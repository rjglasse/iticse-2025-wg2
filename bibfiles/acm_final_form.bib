@inproceedings{10.1145/3568812.3603453,
author = {Tran, Minh},
title = {Prompt Engineering for Large Language Models to Support K-8 Computer Science Teachers in Creating Culturally Responsive Projects},
year = {2023},
isbn = {9781450399753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568812.3603453},
doi = {10.1145/3568812.3603453},
abstract = {The power of large language models has opened up opportunities for educational use. In computing education, recent studies have demonstrated the potential of these models to improve learning and teaching experiences in university-level programming courses. However, research into leveraging them to aid computer science instructors in curriculum development and course material design is relatively sparse, especially at the K-12 level. This work aims to fill this gap by exploring the capability of large language models in ideating and designing culturally responsive projects for elementary and middle school programming classes. Our ultimate goal is to support K-8 teachers in effectively extracting suggestions from large language models by only using natural language modifications. Furthermore, we aim to develop a comprehensive assessment framework for culturally responsive AI-generated project ideas. We also hope to provide valuable insight into teachers’ perspectives on large language models and their integration into teaching practices.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2},
pages = {110–112},
numpages = {3},
keywords = {culturally responsive pedagogy, large language models},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3545947.3569630,
author = {MacNeil, Stephen and Tran, Andrew and Leinonen, Juho and Denny, Paul and Kim, Joanne and Hellas, Arto and Bernstein, Seth and Sarsa, Sami},
title = {Automatically Generating CS Learning Materials with Large Language Models},
year = {2023},
isbn = {9781450394338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545947.3569630},
doi = {10.1145/3545947.3569630},
abstract = {Recent breakthroughs in Large Language Models (LLMs), such as GPT-3 and Codex, now enable software developers to generate code based on a natural language prompt. Within computer science education, researchers are exploring the potential for LLMs to generate code explanations and programming assignments using carefully crafted prompts. These advances may enable students to interact with code in new ways while helping instructors scale their learning materials. However, LLMs also introduce new implications for academic integrity, curriculum design, and software engineering careers. This workshop will demonstrate the capabilities of LLMs to help attendees evaluate whether and how LLMs might be integrated into their pedagogy and research. We will also engage attendees in brainstorming to consider how LLMs will impact our field.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1176},
numpages = {1},
keywords = {code generation, computer science education, copilot, explanations, large language models},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3585059.3611431,
author = {Zheng, Yong},
title = {ChatGPT for Teaching and Learning: An Experience from Data Science Education},
year = {2023},
isbn = {9798400701306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585059.3611431},
doi = {10.1145/3585059.3611431},
abstract = {ChatGPT, an implementation and application of large language models, has gained significant popularity since its initial release. Researchers have been exploring ways to harness the practical benefits of ChatGPT in real-world scenarios. Educational researchers have investigated its potential in various subjects, e.g., programming, mathematics, finance, clinical decision support, etc. However, there has been limited attention given to its application in data science education. This paper aims to bridge that gap by utilizing ChatGPT in a data science course, gathering perspectives from students, and presenting our experiences and feedback on using ChatGPT for teaching and learning in data science education. The findings not only distinguish data science education from other disciplines but also uncover new opportunities and challenges associated with incorporating ChatGPT into the data science curriculum.},
booktitle = {Proceedings of the 24th Annual Conference on Information Technology Education},
pages = {66–72},
numpages = {7},
keywords = {ChatGPT, data analytics, data science, large language model},
location = {Marietta, GA, USA},
series = {SIGITE '23}
}

@article{10.5555/3636517.3636522,
author = {Crandall, Aaron S. and Sprint, Gina and Fischer, Bryan},
title = {Generative Pre-Trained Transformer (GPT) Models as a Code Review Feedback Tool in Computer Science Programs},
year = {2023},
issue_date = {October 2023},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {1},
issn = {1937-4771},
abstract = {Undergraduate computer science and software engineering students benefit significantly from in-depth reviews of their code early and often in their courses. Performing these reviews is time-consuming for teaching assistants and professors to complete, consequently impacting the timeliness and consistency of the provided feedback. When code feedback is not delivered close to the time of authorship, the utility of the review for students is diminished. Prior work with Automatic Static Analysis Tools has shown promise at using artificial intelligence to automate code reviews, with some success integrating them into classroom environments. To leverage new advances in Generative Pre-Trained Transformer (GPT) models, this work reports on an Automatic Review Tool (ART) to provide timely, automatically generated code reviews. ART was evaluated in a second-semester computer science course by integrating ART into the course's Github-based assignment submission system. A cohort of student volunteers (N = 74) read the ART reviews and provided feedback using a survey spanning two of their course assignments. The results of this pilot study show that students perceived ART was successful at detecting defects and offering style-based suggestions, and students were receptive to receiving future automated reviews of their work.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {38–47},
numpages = {10}
}

@inproceedings{10.1145/3616961.3616974,
author = {Rajala, Jaakko and Hukkanen, Jenni and Hartikainen, Maria and Niemel\"{a}, Pia},
title = {"\"Call me Kiran\" – ChatGPT as a Tutoring Chatbot in a Computer Science Course"},
year = {2023},
isbn = {9798400708749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616961.3616974},
doi = {10.1145/3616961.3616974},
abstract = {Natural language processing has taken enormous steps during the last few years. The development of large language models and generative AI has elevated natural language processing to the level that it can output coherent and contextually relevant text for a given natural language prompt. ChatGPT is one incarnation of these steps, and its use in education is a rather new phenomenon. In this paper, we study students’ perception on ChatGPT during a computer science course. On the course, we integrated ChatGPT into Teams private discussion groups. In addition, all the students had freedom to employ ChatGPT and related technologies to help them in their coursework. The results show that the majority of students had at least tested AI-powered chatbots, and that students are using AI-powered chatbots for multiple tasks, e.g., debugging code, tutoring, and enhancing comprehension. The amount of positive implications of using ChatGPT takes over the negative implications, when the implications were considered from an understanding, learning and creativity perspective. Relatively many students reported reliability issues with the outputs and that the iterations with prompts might be necessary for satisfactory outputs. It is important to try to steer the usage of ChatGPT so that it complements students’ learning processes, but does not replace it.},
booktitle = {Proceedings of the 26th International Academic Mindtrek Conference},
pages = {83–94},
numpages = {12},
keywords = {tutoring, student perceptions, generative AI, education, discussion forum, chatbots, artificial intelligence, ChatGPT},
location = {Tampere, Finland},
series = {Mindtrek '23}
}

@inproceedings{10.1145/3587102.3588815,
author = {Daun, Marian and Brings, Jennifer},
title = {How ChatGPT Will Change Software Engineering Education},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588815},
doi = {10.1145/3587102.3588815},
abstract = {This position paper discusses the potential for using generative AIs like ChatGPT in software engineering education. Currently, discussions center around potential threats emerging from student's use of ChatGPT. For instance, generative AI will limit the usefulness of graded homework dramatically. However, there exist potential opportunities as well. For example, ChatGPT's ability to understand and generate human language allows providing personalized feedback to students, and can thus accompany current software engineering education approaches. This paper highlights the potential for enhancing software engineering education. The availability of generative AI will improve the individualization of education approaches. In addition, we discuss the need to adapt software engineering curricula to the changed profiles of software engineers. Moreover, we point out why it is important to provide guidance for using generative AI and, thus, integrate it in courses rather than accepting the unsupervised use by students, which can negatively impact the students' learning.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {110–116},
numpages = {7},
keywords = {ChatGPT, generative AI, software engineering education},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3581783.3610953,
author = {Wang, Zheng and Long, Cheng and Xu, Shihao and Gan, Bingzheng and Shi, Wei and Cao, Zhao and Chua, Tat-Seng},
title = {LGM3A '23: 1st Workshop on Large Generative Models Meet Multimodal Applications},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3610953},
doi = {10.1145/3581783.3610953},
abstract = {A large language model is a type of artificial intelligence model designed to understand and generate natural language text, such as GPT, T5, RoBERTa, BERT, etc. These models are trained on vast amounts of text data, allowing them to learn the patterns and structures of human language. With the increasing amount of multimodal information such as audio, visual, and text data generated, there is a growing need of leveraging large generative language model for multimodal applications. Recently, a few notable multimodal models (e.g., BLIP, Flamingo, KOSMOS, PaLM-E, LLaVA, Visual ChatGPT, GPT-4, etc.) with a combination of large language models significantly enhanced their understanding and generate more accurate and nuanced responses. The workshop will provide an opportunity for researchers, practitioners, and industry professionals to explore the latest trends and best practices in the field of multimodal applications of large generative models. The workshop will also focus on exploring the challenges and opportunities of integrating large language models with other AI technologies such as computer vision and speech recognition.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {9744–9745},
numpages = {2},
keywords = {generative models, large language models, multimodal applications},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3568813.3600139,
author = {Hellas, Arto and Leinonen, Juho and Sarsa, Sami and Koutcheme, Charles and Kujanp\"{a}\"{a}, Lilja and Sorva, Juha},
title = {Exploring the Responses of Large Language Models to Beginner Programmers’ Help Requests},
year = {2023},
isbn = {9781450399760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568813.3600139},
doi = {10.1145/3568813.3600139},
abstract = {Background and Context: Over the past year, large language models (LLMs) have taken the world by storm. In computing education, like in other walks of life, many opportunities and threats have emerged as a consequence. Objectives: In this article, we explore such opportunities and threats in a specific area: responding to student programmers’ help requests. More specifically, we assess how good LLMs are at identifying issues in problematic code that students request help on. Method: We collected a sample of help requests and code from an online programming course. We then prompted two different LLMs (OpenAI Codex and GPT-3.5) to identify and explain the issues in the students’ code and assessed the LLM-generated answers both quantitatively and qualitatively. Findings: GPT-3.5 outperforms Codex in most respects. Both LLMs frequently find at least one actual issue in each student program (GPT-3.5 in 90% of the cases). Neither LLM excels at finding all the issues (GPT-3.5 finding them 57% of the time). False positives are common (40% chance for GPT-3.5). The advice that the LLMs provide on the issues is often sensible. The LLMs perform better on issues involving program logic rather than on output formatting. Model solutions are frequently provided even when the LLM is prompted not to. LLM responses to prompts in a non-English language are only slightly worse than responses to English prompts. Implications: Our results continue to highlight the utility of LLMs in programming education. At the same time, the results highlight the unreliability of LLMs: LLMs make some of the same mistakes that students do, perhaps especially when formatting output as required by automated assessment systems. Our study informs teachers interested in using LLMs as well as future efforts to customize LLMs for the needs of programming education.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1},
pages = {93–105},
numpages = {13},
keywords = {CS1, GPT, OpenAI Codex, automatic feedback, help seeking, introductory programming education, large language models, student questions},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3587102.3588814,
author = {Cipriano, Bruno Pereira and Alves, Pedro},
title = {GPT-3 vs Object Oriented Programming Assignments: An Experience Report},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588814},
doi = {10.1145/3587102.3588814},
abstract = {Recent studies show that AI-driven code generation tools, such as Large Language Models, are able to solve most of the problems usually presented in introductory programming classes. However, it is still unknown how they cope with Object Oriented Programming assignments, where the students are asked to design and implement several interrelated classes (either by composition or inheritance) that follow a set of best-practices. Since the majority of the exercises in these tools' training dataset are written in English, it is also unclear how well they function with exercises published in other languages.In this paper, we report our experience using GPT-3 to solve 6 real-world tasks used in an Object Oriented Programming course at a Portuguese University and written in Portuguese. Our observations, based on an objective evaluation of the code, performed by an open-source Automatic Assessment Tool, show that GPT-3 is able to interpret and handle direct functional requirements, however it tends not to give the best solution in terms of object oriented design. We perform a qualitative analysis of GPT-3's output, and gather a set of recommendations for computer science educators, since we expect students to use and abuse this tool in their academic work.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {61–67},
numpages = {7},
keywords = {GPT-3, large language models, object oriented programming, programming assignments, teaching},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3545947.3573358,
author = {MacNeil, Stephen and Kim, Joanne and Leinonen, Juho and Denny, Paul and Bernstein, Seth and Becker, Brett A. and Wermelinger, Michel and Hellas, Arto and Tran, Andrew and Sarsa, Sami and Prather, James and Kumar, Viraj},
title = {The Implications of Large Language Models for CS Teachers and Students},
year = {2023},
isbn = {9781450394338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545947.3573358},
doi = {10.1145/3545947.3573358},
abstract = {The introduction of Large Language Models (LLMs) has generated a significant amount of excitement both in industry and among researchers. Recently, tools that leverage LLMs have made their way into the classroom where they help students generate code and help instructors generate learning materials. There are likely many more uses of these tools -- both beneficial to learning and possibly detrimental to learning. To help ensure that these tools are used to enhance learning, educators need to not only be familiar with these tools, but with their use and potential misuse. The goal of this BoF is to raise awareness about LLMs and to build a learning community around their use in computing education. Aligned with this goal of building an inclusive learning community, our BoF is led by globally distributed discussion leaders, including undergraduate researchers, to facilitate multiple coordinated discussions that can lead to a broader conversation about the role of LLMs in CS education.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1255},
numpages = {1},
keywords = {artificial intelligence, code explanations, code generation, computer science education, copilot, gpt-3, large language models},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3613944.3613946,
author = {Qureshi, Basit},
title = {ChatGPT in Computer Science Curriculum Assessment: An analysis of Its Successes and Shortcomings},
year = {2023},
isbn = {9798400700415},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613944.3613946},
doi = {10.1145/3613944.3613946},
abstract = {The application of Artificial intelligence for teaching and learning in the academic sphere is a trending subject of interest in computing education. ChatGPT, as an AI-based tool, provides various advantages, such as heightened student involvement, cooperation, accessibility, and availability. This paper addresses the prospects and obstacles associated with utilizing ChatGPT as a tool for learning and assessment in undergraduate Computer Science curriculum in particular to teaching and learning fundamental programming courses. Students having completed the course work for a Data Structures and Algorithms (a sophomore-level course) participated in this study. Two groups of students were given programming challenges to solve within a short period of time. The control group (group A) had access to textbooks and notes of programming courses, however, no Internet access was provided. Group B students were given access to ChatGPT and were encouraged to use it to help solve the programming challenges. The challenge was conducted in a computer lab environment using Programming Contest Control (PC2) environment which is widely used in ACM International Collegiate Programming Contest (ICPC). Each team of students addresses the problem by writing executable code that satisfies a certain number of test cases. Student teams were scored based on their performance in terms of the number of successfully passed test cases. Results show that students using ChatGPT had an advantage in terms of earned scores, however, there were inconsistencies and inaccuracies in the submitted code consequently affecting the overall performance. After a thorough analysis, the paper’s findings indicate that incorporating AI in higher education brings about various opportunities and challenges. Nonetheless, universities can efficiently manage these apprehensions by adopting a proactive and ethical stance toward the implementation of such tools.},
booktitle = {Proceedings of the 2023 9th International Conference on E-Society, e-Learning and e-Technologies},
pages = {7–13},
numpages = {7},
keywords = {Academic assessment, ChatGPT, Data Structures and Algorithms, programming concepts},
location = {Portsmouth, United Kingdom},
series = {ICSLT '23}
}

@inproceedings{10.1145/3568812.3603476,
author = {Phung, Tung and P\u{a}durean, Victor-Alexandru and Cambronero, Jos\'{e} and Gulwani, Sumit and Kohn, Tobias and Majumdar, Rupak and Singla, Adish and Soares, Gustavo},
title = {Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors},
year = {2023},
isbn = {9781450399753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568812.3603476},
doi = {10.1145/3568812.3603476},
abstract = {Generative AI and large language models hold great promise in enhancing computing education by powering next-generation educational technologies. State-of-the-art models like OpenAI’s ChatGPT&nbsp;[8] and GPT-4&nbsp;[9] could enhance programming education in various roles, e.g., by acting as a personalized digital tutor for a student, a digital assistant for an educator, and a digital peer for collaborative learning&nbsp;[1, 2, 7]. In our work, we seek to comprehensively evaluate and benchmark state-of-the-art large language models for various scenarios in programming education. Recent works have evaluated several large language models in the context of programming education&nbsp;[4, 6, 10, 11, 12]. However, these works are limited for several reasons: they have typically focused on evaluating a specific model for a specific education scenario (e.g., generating explanations), or have considered models that are already outdated (e.g., OpenAI’s Codex&nbsp;[3] is no longer publicly available since March 2023). Consequently, there is a lack of systematic study that benchmarks state-of-the-art models for a comprehensive set of programming education scenarios. In our work, we systematically evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human tutors for a variety of scenarios in programming education. These scenarios are designed to capture distinct roles these models could play, namely digital tutors, assistants, and peers, as discussed above. More concretely, we consider the following six scenarios: (1) program repair, i.e., fixing a student’s buggy program; (2) hint generation, i.e., providing a natural language hint to the student to help resolve current issues; (3) grading feedback, i.e., grading a student’s program w.r.t. a given rubric; (4) peer programming, i.e., completing a partially written program or generating a sketch for the solution program; (5) task creation, i.e., generating new tasks that exercise specific types of concepts or bugs; (6) contextualized explanation, i.e., explaining specific concepts or functions in the context of a given program. Our study uses a mix of quantitative and qualitative evaluation to compare the performance of these models with the performance of human tutors. We conduct our evaluation based on 5 introductory Python programming problems with a diverse set of input/output specifications. For each of these problems, we consider 5 buggy programs based on publicly accessible submissions from geeksforgeeks.org &nbsp;[5] (see Figure&nbsp;1); these buggy programs are picked to capture different types of bugs for each problem. We will provide a detailed analysis of the data and results in a longer version of this poster. Our preliminary results show that GPT-4 drastically outperforms ChatGPT (based on GPT-3.5) and comes close to human tutors’ performance for several scenarios.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2},
pages = {41–42},
numpages = {2},
keywords = {ChatGPT, generative AI, introductory programming education, large language models},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@article{10.1145/3631504.3631518,
author = {Amer-Yahia, Sihem and Bonifati, Angela and Chen, Lei and Li, Guoliang and Shim, Kyuseok and Xu, Jianliang and Yang, Xiaochun},
title = {From Large Language Models to Databases and Back: A Discussion on Research and Education},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/3631504.3631518},
doi = {10.1145/3631504.3631518},
abstract = {In recent years, large language models (LLMs) have garnered increasing attention from both academia and industry due to their potential to facilitate natural language processing (NLP) and generate highquality text. Despite their benefits, however, the use of LLMs is raising concerns about the reliability of knowledge extraction. The combination of DB research and data science has advanced the state of the art in solving real-world problems, such as merchandise recommendation and hazard prevention [30]. In this discussion, we explore the challenges and opportunities related to LLMs in DB and data science research and education.},
journal = {SIGMOD Rec.},
month = nov,
pages = {49–56},
numpages = {8}
}

@inproceedings{10.1145/3593663.3593695,
author = {Dobslaw, Felix and Bergh, Peter},
title = {Experiences with Remote Examination Formats in Light of GPT-4},
year = {2023},
isbn = {9781450399562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593663.3593695},
doi = {10.1145/3593663.3593695},
abstract = {Sudden access to the rapidly improving large language model GPT by OpenAI forces educational institutions worldwide to revisit their exam procedures. In the pre-GPT era, we successfully applied oral and open-book home exams for two courses in the third year of our predominantly remote Software Engineering BSc program. We ask in this paper whether our current open-book exams are still viable or whether a move back to a legally compliant but less scalable oral exam is the only workable alternative. We further compare work-effort estimates between oral and open-book exams and report on differences in throughput and grade distribution over eight years to better understand the impact of examination format on the outcome. Examining GPT-4 on the most recent open-book exams showed that our current Artificial Intelligence and Reactive Programming exams are not GPT v4 proof. Three potential weaknesses of GPT are outlined. We also found that grade distributions have largely been unaffected by the examination format, opening up for a move to oral examinations only if needed. Throughput was higher for open-book exam course instances (73% vs 64%), while fail rates were too (12% vs 7%), with teacher workload increasing even for smaller classes. We also report on our experience regarding effort. Oral examinations are efficient for smaller groups but come with caveats regarding intensity and stress.},
booktitle = {Proceedings of the 5th European Conference on Software Engineering Education},
pages = {220–225},
numpages = {6},
keywords = {Software Engineering Education, Oral Examinations, Examination Formats, ChatGPT},
location = {Seeon/Bavaria, Germany},
series = {ECSEE '23}
}

@inproceedings{10.1145/3551349.3559555,
author = {Ahmed, Toufique and Devanbu, Premkumar},
title = {Few-shot training LLMs for project-specific code-summarization},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559555},
doi = {10.1145/3551349.3559555},
abstract = {Very large language models (LLMs), such as GPT-3 and Codex have achieved state-of-the-art performance on several natural-language tasks, and show great promise also for code. A particularly exciting aspect of LLMs is their knack for few-shot and zero-shot learning: they can learn to perform a task with very few examples. Few-shotting has particular synergies in software engineering, where there are a lot of phenomena (identifier names, APIs, terminology, coding patterns) that are known to be highly project-specific. However, project-specific data can be quite limited, especially early in the history of a project; thus the few-shot learning capacity of LLMs might be very relevant. In this paper, we investigate the use few-shot training with the very large GPT (Generative Pre-trained Transformer) Codex model, and find evidence suggesting that one can significantly surpass state-of-the-art models for code-summarization, leveraging project-specific training.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {177},
numpages = {5},
keywords = {code summarization, deep learning, large language model},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@article{10.5555/3575618.3575622,
author = {Puryear, Ben and Sprint, Gina},
title = {Github copilot in the classroom: learning to code with AI assistance},
year = {2022},
issue_date = {November 2022},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {38},
number = {1},
issn = {1937-4771},
abstract = {Recent advances in deep machine learning have enabled artificial intelligence-driven development environments (AIDEs). AIDEs are programming tools that, given comments or starter code, can generate code solution suggestions. As the accuracy of these tools continues to increase, one particular AIDE from Github, Copilot, has been gaining significant attention for its performance and ease of use. The rise of Copilot suggests that code solution generation tools will soon be commonplace in both the industry and in computer science courses, with expert and novice programmers alike benefiting from using these tools. More specifically for novices, the effects of Copilot on the process of learning to code are mostly unknown. In this paper, we perform initial explorations into these effects. Using introductory computer science and data science courses, we evaluate Copilot-generated programming assignment solutions for correctness, style, skill level appropriateness, grade scores, and potential plagiarism. Our findings indicate Copilot generates mostly unique code that can solve introductory assignments with human-graded scores ranging from 68% to 95%. Based on these results, we provide recommendations for educators to help adapt their courses to incorporate new AIDE-based programming workflows.},
journal = {J. Comput. Sci. Coll.},
month = nov,
pages = {37–47},
numpages = {11}
}

@inproceedings{10.1145/3568812.3603474,
author = {Singla, Adish},
title = {Evaluating ChatGPT and GPT-4 for Visual Programming},
year = {2023},
isbn = {9781450399753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568812.3603474},
doi = {10.1145/3568812.3603474},
abstract = {Generative AI has the potential to drastically improve the landscape of computing education by automatically generating personalized feedback and content. In particular, this potential lies in the advanced capabilities of state-of-the-art deep generative and large language models such as OpenAI’s Codex&nbsp;[7], ChatGPT&nbsp;[11], and GPT-4&nbsp;[12]. In our work, we seek to investigate the capabilities of these models in visual programming domains popularly used for K-8 programming education, including domains like Scratch&nbsp;[17], Hour of Code: Maze Challenge by Code.org&nbsp;[4, 5], and Karel&nbsp;[13]. Recent works have shown us sparks of advanced capabilities of such models for various education scenarios in introductory Python programming&nbsp;[2, 14, 18, 20]. In fact, a study in 2022 had ranked Codex in the top quartile w.r.t students in a large Python programming course&nbsp;[8]. However, all these works consider only text-based Python programming and leave open the question of how well these models would perform for visual programming. The main research question is: Do state-of-the-art neural generative models show advanced capabilities for visual programming on par with their capabilities on text-based Python programming?In our work, we evaluate these models for visual programming based on the following three settings designed to capture various generative and problem-solving capabilities: We conduct our evaluation based on 10 representative tasks from two visual programming domains: Hour of Code: Maze Challenge by Code.org&nbsp;[4, 5] and Intro to Programming with Karel course by CodeHS.com&nbsp;[3, 13]. As illustrative examples, Figures&nbsp;1,&nbsp;2,&nbsp;and&nbsp;3 show the output of GPT-4 in three settings for Maze18 task. We will provide the detailed analysis and prompts used in a longer version of this poster. Our preliminary results for ChatGPT (based on GPT-3.5) and GPT-4 show that these models perform poorly and produce incorrect output the majority of the time. These results highlight that state-of-the-art neural generative models like GPT-4 still struggle to combine spatial, logical, and programming skills crucial for visual programming. As the next step, it would be important to curate novel benchmarks that the research community can use to evaluate improvements in future versions of these models for visual programming.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2},
pages = {14–15},
numpages = {2},
keywords = {ChatGPT, block-based visual programming, generative AI, introductory programming education, large language models},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3545947.3573353,
author = {Brusilovsky, Peter and Ericson, Barbara J. and Horstmann, Cay S. and Servin, Christian and Vahid, Frank and Zilles, Craig},
title = {Significant Trends in CS Educational Material: Current and Future},
year = {2023},
isbn = {9781450394338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545947.3573353},
doi = {10.1145/3545947.3573353},
abstract = {To recognize the current and future trends and challenges in computer science education educational materials for the next decade, the authors of this work provide a conversation to voice the computer science community's experience and expertise on these trends. One of the biggest challenges for introductory computing courses in the next few years will be leveraging the new capabilities of Artificial Intelligent systems such as Open AI CodeX and GPT3 that can generate code from a textual description, explain code, and translate code between programming languages. These tools could drastically change how introductory programming is taught by allowing students to focus more on understanding code, modifying code, and testing code than on writing code. Learning content is increasingly shifting from paper textbooks to online learning systems, which include not just traditional text and figures, but increasingly use interactive items to provide students with better explanations and illustrations, extensive practice, and frequent immediate formative feedback, typically at a lower cognitive load than classical programming assignment. We will discuss challenges and opportunities for interoperability with publishing and learning management platforms. Another example is how guided-based instruments, such as peer team learning, open educational resources, or workbooks, are adaptive and hybrid according to students' needs.Feedback and point of view from the CS community will be considered as part of the curricular practices "Future of CS educational materials" document, featured in the new version of the CS2023: ACM/IEEE-CS/AAAI Computer Science Curricula.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1253},
numpages = {1},
keywords = {adaptive, animation, assessment, automation, computer science, educational materials, feedback, homework, learning, sharing, textbook, videos},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3623762.3633499,
author = {Prather, James and Denny, Paul and Leinonen, Juho and Becker, Brett A. and Albluwi, Ibrahim and Craig, Michelle and Keuning, Hieke and Kiesler, Natalie and Kohn, Tobias and Luxton-Reilly, Andrew and MacNeil, Stephen and Petersen, Andrew and Pettit, Raymond and Reeves, Brent N. and Savelka, Jaromir},
title = {The Robots Are Here: Navigating the Generative AI Revolution in Computing Education},
year = {2023},
isbn = {9798400704055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623762.3633499},
doi = {10.1145/3623762.3633499},
abstract = {Recent advancements in artificial intelligence (AI) and specifically generative AI (GenAI) are threatening to fundamentally reshape computing and society. Largely driven by large language models (LLMs), many tools are now able to interpret and generate both natural language instructions and source code. These capabilities have sparked urgent questions in the computing education community around how educators should adapt their pedagogy to address the challenges and to leverage the opportunities presented by this new technology. In this working group report, we undertake a comprehensive exploration of generative AI in the context of computing education and make five significant contributions. First, we provide a detailed review of the literature on LLMs in computing education and synthesise findings from 71 primary articles, nearly 80% of which have been published in the first 8 months of 2023. Second, we report the findings of a survey of computing students and instructors from across 20 countries, capturing prevailing attitudes towards GenAI/LLMs and their use in computing education contexts. Third, to understand how pedagogy is already changing, we offer insights collected from in-depth interviews with 22 computing educators from five continents. Fourth, we use the ACM Code of Ethics to frame a discussion of ethical issues raised by the use of large language models in computing education, and we provide concrete advice for policy makers, educators, and students. Finally, we benchmark the performance of several current GenAI models/tools on various computing education datasets, and highlight the extent to which the capabilities of current models are rapidly improving.There is little doubt that LLMs and other forms of GenAI will have a profound impact on computing education over the coming years. However, just as the technology will continue to improve, so will our collective knowledge about how to leverage these new models and tools in educational settings. We expect many important conversations around this topic will emerge as the community explores how to provide more effective, inclusive, and personalised learning experiences. Our aim is that this report will serve as a focal point for both researchers and practitioners who are exploring, adapting, using, and evaluating GenAI and LLM-based tools in computing classrooms.},
booktitle = {Proceedings of the 2023 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {108–159},
numpages = {52},
keywords = {ai, artificial intelligence, chatgpt, code generation, codex, computer programming, copilot, cs1, curriculum, generative ai, github, gpt, gpt-3, gpt-4, large language models, llm, llms, novice programming, openai, pedagogical practices, programming},
location = {Turku, Finland},
series = {ITiCSE-WGR '23}
}

@inproceedings{10.1145/3605468.3605471,
author = {Vo, Gia Minh and Pancratz, Nils},
title = {AI Education in German K-10 Computer Science Curricula},
year = {2023},
isbn = {9798400708510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605468.3605471},
doi = {10.1145/3605468.3605471},
abstract = {The growing importance of artificial intelligence (AI) in our daily lives leads to an increasing demand for AI in learning, teaching, and education. Recent developments, such as ChatGPT, have further pushed the significance of AI, garnering media attention and prompting politicians to require stakeholders in education to place a stronger emphasis on AI education in schools. As a result, a growing number of computer science (CS) curricula are expanding to include the topic of AI. This paper aims to contribute to the understanding of AI in K-10 education in Germany by analyzing CS curricula for lower secondary school education across the 16 federal states of Germany. The results indicate that AI-related content is inconsistently addressed in the CS curricula of various federal states, with a noticeable absence of standardized AI competencies for K-10 education. In several federal states, AI-related content is only implicitly addressed from a socio-cultural perspective. To ensure up-to-date education, it is essential to include mandatory AI content in K-10 CS curricula. These contents should be considered holistically by taking into account the technological, socio-cultural, and user-oriented perspectives, in accordance with the Dagstuhl Triangle.},
booktitle = {Proceedings of the 18th WiPSCE Conference on Primary and Secondary Computing Education Research},
articleno = {15},
numpages = {4},
keywords = {K-10 education, artificial intelligence, computer science education, curriculum analysis},
location = {Cambridge, United Kingdom},
series = {WiPSCE '23}
}

@inproceedings{10.1145/3593434.3593468,
author = {Ahmad, Aakash and Waseem, Muhammad and Liang, Peng and Fahmideh, Mahdi and Aktar, Mst Shamima and Mikkonen, Tommi},
title = {Towards Human-Bot Collaborative Software Architecting with ChatGPT},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3593468},
doi = {10.1145/3593434.3593468},
abstract = {Architecting software-intensive systems can be a complex process. It deals with the daunting tasks of unifying stakeholders’ perspectives, designers’ intellect, tool-based automation, pattern-driven reuse, and so on, to sketch a blueprint that guides software implementation and evaluation. Despite its benefits, architecture-centric software engineering (ACSE) suffers from a multitude of challenges. ACSE challenges could stem from a lack of standardized processes, socio-technical limitations, and scarcity of human expertise etc. that can impede the development of existing and emergent classes of software. Software Development Bots (DevBots) trained on large language models can help synergise architects’ knowledge with artificially intelligent decision support to enable rapid architecting in a human-bot collaborative ACSE. An emerging solution to enable this collaboration is ChatGPT, a disruptive technology not primarily introduced for software engineering, but is capable of articulating and refining architectural artifacts based on natural language processing. We detail a case study that involves collaboration between a novice software architect and ChatGPT to architect a service-based software. Future research focuses on harnessing empirical evidence about architects’ productivity and explores socio-technical aspects of architecting with ChatGPT to tackle challenges of ACSE.},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {279–285},
numpages = {7},
keywords = {Software Architecture, Large Language Models, DevBots, ChatGPT},
location = {Oulu, Finland},
series = {EASE '23}
}

@inproceedings{10.1145/3610661.3616129,
author = {Brooks, Jeffrey A. and Tiruvadi, Vineet and Baird, Alice and Tzirakis, Panagiotis and Li, Haoqi and Gagne, Chris and Oh, Moses and Cowen, Alan},
title = {Emotion Expression Estimates to Measure and Improve Multimodal Social-Affective Interactions},
year = {2023},
isbn = {9798400703218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610661.3616129},
doi = {10.1145/3610661.3616129},
abstract = {Large language models (LLMs) are being adopted in a wide range of applications, but an understanding of other social-affective signals is needed to support effective human-computer-interaction (HCI) in multimodal interfaces. In particular, robust, accurate measurements of human emotional expression can be used to tailor responses to human values and preferences. In this paper, we present two models available from an API-based suite of emotional expression models that measure nuanced facial and vocal signals, providing rich, high-dimensional emotional expression estimates (EEEs). We demonstrate the ability of EEEs to provide insight into two established datasets and present methods for integrating EEEs into large language model (LLM) applications. We discuss how this approach is a step towards more reliable tools for clinical screening and scientific study, as well as empathic digital assistants that can be used in therapeutic settings.},
booktitle = {Companion Publication of the 25th International Conference on Multimodal Interaction},
pages = {353–358},
numpages = {6},
keywords = {Multimodal Sentiment Analysis, Mental Health, Emotion Science, Emotion Recognition, Affective Computing},
location = {Paris, France},
series = {ICMI '23 Companion}
}

@proceedings{10.1145/3580305,
title = {KDD '23: Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining KDD 2023. This year's conference continues its tradition of being the premier forum for presentation of research results and experience reports on leading edge issues of knowledge discovery, data science, and machine learning. The mission of the conference is to provide the premier forum for advancement, education, and adoption of the "science" of knowledge discovery and machine learning from all types of data; to share novel methodologies that fulfill the needs of heterogeneous applications and environments and identify new directions for future research and development. These ideas have the potential to shape and impact our society and environment, and are becoming particularly important with the emergence of AI in all fields. KDD provides researchers and practitioners a unique opportunity to share their perspectives with others interested in various aspects of data science and machine learning.KDD '23 has a program of three keynotes, 313 research track papers, 184 ADS (Applied Data Science) track papers, 34 workshops, 33 tutorials, nine special days, three panels, and eight ADS invited talks. For the first time, we switched to OpenReview with the mission to further improve the review quality and facilitate the interaction between reviewers and authors. We have introduced several new special days, such as Large Language Model (LLM) Day, Finance Day, AI for Open Society Day, Entertainment, Sports, and Media (ESM) Day, Southern California Data Science; and several new panels, such as AI for Science and LLMs for education &amp; research. The rise of LLMs has been historic and the nature of creativity itself may change. With this in mind, we have emphasized LLMs in our keynotes, special days, and panels. Only time will tell whether we went too far or not far enough!},
location = {Long Beach, CA, USA}
}

@inproceedings{10.1145/3628797.3628837,
author = {Nguyen, Duc-Vu and Nguyen, Quoc-Nam},
title = {Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education},
year = {2023},
isbn = {9798400708916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628797.3628837},
doi = {10.1145/3628797.3628837},
abstract = {In this paper, we evaluate the ability of large language models (LLMs) to perform multiple choice symbol binding (MCSB) for multiple choice question answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus on Vietnamese, with fewer challenging MCQA datasets than in English. The two existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent research in Vietnamese natural language processing (NLP) has focused on the Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to 2023 to evaluate ChatGPT. However, these studies have mainly focused on how ChatGPT solves the VNHSGE step by step. We aim to create a novel and high-quality dataset by providing structured guidelines for typing LaTeX formulas for mathematics, physics, chemistry, and biology. This dataset can be used to evaluate the MCSB ability of LLMs and smaller language models (LMs) because it is typed in a strict LaTeX style. We determine the most probable character answer (A, B, C, or D) based on context, instead of finding the answer step by step as in previous Vietnamese works. This reduces computational costs and accelerates the evaluation of LLMs. Our evaluation of six well-known LLMs, namely BLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the ViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising results on the MCSB ability of LLMs for Vietnamese. The dataset is available1 for research purposes only.},
booktitle = {Proceedings of the 12th International Symposium on Information and Communication Technology},
pages = {379–386},
numpages = {8},
keywords = {Analysis of Language Models, Language Modeling, Multiple Choice Question Answering, Multiple Choice Symbol Binding},
location = {Ho Chi Minh, Vietnam},
series = {SOICT '23}
}

@inproceedings{10.1145/3617553.3617887,
author = {Fulcini, Tommaso and Torchiano, Marco},
title = {Is ChatGPT Capable of Crafting Gamification Strategies for Software Engineering Tasks?},
year = {2023},
isbn = {9798400703737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617553.3617887},
doi = {10.1145/3617553.3617887},
abstract = {Gamification has gained significant attention in the last decade for its potential to enhance engagement and motivation in various domains. During the last year ChatGPT, a state-of-the-art large language model has received even more attention both in the field of scientific research and in common use by individuals or companies.  
In this study, we investigate the possibility of adopting ChatGPT as a tool for designing gamification platforms in the Software Engineering domain. Leveraging the capabilities of ChatGPT, we assess how good is it at generating effective suggestions and ideas for designers or developers.  
To evaluate ChatGPT's potential as a gamification platform creator we narrowed the context to one particular Software Engineering activity, asking for possible aspects of the activity to be gamified. Each proposed aspect was subsequently unraveled by ChatGPT both asking in a shared and separate context, first following the conversational nature of the model, then applying a validated design framework. The study assesses ChatGPT's ability to select and integrate game elements to build a thriving gamification environment by framing the design of the platform to a state-of-the-art conceptual framework. To evaluate the goodness of the design choices made we relied both on the Octalysis framework and on personal experience.  
The findings of the papers show that ChatGPT can only create simple playful experiences not very effective. Although, by instructing the model with more specific desired mechanics and dynamics, it is possible to guide it toward the application of the ideas suggested. We argue that ChatGPT is not capable of building a gamified environment on its own, but it could still be used to build the foundation of a gamification platform as long as the designers refine and rough out the advice gained from a user-centered solution.},
booktitle = {Proceedings of the 2nd International Workshop on Gamification in Software Development, Verification, and Validation},
pages = {22–28},
numpages = {7},
keywords = {Software Lifecycle, Software Engineering, Large Language Model, Gamification, Artificial Intelligence},
location = {San Francisco, CA, USA},
series = {Gamify 2023}
}

@inproceedings{10.1145/3633083.3633099,
author = {Stone, Irene},
title = {Exploring the Research Gap: Generative AI and Learning of Python Programming among Post-Primary Students},
year = {2023},
isbn = {9798400716461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3633083.3633099},
doi = {10.1145/3633083.3633099},
abstract = {The introduction of Leaving Certificate Computer Science (LCCS) in Ireland in 2018 signifies a notable advancement in post-primary education. Moreover, developments in generative Artificial Intelligence (GAI) in education, are gaining prominence, yet we do not understand its value or how best to implement it in post-primary educational settings. Despite a growing international body of research in this area, my scoping review highlights that many aspects of these topics have yet to be explored, particularly in the context of post-primary students in Ireland. My study will begin to bridge this gap by exploring how a purposeful sample of LCCS post-primary students in Ireland engage with GAI tools, such as ChatGPT, during their initial experiences learning Python programming. These findings, when approached through the lens of Human-Centred Artificial Intelligence (HCAI), can help enhance pedagogical strategies and lead to improved learning experiences for students.},
booktitle = {Proceedings of the 2023 Conference on Human Centered Artificial Intelligence: Education and Practice},
pages = {51},
numpages = {1},
location = {Dublin, Ireland},
series = {HCAIep '23}
}

@inproceedings{10.1145/3581754.3584111,
author = {Cao, Chen},
title = {Scaffolding CS1 Courses with a Large Language Model-Powered Intelligent Tutoring System},
year = {2023},
isbn = {9798400701078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581754.3584111},
doi = {10.1145/3581754.3584111},
abstract = {Programming skills are rapidly becoming essential for many educational paths and career opportunities. Yet, for many international students, the traditional approach to teaching introductory programming courses can be a significant challenge due to the complexities of the language, the lack of prior programming knowledge, and the language and cultural barriers. This study explores how large language models and gamification can scaffold coding learning and increase Chinese students’ sense of belonging in introductory programming courses. In this project, a gamification intelligent tutoring system was developed to adapt to Chinese international students’ learning needs and provides scaffolding to support their success in introductory computer programming courses. My research includes three studies: a formative study, a user study of an initial prototype, and a computer simulation study with a user study in progress. Both qualitative and quantitative data were collected through surveys, observations, focus group discussions and computer simulation. The preliminary findings suggest that GPT-3-enhanced gamification has great potential in scaffolding introductory programming learning by providing adaptive and personalised feedback, increasing students’ sense of belonging, and reducing their anxiety about learning programming.},
booktitle = {Companion Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {229–232},
numpages = {4},
location = {Sydney, NSW, Australia},
series = {IUI '23 Companion}
}

@inproceedings{10.1145/3587102.3588785,
author = {Leinonen, Juho and Denny, Paul and MacNeil, Stephen and Sarsa, Sami and Bernstein, Seth and Kim, Joanne and Tran, Andrew and Hellas, Arto},
title = {Comparing Code Explanations Created by Students and Large Language Models},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588785},
doi = {10.1145/3587102.3588785},
abstract = {Reasoning about code and explaining its purpose are fundamental skills for computer scientists. There has been extensive research in the field of computing education on the relationship between a student's ability to explain code and other skills such as writing and tracing code. In particular, the ability to describe at a high-level of abstraction how code will behave over all possible inputs correlates strongly with code writing skills. However, developing the expertise to comprehend and explain code accurately and succinctly is a challenge for many students. Existing pedagogical approaches that scaffold the ability to explain code, such as producing exemplar code explanations on demand, do not currently scale well to large classrooms. The recent emergence of powerful large language models (LLMs) may offer a solution. In this paper, we explore the potential of LLMs in generating explanations that can serve as examples to scaffold students' ability to understand and explain code. To evaluate LLM-created explanations, we compare them with explanations created by students in a large course (n ≈ 1000) with respect to accuracy, understandability and length. We find that LLM-created explanations, which can be produced automatically on demand, are rated as being significantly easier to understand and more accurate summaries of code than student-created explanations. We discuss the significance of this finding, and suggest how such models can be incorporated into introductory programming education.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {124–130},
numpages = {7},
keywords = {CS1, ChatGPT, GPT-3, GPT-4, code comprehension, code explanations, foundation models, large language models, natural language generation, resource generation},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3605770.3625214,
author = {Singla, Tanmay and Anandayuvaraj, Dharun and Kalu, Kelechi G. and Schorlemmer, Taylor R. and Davis, James C.},
title = {An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security Failures},
year = {2023},
isbn = {9798400702631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605770.3625214},
doi = {10.1145/3605770.3625214},
abstract = {As we increasingly depend on software systems, the consequences of breaches in the software supply chain become more severe. High-profile cyber attacks like SolarWinds and ShadowHammer have resulted in significant financial and data losses, underlining the need for stronger cybersecurity. One way to prevent future breaches is by studying past failures. However, traditional methods of analyzing past failures require manually reading and summarizing reports about them. Automated support could reduce costs and allow analysis of more failures. Natural Language Processing (NLP) techniques such as Large Language Models (LLMs) could be leveraged to assist the analysis of failures. In this study, we assessed the ability of Large Language Models (LLMs) to analyze historical software supply chain breaches. We used LLMs to replicate the manual analysis of 69 software supply chain security failures performed by members of the Cloud Native Computing Foundation (CNCF). We developed prompts for LLMs to categorize these by four dimensions: type of compromise, intent, nature, and impact. GPT 3.5's categorizations had an average accuracy of 68% and Bard's had an accuracy of 58% over these dimensions. We report that LLMs effectively characterize software supply chain failures when the source articles are detailed enough for consensus among manual analysts, but cannot yet replace human analysts. Future work can improve LLM performance in this context, and study a broader range of articles and failures.},
booktitle = {Proceedings of the 2023 Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses},
pages = {5–15},
numpages = {11},
keywords = {cybersecurity, empirical software engineering, failure analysis, large language models, software security, software supply chain},
location = {Copenhagen, Denmark},
series = {SCORED '23}
}

@inproceedings{10.1145/3586182.3615825,
author = {Aveni, Timothy J. and Fox, Armando and Hartmann, Bj\"{o}rn},
title = {Bringing Context-Aware Completion Suggestions to Arbitrary Text Entry Interfaces},
year = {2023},
isbn = {9798400700965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586182.3615825},
doi = {10.1145/3586182.3615825},
abstract = {Large language models (LLMs) can predict “obvious” next steps that users will take in text entry fields, especially the tedious components of tasks like software engineering or email composition. These models are not only useful in large, unbroken text fields, however. We present OmniFill, a browser extension that detects text entry fields and offers “autofill”-style suggestions based on context from the browsing session. The system constructs an LLM prompt that includes three main components: (a) a description of the active tab’s text fields and their current values, (b) information from the user’s recent web browsing context, and (c) a history, if available, of the user’s prior submissions to the web form (alongside those submissions’ associated browsing context). Suggestions from the LLM’s response are offered to the user to be automatically typed into each corresponding text field. We offer a motivating example of a time-saving interaction and discuss the broader utility of interface-agnostic LLM integrations.},
booktitle = {Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {77},
numpages = {3},
keywords = {Web accessibility, intelligent user interfaces, large language models},
location = {San Francisco, CA, USA},
series = {UIST '23 Adjunct}
}

@inproceedings{10.1145/3593342.3593360,
author = {Rajabi, Parsa and Taghipour, Parnian and Cukierman, Diana and Doleck, Tenzin},
title = {Exploring ChatGPT’s impact on post-secondary education: A qualitative study},
year = {2023},
isbn = {9798400707896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593342.3593360},
doi = {10.1145/3593342.3593360},
abstract = {As Chat Generative Pre-trained Transformer (ChatGPT) gains traction, its impact on post-secondary education is increasingly being debated. This qualitative study explores the perception of students and faculty members at a research university in Canada regarding ChatGPT’s use in a post-secondary setting, focusing on how it could be incorporated and what ways instructors can respond to this technology. We present the summary of a discussion that took place in a two-hour focus group session with 40 participants from the computer science and engineering departments, and highlight issues surrounding plagiarism, assessment methods, and the appropriate use of ChatGPT. Findings suggest that students are likely to use ChatGPT, but there is a need for specific guidelines, more classroom assessments, and mandatory reporting of ChatGPT use. The study contributes to the emergent research on ChatGPT in higher education and emphasizes the importance of proactively addressing challenges and opportunities associated with ChatGPT adoption and use.},
booktitle = {Proceedings of the 25th Western Canadian Conference on Computing Education},
articleno = {9},
numpages = {6},
keywords = {post-secondary, higher education, education, conversational AI, assessment, ChatGPT, Artificial Intelligence in education},
location = {Vancouver, BC, Canada},
series = {WCCCE '23}
}

@article{10.1145/3585060.3585063,
author = {Lopez, Patty},
title = {Reflections on the Design of Systems that Impact Computers and Society},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {3},
issn = {0095-2737},
url = {https://doi.org/10.1145/3585060.3585063},
doi = {10.1145/3585060.3585063},
abstract = {Having spent the past year post-retirement working with my alma mater, New Mexico State University's (NMSU) Computer Science department to broaden computing, increase student engagement, and to improve graduation completion, as well as reflecting on the state of computing in society at large, I thought I'd share some observations. In March of this year, I had the opportunity to participate in the SIGCSE 2022 Technical Symposium. I was struck by Dr. Shaundra Daily's plenary keynote, entitled "Diversifying Computing: Real Change Must Come from Within", and her use of the phrase "navigating systems that were not designed for me" as she described her exploration of STEM as a first-generation college student, as both a dance and an engineering student, and as a graduate student preparing for motherhood lacking flexibility during her pregnancy, no maternity leave, no livable stipend, and a lack of affordable childcare, as well as the coping strategies she needed to develop to deal with academic culture. In my work with NMSU this past spring, co-teaching a problem solving course, my work this fall advising CS students, and my board roles serving on the National Academy of Science, Engineering, and Medicine's Roundtable for Systemic Change in Undergraduate STEM Education co-chairing the "Culture of STEM" workgroup, on the Computing Alliance of Hispanic Serving Institution's (CAHSI) Advisory Board, and on the Computing Research Association for Widening Participation (CRA-WP), co-editing the "Expanding the Pipeline" column, it's clear that system design adversely impacts society in terms of determining not only who gets to participate in the design of computer hardware and software, but also who gets to advance in social and economic mobility. Academic institutions are complex systems in need of an overhaul, by the University of California's academic workers strike for better pay and benefits. The design and commercialization of AI without fully understanding the implications of bias and ethics is inherently a system design problem. The application to everything from AI generated art and images (and how to spot deep fakes), the ability of large language models (LLMs) to create volumes of text generated articles that appear legitimate with the capacity to spread hate and misinformation globally are but just a few examples of the potentially horrific impact to society, because humans cannot work at the pace and scale to validate and/or authenticate them, with few if any meaningful domestic and international laws or policies in place to safeguard us.},
journal = {SIGCAS Comput. Soc.},
month = feb,
pages = {9},
numpages = {1},
keywords = {system design, ethics, diversity, bias}
}

@inproceedings{10.1145/3585059.3611447,
author = {Sakib, Nazmus and Anik, Fahim Islam and Li, Lei},
title = {ChatGPT in IT Education Ecosystem: Unraveling Long-Term Impacts on Job Market, Student Learning, and Ethical Practices},
year = {2023},
isbn = {9798400701306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585059.3611447},
doi = {10.1145/3585059.3611447},
abstract = {The use of ChatGPT in the educational ecosystem has opened up new avenues for learning but also raises questions about its multifarious long-term effects. This scientific study explores how ChatGPT, an AI chatbot, may impact the career prospects of Information Technology and Computer Science graduates in the long term, focusing on job automation and displacement. This study also investigates the enduring impact of ChatGPT on students' attitudes toward learning and developing skills in this education domain while examining ethical practices for incorporating this AI-based aid. This research provides methods to deter unethical actions related to ChatGPT and encourage ethical conduct among students for optimal performance. Moreover, it divulges the impact of ChatGPT on job opportunities, positive outlook, and the pressing necessity for ethical regulations in artificial intelligence use and deployment.},
booktitle = {Proceedings of the 24th Annual Conference on Information Technology Education},
pages = {73–78},
numpages = {6},
keywords = {Artificial Intelligence, ChatGPT, Ethical Practices in IT Education, Job Transformation, Student Attitudes},
location = {Marietta, GA, USA},
series = {SIGITE '23}
}

@inproceedings{10.1145/3568812.3603482,
author = {Tran, Andrew and Li, Linxuan and Rama, Egi and Angelikas, Kenneth and Macneil, Stephen},
title = {Using Large Language Models to Automatically Identify Programming Concepts in Code Snippets},
year = {2023},
isbn = {9781450399753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568812.3603482},
doi = {10.1145/3568812.3603482},
abstract = {Curating course material that aligns with students’ learning goals is a challenging and time-consuming task that instructors undergo when preparing their curricula. For instance, it is a challenge to find multiple-choice questions or example codes that demonstrate recursion in an unlabeled question bank or repository. Recently, Large Language Models (LLMs) have demonstrated the capability to generate high-quality learning materials at scale. In this poster, we use LLMs to identify programming concepts found within code snippets, allowing instructors to quickly curate their course materials. We compare programming concepts generated by LLMs with concepts generated by experts to see the extent to which they agree. The agreement was calculated using Cohen’s Kappa.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2},
pages = {22–23},
numpages = {2},
keywords = {computer science education, explanations, large language models},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@article{10.1145/3628162,
author = {Shoufan, Abdulhadi},
title = {Can Students without Prior Knowledge Use ChatGPT to Answer Test Questions? An Empirical Study},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {4},
url = {https://doi.org/10.1145/3628162},
doi = {10.1145/3628162},
abstract = {With the immense interest in ChatGPT worldwide, education has seen a mix of both excitement and skepticism. To properly evaluate its impact on education, it is crucial to understand how far it can help students without prior knowledge answer assessment questions. This study aims to address this question as well as the impact of the question type. We conducted multiple experiments with computer engineering students (experiment group: n=41 to 56), who were asked to use ChatGPT to answer previous test questions before learning about the related topics. Their scores were then compared with the scores of previous-term students who answered the same questions in a quiz or exam setting (control group: n=24 to 61). The results showed a wide range of effect sizes, from -2.55 to 1.23, depending on the question type and content. The experiment group performed best answering code analysis and conceptual questions but struggled with code completion and questions that involved images. However, the performance in code generation tasks was inconsistent. Overall, the ChatGPT group’s answers lagged slightly behind the control group’s answers with an effect size of -0.16. We conclude that ChatGPT, at least in the field of this study, is not yet ready to rely on by students who do not have sufficient background to evaluate generated answers. We suggest that educators try using ChatGPT and educate students on effective questioning techniques and how to assess the generated responses. This study provides insights into the capabilities and limitations of ChatGPT in education and informs future research and development.},
journal = {ACM Trans. Comput. Educ.},
month = dec,
articleno = {45},
numpages = {29},
keywords = {large language models, ChatGPT}
}

@inproceedings{10.1145/3611643.3613093,
author = {Cabra-Acela, Laura and Mojica-Hanke, Anamaria and Linares-V\'{a}squez, Mario and Herbold, Steffen},
title = {On Using Information Retrieval to Recommend Machine Learning Good Practices for Software Engineers},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613093},
doi = {10.1145/3611643.3613093},
abstract = {Machine learning (ML) is nowadays widely used for different purposes and with several disciplines. From self-driving cars to automated medical diagnosis, machine learning models extensively support users’ daily activities, and software engineering tasks are no exception. Not embracing good ML practices may lead to pitfalls that hinder the performance of an ML system and potentially lead to unexpected results. Despite the existence of documentation and literature about ML best practices, many non-ML experts turn towards gray literature like blogs and Q&amp;A systems when looking for help and guidance when implementing ML systems. To better aid users in distilling relevant knowledge from such sources, we propose a recommender system that recommends ML practices based on the user’s context. As a first step in creating a recommender system for machine learning practices, we implemented Idaka. A tool that provides two different approaches for retrieving/generating ML best practices: i) an information retrieval (IR) engine and ii) a large language model. The IR-engine uses BM25 as the algorithm for retrieving the practices, and a large language model, in our case Alpaca. The platform has been designed to allow comparative studies of best practices retrieval tools. Idaka is publicly available at  GitHub: https://bit.ly/idaka. Video: https://youtu.be/cEb-AhIPxnM},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {2142–2146},
numpages = {5},
keywords = {Good practices, Information retrieval, Large language models, Machine learning},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3583780.3615308,
author = {Makrehchi, Masoud and Zhang, Dell and Petrova, Alina and Armour, John},
title = {The 3rd International Workshop on Mining and Learning in the Legal Domain},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615308},
doi = {10.1145/3583780.3615308},
abstract = {The increasing accessibility of legal corpora and databases create opportunities to develop data-driven techniques and advanced tools that can facilitate a variety of tasks in the legal domain, such as legal search and research, legal document review and summary, legal contract drafting, and legal outcome prediction. Compared with other application domains, the legal domain is characterized by the huge scale of natural language text data, the high complexity of specialist knowledge, and the critical importance of ethical considerations. The MLLD workshop aims to bring together researchers and practitioners to share the latest research findings and innovative approaches in employing data mining, machine learning, information retrieval, and knowledge management techniques to transform the legal sector. Building upon the previous successes, the third edition of the MLLD workshop will emphasize the exploration of new research opportunities brought about by recent rapid advances in Large Language Models and Generative AI. We encourage submissions that intersect computer science and law, from both academia and industry, embodying the interdisciplinary spirit of CIKM.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {5277–5280},
numpages = {4},
keywords = {legal natural language processing, legal knowledge management, legal information retrieval, legal data mining, large language models},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@article{10.14778/3611540.3611634,
author = {Halevy, Alon and Choi, Yejin and Floratou, Avrilia and Franklin, Michael J. and Noy, Natasha and Wang, Haixun},
title = {Will LLMs Reshape, Supercharge, or Kill Data Science? (VLDB 2023 Panel)},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611634},
doi = {10.14778/3611540.3611634},
abstract = {Large language models (LLMs) have recently taken the world by storm, promising potentially game changing opportunities in multiple fields. Naturally, there is significant promise in applying LLMs to the management of structured data, or more generally, to the processes involved in data science. At the very least, LLMs have the potential to provide substantial advancements in long-standing challenges that our community has been tackling for decades. On the other hand, they may introduce completely new capabilities that we have only dreamed of thus far. This panel will bring together a few leading experts who have been thinking about these opportunities from various perspectives and fielding them in research prototypes and even in commercial applications.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {4114–4115},
numpages = {2}
}

@inproceedings{10.1145/3540250.3569444,
author = {Gulwani, Sumit},
title = {AI-assisted programming: applications, user experiences, and neuro-symbolic techniques (keynote)},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3569444},
doi = {10.1145/3540250.3569444},
abstract = {AI can enhance programming experiences for a diverse set of programmers: from professional developers and data scientists (proficient programmers) who need help in software engineering and data wrangling, all the way to spreadsheet users (low-code programmers) who need help in authoring formulas, and students (novice programmers) who seek hints when stuck with their programming homework. To communicate their need to AI, users can express their intent explicitly—as input-output examples or natural-language specification—or implicitly—where they encounter a bug (and expect AI to suggest a fix), or simply allow AI to observe their last few lines of code or edits (to have it suggest the next steps).  

The task of synthesizing an intended program snippet from the user’s intent is both a search and a ranking problem. Search is required to discover candidate programs that correspond to the (often ambiguous) intent, and ranking is required to pick the best program from multiple plausible alternatives. This creates a fertile playground for combining symbolic-reasoning techniques, which model the semantics of programming operators, and machine-learning techniques, which can model human preferences in programming. Recent advances in large language models like Codex offer further promise to advance such neuro-symbolic techniques.  

Finally, a few critical requirements in AI-assisted programming are usability, precision, and trust; and they create opportunities for innovative user experiences and interactivity paradigms. In this talk, I will explain these concepts using some existing successes, including the Flash Fill feature in Excel, Data Connectors in PowerQuery, and IntelliCode/CoPilot in Visual Studio. I will also describe several new opportunities in AI-assisted programming, which can drive the next set of foundational neuro-symbolic advances.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1},
numpages = {1},
keywords = {Symbolic Reasoning, Program Synthesis, Machine Learning, Interactive Programming},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@article{10.1145/3593230,
author = {Brie, Paul and Burny, Nicolas and Slu\"{y}ters, Arthur and Vanderdonckt, Jean},
title = {Evaluating a Large Language Model on Searching for GUI Layouts},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {EICS},
url = {https://doi.org/10.1145/3593230},
doi = {10.1145/3593230},
abstract = {The field of generative artificial intelligence has seen significant advancements in recent years with the advent of large language models, which have shown impressive results in software engineering tasks but not yet in engineering user interfaces. Thus, we raise a specific research question: would an LLM-based system be able to search for relevant GUI layouts? To address this question, we conducted a controlled study evaluating how Instigator, an LLM-based system for searching GUI layouts of web pages by generative pre-trained training, would return GUI layouts that are relevant to a given instruction and what would be the user experience of (N =34) practitioners interacting with Instigator. Our results identify a very high similarity and a moderate correlation between the rankings of the GUI layouts generated by Instigator and the rankings of the practitioners with respect to their relevance to a given design instruction. We highlight the results obtained through thirteen UEQ+ scales that characterize the user experience of the practitioner with Instigator, which we use to discuss perspectives for improving such future tools.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {178},
numpages = {37},
keywords = {web pages, large language model, gui layout, gui design, generative pre-training}
}

@inproceedings{10.1145/3587102.3588827,
author = {Malinka, Kamil and Peres\'{\i}ni, Martin and Firc, Anton and Hujn\'{a}k, Ondrej and Janus, Filip},
title = {On the Educational Impact of ChatGPT: Is Artificial Intelligence Ready to Obtain a University Degree?},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588827},
doi = {10.1145/3587102.3588827},
abstract = {In late 2022, OpenAI released a new version of ChatGPT, a sophisticated natural language processing system capable of holding natural conversations while preserving and responding to the context of the discussion. ChatGPT has exceeded expectations in its abilities, leading to extensive considerations of its potential applications and misuse. In this work, we evaluate the influence of ChatGPT on university education, with a primary focus on computer security-oriented specialization. We gather data regarding the effectiveness and usability of this tool for completing exams, programming assignments, and term papers. We evaluate multiple levels of tool misuse, ranging from utilizing it as a consultant to simply copying its outputs. While we demonstrate how easily ChatGPT can be used to cheat, we also discuss the potentially significant benefits to the educational system. For instance, it might be used as an aid (assistant) to discuss problems encountered while solving an assignment or to speed up the learning process. Ultimately, we discuss how computer science higher education should adapt to tools like ChatGPT.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {47–53},
numpages = {7},
keywords = {ChatGPT, academic education, artificial intelligence, computer security, virtual assistant},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3545945.3569770,
author = {Leinonen, Juho and Hellas, Arto and Sarsa, Sami and Reeves, Brent and Denny, Paul and Prather, James and Becker, Brett A.},
title = {Using Large Language Models to Enhance Programming Error Messages},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569770},
doi = {10.1145/3545945.3569770},
abstract = {A key part of learning to program is learning to understand programming error messages. They can be hard to interpret and identifying the cause of errors can be time-consuming. One factor in this challenge is that the messages are typically intended for an audience that already knows how to program, or even for programming environments that then use the information to highlight areas in code. Researchers have been working on making these errors more novice friendly since the 1960s, however progress has been slow. The present work contributes to this stream of research by using large language models to enhance programming error messages with explanations of the errors and suggestions on how to fix them. Large language models can be used to create useful and novice-friendly enhancements to programming error messages that sometimes surpass the original programming error messages in interpretability and actionability. These results provide further evidence of the benefits of large language models for computing educators, highlighting their use in areas known to be challenging for students. We further discuss the benefits and downsides of large language models and highlight future streams of research for enhancing programming error messages.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {563–569},
numpages = {7},
keywords = {ai, codex, compiler error messages, large language models, programming error messages, syntax error messages},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3587103.3594206,
author = {Prather, James and Denny, Paul and Leinonen, Juho and Becker, Brett A. and Albluwi, Ibrahim and Caspersen, Michael E. and Craig, Michelle and Keuning, Hieke and Kiesler, Natalie and Kohn, Tobias and Luxton-Reilly, Andrew and MacNeil, Stephen and Petersen, Andrew and Pettit, Raymond and Reeves, Brent N. and Savelka, Jaromir},
title = {Transformed by Transformers: Navigating the AI Coding Revolution for Computing Education: An ITiCSE Working Group Conducted by Humans},
year = {2023},
isbn = {9798400701399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587103.3594206},
doi = {10.1145/3587103.3594206},
abstract = {The recent advent of highly accurate and scalable large language models (LLMs) has taken the world by storm. From art to essays to computer code, LLMs are producing novel content that until recently was thought only humans could produce. Recent work in computing education has sought to understand the capabilities of LLMs for solving tasks such as writing code, explaining code, creating novel coding assignments, interpreting programming error messages, and more. However, these technologies continue to evolve at an astonishing rate leaving educators little time to adapt. This working group seeks to document the state-of-the-art for code generation LLMs, detail current opportunities and challenges related to their use, and present actionable approaches to integrating them into computing curricula.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2},
pages = {561–562},
numpages = {2},
keywords = {AI, CS1, GPT, GitHub, LLM, artificial intelligence, code generation, codex, computer programming, copilot, large language models, novice programming, openAI, pedagogical practices},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3611643.3613892,
author = {Jin, Matthew and Shahriar, Syed and Tufano, Michele and Shi, Xin and Lu, Shuai and Sundaresan, Neel and Svyatkovskiy, Alexey},
title = {InferFix: End-to-End Program Repair with LLMs},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613892},
doi = {10.1145/3611643.3613892},
abstract = {Software development life cycle is profoundly influenced by bugs; their introduction, identification, and eventual resolution account for a significant portion of software development cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large Language Models (LLMs) have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose : a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs.  combines a Retriever – transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator – an LLM (12 billion parameter Codex Cushman model) finetuned on supervised bug-fix data with prompts augmented via adding bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated , a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that  outperforms strong LLM baselines, with a top-1 accuracy of 65.6% for generating fixes in C# and 76.8% in Java. We discuss the deployment of  alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration (CI) pipeline to automate the software development workflow.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1646–1656},
numpages = {11},
keywords = {Program repair, finetuning, prompt augmentation, static analyses},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3613372.3614189,
author = {Albonico, Michel and Varela, Paulo J\'{u}nior},
title = {A Report on the Use of ChatGPT in Software Engineering and Systems Analysis Courses},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613372.3614189},
doi = {10.1145/3613372.3614189},
abstract = {ChatGPT is a natural language model that works as a virtual chat assistant. It has the potential to be used for fostering classroom discussions and addressing student needs when the professor is not accessible. Although it is still early to assess the impact of ChatGPT and similar technologies, there is a considerable discussion on social media and blogs regarding the aspirations and opportunities of utilizing ChatGPT in the software industry and education. The main perception is that ChatGPT can serve as a support tool but should not completely replace interpersonal interaction, as face-to-face dialogue remains crucial for the development of interpersonal skills and a deeper understanding of concepts. This article reports a recent classroom experience in the subjects of Software Engineering and Systems Analysis, while also analyzing ChatGPT’s responses to student inquiries.},
booktitle = {Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
pages = {303–311},
numpages = {9},
keywords = {ChatGPT, Software Engineering, Student Support, System Analysis},
location = {Campo Grande, Brazil},
series = {SBES '23}
}

@inproceedings{10.1145/3610969.3610982,
author = {Mahon, Joyce and Mac Namee, Brian and Becker, Brett A.},
title = {No More Pencils No More Books: Capabilities of Generative AI on Irish and UK Computer Science School Leaving Examinations},
year = {2023},
isbn = {9798400708763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610969.3610982},
doi = {10.1145/3610969.3610982},
abstract = {We investigate the capabilities of ChatGPT (GPT-4) on second-level (high-school) computer science examinations: the UK A-Level and Irish Leaving Certificate. Both are national, government-set / approved, and centrally assessed examinations. We also evaluate performance differences in exams made publicly available before and after the ChatGPT knowledge cutoff date, and investigate what types of question ChatGPT struggles with. We find that ChatGPT is capable of achieving very high marks on both exams and that the performance difference before and after the knowledge cutoff date are minimal. We also observe that ChatGPT struggles with questions involving symbols or images, which can be mitigated when in-text information ‘fills in the gaps’. Additionally, GPT-4 performance can be negatively impacted when an initial inaccurate answer leads to further inaccuracies in subsequent parts of the same question. Finally, the element of choice on the Leaving Certificate is a significant advantage in achieving a high grade. Notably, there are minimal occurrences of hallucinations in answers and few errors in solutions not involving images. These results reveal several strengths and weaknesses of these exams in terms of how generative AI performs on them and have implications for exam design, the construction of marking schemes, and could also shift the focus of what is examined and how.},
booktitle = {Proceedings of the 2023 Conference on United Kingdom &amp; Ireland Computing Education Research},
articleno = {2},
numpages = {7},
keywords = {A-Level, Artificial Intelligence, ChatGPT, GPT-4, Generative AI, Ireland, K-12, LCCS, Leaving Certificate, UK, examinations, high school, school, second-level},
location = {Swansea, Wales Uk},
series = {UKICER '23}
}

@inproceedings{10.1145/3622780.3623648,
author = {Kuramitsu, Kimio and Obara, Yui and Sato, Miyu and Obara, Momoka},
title = {KOGI: A Seamless Integration of ChatGPT into Jupyter Environments for Programming Education},
year = {2023},
isbn = {9798400703904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622780.3623648},
doi = {10.1145/3622780.3623648},
abstract = {The impact of ChatGPT has brought both anxiety and anticipation to schools and universities. Exploring a positive method to improve programming skills with ChatGPT is a new and pressing challenge.  
In pursuit of this goal, we have developed KOGI, a learning support system that integrates ChatGPT into the Jupyter environment. This paper demonstrates how KOGI enables students to receive timely advice from ChatGPT in response to errors and other questions they encounter.  

We immediately introduced KOGI in our two introductory courses: Algorithms and Data Science. The introduction of KOGI resulted in a significant decrease in the number of unresolved student errors. In addition, we report on student trends observed in the classroom regarding the type and frequency of help requested. Although our findings are preliminary, they are informative for programming instructors interested in using ChatGPT.},
booktitle = {Proceedings of the 2023 ACM SIGPLAN International Symposium on SPLASH-E},
pages = {50–59},
numpages = {10},
keywords = {ChatGPT, LLM, classroom experience, programming education},
location = {Cascais, Portugal},
series = {SPLASH-E 2023}
}

@inproceedings{10.1145/3587102.3588805,
author = {Reeves, Brent and Sarsa, Sami and Prather, James and Denny, Paul and Becker, Brett A. and Hellas, Arto and Kimmel, Bailey and Powell, Garrett and Leinonen, Juho},
title = {Evaluating the Performance of Code Generation Models for Solving Parsons Problems With Small Prompt Variations},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588805},
doi = {10.1145/3587102.3588805},
abstract = {The recent emergence of code generation tools powered by large language models has attracted wide attention. Models such as OpenAI Codex can take natural language problem descriptions as input and generate highly accurate source code solutions, with potentially significant implications for computing education. Given the many complexities that students face when learning to write code, they may quickly become reliant on such tools without properly understanding the underlying concepts. One popular approach for scaffolding the code writing process is to use Parsons problems, which present solution lines of code in a scrambled order. These remove the complexities of low-level syntax, and allow students to focus on algorithmic and design-level problem solving. It is unclear how well code generation models can be applied to solve Parsons problems, given the mechanics of these models and prior evidence that they underperform when problems include specific restrictions. In this paper, we explore the performance of the Codex model for solving Parsons problems over various prompt variations. Using a corpus of Parsons problems we sourced from the computing education literature, we find that Codex successfully reorders the problem blocks about half of the time, a much lower rate of success when compared to prior work on more free-form programming tasks. Regarding prompts, we find that small variations in prompting have a noticeable effect on model performance, although the effect is not as pronounced as between different problems.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {299–305},
numpages = {7},
keywords = {CS1, GPT-3, GitHub, ML, academic integrity, ai, artificial intelligence, chatgpt, code generation, code writing, codex, computer programming, copilot, deep learning, generative ai, introductory programming, large language models, machine learning, natural language processing, neural networks, novice programming, openAI},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3600061.3603136,
author = {Fu, Shuhao and Liao, Yong and Zhou, Pengyuan},
title = {Training ChatGPT-like Models with In-network Computation},
year = {2023},
isbn = {9798400707827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600061.3603136},
doi = {10.1145/3600061.3603136},
abstract = {ChatGPT shows the enormous potential of large language models (LLMs). These models can easily reach the size of billions of parameters and create training difficulties for the majority. We propose a paradigm to train LLMs using distributed in-network computation on routers. Our preliminary result shows that our design allows LLMs to be trained at a reasonable learning rate without demanding extensive GPU resources.},
booktitle = {Proceedings of the 7th Asia-Pacific Workshop on Networking},
pages = {206–207},
numpages = {2},
keywords = {ChatGPT, In-network Computation, Large Language Model, Pipeline Parallelism},
location = {Hong Kong, China},
series = {APNet '23}
}

@inproceedings{10.1145/3580305.3599199,
author = {Gaur, Manas and Tsamoura, Efthymia and Sreedharan, Sarath and Mittal, Sudip},
title = {KiL 2023 : 3rd International Workshop on Knowledge-infused Learning},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599199},
doi = {10.1145/3580305.3599199},
abstract = {Recent prolific advances in artificial intelligence through the incorporation of domain knowledge have constituted a new paradigm for AI and data mining communities. For example, the human feedback-based language generation in ChatGPT (a large language model (LLM)), the use of Protein Bank in DeepMind's AlphaFold, and the use of 23 rules of safety in DeepMind's Sparrow have demonstrated the success of teaming human knowledge and AI. In addition, the knowledge retrieval-guided language modeling methods have strengthened the association between knowledge and AI. However, translating research methods and resources into practice presents a new challenge for the machine learning and data/knowledge mining communities. For example, in DARPA's Explainable AI seminar, the need for explainable contextual adaptation is seen as the 3rd phase of AI, facilitating the interplay between data and knowledge for explainability, safety, and, eventually, trust. However, policymakers and practitioners assert serious usability and privacy concerns that constrain adoption, notably in high-consequence domains, such as cybersecurity, healthcare, and other social good domains. In addition, limitations in output quality, measurement, and interactive ability, including both the provision of explanations and the acceptance of user preferences, result in low adoption rates in such domains. This workshop aims to accelerate our pace towards creating innovative methods for integrating knowledge into contemporary AI and data science methods and develop metrics for assessing performance in various applications.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5857–5858},
numpages = {2},
keywords = {explainable ai, games, knowledge-infused learning, language models, neurosymbolic ai, programming languages, safe ai},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3544549.3582749,
author = {Byun, Courtni and Vasicek, Piper and Seppi, Kevin},
title = {Dispensing with Humans in Human-Computer Interaction Research},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3582749},
doi = {10.1145/3544549.3582749},
abstract = {Machine Learning models have become more advanced than could have been supposed even a few years ago, often surpassing human performance on many tasks. Large language models (LLM) can produce text indistinguishable from human-produced text. This begs the question, how necessary are humans - even for tasks where humans appear indispensable? Qualitative Analysis (QA) is integral to human-computer interaction research, requiring both human-produced data and human analysis of that data to illuminate human opinions about and experiences with technology. We use GPT-3 and ChatGPT to replace human analysis and then to dispense with human-produced text altogether. We find GPT-3 is capable of automatically identifying themes and generating nuanced analyses of qualitative data arguably similar to those written by human researchers. We also briefly ponder philosophical implications of this research.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {413},
numpages = {26},
keywords = {gpt-3, prompt engineering, qualitative analysis},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3605390.3610821,
author = {Di Caro, Luigi and Rapp, Amon and Torrielli, Federico},
title = {GENERAL: GENerative, Explainable and Reasonable Artificial Learning},
year = {2023},
isbn = {9798400708060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605390.3610821},
doi = {10.1145/3605390.3610821},
abstract = {The GENERAL (GENerative, Explainable and Reasonable Artificial Learning) workshop, held at CHITALY 2023, delves into the advancements in General and Generative Artificial Intelligence (GGAI), with a focus on breakthroughs in natural language processing (NLP) and computer vision (CV). The workshop highlights the capabilities of Large Language Models (LLMs) and Latent Diffusion Models (LDMs) in generating human-like content across text and images. It emphasizes the importance of AI explainability, aiming to understand, explain, and control the complexities of these AI systems in terms of fairness, accountability, and transparency. The workshop encourages interdisciplinary collaboration across fields like HCI, psychology, social studies, and the arts to better understand AI’s societal and cultural impacts. Topics of interest include user perceptions of generative AIs, machine psychology, AI assistants, ethical issues in Generative AI, and safety and control mechanisms for large language models.},
booktitle = {Proceedings of the 15th Biannual Conference of the Italian SIGCHI Chapter},
articleno = {35},
numpages = {2},
keywords = {AI Explainability, Ethical Issues in AI, Generative Artificial Intelligence, Interdisciplinary Collaboration, Large Language Models},
location = {Torino, Italy},
series = {CHItaly '23}
}

@inproceedings{10.1145/3624062.3624172,
author = {Ding, Xianzhong and Chen, Le and Emani, Murali and Liao, Chunhua and Lin, Pei-Hung and Vanderbruggen, Tristan and Xie, Zhen and Cerpa, Alberto and Du, Wan},
title = {HPC-GPT: Integrating Large Language Model for High-Performance Computing},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624172},
doi = {10.1145/3624062.3624172},
abstract = {Large Language Models (LLMs), including the LLaMA model, have exhibited their efficacy across various general-domain natural language processing (NLP) tasks. However, their performance in high-performance computing (HPC) domain tasks has been less than optimal due to the specialized expertise required to interpret the model’s responses. In response to this challenge, we propose HPC-GPT, a novel LLaMA-based model that has been supervised fine-tuning using generated QA (Question-Answer) instances for the HPC domain. To evaluate its effectiveness, we concentrate on two HPC tasks: managing AI models and datasets for HPC, and data race detection. By employing HPC-GPT, we demonstrate comparable performance with existing methods on both tasks, exemplifying its excellence in HPC-related scenarios. Our experiments on open-source benchmarks yield extensive results, underscoring HPC-GPT’s potential to bridge the performance gap between LLMs and HPC-specific tasks. With HPC-GPT, we aim to pave the way for LLMs to excel in HPC domains, simplifying the utilization of language models in complex computing applications.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {951–960},
numpages = {10},
keywords = {Data Race Detection, High-performance Computing, Large Language Model, Neural Network., OpenMP},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@inproceedings{10.1145/3610969.3611132,
author = {Petrovska, Olga and Clift, Lee and Moller, Faron},
title = {Generative AI in Software Development Education: Insights from a Degree Apprenticeship Programme},
year = {2023},
isbn = {9798400708763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610969.3611132},
doi = {10.1145/3610969.3611132},
abstract = {We describe insights gained from incorporating ChatGPT into assignments for our Software Engineering Degree Apprenticeship programme, including attitudes expressed by the learners and their employers regarding our approach.},
booktitle = {Proceedings of the 2023 Conference on United Kingdom &amp; Ireland Computing Education Research},
articleno = {19},
numpages = {1},
keywords = {Apprenticeships, Education, Generative AI, Software Engineering},
location = {Swansea, Wales Uk},
series = {UKICER '23}
}

@article{10.5555/3636988.3636989,
author = {Conrad, Susan and Dimitoglou, George and Flinn, Michael B. and Morgan, Jacob and Gupta, Pranshu and Mengistu, Zelalem},
title = {Current Challenges in Computing Education},
year = {2023},
issue_date = {October 2023},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {3},
issn = {1937-4771},
abstract = {Discussion about topics related to current issues in computing science education focusing on three themes: "That AI thing...", "Post-Pandemic Strategies," and "Partnerships."The first theme attempts to address the benefits, challenges, and practical applications of integrating Generative AI technologies, such as ChatGPT, Bard, and CoPilot, into educational settings. Exploration of academic honesty and intellectual property and strategies for how these AI tools can be utilized in classrooms, labs, student projects, assignments, academic programs, and even preparing students for future job opportunities.The second theme revolves around post-pandemic approaches and initiatives to explore aimed at re-engaging students in both classroom activities and extracurricular pursuits. Exploration of strategies to enhance undergraduate and graduate student participation in internships, research opportunities, and the unique challenges and characteristics of job hunting in the current educational and economic landscape.The third theme highlights the significance of forging partnerships between educational institutions and industry stakeholders. Exploring campus ideas and efforts to establish and strengthen relationships with industry partners. Discussion on collaborative projects, research initiatives, mentorship programs, and ways to bridge the gap between academia and industry to benefit both students and the workforce.The final theme is open-ended, encouraging attendees to contemplate additional questions that may initiate reflection on emerging trends, pedagogical challenges, technological advancements, and any other critical issues that computing science educators should address to stay effective and responsive in their roles.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {16–17},
numpages = {2}
}

@inproceedings{10.1145/3583133.3596401,
author = {Pluhacek, Michal and Kazikova, Anezka and Kadavy, Tomas and Viktorin, Adam and Senkerik, Roman},
title = {Leveraging Large Language Models for the Generation of Novel Metaheuristic Optimization Algorithms},
year = {2023},
isbn = {9798400701207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583133.3596401},
doi = {10.1145/3583133.3596401},
abstract = {In this paper, we investigate the potential of using Large Language Models (LLMs) such as GPT-4 to generate novel hybrid swarm intelligence optimization algorithms. We use the LLM to identify and decompose six well-performing swarm algorithms for continuous optimization: Particle Swarm Optimization (PSO), Cuckoo Search (CS), Artificial Bee Colony (ABC), Grey Wolf Optimizer (GWO), Self-Organizing Migrating Algorithm (SOMA), and Whale Optimization Algorithm (WOA). We leverage GPT-4 to propose a hybrid algorithm that combines the strengths of these techniques for two distinct use-case scenarios. Our focus is on the process itself and various challenges that emerge during the use of GPT-4 to fulfill a series of set tasks. Furthermore, we discuss the potential impact of LLM-generated algorithms in the metaheuristics domain and explore future research directions.},
booktitle = {Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
pages = {1812–1820},
numpages = {9},
keywords = {large language models, metaheuristic optimization, swarm algorithms, algorithm generation, decomposition and construction, GPT-4},
location = {Lisbon, Portugal},
series = {GECCO '23 Companion}
}

@inproceedings{10.1145/3581641.3584037,
author = {Ross, Steven I. and Martinez, Fernando and Houde, Stephanie and Muller, Michael and Weisz, Justin D.},
title = {The Programmer’s Assistant: Conversational Interaction with a Large Language Model for Software Development},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584037},
doi = {10.1145/3581641.3584037},
abstract = {Large language models (LLMs) have recently been applied in software engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. When used within development tools, these systems typically treat each model invocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model’s responses. We developed a prototype system – the Programmer’s Assistant – in order to explore the utility of conversational interactions grounded in code, as well as software engineers’ receptiveness to the idea of conversing with, rather than invoking, a code-fluent LLM. Through an evaluation with 42 participants with varied levels of programming experience, we found that our system was capable of conducting extended, multi-turn discussions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the LLM. Despite skeptical initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistant’s capabilities, the quality of its responses, and its potential for improving their productivity. Our work demonstrates the unique potential of conversational interactions with LLMs for co-creative processes like software development.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {491–514},
numpages = {24},
keywords = {code-fluent large language models, conversational interaction, foundation models, human-centered AI},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@inproceedings{10.1145/3586182.3615817,
author = {Du, Ruofei and Li, Na and Jin, Jing and Carney, Michelle and Yuan, Xiuxiu and Wright, Kristen and Sherwood, Mark and Mayes, Jason and Chen, Lin and Jiang, Jun and Zhou, Jingtao and Zhou, Zhongyi and Yu, Ping and Kowdle, Adarsh and Iyengar, Ram and Olwal, Alex},
title = {Experiencing Visual Blocks for ML: Visual Prototyping of AI Pipelines},
year = {2023},
isbn = {9798400700965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586182.3615817},
doi = {10.1145/3586182.3615817},
abstract = {We demonstrate Visual Blocks for ML, a visual programming platform that facilitates rapid prototyping of ML-based multimedia applications. As the public version of Rapsai&nbsp;[3], we further integrated large language models and custom APIs into the platform. In this demonstration, we will showcase how to build interactive AI pipelines in a few drag-and-drops, how to perform interactive data augmentation, and how to integrate pipelines into Colabs. In addition, we demonstrate a wide range of community-contributed pipelines in Visual Blocks for ML, covering various aspects including interactive graphics, chains of large language models, computer vision, and multi-modal applications. Finally, we encourage students, designers, and ML practitioners to contribute ML pipelines through https://github.com/google/visualblocks/tree/main/pipelines to inspire creative use cases. Visual Blocks for ML is available at http://visualblocks.withgoogle.com.},
booktitle = {Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {76},
numpages = {3},
keywords = {data augmentation, deep learning, deep neural networks, large language models, multi-modal models, node-graph editor, visual analytics, visual programming, visual prototyping},
location = {San Francisco, CA, USA},
series = {UIST '23 Adjunct}
}

@inproceedings{10.1145/3600100.3626262,
author = {Berger, Markus and Ploennigs, Joern},
title = {ArchiGuesser – Teaching Architecture Styles using Generative AI},
year = {2023},
isbn = {9798400702303},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600100.3626262},
doi = {10.1145/3600100.3626262},
abstract = {Generative AIs are opening new possibilities to create content from text, speech, and images based on simple input prompts. Users use this to improve their productivity when summarizing knowledge, templating communication, and inspiring their creativity. But, can it also be used to teach, e.g. about our architectural history? With this demo we are exploring this question. We created an educational game that combines various AI technologies from large language models and image generation to computer vision, in order to serve a single purpose: Teach users about architecture in an entertaining way. We wanted to enable students to explore and learn the diversity of our architectural history in a playful and exploratory way and at the same time experience and understand what current AI technologies can achieve.},
booktitle = {Proceedings of the 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {284–285},
numpages = {2},
location = {Istanbul, Turkey},
series = {BuildSys '23}
}

@inproceedings{10.1145/3478432.3499265,
author = {Koornneef, Stacey A. and Bradbury, Jeremy S. and Miljanovic, Michael A.},
title = {Run, Llama, Run: A Collaborative Physical and Online Coding Game for Children},
year = {2022},
isbn = {9781450390712},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478432.3499265},
doi = {10.1145/3478432.3499265},
abstract = {Computational thinking and computer science are now being introduced in K-5 classrooms and this has led to a demand for more engaging and interactive tools designed for a younger audience. Educational games and block-based programming are two approaches that have been shown to be effective at engaging children to learn computer science. While existing tools have value, they also have limitations with respect to their support for collaborative learning and with respect to equitable access. Run, Llama, Run, is a collaborative educational game designed to be played by K-5 students both with and without access to a tablet or computer. The game includes physical programming blocks where a group of students work together to find a solution for a given scenario. A digital interface is available to execute and animate student solutions and a non-digital alternative allows students to act out their solutions. This demo of Run, Llama, Run provides a chance for participants to play both versions of the game and observe the potential impact this game could have for students.},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education V. 2},
pages = {1177},
numpages = {1},
keywords = {collaborative learning, educational game, k-5 computer science education},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}

@inproceedings{10.1145/3568813.3600138,
author = {Lau, Sam and Guo, Philip},
title = {From "Ban It Till We Understand It" to "Resistance is Futile": How University Programming Instructors Plan to Adapt as More Students Use AI Code Generation and Explanation Tools such as ChatGPT and GitHub Copilot},
year = {2023},
isbn = {9781450399760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568813.3600138},
doi = {10.1145/3568813.3600138},
abstract = {Over the past year (2022–2023), recently-released AI tools such as ChatGPT and GitHub Copilot have gained significant attention from computing educators. Both researchers and practitioners have discovered that these tools can generate correct solutions to a variety of introductory programming assignments and accurately explain the contents of code. Given their current capabilities and likely advances in the coming years, how do university instructors plan to adapt their courses to ensure that students still learn well? To gather a diverse sample of perspectives, we interviewed 20 introductory programming instructors (9 women + 11 men) across 9 countries (Australia, Botswana, Canada, Chile, China, Rwanda, Spain, Switzerland, United States) spanning all 6 populated continents. To our knowledge, this is the first empirical study to gather instructor perspectives about how they plan to adapt to these AI coding tools that more students will likely have access to in the future. We found that, in the short-term, many planned to take immediate measures to discourage AI-assisted cheating. Then opinions diverged about how to work with AI coding tools longer-term, with one side wanting to ban them and continue teaching programming fundamentals, and the other side wanting to integrate them into courses to prepare students for future jobs. Our study findings capture a rare snapshot in time in early 2023 as computing instructors are just starting to form opinions about this fast-growing phenomenon but have not yet converged to any consensus about best practices. Using these findings as inspiration, we synthesized a diverse set of open research questions regarding how to develop, deploy, and evaluate AI coding tools for computing education.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1},
pages = {106–121},
numpages = {16},
keywords = {AI coding tools, ChatGPT, Copilot, LLM, instructor perspectives},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3583131.3590351,
author = {Lanzi, Pier Luca and Loiacono, Daniele},
title = {ChatGPT and Other Large Language Models as Evolutionary Engines for Online Interactive Collaborative Game Design},
year = {2023},
isbn = {9798400701191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583131.3590351},
doi = {10.1145/3583131.3590351},
abstract = {Large language models (LLMs) have taken the scientific world by storm, changing the landscape of natural language processing and human-computer interaction. These powerful tools can answer complex questions and, surprisingly, perform challenging creative tasks (e.g., generate code and applications to solve problems, write stories, pieces of music, etc.). In this paper, we present a collaborative game design framework that combines interactive evolution and large language models to simulate the typical human design process. We use the former to exploit users' feedback for selecting the most promising ideas and large language models for a very complex creative task---the recombination and variation of ideas. In our framework, the process starts with a brief and a set of candidate designs, either generated using a language model or proposed by the users. Next, users collaborate on the design process by providing feedback to an interactive genetic algorithm that selects, recombines, and mutates the most promising designs. We evaluated our framework on three game design tasks with human designers who collaborated remotely.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1383–1390},
numpages = {8},
keywords = {collaborative design, large language models, interactive evolution},
location = {Lisbon, Portugal},
series = {GECCO '23}
}

@inproceedings{10.1145/3583780.3615047,
author = {Hoq, Muntasir and Chilla, Sushanth Reddy and Ahmadi Ranjbar, Melika and Brusilovsky, Peter and Akram, Bita},
title = {SANN: Programming Code Representation Using Attention Neural Network with Optimized Subtree Extraction},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615047},
doi = {10.1145/3583780.3615047},
abstract = {Automated analysis of programming data using code representation methods offers valuable services for programmers, from code completion to clone detection to bug detection. Recent studies show the effectiveness of Abstract Syntax Trees (AST), pre-trained Transformer-based models, and graph-based embeddings in programming code representation. However, pre-trained large language models lack interpretability, while other embedding-based approaches struggle with extracting important information from large ASTs. This study proposes a novel Subtree-based Attention Neural Network (SANN) to address these gaps by integrating different components: an optimized sequential subtree extraction process using Genetic algorithm optimization, a two-way embedding approach, and an attention network. We investigate the effectiveness of SANN by applying it to two different tasks: program correctness prediction and algorithm detection on two educational datasets containing both small and large-scale code snippets written in Java and C, respectively. The experimental results show SANN's competitive performance against baseline models from the literature, including code2vec, ASTNN, TBCNN, CodeBERT, GPT-2, and MVG, regarding accurate predictive power. Finally, a case study is presented to show the interpretability of our model prediction and its application for an important human-centered computing application, student modeling. Our results indicate the effectiveness of the SANN model in capturing important syntactic and semantic information from students' code, allowing the construction of accurate student models, which serve as the foundation for generating adaptive instructional support such as individualized hints and feedback.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {783–792},
numpages = {10},
keywords = {algorithm detection, code representation, program analysis, program correctness prediction, static analysis},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3545945.3569785,
author = {MacNeil, Stephen and Tran, Andrew and Hellas, Arto and Kim, Joanne and Sarsa, Sami and Denny, Paul and Bernstein, Seth and Leinonen, Juho},
title = {Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569785},
doi = {10.1145/3545945.3569785},
abstract = {Advances in natural language processing have resulted in large language models (LLMs) that can generate code and code explanations. In this paper, we report on our experiences generating multiple code explanation types using LLMs and integrating them into an interactive e-book on web software development. Three different types of explanations -- a line-by-line explanation, a list of important concepts, and a high-level summary of the code -- were created. Students could view explanations by clicking a button next to code snippets, which showed the explanation and asked about its utility. Our results show that all explanation types were viewed by students and that the majority of students perceived the code explanations as helpful to them. However, student engagement varied by code snippet complexity, explanation type, and code snippet length. Drawing on our experiences, we discuss future directions for integrating explanations generated by LLMs into CS classrooms.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {931–937},
numpages = {7},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3610661.3617514,
author = {A, Rajagopal and V, Nirmala and Jebadurai, Immanuel Johnraja and Vedamanickam, Arun Muthuraj and Kumar, Prajakta Uthaya},
title = {Design of Generative Multimodal AI Agents to Enable Persons with Learning Disability},
year = {2023},
isbn = {9798400703218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610661.3617514},
doi = {10.1145/3610661.3617514},
abstract = {The recent advances in Multimodal AI &amp; Generative AI open doors to the possibilities of solving key challenges for Persons with Learning Disability. To assist individuals facing difficulty in visual or auditory perception, this paper designs &amp; develops a multimodal AI agent using recent advances in the field. We aim to solve the challenge of enabling persons with Visual or Auditory Processing Disorders to learn &amp; communicate. We do this by exploring a design that allows the transformation of information across visual and language modalities. This design can be realized with the recent advances in Generative Multimodal AI. Based on each individual's needs, the AI agent dynamically adapts the Human Computer interaction model. For instance, for a child with Visual Processing Disorder (VPD), given the child's hindered ability to make sense of information taken in through the eyes, the Multimodal AI agent transforms any visual information into auditory user interaction. In another instance, for a person with Central Auditory Processing Disorder (CAPD), given the hindrance in the individual's ability to analyze information taken in through the ears, the AI dynamically translates any speech modality into visual cues. Thus the AI agent adapts dynamically to the strengths and abilities of the individual. To enable students with VPD to learn, the design allows the student to ask questions about an image. This design is realized as a Visual Question Answering task in Vision Language Transformer models. We explore interactive multimodal conversations with Few shot Learning and In-Context Instruction Tuning of Multimodal Large Language Models to address difficulty in visual reasoning. To enable persons with CAPD to learn, the design translates audio lectures into visual cues. This visual cue consists of a combination of words using speech recognition and Large Language Models based re-phrasing to simpler words, cross-modal retrieval of images to address auditory memory challenges, and AI-generated images. To identify the strengths of each child, we also explore Multimodal embedding based Multimodal latent space arithmetic to link AI across senses. To effectively integrate the proposed design into the mainstream, we explore a universal design based inclusive approach to extend the use case to create AI assistants for assisting children with different learning styles such as visual learners or auditory learners. To enable future research on the proposed design, we explore an architecture to compose a pipeline of AI models, and to connect with external systems via plugin connectors. We implement lab scale prototypes of this design and present a demo on the project webpage at https://sites.google.com/view/multimodallearningdisability.},
booktitle = {Companion Publication of the 25th International Conference on Multimodal Interaction},
pages = {259–271},
numpages = {13},
keywords = {Assistive Technology, Central Auditory Processing Disorder, Generative AI, Human Computer Interaction, Learning disability, Multimodal AI, Multimodal Few shot Learning, Multimodal In-context Tuning, Multimodal Large Language Transformers, Multimodal latent space, Person with Disability, Vision Language Models, Visual Processing Disorder},
location = {Paris, France},
series = {ICMI '23 Companion}
}

@inproceedings{10.1145/3584371.3612956,
author = {Shi, Wenqi and Zhuang, Yuchen and Zhu, Yuanda and Iwinski, Henry and Wattenbarger, Michael and Wang, May Dongmei},
title = {Retrieval-Augmented Large Language Models for Adolescent Idiopathic Scoliosis Patients in Shared Decision-Making},
year = {2023},
isbn = {9798400701269},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584371.3612956},
doi = {10.1145/3584371.3612956},
abstract = {As health-related decision-making evolves, patients increasingly seek help from additional online resources such as "Dr. Google" and ChatGPT. Despite their potential, these tools encounter limitations, including the risk of potentially inaccurate information, a lack of specialized medical knowledge, the risk of generating unrealistic outputs (hallucinations), and significant computational demands. In this study, we develop and validate an innovative shared decisionmaking (SDM) tool, Chat-Orthopedist, for adolescent idiopathic scoliosis (AIS) patients and families to prepare a meaningful discussion with clinicians based on retrieval-augmented large language models. Firstly, we establish an external knowledge base with information on AIS disease and treatment options Secondly, we develop a retrieval-augmented ChatGPT to feed LLMs with AIS domain knowledge, providing accurate and comprehensible responses to patient inquiries. In addition, we perform a cyclical process of human-in-the-loop evaluations for system validation and improvement. ment. Chat-Orthopedist may optimize SDM workflow by enabling better interactive learning experiences, more effective clinical visits, and better-informed treatment decision-making.},
booktitle = {Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {14},
numpages = {10},
keywords = {large language models, information retrieval, pediatric healthcare, shared decision-making, adolescent idiopathic scoliosis},
location = {Houston, TX, USA},
series = {BCB '23}
}

@inproceedings{10.1145/3581784.3613215,
author = {Yin, Junqi and Dash, Sajal and Wang, Feiyi and Shankar, Mallikarjun},
title = {FORGE: Pre-Training Open Foundation Models for Science},
year = {2023},
isbn = {9798400701092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581784.3613215},
doi = {10.1145/3581784.3613215},
abstract = {Large language models (LLMs) are poised to revolutionize the way we conduct scientific research. However, both model complexity and pre-training cost are impeding effective adoption for the wider science community. Identifying suitable scientific use cases, finding the optimal balance between model and data sizes, and scaling up model training are among the most pressing issues that need to be addressed. In this study, we provide practical solutions for building and using LLM-based foundation models targeting scientific research use cases. We present an end-to-end examination of the effectiveness of LLMs in scientific research, including their scaling behavior and computational requirements on Frontier, the first Exascale supercomputer. We have also developed for release to the scientific community a suite of open foundation models called FORGE with up to 26B parameters using 257B tokens from over 200M scientific articles, with performance either on par or superior to other state-of-the-art comparable models. We have demonstrated the use and effectiveness of FORGE on scientific downstream tasks. Our research establishes best practices that can be applied across various fields to take advantage of LLMs for scientific discovery.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {81},
numpages = {13},
location = {Denver, CO, USA},
series = {SC '23}
}

@inproceedings{10.1145/3573381.3596471,
author = {Stragier, Vincent and Seddati, Omar and Dutoit, Thierry},
title = {Developing an Interactive Agent for Blind and Visually Impaired People},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573381.3596471},
doi = {10.1145/3573381.3596471},
abstract = {The aim of this project is to create an interactive assistant that incorporates different assistive features for blind and visually impaired people. The assistant might incorporate screen readers, magnifiers, voice synthesis, OCR, GPS, face recognition, and object recognition among other tools. Recently, the work done by OpenAI and Be My Eyes with the implementation of GPT-4 is comparable to the aim of this project. It shows the development of an interactive assistant has become simpler due to recent developments in large language models. However, older methods like named entity recognition and intent classification are still valuable to build lightweight assistants. A hybrid solution combining both methods seems possible, would help to reduce the computational cost of the assistant, and would facilitate the data collection process. Despite being more complex to implement in a multilingual and multimodal context, a hybrid solution has the potential to be used offline and to consume less resources.},
booktitle = {Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
pages = {248–253},
numpages = {6},
keywords = {BLOOMZ, GPT-4, OCR, Open Source, OpenAI, accessibility, assistive technology, blind, face recognition, object recognition, visually impaired},
location = {Nantes, France},
series = {IMX '23}
}

@inproceedings{10.5555/3615924.3615927,
author = {Nascimento, Nathalia and Alencar, Paulo and Cowan, Donald},
title = {Artificial Intelligence vs. Software Engineers: An Empirical Study on Performance and Efficiency using ChatGPT},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {In the realm of Software Engineering (SE), automation has become a tangible reality. Artificial Intelligence (AI) has suc-cessfully addressed challenges in project management, mod-eling, testing, and development. Among the latest innova-tions is ChatGPT, an ML-infused chatbot capable of gen-erating programming codes and software testing strategies. Although there is speculation that AI-based computation can boost productivity and even substitute software engineers in software development, empirical evidence supporting such claims is lacking. Moreover, questions remain about their po-tential to address overlooked evaluation metrics like energy efficiency, vulnerability, fairness (i.e., human bias), and safety. This paper probes into these issues with an empirical study, comparing ChatGPT with both novice and expert program-mers using LeetCode contest problems. The investigation focuses on performance and memory-efficiency, while also acknowledging the need for a broader assessment of non-functional requirements. The results suggest that ChatGPT is better than beginners at solving easy and medium prob-lems, but it is not yet proven to beat expert programmers. This paper posits that a comprehensive comparison of soft-ware engineers and AI-based solutions, considering various evaluation criteria, is pivotal in fostering human-machine collaboration, enhancing the reliability of AI-based meth-ods, and understanding task suitability for humans or AI. Furthermore, it facilitates the effective implementation of co-operative work structures and human-in-the-loop processes.},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {24–33},
numpages = {10},
keywords = {Software Engineering, AI-based solutions, Performance Evaluation, ChatGPT, Machine Learning},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@inproceedings{10.1145/3576123.3576134,
author = {Finnie-Ansley, James and Denny, Paul and Luxton-Reilly, Andrew and Santos, Eddie Antonio and Prather, James and Becker, Brett A.},
title = {My AI Wants to Know if This Will Be on the Exam: Testing OpenAI’s Codex on CS2 Programming Exercises},
year = {2023},
isbn = {9781450399418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576123.3576134},
doi = {10.1145/3576123.3576134},
abstract = {The introduction of OpenAI Codex sparked a surge of interest in the impact of generative AI models on computing education practices. Codex is also the underlying model for GitHub Copilot, a plugin which makes AI-generated code accessible to students through auto-completion in popular code editors. Research in this area, particularly on the educational implications, is nascent and has focused almost exclusively on introductory programming (or CS1) questions. Very recent work has shown that Codex performs considerably better on typical CS1 exam questions than most students. It is not clear, however, what Codex’s limits are with regard to more complex programming assignments and exams. In this paper, we present results detailing how Codex performs on more advanced CS2 (data structures and algorithms) exam questions taken from past exams. We compare these results to those of students who took the same exams under normal conditions, demonstrating that Codex outscores most students. We consider the implications of such tools for the future of undergraduate computing education.},
booktitle = {Proceedings of the 25th Australasian Computing Education Conference},
pages = {97–104},
numpages = {8},
keywords = {AI, AlphaCode, CS1, CS2, Codex, DeepMind, GPT-3, GitHub, OpenAI, academic integrity, algorithms, artificial intelligence, code generation, copilot, data structures, deep learning, introductory programming, machine learning, neural networks, novice programming},
location = {Melbourne, VIC, Australia},
series = {ACE '23}
}

@inproceedings{10.1145/3579027.3608973,
author = {Galindo, Jos\'{e} A. and Dominguez, Antonio J. and White, Jules and Benavides, David},
title = {Large Language Models to generate meaningful feature model instances},
year = {2023},
isbn = {9798400700910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579027.3608973},
doi = {10.1145/3579027.3608973},
abstract = {Feature models are the "de facto" standard for representing variability in software-intensive systems. Automated analysis of feature models is the computer-aided extraction of information of feature models and is used in testing, maintenance, configuration, and derivation, among other tasks. Testing the analyses of feature models often requires relying on a large number of models that are as realistic as possible. There exist different proposals to generate synthetic feature models using random techniques or metamorphic relations; however, the existing methods do not take into account the semantics of the concepts of the domain that are being represented and the interrelations between them, leading to less realistic feature models. In this paper, we propose a novel approach that uses Large Language Models (LLMs), such as Codex or GPT-3, to generate realistic feature models that preserve semantic coherence while maintaining syntactic validity. The approach automatically generates instances of feature models from a given domain. Concretely, two language models were used, first OpenAI's Codex to generate new instances of feature models using the Universal Variability Language (UVL) syntax and then Cohere's semantic analysis to verify if the newly introduced concepts are from the same domain. This approach enabled the generation of 90% of valid instances according to the UVL syntax. In addition, the valid models score well on model complexity metrics, and the generated features mirror the domain of the original UVL instance used as prompts. With this work, we envision a new thread of research where variability is generated and analyzed using LLMs. This opens the door for a new generation of techniques and tools for variability management.},
booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume A},
pages = {15–26},
numpages = {12},
keywords = {deep learning, large language models, synthetic models, universal variability language},
location = {Tokyo, Japan},
series = {SPLC '23}
}

@inproceedings{10.1145/3539618.3592067,
author = {Ferraretto, Fernando and Laitz, Thiago and Lotufo, Roberto and Nogueira, Rodrigo},
title = {ExaRanker: Synthetic Explanations Improve Neural Rankers},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3592067},
doi = {10.1145/3539618.3592067},
abstract = {Recent work has shown that incorporating explanations into the output generated by large language models (LLMs) can significantly enhance performance on a broad spectrum of reasoning tasks. Our study extends these findings by demonstrating the benefits of explanations for neural rankers. By utilizing LLMs such as GPT-3.5 to enrich retrieval datasets with explanations, we trained a sequence-to-sequence ranking model, dubbed ExaRanker, to generate relevance labels and explanations for query-document pairs. The ExaRanker model, finetuned on a limited number of examples and synthetic explanations, exhibits performance comparable to models finetuned on three times more examples, but without explanations. Moreover, incorporating explanations imposes no additional computational overhead into the reranking step and allows for on-demand explanation generation. The codebase and datasets used in this study will be available at https://github.com/unicamp-dl/ExaRanker},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2409–2414},
numpages = {6},
keywords = {explanations, few-shot models, generative models, large language models, multi-stage ranking, synthetic datasets},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3604237.3626869,
author = {Li, Yinheng and Wang, Shaofei and Ding, Han and Chen, Hang},
title = {Large Language Models in Finance: A Survey},
year = {2023},
isbn = {9798400702402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604237.3626869},
doi = {10.1145/3604237.3626869},
abstract = {Recent advances in large language models (LLMs) have opened new possibilities for artificial intelligence applications in finance. In this paper, we provide a practical survey focused on two key aspects of utilizing LLMs for financial tasks: existing solutions and guidance for adoption. First, we review current approaches employing LLMs in finance, including leveraging pretrained models via zero-shot or few-shot learning, fine-tuning on domain-specific data, and training custom LLMs from scratch. We summarize key models and evaluate their performance improvements on financial natural language processing tasks. Second, we propose a decision framework to guide financial professionals in selecting the appropriate LLM solution based on their use case constraints around data, compute, and performance needs. The framework provides a pathway from lightweight experimentation to heavy investment in customized LLMs. Lastly, we discuss limitations and challenges around leveraging LLMs in financial applications. Overall, this survey aims to synthesize the state-of-the-art and provide a roadmap for responsibly applying LLMs to advance financial AI.},
booktitle = {Proceedings of the Fourth ACM International Conference on AI in Finance},
pages = {374–382},
numpages = {9},
keywords = {Finance, Generative AI, Large Language Models, Natural Language Processing},
location = {Brooklyn, NY, USA},
series = {ICAIF '23}
}

@inproceedings{10.1145/3605764.3623985,
author = {Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
title = {Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection},
year = {2023},
isbn = {9798400702600},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605764.3623985},
doi = {10.1145/3605764.3623985},
abstract = {Large Language Models (LLMs) are increasingly being integrated into applications, with versatile functionalities that can be easily modulated via natural language prompts. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We show that LLM-Integrated Applications blur the line between data and instructions and reveal several new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (i.e., without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved at inference time. We derive a comprehensive taxonomy from a computer security perspective to broadly investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We then demonstrate the practical viability of our attacks against both real-world systems, such as Bing Chat and code-completion engines, and GPT-4 synthetic applications. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing reliance on LLMs, effective mitigations of these emerging threats are lacking. By raising awareness of these vulnerabilities, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users from potential attacks.},
booktitle = {Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security},
pages = {79–90},
numpages = {12},
keywords = {indirect prompt injection, large language models},
location = {Copenhagen, Denmark},
series = {AISec '23}
}

@article{10.14778/3611479.3611527,
author = {Fernandez, Raul Castro and Elmore, Aaron J. and Franklin, Michael J. and Krishnan, Sanjay and Tan, Chenhao},
title = {How Large Language Models Will Disrupt Data Management},
year = {2023},
issue_date = {July 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611479.3611527},
doi = {10.14778/3611479.3611527},
abstract = {Large language models (LLMs), such as GPT-4, are revolutionizing software's ability to understand, process, and synthesize language. The authors of this paper believe that this advance in technology is significant enough to prompt introspection in the data management community, similar to previous technological disruptions such as the advents of the world wide web, cloud computing, and statistical machine learning. We argue that the disruptive influence that LLMs will have on data management will come from two angles. (1) A number of hard database problems, namely, entity resolution, schema matching, data discovery, and query synthesis, hit a ceiling of automation because the system does not fully understand the semantics of the underlying data. Based on large training corpora of natural language, structured data, and code, LLMs have an unprecedented ability to ground database tuples, schemas, and queries in real-world concepts. We will provide examples of how LLMs may completely change our approaches to these problems. (2) LLMs blur the line between predictive models and information retrieval systems with their ability to answer questions. We will present examples showing how large databases and information retrieval systems have complementary functionality.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {3302–3309},
numpages = {8}
}

@inproceedings{10.1145/3587102.3588773,
author = {Denny, Paul and Becker, Brett A. and Leinonen, Juho and Prather, James},
title = {Chat Overflow: Artificially Intelligent Models for Computing Education - renAIssance or apocAIypse?},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588773},
doi = {10.1145/3587102.3588773},
abstract = {Recent breakthroughs in deep learning have led to the emergence of generative AI models that exhibit extraordinary performance at producing human-like outputs. Using only simple input prompts, it is possible to generate novel text, images, video, music, and source code, as well as tackle tasks such as answering questions and translating and summarising text.However, the potential for these models to impact computing education practice is only just beginning to be explored. For example, novices learning to code can now use free tools that automatically suggest solutions to programming exercises and assignments; yet these tools were not designed with novices in mind and little to nothing is known about how they will impact learning. Furthermore, much attention has focused on the immediate challenges these models present, such as academic integrity concerns. It seems that even in the AI-era a pending apocalypse sells better than a promising renaissance.Generative AI will likely play an increasing role in people's lives in the reasonably foreseeable future. Model performance seems set to continue accelerating while novel uses and new possibilities multiply. Given this, we should devote just as much effort to identifying and exploiting new opportunities as we do to identifying and mitigating challenges.In this talk, we begin by discussing several concrete and research-backed opportunities for computing educators. Many of these have already shown great promise in positively impacting current practice. We then discuss more short- to medium-term possibilities in areas such as student recruitment, and curricular changes. Finally - against our better judgement - we speculate over the longer-term, including rethinking the very fundamentals of the practice of teaching introductory and advanced computing courses. In these discussions we suggest potential research questions and directions. Although making remotely accurate predictions in such a fast-changing landscape is foolhardy, we believe that now is the time to explore and embrace opportunities to help make positive change in as many computing classrooms as possible.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {3–4},
numpages = {2},
keywords = {ai, artificial intelligence, chatgpt, computer programming, computer science education, computing education, copilot, deep learning, generative ai, large language models, llm, machine learning},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3540250.3549127,
author = {Robe, Peter and Kuttal, Sandeep K. and AuBuchon, Jake and Hart, Jacob},
title = {Pair programming conversations with agents vs. developers: challenges and opportunities for SE community},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549127},
doi = {10.1145/3540250.3549127},
abstract = {Recent research has shown feasibility of an interactive pair-programming conversational agent, but implementing such an agent poses three challenges: a lack of benchmark datasets, absence of software engineering specific labels, and the need to understand developer conversations. To address these challenges, we conducted a Wizard of Oz study with 14 participants pair programming with a simulated agent and collected 4,443 developer-agent utterances. Based on this dataset, we created 26 software engineering labels using an open coding process to develop a hierarchical classification scheme. To understand labeled developer-agent conversations, we compared the accuracy of three state-of-the-art transformer-based language models, BERT, GPT-2, and XLNet, which performed interchangeably. In order to begin creating a developer-agent dataset, researchers and practitioners need to conduct resource intensive Wizard of Oz studies. Presently, there exists vast amounts of developer-developer conversations on video hosting websites. To investigate the feasibility of using developer-developer conversations, we labeled a publicly available developer-developer dataset (3,436 utterances) with our hierarchical classification scheme and found that a BERT model trained on developer-developer data performed ~10% worse than the BERT trained on developer-agent data, but when using transfer-learning, accuracy improved. Finally, our qualitative analysis revealed that developer-developer conversations are more implicit, neutral, and opinionated than developer-agent conversations. Our results have implications for software engineering researchers and practitioners developing conversational agents.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {319–331},
numpages = {13},
keywords = {Classification, Conversational agents, Datasets, Labels, Language models, Pair programming conversations, Pair programming questions},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@article{10.5555/3636988.3636996,
author = {Carter, Karla},
title = {"I, ChatBot": Co-Teaching Cybersecurity Courses With Generative AI},
year = {2023},
issue_date = {October 2023},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {3},
issn = {1937-4771},
abstract = {This tutorial is for computing science faculty who are intrigued by the notion that generative AI, such as OpenAI's ChatGPT or Google's Bard, can enhance the way we teach and students learn cybersecurity. Rather than questioning if faculty and students should use generative AI in the classroom, you're asking how faculty and students can use generative AI appropriately and responsibly in the classroom. Our students deserve to understand the tools shaping their future; generative AI is not going away and we need to prepare our students for a future where not knowing how to write generative AI prompts isn't an option.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {27–28},
numpages = {2}
}

@inproceedings{10.1145/3611643.3613891,
author = {Jin, Pengxiang and Zhang, Shenglin and Ma, Minghua and Li, Haozhe and Kang, Yu and Li, Liqun and Liu, Yudong and Qiao, Bo and Zhang, Chaoyun and Zhao, Pu and He, Shilin and Sarro, Federica and Dang, Yingnong and Rajmohan, Saravan and Lin, Qingwei and Zhang, Dongmei},
title = {Assess and Summarize: Improve Outage Understanding with Large Language Models},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613891},
doi = {10.1145/3611643.3613891},
abstract = {Cloud systems have become increasingly popular in recent years due to their flexibility and scalability. Each time cloud computing applications and services hosted on the cloud are affected by a cloud outage, users can experience slow response times, connection issues or total service disruption, resulting in a significant negative business impact. Outages are usually comprised of several concurring events/source causes, and therefore understanding the context of outages is a very challenging yet crucial first step toward mitigating and resolving outages. In current practice, on-call engineers with in-depth domain knowledge, have to manually assess and summarize outages when they happen, which is time-consuming and labor-intensive. In this paper, we first present a large-scale empirical study investigating the way on-call engineers currently deal with cloud outages at Microsoft, and then present and empirically validate a novel approach (dubbed Oasis) to help the engineers in this task. Oasis is able to automatically assess the impact scope of outages as well as to produce human-readable summarization. Specifically, Oasis first assesses the impact scope of an outage by aggregating relevant incidents via multiple techniques. Then, it generates a human-readable summary by leveraging fine-tuned large language models like GPT-3.x. The impact assessment component of Oasis was introduced in Microsoft over three years ago, and it is now widely adopted, while the outage summarization component has been recently introduced, and in this article we present the results of an empirical evaluation we carried out on 18 real-world cloud systems as well as a human-based evaluation with outage owners. The results obtained show that Oasis can effectively and efficiently summarize outages, and lead Microsoft to deploy its first prototype which is currently under experimental adoption by some of the incident teams.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1657–1668},
numpages = {12},
keywords = {Cloud Systems, Large Language Model, Outage Understanding},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3584931.3606965,
author = {Zhu, Qingxiaoyang and Wang, Hao-Chuan},
title = {Leveraging Large Language Model as Support for Human Problem Solving: An Exploration of Its Appropriation and Impact},
year = {2023},
isbn = {9798400701290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584931.3606965},
doi = {10.1145/3584931.3606965},
abstract = {The emergence of pre-trained Large Language Model (LLM) has opened up new possibilities for people to access language resources at their fingertips. Previously, patterns of language could be difficult to derive from large-scale documents, which impeded people from processing and extracting information contained within. Observations from common users’ practices and experiences suggest that LLM may appear to possess certain capacities for processing, handling and working with not just human language, but also the associated knowledge. However, the original construction of LLM is essentially language-centric, which is not more than a probabilistic model representing and summarizing language patterns from large language corpora, without deliberately incorporating other types of data or information (e.g., user behaviors, domain concepts) into the model construction. Consequently, when using LLM in the real-world, it’s not uncommon to appropriate and re-purpose an LLM for handling tasks that don’t necessarily match what it’s built for. In this poster, we present an exploratory study aimed at understanding how people interact with an LLM, chatGPT, to obtain instructions to work on a problem-solving task, installing Python on a remote computer. The results reveal that users’ literacy and expectations concerning LLM can influence how they perceive and utilize it. Surprisingly, low-literacy participants with limited understanding of LLM appear to benefit more, producing implications for designing user-centric AI/ML tools.},
booktitle = {Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing},
pages = {333–337},
numpages = {5},
keywords = {LLM, appropriation, conversation, end-users, literacy, problem solving, prompt analysis, transparency},
location = {Minneapolis, MN, USA},
series = {CSCW '23 Companion}
}

@article{10.5555/3648699.3649076,
author = {Roberts, Adam and Chung, Hyung Won and Mishra, Gaurav and Levskaya, Anselm and Bradbury, James and Andor, Daniel and Narang, Sharan and Lester, Brian and Gaffney, Colin and Mohiuddin, Afroz and Hawthorne, Curtis and Lewkowycz, Aitor and Salcianu, Alex and Hu, Haitang and Tsvyashchenko, Sasha and Chowdhery, Aakanksha and Bastings, Jasmijn and Bulian, Jannis and Garcia, Xavier and Ni, Jianmo and Chen, Andrew and Kenealy, Kathleen and Han, Kehang and Casbon, Michelle and Clark, Jonathan H. and Lee, Stephan and Garrette, Dan and Lee-Thorp, James and Raffel, Colin and Shazeer, Noam and Ritter, Marvin and Bosma, Maarten and Passos, Alexandre and Maitin-Shepard, Jeremy and Fiedel, Noah and Omernick, Mark and Saeta, Brennan and Sepassi, Ryan and Spiridonov, Alexander and Newlan, Joshua and Gesmundo, Andrea and Van Zee, Marc and Austin, Jacob and Goodman, Sebastian and Soares, Livio Baldini},
title = {Scaling up models and data with t5x and seqio},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Scaling up training datasets and model parameters have benefited neural network-based language models, but also present challenges like distributed compute, input data bottlenecks and reproducibility of results. We introduce two simple and scalable software libraries that simplify these issues: t5x enables training large language models at scale, while seqio enables reproducible input and evaluation pipelines. These open-source libraries have been used to train models with hundreds of billions of parameters on multiterabyte datasets. Configurations and instructions for T5-like and GPT-like models are also provided. The libraries can be found at https://github.com/google-research/t5x and https://github.com/google/seqio.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {377},
numpages = {8},
keywords = {large language models, data parallelism, model parallelism, data processing}
}

@inproceedings{10.1145/3618305.3623587,
author = {Ribeiro, Francisco},
title = {Large Language Models for Automated Program Repair},
year = {2023},
isbn = {9798400703843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3618305.3623587},
doi = {10.1145/3618305.3623587},
abstract = {This paper introduces two methods for automated program repair (APR) utilizing pre-trained language models. The first method demonstrates program repair as a code completion task and is validated on a dataset of Java programs. The second method, Mentat, leverages OCaml’s parser and type system as fault localization techniques to generate prompts for GPT-3, producing candidate patches. Evaluation results show promising repair rates, with 27% and 39.2% effectiveness, respectively. For OCaml, a comparative study employing an automated validation strategy is presented in which the technique outperforms other tools. Language models are effective at APR, enhancing bug fixing and freeing developers to focus on other critical aspects of software engineering.},
booktitle = {Companion Proceedings of the 2023 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity},
pages = {7–9},
numpages = {3},
keywords = {automated program repair, code generation, fault localization, type systems},
location = {Cascais, Portugal},
series = {SPLASH 2023}
}

@inproceedings{10.1145/3627217.3627234,
author = {Pawagi, Mrigank and Kumar, Viraj},
title = {GuardRails: Automated Suggestions for Clarifying Ambiguous Purpose Statements},
year = {2023},
isbn = {9798400708404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627217.3627234},
doi = {10.1145/3627217.3627234},
abstract = {Before implementing a function, programmers are encouraged to write a purpose statement i.e., a short, natural-language explanation of what the function computes. A purpose statement may be ambiguous i.e., it may fail to specify the intended behaviour when two or more inequivalent computations are plausible on certain inputs. Our paper makes four contributions. First, we propose a novel heuristic that suggests such inputs using Large Language Models (LLMs). Using these suggestions, the programmer may choose to clarify the purpose statement (e.g., by providing a functional example that specifies the intended behaviour on such an input). Second, to assess the quality of inputs suggested by our heuristic, and to facilitate future research, we create an open dataset of purpose statements with known ambiguities. Third, we compare our heuristic against GitHub Copilot’s Chat feature, which can suggest similar inputs when prompted to generate unit tests. Fourth, we provide an open-source implementation of our heuristic as an extension to Visual Studio Code for the Python programming language, where purpose statements and functional examples are specified as docstrings and doctests respectively. We believe that this tool will be particularly helpful to novice programmers and instructors.},
booktitle = {Proceedings of the 16th Annual ACM India Compute Conference},
pages = {55–60},
numpages = {6},
keywords = {CS1, function design, purpose statement},
location = {Hyderabad, India},
series = {COMPUTE '23}
}

@inproceedings{10.1145/3626705.3626706,
author = {Lingler, Alexander and Talypova, Dinara and Draxler, Fiona and Schneegass, Christina and Dingler, Tilman and Wintersberger, Philipp},
title = {MuM'23 Workshop on Interruptions and Attention Management},
year = {2023},
isbn = {9798400709210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626705.3626706},
doi = {10.1145/3626705.3626706},
abstract = {Attention management systems seek to minimize disruption by intelligently timing interruptions and helping users navigate multiple tasks and activities. While there is a solid theoretical basis and rich history in HCI research for attention management, little progress has been made regarding their practical implementation and deployment. Building sophisticated attention management systems requires a great variety of sensors, task- and user models, and multiple devices while considering the complexity of user context and human behavior. Novel AI technologies, such as generative systems, reinforcement learning, and large language models, open new possibilities to create intelligent, practical, and user-centered attention management systems. This proposed workshop aims to bring together researchers and practitioners from diverse backgrounds to discuss and formulate a research agenda to advance attention management systems using novel AI tools to manage and mitigate interruptions from computing systems effectively.},
booktitle = {Proceedings of the 22nd International Conference on Mobile and Ubiquitous Multimedia},
pages = {548–551},
numpages = {4},
keywords = {Human-computer interaction, attention management, attentive user interfaces, cognitive load, interruptions, notifications, ubiquitous computing, workload},
location = {Vienna, Austria},
series = {MUM '23}
}

@inproceedings{10.1145/3624918.3625339,
author = {Hua, Wenyue and Xu, Shuyuan and Ge, Yingqiang and Zhang, Yongfeng},
title = {How to Index Item IDs for Recommendation Foundation Models},
year = {2023},
isbn = {9798400704086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624918.3625339},
doi = {10.1145/3624918.3625339},
abstract = {Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item as in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text and hallucinated recommendations when deciding which item(s) to recommend, creating LLM-compatible item IDs to uniquely identify each item is essential for recommendation foundation models. In this study, we systematically examine the item ID creation and indexing problem for recommendation foundation models, using P5 as an example of the backbone LLM. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as random indexing, title indexing, and independent indexing. We then propose four simple yet effective solutions, including sequential indexing, collaborative indexing, semantic (content-based) indexing, and hybrid indexing. Our study highlights the significant influence of item indexing methods on the performance of LLM-based recommendation, and our results on real-world datasets validate the effectiveness of our proposed solutions. The research also demonstrates how recent advances on language modeling and traditional IR principles such as indexing can help each other for better learning and inference. Source code and data are available at https://github.com/Wenyueh/LLM-RecSys-ID.},
booktitle = {Proceedings of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region},
pages = {195–204},
numpages = {10},
keywords = {Item ID and Indexing, Large Language Model, Recommendation},
location = {Beijing, China},
series = {SIGIR-AP '23}
}

@inproceedings{10.1145/3591106.3592278,
author = {Alonso del Barrio, David and Gatica-Perez, Daniel},
title = {Framing the News: From Human Perception to Large Language Model Inferences},
year = {2023},
isbn = {9798400701788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591106.3592278},
doi = {10.1145/3591106.3592278},
abstract = {Identifying the frames of news is important to understand the articles’ vision, intention, message to be conveyed, and which aspects of the news are emphasized. Framing is a widely studied concept in journalism, and has emerged as a new topic in computing, with the potential to automate processes and facilitate the work of journalism professionals. In this paper, we study this issue with articles related to the Covid-19 anti-vaccine movement. First, to understand the perspectives used to treat this theme, we developed a protocol for human labeling of frames for 1786 headlines of No-Vax movement articles of European newspapers from 5 countries. Headlines are key units in the written press, and worth of analysis as many people only read headlines (or use them to guide their decision for further reading.) Second, considering advances in Natural Language Processing (NLP) with large language models, we investigated two approaches for frame inference of news headlines: first with a GPT-3.5 fine-tuning approach, and second with GPT-3.5 prompt-engineering. Our work contributes to the study and analysis of the performance that these models have to facilitate journalistic tasks like classification of frames, while understanding whether the models are able to replicate human perception in the identification of these frames.},
booktitle = {Proceedings of the 2023 ACM International Conference on Multimedia Retrieval},
pages = {627–635},
numpages = {9},
keywords = {Covid-19 no-vax, GPT-3, large language models, news framing, prompt-engineering, transformers},
location = {Thessaloniki, Greece},
series = {ICMR '23}
}

@inproceedings{10.1145/3609510.3609815,
author = {Park, Daon and Jo, Sungbin and Egger, Bernhard},
title = {Improving Throughput-oriented Generative Inference with CPUs},
year = {2023},
isbn = {9798400703058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609510.3609815},
doi = {10.1145/3609510.3609815},
abstract = {Despite recent attempts to reduce the number of parameters of large language models (LLMs), their parameter data is still too large to fit into a single GPU. With the emergence of throughput-oriented tasks, high-throughput generative inference frameworks for LLMs on a single commodity GPU leverage GPU, DRAM, and NVMe to run inference on large models with terabytes of data. Our analysis of the technique shows that the runtime is dominated by data transfers of the weights, leading to a low utilization of both the GPU and the CPU. In this paper, we increase the throughput and decrease the total latency of state-of-the-art frameworks by including the CPU as a compute device and overlapping computations on the CPU with GPU data transfers. Our work shows a promising improvement of around 40% in throughput and total latency, with potential room for further improvements.},
booktitle = {Proceedings of the 14th ACM SIGOPS Asia-Pacific Workshop on Systems},
pages = {37–42},
numpages = {6},
keywords = {CPU offloading, Large language models, latency reduction},
location = {Seoul, Republic of Korea},
series = {APSys '23}
}

@inproceedings{10.5555/3523760.3523920,
author = {Pittman, Daniel E. and Haring, Kerstin S. and Kim, Pilyoung and Dossett, Benjamin and Ehman, Gillian and Gutierrez-Gutierrez, Elizabeth and Patil, Sneha and Sanchez, Ashley},
title = {A Novel Online Robot Design Research Platform to Determine Robot Mind Perception},
year = {2022},
publisher = {IEEE Press},
abstract = {A common issue in Human-Robot Interaction is a gap in understanding how robot designs are perceived by the user. A common issue encountered by practitioners of Machine Learning (ML) is a lack of salient data to use in training. The "Build-A-Bot" project is developing a novel research platform implemented as a web-accessible 3D game that affords data collection of many user-provided robot designs. The designs are used to train ML models to better evaluate robot designs, predict how a design will be perceived using Convolutional Neural Networks (CNNs), and create new robot designs using Generative Adversarial Networks (GANs). This paper outlines the current and future work accomplished by an interdisciplinary undergraduate student team at the University of Denver across Computer Science, Music, Psychology, and other related STEM fields that have created Build-A-Bot.},
booktitle = {Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {986–990},
numpages = {5},
keywords = {machine learning, fnirs, robot design},
location = {Sapporo, Hokkaido, Japan},
series = {HRI '22}
}

@article{10.5555/3636988.3637001,
author = {Tu, Junyi},
title = {How to Integrate ChatGPT into the CS1/CS2 Sequence},
year = {2023},
issue_date = {October 2023},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {3},
issn = {1937-4771},
abstract = {ChatGPT is one of most revolutionary technologies and fiercely debated on the benefit and disaster of its effect on human society. How to guide students to use this new technology is an inevitably topic as CS educators. This nifty idea includes ways to integrate ChatGPT into the CS1/CS2 sequence, how to guide students to use ChatGPT in a constructive way, instead of cheating on their homework.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {34},
numpages = {1}
}

@inproceedings{10.1145/3597926.3598067,
author = {Deng, Yinlin and Xia, Chunqiu Steven and Peng, Haoran and Yang, Chenyuan and Zhang, Lingming},
title = {Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598067},
doi = {10.1145/3597926.3598067},
abstract = {Deep Learning (DL) systems have received exponential growth in popularity and have become ubiquitous in our everyday life. Such systems are built on top of popular DL libraries, e.g., TensorFlow and PyTorch which provide APIs as building blocks for DL systems. Detecting bugs in these DL libraries is critical for almost all downstream DL systems in ensuring effectiveness/safety for end users. Meanwhile, traditional fuzzing techniques can be hardly effective for such a challenging domain since the input DL programs need to satisfy both the input language (e.g., Python) syntax/semantics and the DL API input/shape constraints for tensor computations.  
To address these limitations, we propose TitanFuzz – the first approach to directly leveraging Large Language Models (LLMs) to generate input programs for fuzzing DL libraries. LLMs are titanic models trained on billions of code snippets and can autoregressively generate human-like code snippets. Our key insight is that modern LLMs can also include numerous code snippets invoking DL library APIs in their training corpora, and thus can implicitly learn both language syntax/semantics and intricate DL API constraints for valid DL program generation. More specifically, we use both generative and infilling LLMs (e.g., Codex/InCoder) to generate and mutate valid/diverse input DL programs for fuzzing. Our experimental results demonstrate that TitanFuzz can achieve 30.38%/50.84% higher code coverage than state-of-the-art fuzzers on TensorFlow/PyTorch. Furthermore, TitanFuzz is able to detect 65 bugs, with 44 already confirmed as previously unknown bugs.  
This paper demonstrates that modern titanic LLMs can be leveraged to directly perform both generation-based and mutation-based fuzzing studied for decades, while being fully automated, generalizable, and applicable to domains challenging for traditional approaches (such as DL systems). We hope TitanFuzz can stimulate more work in this promising direction of LLMs for fuzzing.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {423–435},
numpages = {13},
keywords = {Fuzz Testing, Large Language Model, Test Generation},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3511861.3511863,
author = {Finnie-Ansley, James and Denny, Paul and Becker, Brett A. and Luxton-Reilly, Andrew and Prather, James},
title = {The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming},
year = {2022},
isbn = {9781450396431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511861.3511863},
doi = {10.1145/3511861.3511863},
abstract = {Recent advances in artificial intelligence have been driven by an exponential growth in digitised data. Natural language processing, in particular, has been transformed by machine learning models such as OpenAI’s GPT-3 which generates human-like text so realistic that its developers have warned of the dangers of its misuse. In recent months OpenAI released Codex, a new deep learning model trained on Python code from more than 50 million GitHub repositories. Provided with a natural language description of a programming problem as input, Codex generates solution code as output. It can also explain (in English) input code, translate code between programming languages, and more. In this work, we explore how Codex performs on typical introductory programming problems. We report its performance on real questions taken from introductory programming exams and compare it to results from students who took these same exams under normal conditions, demonstrating that Codex outscores most students. We then explore how Codex handles subtle variations in problem wording using several published variants of the well-known “Rainfall Problem” along with one unpublished variant we have used in our teaching. We find the model passes many test cases for all variants. We also explore how much variation there is in the Codex generated solutions, observing that an identical input prompt frequently leads to very different solutions in terms of algorithmic approach and code length. Finally, we discuss the implications that such technology will have for computing education as it continues to evolve, including both challenges and opportunities.},
booktitle = {Proceedings of the 24th Australasian Computing Education Conference},
pages = {10–19},
numpages = {10},
keywords = {AI, CS1, Codex, GPT-3, GitHub, OpenAI, academic integrity, artificial intelligence, code generation, code writing, copilot, deep learning, introductory programming, machine learning, neural networks, novice programming},
location = {Virtual Event, Australia},
series = {ACE '22}
}

@inproceedings{10.1145/3624062.3624128,
author = {Quan, Andres and Howell, Leah and Greenberg, Hugh},
title = {Heterogeneous Syslog Analysis: There Is Hope},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624128},
doi = {10.1145/3624062.3624128},
abstract = {Identifying system hardware failures and anomalies is a unique challenge in heterogeneous testbed clusters because of variation in the ways that the system log reports errors and warnings. We present a novel approach for the real-time classification of syslog messages generated by a heterogeneous testbed cluster to proactively identify potential hardware issues and security events. By integrating machine learning models with high-performance computing systems, our system facilitates continuous system health monitoring. The paper introduces a taxonomy for classifying system issues into actionable categories of problems, while filtering out groups of messages that the system administrators would consider unimportant "noise". Finally, we experiment with using large language models as a message classifier, and share our results and experience with doing so. Results demonstrate promising performance, and more explainable results compared to currently available techniques, but the computational costs may offset the benefits.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {581–587},
numpages = {7},
keywords = {Applications of Large-Language-Models, Cross-platform Software, Error detection, Failure detection, Heterogeneous Clusters, Log Analysis, Monitoring, Syslog, Testbeds},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@inproceedings{10.1145/3545945.3569823,
author = {Denny, Paul and Kumar, Viraj and Giacaman, Nasser},
title = {Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569823},
doi = {10.1145/3545945.3569823},
abstract = {GitHub Copilot is an artificial intelligence tool for automatically generating source code from natural language problem descriptions. Since June 2022, Copilot has officially been available for free to all students as a plug-in to development environments like Visual Studio Code. Prior work exploring OpenAI Codex, the underlying model that powers Copilot, has shown it performs well on typical CS1 problems thus raising concerns about its potential impact on how introductory programming courses are taught. However, little is known about the types of problems for which Copilot does not perform well, or about the natural language interactions that a student might have with Copilot when resolving errors. We explore these questions by evaluating the performance of Copilot on a publicly available dataset of 166 programming problems. We find that it successfully solves around half of these problems on its very first attempt, and that it solves 60% of the remaining problems using only natural language changes to the problem description. We argue that this type of prompt engineering, which we believe will become a standard interaction between human and Copilot when it initially fails, is a potentially useful learning activity that promotes computational thinking skills, and is likely to change the nature of code writing skill development.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1136–1142},
numpages = {7},
keywords = {artificial intelligence, cs1, foundation models, github copilot, introductory programming, large language models, openai},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3582269.3615596,
author = {Cai, Alice and Rick, Steven R and Heyman, Jennifer L and Zhang, Yanxia and Filipowicz, Alexandre and Hong, Matthew and Klenk, Matt and Malone, Thomas},
title = {DesignAID: Using Generative AI and Semantic Diversity for Design Inspiration},
year = {2023},
isbn = {9798400701139},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582269.3615596},
doi = {10.1145/3582269.3615596},
abstract = {Designers often struggle to sufficiently explore large design spaces, which can lead to design fixation and suboptimal outcomes. Here we introduce DesignAID, a generative AI tool that supports broader design space exploration by first using large language models to produce a range of diverse ideas expressed in words, and then using image generation software to create images from these words. This innovative combination of AI-based capabilities allows human-computer pairs to rapidly create a diverse set of visual concepts without time-consuming drawing. In a study with 87 crowd-sourced designers, we found that designers rated the automatic generation of images from words as significantly more inspirational, enjoyable, and useful than a conventional baseline condition of image search using Pinterest. Surprisingly, however, we found that automatically generating highly diverse ideas had less value. For image generation, the high diversity condition was somewhat better in inspiration but no better in the other dimensions, and for image search it was significantly worse in all dimensions.},
booktitle = {Proceedings of The ACM Collective Intelligence Conference},
pages = {1–11},
numpages = {11},
keywords = {AI assistance, creativity support, generative AI, human AI collaboration, human-computer collaboration, machine learning},
location = {Delft, Netherlands},
series = {CI '23}
}

@inproceedings{10.1145/3624062.3624257,
author = {Zhang, Chengming and Sun, Baixi and Yu, Xiaodong and Xie, Zhen and Zheng, Weijian and Iskra, Kamil A. and Beckman, Pete and Tao, Dingwen},
title = {Benchmarking and In-depth Performance Study of Large Language Models on Habana Gaudi Processors},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624257},
doi = {10.1145/3624062.3624257},
abstract = {Transformer models have achieved remarkable success in various machine learning tasks but suffer from high computational complexity and resource requirements. The quadratic complexity of the self-attention mechanism further exacerbates these challenges when dealing with long sequences and large datasets. Specialized AI hardware accelerators, such as the Habana GAUDI architecture, offer a promising solution to tackle these issues. GAUDI features a Matrix Multiplication Engine (MME) and a cluster of fully programmable Tensor Processing Cores (TPC). This paper explores the untapped potential of using GAUDI processors to accelerate Transformer-based models, addressing key challenges in the process. Firstly, we provide a comprehensive performance comparison between the MME and TPC components, illuminating their relative strengths and weaknesses. Secondly, we explore strategies to optimize MME and TPC utilization, offering practical insights to enhance computational efficiency. Thirdly, we evaluate the performance of Transformers on GAUDI, particularly in handling long sequences and uncovering performance bottlenecks. Lastly, we evaluate the end-to-end performance of two Transformer-based large language models (LLM) on GAUDI. The contributions of this work encompass practical insights for practitioners and researchers alike. We delve into GAUDI’s capabilities for Transformers through systematic profiling, analysis, and optimization exploration. Our study bridges a research gap and offers a roadmap for optimizing Transformer-based model training on the GAUDI architecture.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {1759–1766},
numpages = {8},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@inproceedings{10.1145/3586183.3606719,
author = {Angert, Tyler and Suzara, Miroslav and Han, Jenny and Pondoc, Christopher and Subramonyam, Hariharan},
title = {Spellburst: A Node-based Interface for Exploratory Creative Coding with Natural Language Prompts},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606719},
doi = {10.1145/3586183.3606719},
abstract = {Creative coding tasks are often exploratory in nature. When producing digital artwork, artists usually begin with a high-level semantic construct such as a “stained glass filter” and programmatically implement it by varying code parameters such as shape, color, lines, and opacity to produce visually appealing results. Based on interviews with artists, it can be effortful to translate semantic constructs to program syntax, and current programming tools don’t lend well to rapid creative exploration. To address these challenges, we introduce Spellburst, a large language model (LLM) powered creative-coding environment. Spellburst provides (1) a node-based interface that allows artists to create generative art and explore variations through branching and merging operations, (2) expressive prompt-based interactions to engage in semantic programming, and (3) dynamic prompt-driven interfaces and direct code editing to seamlessly switch between semantic and syntactic exploration. Our evaluation with artists demonstrates Spellburst’s potential to enhance creative coding practices and inform the design of computational creativity tools that bridge semantic and syntactic spaces.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {100},
numpages = {22},
keywords = {creative coding, exploratory programming, generative art, large language models, prompt engineering},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3539618.3591740,
author = {Zamani, Hamed and Bendersky, Michael},
title = {Multivariate Representation Learning for Information Retrieval},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591740},
doi = {10.1145/3539618.3591740},
abstract = {Dense retrieval models use bi-encoder network architectures for learning query and document representations. These representations are often in the form of a vector representation and their similarities are often computed using the dot product function. In this paper, we propose a new representation learning framework for dense retrieval. Instead of learning a vector for each query and document, our framework learns a multivariate distribution and uses negative multivariate KL divergence to compute the similarity between distributions. For simplicity and efficiency reasons, we assume that the distributions are multivariate normals and then train large language models to produce mean and variance vectors for these distributions. We provide a theoretical foundation for the proposed framework and show that it can be seamlessly integrated into the existing approximate nearest neighbor algorithms to perform retrieval efficiently. We conduct an extensive suite of experiments on a wide range of datasets, and demonstrate significant improvements compared to competitive dense retrieval models.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {163–173},
numpages = {11},
keywords = {approximate nearest neighbor search, dense retrieval, learning to rank, neural information retrieval},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3605468.3609775,
author = {Philbin, Carrie Anne},
title = {Impact of Generative AI on K-12 Students’ Perceptions of Computing: A Research Proposal},
year = {2023},
isbn = {9798400708510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605468.3609775},
doi = {10.1145/3605468.3609775},
abstract = {The rapid progress of generative artificial intelligence (AI) is fundamentally reshaping traditional perspectives on knowledge and skills, with profound implications for computing education. This necessitates a thorough examination of the relevance and timeliness of computing as a subject, especially for K-12 students who are making critical decisions about their future qualifications. This abstract proposes an empirical research study that aims to explore the effects of integrating generative AI in the creation of digital artefacts on K-12 students’ perceptions of the value of computing, as well as their understanding of ownership and achievement. Constructive discussions regarding the outlined approach are encouraged.},
booktitle = {Proceedings of the 18th WiPSCE Conference on Primary and Secondary Computing Education Research},
articleno = {28},
numpages = {2},
keywords = {Artificial Intelligence education, Creative computing, Generative AI, K-12 education, Student perceptions},
location = {Cambridge, United Kingdom},
series = {WiPSCE '23}
}

@inproceedings{10.1145/3604237.3626898,
author = {Nagy, Peer and Frey, Sascha and Sapora, Silvia and Li, Kang and Calinescu, Anisoara and Zohren, Stefan and Foerster, Jakob},
title = {Generative AI for End-to-End Limit Order Book Modelling: A Token-Level Autoregressive Generative Model of Message Flow Using a Deep State Space Network},
year = {2023},
isbn = {9798400702402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604237.3626898},
doi = {10.1145/3604237.3626898},
abstract = {Developing a generative model of realistic order flow in financial markets is a challenging open problem, with numerous applications for market participants. Addressing this, we propose the first end-to-end autoregressive generative model that generates tokenized limit order book (LOB) messages. These messages are interpreted by the JAX-LOB simulator, which updates the LOB state. To handle long sequences efficiently, the model employs simplified structured state-space layers to process sequences of order book states and tokenized messages. Using LOBSTER data of NASDAQ equity LOBs, we develop a custom tokenizer for message data, converting groups of successive digits to tokens, similar to tokenization in large language models. Out-of-sample results show promising performance in approximating the data distribution, as evidenced by low model perplexity. Furthermore, the mid-price returns calculated from the generated order flow exhibit a significant correlation with the data, indicating impressive conditional forecast performance. Due to the granularity of generated data, and the accuracy of the model, it offers new application areas for future work beyond forecasting, e.g. acting as a world model in high-frequency financial reinforcement learning applications. Overall, our results invite the use and extension of the model in the direction of autoregressive large financial models for the generation of high-frequency financial data. 1},
booktitle = {Proceedings of the Fourth ACM International Conference on AI in Finance},
pages = {91–99},
numpages = {9},
keywords = {ML, generative AI, limit order books, structured state space models},
location = {Brooklyn, NY, USA},
series = {ICAIF '23}
}

@inproceedings{10.1145/3604237.3626861,
author = {Chung, Andy and Tanaka-Ishii, Kumiko},
title = {Predictability of Post-Earnings Announcement Drift with Textual and Contextual Factors of Earnings Calls},
year = {2023},
isbn = {9798400702402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604237.3626861},
doi = {10.1145/3604237.3626861},
abstract = {Post-Earnings Announcement Drift (PEAD), a well-known anomaly in financial markets, describes the tendency of cumulative stock returns to drift in the direction of an earnings surprise for a prolonged period following an earnings announcement. Numerous studies have used a supervised learning approach to predict PEAD, using earnings, fundamental and technical factors. However, there is a lack of study on how the context of the earnings call can be used for the PEAD prediction task. This paper uses computational linguistics techniques and large language models to examine the effectiveness of incorporating textual and contextual features from earnings calls for the PEAD prediction task. Our proposed supervised model includes four categories of features: 1) textual features, 2) contextual features, 3) earnings features, and 4) fundamental and technical features. We study the proposed model using earnings from 2010/01/01 to 2022/12/31 of all point-in-time S&amp;P500 constituents in the US stock market. Our results show that contextual features provide information unexplained by earnings, fundamental and technical features, improving the average returns per trade of a hypothetical long-short portfolio against baseline solution in out-of-sample across all four different abnormal return calculations, ranging from 53 to 354 basis points and 16.9% to 108.5% improvement from baseline model, which uses only earnings, fundamental and technical features.},
booktitle = {Proceedings of the Fourth ACM International Conference on AI in Finance},
pages = {401–408},
numpages = {8},
keywords = {Post-earnings announcement drift, computational linguistics, earnings call, large language models, machine learning},
location = {Brooklyn, NY, USA},
series = {ICAIF '23}
}

@inproceedings{10.1145/3539618.3591913,
author = {Alessio, Marco and Faggioli, Guglielmo and Ferro, Nicola},
title = {DECAF: A Modular and Extensible Conversational Search Framework},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591913},
doi = {10.1145/3539618.3591913},
abstract = {The Conversational Search (CS) paradigm allows for an intuitive interaction between the user and the system through natural language sentences and it is increasingly being adopted in various scenarios. However, its widespread experimentation has led to the birth of a multitude of conversational search systems with custom implementations and variants of information retrieval models. This exacerbates the reproducibility crisis already observed in several research areas, including Information Retrieval (IR). To address this issue, we propose DECAF: a modular and extensible conversational search framework designed for fast prototyping and development of conversational agents. Our framework integrates all the components that characterize a modern conversational search system and allows for the seamless integration of Machine Learning (ML) and Large Language Models (LLMs)-based techniques. Furthermore, thanks to its uniform interface, DECAF allows for experiments characterized by a high degree of reproducibility. DECAF contains several state-of-the-art components including query rewriting, search functions under BoW and dense paradigms, and re-ranking functions. Our framework is tested on two well-known conversational collections: TREC CAsT 2019 and TREC CAsT 2020 and the results can be used by future practitioners as baselines. Our contributions include the identification of a series of state-of-the-art components for the conversational search task and the definition of a modular framework for its implementation.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3075–3085},
numpages = {11},
keywords = {conversational search, decaf, information retrieval, modular framework},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3607827.3616846,
author = {Li, Boyang},
title = {Unlocking Multimedia Capabilities of Gigantic Pretrained Language Models},
year = {2023},
isbn = {9798400702839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607827.3616846},
doi = {10.1145/3607827.3616846},
abstract = {Benefitting from unprecedented computational power, massive data throughput, and superhuman memory, large language models (LLMs) are fundamentally transforming multimodal machine learning. An LLM can be analogized to an enormous treasure box guarded by a lock. It contains extensive knowledge, but it can be non-trivial to access and apply appropriate knowledge to solve the problem at hand. Researchers have developed many techniques to unlock the capabilities of LLMs. Some well-known examples include chain-of-thought prompting, "let's think step by step'', and instruction tuning. In this talk, I will discuss techniques to unlock the capability of LLMs to process both visual and linguistic information. VisualGPT is one of the earliest works that finetunes an LLM for a vision-language task. InstructBLIP is an instruction-tuned large vision-language model, which set new states of the art on several vision-language tasks and snatched top positions on several comprehensive evaluation suites. In addition, I will talk about how to unlock zero-shot capabilities without end-to-end finetuning, or any form of finetuning at all. In Plug-and-Play VQA and Img2LLM, we achieve excellent results on visual question-answering datasets by connecting existing pretrained models using natural language and model interpretations, demonstrating a feasible alternative to the mainstream finetuning approach. Finally, I will describe a new multimodal dataset, Synopses of Movie Narratives, or SyMoN, for story understanding, which constitutes a new challenge for large vision-language models. I will argue that story understanding is an important objective in the pursuit of artificial general intelligence (AGI) because stories are a preeminent form of human communication and story understanding requires many AGI capabilities such as cause-effect reasoning and theory of mind. Compared to other multimodal story datasets, the special advantages of SyMoN include (1) event descriptions at the right level of granularity, (2) abundant mental state descriptions, (3) the use of diverse storytelling techniques, and (4) the provision of easy-to-use automatic performance evaluation.},
booktitle = {Proceedings of the 1st Workshop on Large Generative Models Meet Multimodal Applications},
pages = {3–4},
numpages = {2},
keywords = {large language models, multimodal learning, multimodal story understanding, visual question-answering},
location = {Ottawa ON, Canada},
series = {LGM3A '23}
}

@article{10.1145/3597204,
author = {Liu, Xuanzhe and Gu, Diandian and Chen, Zhenpeng and Wen, Jinfeng and Zhang, Zili and Ma, Yun and Wang, Haoyu and Jin, Xin},
title = {Rise of Distributed Deep Learning Training in the Big Model Era: From a Software Engineering Perspective},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {6},
issn = {1049-331X},
url = {https://doi.org/10.1145/3597204},
doi = {10.1145/3597204},
abstract = {Deep learning (DL) has become a key component of modern software. In the “big model” era, the rich features of DL-based software (i.e., DL software) substantially rely on powerful DL models, e.g., BERT, GPT-3, and the recently emerging GPT-4, which are trained on the powerful cloud with large datasets. Hence, training effective DL models has become a vital stage in the whole software lifecycle. When training deep learning models, especially those big models, developers need to parallelize and distribute the computation and memory resources amongst multiple devices (e.g., a cluster of GPUs) in the training process, which is known as distributed deep learning training, or distributed training for short. However, the unique challenges that developers encounter in distributed training process have not been studied in the software engineering community. Given the increasingly heavy dependence of current DL-based software on distributed training, this paper aims to fill in the knowledge gap and presents the first comprehensive study on developers’ issues in distributed training. To this end, we focus on popular DL frameworks that support distributed training (including TensorFlow, PyTorch, Keras, and Horovod) and analyze 1,131 real-world developers’ issues about using these frameworks reported on Stack Overflow and GitHub. We construct a fine-grained taxonomy consisting of 30 categories regarding the fault symptoms and summarize common fix patterns for different symptoms. We find that: (1) many distributed-specific faults and non-distributed-specific faults inherently share the same fault symptoms, making it challenging to debug; (2) most of the fault symptoms have frequent fix patterns; (3) about half of the faults are related to system-level configurations. Based on the results, we suggest actionable implications on research avenues that can potentially facilitate the distributed training to develop DL-based software, such as focusing on the frequent and common fix patterns when designing testing or debugging tools, developing efficient testing and debugging techniques for communication configuration along with the synthesis of network configuration analysis, designing new multi-device checkpoint-and-replay techniques to help reproduction, and designing serverless APIs for cloud platforms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {156},
numpages = {26},
keywords = {Empirical study, distributed training, software engineering}
}

@inproceedings{10.1145/3620678.3624793,
author = {Zhao, Dan and Samsi, Siddharth and McDonald, Joseph and Li, Baolin and Bestor, David and Jones, Michael and Tiwari, Devesh and Gadepally, Vijay},
title = {Sustainable Supercomputing for AI: GPU Power Capping at HPC Scale},
year = {2023},
isbn = {9798400703874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620678.3624793},
doi = {10.1145/3620678.3624793},
abstract = {As research and deployment of AI grows, the computational burden to support and sustain its progress inevitably does too. To train or fine-tune state-of-the-art models in NLP, computer vision, etc., some form of AI hardware acceleration is virtually a requirement. Recent large language models require considerable resources to train and deploy, resulting in significant energy usage, potential carbon emissions, and massive demand for GPUs and other hardware accelerators. However, this surge carries large implications for energy sustainability at the HPC/datacenter level. In this paper, we study the effects of power-capping GPUs at a research supercomputing center on GPU temperature and power draw; we show significant decreases in both temperature and power draw, reducing power consumption and potentially improving hardware life-span, with minimal impact on job performance. To our knowledge, our work is the first to conduct and make available a detailed analysis of the effects of GPU power-capping at the supercomputing scale. We hope our work will inspire HPCs/datacenters to further explore, evaluate, and communicate the impact of power-capping AI hardware accelerators for more sustainable AI.},
booktitle = {Proceedings of the 2023 ACM Symposium on Cloud Computing},
pages = {588–596},
numpages = {9},
keywords = {artificial intelligence, deep learning, distributed systems, high-performance computing, sustainable computing},
location = {Santa Cruz, CA, USA},
series = {SoCC '23}
}

@article{10.1145/3589324,
author = {Peng, Zhencan and Wang, Zhizhi and Deng, Dong},
title = {Near-Duplicate Sequence Search at Scale for Large Language Model Memorization Evaluation},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3589324},
doi = {10.1145/3589324},
abstract = {Recent studies show that large language models (LLM) unintendedly memorize part of the training data, which brings serious privacy risks. For example, it has been shown that over 1% of tokens generated unprompted by an LLM are part of sequences in the training data. However, current studies mainly focus on the exact memorization behaviors. In this paper, we propose to evaluate how many generated texts have near-duplicates (e.g., only differ by a couple of tokens out of 100) in the training corpus. A major challenge of conducting this evaluation is the huge computation cost incurred by near-duplicate sequence searches. This is because modern LLMs are trained on larger and larger corpora with up to 1 trillion tokens. What's worse is that the number of sequences in a text is quadratic to the text length. To address this issue, we develop an efficient and scalable near-duplicate sequence search algorithm in this paper. It can find (almost) all the near-duplicate sequences of the query sequence in a large corpus with guarantees. Specifically, the algorithm generates and groups the min-hash values of all the sequences with at least t tokens (as very short near-duplicates are often irrelevant noise) in the corpus in linear time to the corpus size. We formally prove that only 2 n+1/t+1 -1 min-hash values are generated for a text with n tokens in expectation. Thus the index time and size are reasonable. When a query arrives, we find all the sequences sharing enough min-hash values with the query using inverted indexes and prefix filtering. Extensive experiments on a few large real-world LLM training corpora show that our near-duplicate sequence search algorithm is efficient and scalable.},
journal = {Proc. ACM Manag. Data},
month = jun,
articleno = {179},
numpages = {18},
keywords = {language model memorization, large language model, near-duplicate detection, text alignment}
}

@inproceedings{10.1109/ICSE48619.2023.00181,
author = {Mastropaolo, Antonio and Pascarella, Luca and Guglielmi, Emanuela and Ciniselli, Matteo and Scalabrino, Simone and Oliveto, Rocco and Bavota, Gabriele},
title = {On the Robustness of Code Generation Techniques: An Empirical Study on GitHub Copilot},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00181},
doi = {10.1109/ICSE48619.2023.00181},
abstract = {Software engineering research has always being concerned with the improvement of code completion approaches, which suggest the next tokens a developer will likely type while coding. The release of GitHub Copilot constitutes a big step forward, also because of its unprecedented ability to automatically generate even entire functions from their natural language description. While the usefulness of Copilot is evident, it is still unclear to what extent it is robust. Specifically, we do not know the extent to which semantic-preserving changes in the natural language description provided to the model have an effect on the generated code function. In this paper we present an empirical study in which we aim at understanding whether different but semantically equivalent natural language descriptions result in the same recommended function. A negative answer would pose questions on the robustness of deep learning (DL)-based code generators since it would imply that developers using different wordings to describe the same code would obtain different recommendations. We asked Copilot to automatically generate 892 Java methods starting from their original Javadoc description. Then, we generated different semantically equivalent descriptions for each method both manually and automatically, and we analyzed the extent to which predictions generated by Copilot changed. Our results show that modifying the description results in different code recommendations in ~46% of cases. Also, differences in the semantically equivalent descriptions might impact the correctness of the generated code (±28%).},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2149–2160},
numpages = {12},
keywords = {empirical study, recommender systems},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3485447.3512225,
author = {Sun, Zhensu and Du, Xiaoning and Song, Fu and Ni, Mingze and Li, Li},
title = {CoProtector: Protect Open-Source Code against Unauthorized Training Usage with Data Poisoning},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512225},
doi = {10.1145/3485447.3512225},
abstract = {Github Copilot, trained on billions of lines of public code, has recently become the buzzword in the computer science research and practice community. Although it is designed to help developers implement safe and effective code with powerful intelligence, practitioners and researchers raise concerns about its ethical and security problems, e.g., should the copyleft licensed code be freely leveraged or insecure code be considered for training in the first place? These problems pose a significant impact on Copilot and other similar products that aim to learn knowledge from large-scale open-source code through deep learning models, which are inevitably on the rise with the fast development of artificial intelligence. To mitigate such impacts, we argue that there is a need to invent effective mechanisms for protecting open-source code from being exploited by deep learning models. Here, we design and implement a prototype, CoProtector, which utilizes data poisoning techniques to arm source code repositories for defending against such exploits. Our large-scale experiments empirically show that CoProtector is effective in achieving its purpose, significantly reducing the performance of Copilot-like deep learning models while being able to stably reveal the secretly embedded watermark backdoors.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {652–660},
numpages = {9},
keywords = {data poisoning, dataset protection, deep learning, open-source code},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3624062.3624238,
author = {Dhakal, Aditya and Raith, Philipp and Ward, Logan and Hong Enriquez, Rolando P. and Rattihalli, Gourav and Chard, Kyle and Foster, Ian and Milojicic, Dejan},
title = {Fine-grained accelerator partitioning for Machine Learning and Scientific Computing in Function as a Service Platform},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624238},
doi = {10.1145/3624062.3624238},
abstract = {Function-as-a-service (FaaS) is a promising execution environment for high-performance computing (HPC) and machine learning (ML) applications as it offers developers a simple way to write and deploy programs. Nowadays, GPUs and other accelerators are indispensable for HPC and ML workloads. These accelerators are expensive to acquire and operate; consequently, multiplexing them can increase their financial profitability. However, we have observed that state-of-the-art FaaS frameworks usually treat accelerator as a single device to run single workload and have little support for multiplexing accelerators. In this work, we have presented techniques to multiplex GPUs with Parsl, a popular FaaS framework. We demonstrate why GPU multiplexing is beneficial for certain applications and how we have implemented GPU multiplexing in Parsl. With our enhancements, we show up to 60% lower task completion time and 250% improvement in the inference throughput of a large language model when multiplexing a GPU compared to running a single instance without multiplexing. We plan to extend the support for GPU multiplexing in FaaS platforms by tackling the challenges of changing compute resources in the partition and approximating how to right-size a GPU partition for a function.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {1606–1613},
numpages = {8},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@inproceedings{10.1145/3583780.3615996,
author = {Bhattacharya, Indranil and Ye, Ze and Pavani, Kaushik and Dasgupta, Sunny},
title = {RT2S: A Framework for Learning with Noisy Labels},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615996},
doi = {10.1145/3583780.3615996},
abstract = {We introduce Robust Training with Trust Scores (RT2S), a framework to train machine learning classifiers with potentially noisy labels. RT2S calculates a trust score for each training sample, which indicates the quality of its corresponding label. These trust scores are employed as sample weights during training and optionally during threshold optimization. The trust scores are generated from two sources: (i) the model's confidence in the observed label, leveraging out-of-fold prediction scores to detect anomalous labels in the training data, and (ii) the probability of the correct label, ascertained by a Large Language Model with the ability to identify biased label noise. We evaluate RT2S by training machine learning models on 6 product classification datasets that utilize low-quality labels generated by a rule-based classification engine acting as a surrogate labeler. Our experimental findings indicate that RT2S outperforms all baselines, and achieves an average accuracy improvement of 4.38% (max 7.18%) over rule-based classifiers in particular.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {5234–5235},
numpages = {2},
keywords = {confident learning, deep learning models, importance re-weighting, large language model, machine learning models, robust training, sample re-weighted loss, trust scores},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3600211.3604695,
author = {Edenberg, Elizabeth and Wood, Alexandra},
title = {Disambiguating Algorithmic Bias: From Neutrality to Justice},
year = {2023},
isbn = {9798400702310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600211.3604695},
doi = {10.1145/3600211.3604695},
abstract = {As algorithms have become ubiquitous in consequential domains, societal concerns about the potential for discriminatory outcomes have prompted urgent calls to address algorithmic bias. In response, a rich literature across computer science, law, and ethics is rapidly proliferating to advance approaches to designing fair algorithms. Yet computer scientists, legal scholars, and ethicists are often not speaking the same language when using the term ‘bias.’ Debates concerning whether society can or should tackle the problem of algorithmic bias are hampered by conflations of various understandings of bias, ranging from neutral deviations from a standard to morally problematic instances of injustice due to prejudice, discrimination, and disparate treatment. This terminological confusion impedes efforts to address clear cases of discrimination. In this paper, we examine the promises and challenges of different approaches to disambiguating bias and designing for justice. While both approaches aid in understanding and addressing clear algorithmic harms, we argue that they also risk being leveraged in ways that ultimately deflect accountability from those building and deploying these systems. Applying this analysis to recent examples of generative AI, our argument highlights unseen dangers in current methods of evaluating algorithmic bias and points to ways to redirect approaches to addressing bias in generative AI at its early stages in ways that can more robustly meet the demands of justice.},
booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {691–704},
numpages = {14},
keywords = {algorithms, bias, discrimination, fairness, generative AI, justice, large language models, law, philosophy, vision-language models},
location = {Montr\'{e}al, QC, Canada},
series = {AIES '23}
}

@inproceedings{10.1145/3536169.3537785,
author = {Sosa, Ricardo and Gibbons, Andrew and O'Riordan, Emma and Iorangi, Keu and Crowe, Andy and Gibson, Leanne and Harris, Sam and Badenhorst, Daniel},
title = {Food for Advanced Computational Thinking: Critical and Creative Approaches to Technology at Te Kura Taurua Manurewa},
year = {2022},
isbn = {9781450393881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3536169.3537785},
doi = {10.1145/3536169.3537785},
abstract = {This paper focuses on a participatory activity that is part of an ongoing partnership formed six years ago between teachers and academics to study creative technology approaches to youth participation. By focusing on a food-based activity in an after-school maker space, we reflect on the pedagogical and methodological innovations, and the ethical and aesthetic qualities of food-based activities for participatory design. The session brought together students and teachers to form a generative dialogue around computation and automation while preparing and sharing food. The results suggest opportunities to rethink current curricular, pedagogical, and education policy strategies. Recommendations for organizers to prepare generative activities where food is used as a design material close the paper.},
booktitle = {Proceedings of the Participatory Design Conference 2022 - Volume 1},
pages = {109–119},
numpages = {11},
location = {Newcastle upon Tyne, United Kingdom},
series = {PDC '22}
}

@inproceedings{10.1145/3596671.3598574,
author = {Goel, Toshali and Shaer, Orit and Delcourt, Catherine and Gu, Quan and Cooper, Angel},
title = {Preparing Future Designers for Human-AI Collaboration in Persona Creation},
year = {2023},
isbn = {9798400708077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3596671.3598574},
doi = {10.1145/3596671.3598574},
abstract = {This paper presents findings from an exploratory study investigating the use of AI text-generation tools to support novice designers in persona creation. We conducted a workshop with 22 undergraduate students enrolled in an introductory human-computer interaction course, who were instructed to use GPT-3 in the creation of personas. These novice designers were able to use GPT-3 to iterate to produce satisfactory personas, particularly when providing detailed prompts. Our findings suggest that personas created with GPT-3 assistance were mostly comparable to those created manually but rated lower on some evaluation dimensions. The study also reveals merits and concerns of using GPT-3 for persona creation. Based on our findings, we propose recommendations for novice designers on how to use text-generative AIs to create personas effectively and responsibly.},
booktitle = {Proceedings of the 2nd Annual Meeting of the Symposium on Human-Computer Interaction for Work},
articleno = {4},
numpages = {14},
keywords = {education, human-AI collaboration, large language models, natural-language generation, novice designers, personas},
location = {Oldenburg, Germany},
series = {CHIWORK '23}
}

@inproceedings{10.1145/3581754.3584165,
author = {Gadiraju, Ujwal and Abbas, Tahir and Allen, Garrett},
title = {DECI: A Tutorial on Designing Effective Conversational Interfaces},
year = {2023},
isbn = {9798400701078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581754.3584165},
doi = {10.1145/3581754.3584165},
abstract = {Conversational User Interfaces (CUIs) have been argued to have advantages over traditional GUIs due to having a more human-like interaction. The growing popularity of conversational agents has enabled humans to interact with machines more naturally. There is an increasing familiarity among people with conversational interactions mediated by technology due to the widespread use of mobile devices and messaging services and a hungry market for conversational agents. Based on the recent advances in conversational AI, as a result of the proliferation of large language models, the signs are that the future of human-computer interaction will have a significant conversational component. Today, over two-thirds of the population on our planet has access to the Internet, with ever-lowering barriers to accessibility. This tutorial will showcase the benefits of employing novel conversational interfaces for crowd computing, human-AI decision making, health and well-being, and information retrieval. Given the widespread adoption of AI systems across several domains, we will discuss the potential of conversational interfaces in facilitating and mediating people’s interactions with AI systems. The tutorial will include interactive elements and discussions and provide participants with insights to inform the design of effective conversational interfaces.},
booktitle = {Companion Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {187–189},
numpages = {3},
keywords = {conversational AI, conversational crowdsourcing, conversational user interfaces, human-AI decision making, human-AI interaction},
location = {Sydney, NSW, Australia},
series = {IUI '23 Companion}
}

@inproceedings{10.1145/3600211.3604754,
author = {Narayanan Venkit, Pranav},
title = {Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models using an Interdisciplinary Lens},
year = {2023},
isbn = {9798400702310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600211.3604754},
doi = {10.1145/3600211.3604754},
abstract = {The rapid growth in the usage and applications of Natural Language Processing (NLP) in various sociotechnical solutions has highlighted the need for a comprehensive understanding of bias and its impact on society. While research on bias in NLP has expanded, several challenges persist that require attention. These include the limited focus on sociodemographic biases beyond race and gender, the narrow scope of analysis predominantly centered on models, and the technocentric implementation approaches. This paper addresses these challenges and advocates for a more interdisciplinary approach to understanding bias in NLP. The work is structured into three facets, each exploring a specific aspect of bias in NLP. The first facet focuses on identifying sociodemographic bias in various NLP architectures, emphasizing the importance of considering both the models themselves and human computation to comprehensively understand and identify bias. In the second facet, we delve into the significance of establishing a shared vocabulary across different fields and disciplines involved in NLP. By highlighting the potential bias stemming from a lack of shared understanding, this facet emphasizes the need for interdisciplinary collaboration to bridge the gap and foster a more inclusive and accurate analysis of bias. Finally, the third facet investigates the development of a holistic solution by integrating frameworks from social science disciplines. This approach recognizes the complexity of bias in NLP and advocates for an interdisciplinary framework that goes beyond purely technical considerations, involving social and ethical perspectives to address bias effectively. The first facet includes the following of my published works [6, 7, 8, 9] to provide results into how the importance of understanding the presence of bias in various minority group that has not been in focus in the prior works of bias in NLP. The work also shows the need to create a method that considers both human and AI indicators of bias, showcasing the importance of the first facet of my research. In my study [9], I delve into sentiment analysis and toxicity detection models to identify explicit bias against race, gender, and people with disabilities (PWDs). Through statistical exploration of conversations on social media platforms such as Twitter and Reddit, I gain insights into how disability bias permeates real-world social settings. To quantify explicit sociodemographic bias in sentiment analysis and toxicity analysis models, I create the Bias Identification Test in Sentiment (BITS) corpus1. Applying BITS, I uncover significant biases in popular AIaaS sentiment analysis tools, including TextBlob, VADER, and Google Cloud Natural Language API, as well as toxicity analysis models like Toxic-BERT. Remarkably, all of these models exhibit statistically significant explicit bias against disability, underscoring the need for comprehensive understanding and mitigation of biases affecting such groups. The work also demonstrates the utility of BITS as a model-independent method of identifying bias by focusing on social groups instead. Expanding on this, my next work [8] delves into the realm of implicit bias in NLP models. While some models may not overtly exhibit bias, they can unintentionally perpetuate harmful stereotypes [4]. To measure and identify implicit bias in commonly used embedding and large language models, I propose a methodology to measure social biases in various NLP architectures. Focusing on people with disabilities (PWD) as a group with complex social dynamics, I analyze various word embedding-based and transformer-based LLMs, revealing significant biases against PWDs in all tested models. These findings expose how models trained on extensive corpora tend to favor ableist language, underscoring the urgency of detecting and addressing implicit bias. The above two works look at both the implicit and explicit nature of bias in NLP, showcasing the need to distinguish the efforts placed in understanding them. The results also demonstrate the utility of identifying such biases as it provides context to the black-box nature of such public models. As the field of NLP evolved from embedding-based models to large language models, the way these models are constructed underwent significant changes [5]. However, the concern arises from the fact that these models often reflect a populist viewpoint [1] that perpetuates majority-held ideas rather than objective truths. This difference in perception can lead to biases perpetuated by the majority’s worldview. To explore this aspect, I investigate how LLMs represent nationality and their impact on societal stereotypes [6]. By examining LLM-generated stories for various nationalities, I establish a correlation between sentiment and the population of internet users in a country. The study reveals the unintentional implicit and explicit nationality biases exhibited by GPT-2, with nations having lower internet representation and economic status generating negative sentiment stories and employing a greater number of negative adjectives. Additionally, I explore potential debiasing methods such as adversarial triggering and prompt engineering, demonstrating their efficacy in mitigating stereotype propagation through LLM models. While prior work predominantly relies on automatic indicators like sentiment scores or vector distances to identify bias [3], the next phase of my research emphasizes the importance of understanding biases through the lens of human readers [7], bringing to light the need for a human lens in understanding bias through human-aided indicators and mixed-method identification. By incorporating concepts of social computation, using human evaluation, we gain a better understanding of biases’ potential societal impact within the context of language models. To achieve this, I conduct open-ended interviews and employ qualitative coding and thematic analysis to comprehend the implications of biases on human readers. The findings demonstrate that biased NLP models tend to replicate and amplify existing societal biases, posing potential harm when utilized in sociotechnical settings. The qualitative analysis from the interviews provides valuable insights into readers’ experiences when encountering biased articles, highlighting the capacity to shift a reader’s perception of a country. These findings emphasize the critical role of public perception in shaping AI’s impact on society and the need to correct biases in AI systems. The second facet of my research aims to bridge the disparity between AI research and society. This disparity has resulted in a lack of shared understanding between these domains, leading to potential biases and harm toward specific groups. Employing an interdisciplinary approach that combines social informatics, philosophy, and AI, I will investigate the similarities and disparities in the concepts utilized by machine learning models. Existing research [2] highlights the insufficient interdisciplinary effort and motivation in comprehending social aspects of NLP. To commence this exploration, I will delve into the shared taxonomy of sentiment and fairness in natural language processing, sociology, and humanities. This research will first delve into the interdisciplinary nature of sentiment and its application in sentiment analysis models. Sentiment analysis, a popular machine learning application for text classification based on sentiment, opinion, and subjectivity, holds significant influence as a sociotechnical system that impacts both social and technical actors within a network. Nevertheless, the definition and connotation of sentiment vary vastly across different research fields, potentially leading to misconceptions regarding the utility of such systems. To address this issue, this study will examine how diverse fields, including psychology, sociology, and technology, define the concept of sentiment. By unraveling the divergent perspectives on sentiment within different fields, the paper will uncover discrepancies and varying applications of this interdisciplinary concept. Additionally, the research will survey commonly utilized sentiment analysis models, aiming to comprehend their standardized definitions and associated issues. Ultimately, the study will pose critical questions that should be considered during the development of social models to mitigate potential biases and harm stemming from an insufficiently defined comprehension of fundamental social concepts. Similar efforts will be dedicated to comprehending the disparity in bias and fairness as an interdisciplinary concept, shedding light on the imperative for inclusive research to cultivate superior AI models as sociotechnical solutions. The third facet of my study embarks upon an exploration of the intricate interplay between human and AI actors, employing the formidable theoretical lens of actor-network theory (ANT). Through the presentation of a robust framework, this facet aims to engender the formation of efficacious development networks that foster collaboration among developers, practitioners, and other essential stakeholders. Such inclusive networks serve as crucibles for the cultivation of holistic solutions that transcend the discriminatory trappings afflicting specific populations. A tangible outcome of this endeavor entails the creation of an all-encompassing bias analysis platform, poised to guide the discernment and amelioration of an array of sociodemographic biases manifesting within any machine-learning system. By catalyzing the development of socially aware and less pernicious technology, this research makes a substantial contribution to the realms of NLP and AI. The significance of this proposed research reverberates beyond the confines of NLP, resonating throughout the broader domain of AI, wherein analogous challenges about social biases loom large. Leveraging the proposed framework, developers, practitioners, and policymakers are empowered to forge practical solutions that embody inclusivity and reliability, especially when used as a service (AIaaS). Moreover, the platform serves as a centralized locus for the identification and rectification of social biases, irrespective of the underlying model or architecture. By furnishing a cogent narrative that underscores the imperative for a comprehensive and interdisciplinary approach, my work strives to propel the ongoing endeavors to comprehend and mitigate biases within the realm of NLP. With its potential to augment the equity, inclusivity, and societal ramifications of NLP technologies, the proposed framework catapults the field towards responsible and ethical practices.},
booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {1004–1005},
numpages = {2},
location = {Montr\'{e}al, QC, Canada},
series = {AIES '23}
}

@inproceedings{10.1145/3604930.3605705,
author = {Chien, Andrew A and Lin, Liuzixuan and Nguyen, Hai and Rao, Varsha and Sharma, Tristan and Wijayawardana, Rajini},
title = {Reducing the Carbon Impact of Generative AI Inference (today and in 2035)},
year = {2023},
isbn = {9798400702426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604930.3605705},
doi = {10.1145/3604930.3605705},
abstract = {Generative AI, exemplified in ChatGPT, Dall-E 2, and Stable Diffusion, are exciting new applications consuming growing quantities of computing. We study the compute, energy, and carbon impacts of generative AI inference. Using ChatGPT as an exemplar, we create a workload model and compare request direction approaches (Local, Balance, CarbonMin), assessing their power use and carbon impacts.Our workload model shows that for ChatGPT-like services, inference dominates emissions, in one year producing 25x the carbon-emissions of training GPT-3. The workload model characterizes user experience, and experiments show that carbon emissions-aware algorithms (CarbonMin) can both maintain user experience and reduce carbon emissions dramatically (35%). We also consider a future scenario (2035 workload and power grids), and show that CarbonMin can reduce emissions by 56%. In both cases, the key is intelligent direction of requests to locations with low-carbon power. Combined with hardware technology advances, CarbonMin can keep emissions increase to only 20% compared to 2022 levels for 55x greater workload. Finally we consider datacenter headroom to increase effectiveness of shifting. With headroom, CarbonMin reduces 2035 emissions by 71%.},
booktitle = {Proceedings of the 2nd Workshop on Sustainable Computer Systems},
articleno = {11},
numpages = {7},
keywords = {generative AI, sustainability, carbon emissions, large language models, geographic shifting},
location = {Boston, MA, USA},
series = {HotCarbon '23}
}

@article{10.1145/3605889,
author = {Mujahid, Muhammad and Kanwal, Khadija and Rustam, Furqan and Aljedaani, Wajdi and Ashraf, Imran},
title = {Arabic ChatGPT Tweets Classification Using RoBERTa and BERT Ensemble Model},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {8},
issn = {2375-4699},
url = {https://doi.org/10.1145/3605889},
doi = {10.1145/3605889},
abstract = {ChatGPT OpenAI, a large-language chatbot model, has gained a lot of attention due to its popularity and impressive performance in many natural language processing tasks. ChatGPT produces superior answers to a wide range of real-world human questions and generates human-like text. The new OpenAI ChatGPT technology may have some strengths and weaknesses at this early stage. Users have reported early opinions about the ChatGPT features, and their feedback is essential to recognize and fix its shortcomings and issues. This study uses the ChatGPT tweets Arabic dataset to automatically find user opinions and sentiments about ChatGPT technology. The dataset is preprocessed and labeled using the TextBlob Arabic Python library into positive, negative, and neutral tweets. Despite extensive works for the English language, languages like Arabic are less studied regarding tweet analysis. Existing literature about Arabic tweet sentiment analysis has mainly focused on machine learning and deep learning models. We collected a total of 27,780 unstructured tweets from Twitter using the Tweepy SNscrape Python library using various hash-tags such as # Chat-GPT, #OpenAI, #Chatbot, Chat-GPT3, and so on. To enhance the model’s performance and reduce computational complexity, unstructured tweets are converted into structured and normalized forms. Tweets contain missing values, URL and HTML tags, stop words, punctuation, diacritics, elongations, and numeric values that have no impact on the model performance; hence, these increase the computational cost. So, these steps are removed with the help of Python preprocessing libraries to enhance text quality and consistency. This study adopts Transformer-based models such as RoBERTa, XLNet, and DistilBERT that automatically classify the tweets. Additionally, a hybrid transformer-based model is proposed to obtain better results. The proposed hybrid model is developed by combining the hidden outputs of the RoBERTA and BERT models using a concatenation layer, then adding dense layers with “Relu” activation employed as a hidden layer to create non-linearity and a “softmax” activation function for multiclass classification. They differ from existing state-of-the-art models due to the enhanced capabilities of both models in text classification. Hybrid models combine the different models to make accurate predictions and reduce bias and enhanced the overall results, while state-of-the-art models are incapable of making accurate predictions. Experiments show that the proposed hybrid model achieves 96.02% accuracy, 100% precision on negative tweets, and 99% recall for neutral tweets. The performance of the proposed model is far better than existing state-of-the-art models.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {204},
numpages = {23},
keywords = {Arabic tweets, low-resource language, ChatGPT, OpenAI, transformer models, BERT, sentiment analysis}
}

@inproceedings{10.1145/3555776.3577652,
author = {Jamil, Hasan M and Naha, Kallol},
title = {Mapping Strategies for Declarative Queries over Online Heterogeneous Biological Databases for Intelligent Responses},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577652},
doi = {10.1145/3555776.3577652},
abstract = {The emergence of Alexa and Siri, and more recently, OpenAI's Chat-GPT, raises the question whether ad hoc biological queries can also be computed without end-users' active involvement in the code writing process. While advances have been made, current querying architectures for biological databases still assume some degree of computational competence and significant structural awareness of the underlying network of databases by biologists, if not active code writing. Given that biological databases are highly distributed and heterogeneous, and most are not FAIR compliant, a significant amount of expertise in data integration is essential for a query to be accurately crafted and meaningfully executed. In this paper, we introduce a flexible and intelligent query reformulation assistant, called Needle, as a back-end query execution engine of a natural language query interface to online biological databases. Needle leverages a data model called BioStar that leverages a meta-knowledgebase, called the schema graph, to map natural language queries to relevant databases and biological concepts. The implementation of Needle using BioStar is the focus of this article.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {567–574},
numpages = {8},
keywords = {schema graph, biological databases, data integration, ad hoc querying, schema abstraction, query reformulation},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3503222.3507778,
author = {Jangda, Abhinav and Huang, Jun and Liu, Guodong and Sabet, Amir Hossein Nodehi and Maleki, Saeed and Miao, Youshan and Musuvathi, Madanlal and Mytkowicz, Todd and Saarikivi, Olli},
title = {Breaking the computation and communication abstraction barrier in distributed machine learning workloads},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507778},
doi = {10.1145/3503222.3507778},
abstract = {Recent trends towards large machine learning models require both training and inference tasks to be distributed. Considering the huge cost of training these models, it is imperative to unlock optimizations in computation and communication to obtain best performance. However, the current logical separation between computation and communication kernels in machine learning frameworks misses optimization opportunities across this barrier. Breaking this abstraction can provide many optimizations to improve the performance of distributed workloads. However, manually applying these optimizations requires modifying the underlying computation and communication libraries for each scenario, which is both time consuming and error-prone.  Therefore, we present CoCoNet, which contains (i) a domain specific language to express a distributed machine learning program in the form of computation and communication operations, (ii) a set of semantics preserving transformations to optimize the program, and (iii) a compiler to generate jointly optimized communication and computation GPU kernels. Providing both computation and communication as first class constructs allows users to work on a high-level abstraction and apply powerful optimizations, such as fusion or overlapping of communication and computation. CoCoNet enabled us to optimize data-, model- and pipeline-parallel workloads in large language models with only a few lines of code. Our experiments show that CoCoNet significantly outperforms state-of-the-art distributed machine learning implementations.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {402–416},
numpages = {15},
keywords = {CUDA, Code Generation, Collective Communication, Compiler Optimizations, Distributed Machine Learning, MPI},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@inproceedings{10.1145/3613424.3614263,
author = {Fan, Hongxiang and Venieris, Stylianos I. and Kouris, Alexandros and Lane, Nicholas},
title = {Sparse-DySta: Sparsity-Aware Dynamic and Static Scheduling for Sparse Multi-DNN Workloads},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614263},
doi = {10.1145/3613424.3614263},
abstract = {Running multiple deep neural networks (DNNs) in parallel has become an emerging workload in both edge devices, such as mobile phones where multiple tasks serve a single user for daily activities, and data centers, where various requests are raised from millions of users, as seen with large language models. To reduce the costly computational and memory requirements of these workloads, various efficient sparsification approaches have been introduced, resulting in widespread sparsity across different types of DNN models. In this context, there is an emerging need for scheduling sparse multi-DNN workloads, a problem that is largely unexplored in previous literature. This paper systematically analyses the use-cases of multiple sparse DNNs and investigates the opportunities for optimizations. Based on these findings, we propose Dysta, a novel bi-level dynamic and static scheduler that utilizes both static sparsity patterns and dynamic sparsity information for the sparse multi-DNN scheduling. Both static and dynamic components of Dysta are jointly designed at the software and hardware levels, respectively, to improve and refine the scheduling approach. To facilitate future progress in the study of this class of workloads, we construct a public benchmark that contains sparse multi-DNN workloads across different deployment scenarios, spanning from mobile phones and AR/VR wearables to data centers. A comprehensive evaluation on the sparse multi-DNN benchmark demonstrates that our proposed approach outperforms the state-of-the-art methods with up to 10% decrease in latency constraint violation rate and nearly 4 \texttimes{} reduction in average normalized turnaround time. Our artifacts and code are publicly available at: https://github.com/SamsungLabs/Sparse-Multi-DNN-Scheduling.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {353–366},
numpages = {14},
keywords = {Algorithm and Hardware Co-Design, Dynamic and Static Approach, Sparse Multi-DNN Scheduling},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3579371.3589350,
author = {Jouppi, Norm and Kurian, George and Li, Sheng and Ma, Peter and Nagarajan, Rahul and Nai, Lifeng and Patil, Nishant and Subramanian, Suvinay and Swing, Andy and Towles, Brian and Young, Clifford and Zhou, Xiang and Zhou, Zongwei and Patterson, David A},
title = {TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589350},
doi = {10.1145/3579371.3589350},
abstract = {In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are &lt;5% of system cost and &lt;3% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x--7x yet use only 5% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus nearly 10x faster overall, which along with OCS flexibility and availability allows a large language model to train at an average of ~60% of peak FLOPS/second. For similar sized systems, it is ~4.3x--4.5x faster than the Graphcore IPU Bow and is 1.2x--1.7x faster and uses 1.3x--1.9x less power than the Nvidia A100. TPU v4s inside the energy-optimized warehouse scale computers of Google Cloud use ~2--6x less energy and produce ~20x less CO2e than contemporary DSAs in typical on-premise data centers.},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {82},
numpages = {14},
keywords = {machine learning, domain specific architecture, TPU, GPU, IPU, supercomputer, optical interconnect, reconfigurable, embeddings, large language model, power usage effectiveness, warehouse scale computer, carbon emissions, energy, CO2 equivalent emissions},
location = {Orlando, FL, USA},
series = {ISCA '23}
}

@inproceedings{10.1145/3540250.3549162,
author = {Chakraborty, Saikat and Ahmed, Toufique and Ding, Yangruibo and Devanbu, Premkumar T. and Ray, Baishakhi},
title = {NatGen: generative pre-training by “naturalizing” source code},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549162},
doi = {10.1145/3540250.3549162},
abstract = {Pre-trained Generative Language models (e.g., PLBART, CodeT5, SPT-Code) for source code yielded strong results on several tasks in the past few years, including code generation and translation. These models have adopted varying pre-training objectives to learn statistics of code construction from very large-scale corpora in a self-supervised fashion; the success of pre-trained models largely hinges on these pre-training objectives. This paper proposes a new pre-training objective, “Naturalizing” of source code, exploiting code’s bimodal, dual-channel (formal &amp; natural channels) nature. Unlike natural language, code’s bimodal, dual-channel nature allows us to generate semantically equivalent code at scale. We introduce six classes of semantic preserving transformations to introduce unnatural forms of code, and then force our model to produce more natural original programs written by developers. Learning to generate equivalent, but more natural code, at scale, over large corpora of open-source code, without explicit manual supervision, helps the model learn to both ingest &amp; generate code. We fine-tune our model in three generative Software Engineering tasks: code generation, code translation, and code refinement with limited human-curated labeled data and achieve state-of-the-art performance rivaling CodeT5. We show that our pre-trained model is especially competitive at zero-shot and few-shot learning, and better at learning code properties (e.g., syntax, data flow)},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {18–30},
numpages = {13},
keywords = {Neural Network, Semantic Preserving Transformation, Source Code Pre-training, Source Code Transformer},
location = {Singapore, Singapore},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3567955.3567959,
author = {Wang, Shibo and Wei, Jinliang and Sabne, Amit and Davis, Andy and Ilbeyi, Berkin and Hechtman, Blake and Chen, Dehao and Murthy, Karthik Srinivasa and Maggioni, Marcello and Zhang, Qiao and Kumar, Sameer and Guo, Tongfei and Xu, Yuanzhong and Zhou, Zongwei},
title = {Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models},
year = {2022},
isbn = {9781450399159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3567955.3567959},
doi = {10.1145/3567955.3567959},
abstract = {Large deep learning models have shown great potential with state-of-the-art results in many tasks. However, running these large models is quite challenging on an accelerator (GPU or TPU) because the on-device memory is too limited for the size of these models. Intra-layer model parallelism is an approach to address the issues by partitioning individual layers or operators across multiple devices in a distributed accelerator cluster. But, the data communications generated by intra-layer model parallelism can contribute to a significant proportion of the overall execution time and severely hurt the computational efficiency. As intra-layer model parallelism is critical to enable large deep learning models, this paper proposes a novel technique to effectively reduce its data communication overheads by overlapping communication with computation. With the proposed technique, an identified original communication collective is decomposed along with the dependent computation operation into a sequence of finer-grained operations. By creating more overlapping opportunities and executing the newly created, finer-grained communication and computation operations in parallel, it effectively hides the data transfer latency and achieves a better system utilization. Evaluated on TPU v4 Pods using different types of large models that have 10 billion to 1 trillion parameters, the proposed technique improves system throughput by 1.14 - 1.38x. The achieved highest peak FLOPS utilization is 72% on 1024 TPU chips with a large language model that has 500 billion parameters.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
pages = {93–106},
numpages = {14},
keywords = {Collective communication hiding, Compiler optimization, Large scale machine learning},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@article{10.1145/3591300,
author = {Beurer-Kellner, Luca and Fischer, Marc and Vechev, Martin},
title = {Prompting Is Programming: A Query Language for Large Language Models},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {PLDI},
url = {https://doi.org/10.1145/3591300},
doi = {10.1145/3591300},
abstract = {Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation.  
On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction.  

Based on this, we present the novel idea of Language Model Programming (LMP). LMP generalizes language model prompting from pure text prompts to an intuitive combination of text prompting and scripting. Additionally, LMP allows constraints to be specified over the language model output. This enables easy adaption to many tasks while abstracting language model internals and providing high-level semantics.  

To enable LMP, we implement LMQL (short for Language Model Query Language), which leverages the constraints and control flow from an LMP prompt to generate an efficient inference procedure that minimizes the number of expensive calls to the underlying language model.  

We show that LMQL can capture a wide range of state-of-the-art prompting methods in an intuitive way, especially facilitating interactive flows that are challenging to implement with existing high-level APIs. Our evaluation shows that we retain or increase the accuracy on several downstream tasks, while also significantly reducing the required amount of computation or cost in the case of pay-to-use APIs (26-85% cost savings).},
journal = {Proc. ACM Program. Lang.},
month = jun,
articleno = {186},
numpages = {24},
keywords = {language model programming, prompt programming}
}

@inproceedings{10.1145/3508398.3511524,
author = {Qachfar, Fatima Zahra and Verma, Rakesh M. and Mukherjee, Arjun},
title = {Leveraging Synthetic Data and PU Learning For Phishing Email Detection},
year = {2022},
isbn = {9781450392204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508398.3511524},
doi = {10.1145/3508398.3511524},
abstract = {Imbalanced data classification has always been one of the most challenging problems in data science especially in the cybersecurity field, where we observe an out-of-balance proportion between benign and phishing examples in security datasets. Even though there are many phishing detection methods in literature, most of them neglect the imbalanced nature of phishing email datasets. In this paper, we examine the imbalanced property by varying legitimate to phishing class ratios. We generate new synthetic instances using a generative adversarial network model for long sentences (LeakGAN) to balance out the training process and ameliorate its impact on classification. These synthetic instances are labeled by positive-unlabeled learning and added to the initial imbalanced training set. The resulting dataset is given to the Bidirectional Encoder Representations from Transformers (BERT) model for sequence classification. We compare several state-of-the-art methods from the literature against our approach, which achieves a high performance throughout all the imbalanced ratios reaching an F1-score of 99.6% for the most extreme imbalanced ratio and an F1-score of 99.8% for balanced cases.},
booktitle = {Proceedings of the Twelfth ACM Conference on Data and Application Security and Privacy},
pages = {29–40},
numpages = {12},
keywords = {generative adversarial network, imbalanced data, positive-unlabeled learning, semi-supervised learning, sequence classification},
location = {Baltimore, MD, USA},
series = {CODASPY '22}
}

@article{10.1145/3622815,
author = {Wang, Shangwen and Lin, Bo and Sun, Zhensu and Wen, Ming and Liu, Yepang and Lei, Yan and Mao, Xiaoguang},
title = {Two Birds with One Stone: Boosting Code Generation and Code Search via a Generative Adversarial Network},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622815},
doi = {10.1145/3622815},
abstract = {Automatically transforming developers' natural language descriptions into source code has been a longstanding goal in software engineering research.  
Two types of approaches have been proposed in the literature to achieve this: code generation, which involves generating a new code snippet, and code search, which involves reusing existing code.  
However, despite existing efforts, the effectiveness of the state-of-the-art techniques remains limited.  
To seek for further advancement, our insight is that code generation and code search can help overcome the limitation of each other:  
the code generator can benefit from feedback on the quality of its generated code, which can be provided by the code searcher, while the code searcher can benefit from the additional training data augmented by the code generator to better understand code semantics.  
Drawing on this insight, we propose a novel approach that combines code generation and code search techniques using a generative adversarial network (GAN), enabling mutual improvement through the adversarial training.  
Specifically, we treat code generation and code search as the generator and discriminator in the GAN framework, respectively, and incorporate several customized designs for our tasks.  
We evaluate our approach in eight different settings, and consistently observe significant performance improvements for both code generation and code search.  
For instance, when using NatGen, a state-of-the-art code generator, as the generator and GraphCodeBERT, a state-of-the-art code searcher, as the discriminator, we achieve a 32% increase in CodeBLEU score for code generation, and a 12% increase in mean reciprocal rank for code search on a large-scale Python dataset, compared to their original performances.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {239},
numpages = {30},
keywords = {Code Generation, Code Search, Generative Adversarial Network}
}

@inproceedings{10.1145/3583780.3615506,
author = {Maiorino, Antonio and Padgett, Zoe and Wang, Chun and Yakubovskiy, Misha and Jiang, Peng},
title = {Application and Evaluation of Large Language Models for the Generation of Survey Questions},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615506},
doi = {10.1145/3583780.3615506},
abstract = {Generative Language Models have shown promising results in various domains, and some of the most successful applications are related to "concept expansion", which is the task of generating extensive text based on concise instructions provided through a "seed" prompt. In this presentation we will discuss the recent work conducted by the Data Science team at SurveyMonkey, where we have recently introduced a new feature that harnesses Generative AI models to streamline the survey design process. With this feature users can effortlessly initiate this process by specifying their desired objectives through a prompt, allowing them to automate the creation of surveys that include the critical aspects they wish to investigate.We will share our findings regarding some of the challenges encountered during the development of this feature. These include techniques for conditioning the model outputs, integrating generated text with industry-standard questions, fine-tuning Language Models using semi-synthetic Data Generation techniques, and more. Moreover, we will showcase the Evaluation Methodology that we have developed to measure the quality of the generated surveys across several dimensions. This evaluation process is crucial in ensuring that the generated surveys align well with user expectations and serve their intended purpose effectively. Our goal is to demonstrate the promising potential of Generative Language Models in the context of Survey Research, and we believe that sharing our learnings on these challenges and how we addressed them will be useful for practitioners working with Language Models on similar problems.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {5244–5245},
numpages = {2},
keywords = {generative AI, survey research, text evaluation},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@article{10.1145/3592367.3617935,
author = {Hines, Jasara},
title = {Review of "Writing in the Clouds: Inventing and Composing in Internetworked Writing Spaces by John Logie," Logie, J. (2021). Writing in the clouds: Inventing and composing in internetworked writing spaces. Parlor Press.},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
url = {https://doi.org/10.1145/3592367.3617935},
doi = {10.1145/3592367.3617935},
abstract = {In the wake of the controversy surrounding the new AI chatbot application, ChatGPT, I wonder how Logie would seek to include this new technology in his work. I ponder this because, throughout the book, Logie presents compelling evidence for why the concepts of invention, composition, and internetworked writing should be embraced and not feared. While some denounce the application and take to social media to disparage the possible negative impact on students, creativity, and composition, ChatGPT, I believe Logie would argue, would be a powerful tool we can implement to become "composers." He believes that through cloud computing services we are now more apt to collaborate, use, remix, and create rhetorical modes that extend far beyond the formulaic argument, therefore we are composers. So, Logie applies the idea of a composer as someone who is a "prosumer" (Toffler). This composer is media literate and transforms traditional rhetorical canons into multimodal compositions such as memes, Google Docs, and digital collages. However, his overarching argument is that internetworked writing tools have democratized writing through that same offering of innovative outlets. His book is arranged in a way that walks the reader through this argument.},
journal = {Commun. Des. Q. Rev},
month = dec,
pages = {80–81},
numpages = {2}
}

@inproceedings{10.1145/3610969.3610973,
author = {Addo, Salomey Afua},
title = {Are You Ready to Teach AI in Schools? Teachers' Perspectives of Teaching AI in K-12 Settings},
year = {2023},
isbn = {9798400708763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610969.3610973},
doi = {10.1145/3610969.3610973},
abstract = {Artificial intelligence (AI) has continually made headlines, even more so with the mass interest in generative AI. The implications of AI on society raises the need for its inclusion in the K-12 computing curriculum. However, little research has been conducted to understand teachers’ preparedness to teach AI concepts in K-12. This exploratory study seeks to understand teachers’ motivation and preparedness to teach AI in schools through the lens of Self Efficacy Theory (SET) and Self Determination Theory (SDT).},
booktitle = {Proceedings of the 2023 Conference on United Kingdom &amp; Ireland Computing Education Research},
articleno = {32},
numpages = {1},
keywords = {Artificial intelligence, K-12 computing education, motivation},
location = {Swansea, Wales Uk},
series = {UKICER '23}
}

@inproceedings{10.1145/3580305.3599573,
author = {Muhamed, Aashiq and Bock, Christian and Solanki, Rahul and Park, Youngsuk and Wang, Yida and Huan, Jun},
title = {Training Large-scale Foundation Models on Emerging AI Chips},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599573},
doi = {10.1145/3580305.3599573},
abstract = {Foundation models such as ChatGPT and GPT-4 have garnered significant interest from both academia and industry due to their emergent capabilities, such as few-shot prompting, multi-step reasoning, instruction following, and model calibration. Such capabilities were previously only attainable with specially designed models, such as those using knowledge graphs, but can now be achieved on a much larger scale with foundation models. As the capabilities of foundation models have increased, so too have their sizes at a rate much faster than Moore's law. For example, the BERT large model was initially released as a 334M model in 2018, and by 2023, the largest GPT-4 models are estimated to range between 200-300B, representing an increase of three orders of magnitude in just five years. The training of foundation models requires massive computing power. For instance, training a BERT model on a single state-of-the-art GPU machine with multi-A100 chips can take several days, while training GPT-3 models on a large multi-instance GPU cluster can take several months to complete the estimated 3 X 1023 flops.This tutorial provides an overview of the latest progress in supporting foundation model training and inference with new AI chips. It reviews progress on the modeling side, with an emphasis on the transformer architecture, and presents the system architecture supporting training and serving foundation models. This includes programming language frameworks such as PyTorch and Tensorflow, graph compilers, 3D parallelisms, and accelerators such as the GPU H100, TPU, and Trainium. Finally, the tutorial presents our experience of training foundation models using different systems.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5821–5822},
numpages = {2},
keywords = {ai accelerator, foundation models, gpu, tpu, trainium},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3604915.3608795,
author = {Zheng, Zhi and Sun, Ying and Song, Xin and Zhu, Hengshu and Xiong, Hui},
title = {Generative Learning Plan Recommendation for Employees: A Performance-aware Reinforcement Learning Approach},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3608795},
doi = {10.1145/3604915.3608795},
abstract = {With the rapid development of enterprise Learning Management Systems (LMS), more and more companies are trying to build enterprise training and course learning platforms for promoting the career development of employees. Indeed, through course learning, many employees have the opportunity to improve their knowledge and skills. For these systems, a major issue is how to recommend learning plans, i.e., a set of courses arranged in the order they should be learned, that can help employees improve their work performance. Existing studies mainly focus on recommending courses that users are most likely to click on by capturing their learning preferences. However, the learning preference of employees may not be the right fit for their career development, and thus it may not necessarily mean their work performance can be improved accordingly. Furthermore, how to capture the mutual correlation and sequential effects between courses, and ensure the rationality of the generated results, is also a major challenge. To this end, in this paper, we propose the Generative Learning plAn recommenDation (GLAD) framework, which can generate personalized learning plans for employees to help them improve their work performance. Specifically, we first design a performance predictor and a rationality discriminator, which have the same transformer-based model architecture, but with totally different parameters and functionalities. In particular, the performance predictor is trained for predicting the work performance of employees based on their work profiles and historical learning records, while the rationality discriminator aims to evaluate the rationality of the generated results. Then, we design a learning plan generator based on the gated transformer and the cross-attention mechanism for learning plan generation. We calculate the weighted sum of the output from the performance predictor and the rationality discriminator as the reward, and we use Self-Critical Sequence Training (SCST) based policy gradient methods to train the generator following the Generative Adversarial Network (GAN) paradigm. Finally, extensive experiments on real-world data clearly validate the effectiveness of our GLAD framework compared with state-of-the-art baseline methods and reveal some interesting findings for talent management.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {443–454},
numpages = {12},
keywords = {generative recommendation, learning management system, reinforcement learning},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@article{10.14778/3636218.3636227,
author = {Nagrecha, Kabir and Kumar, Arun},
title = {Saturn: An Optimized Data System for Multi-Large-Model Deep Learning Workloads},
year = {2023},
issue_date = {December 2023},
publisher = {VLDB Endowment},
volume = {17},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3636218.3636227},
doi = {10.14778/3636218.3636227},
abstract = {Large models such as GPT-3 and ChatGPT have transformed deep learning (DL), powering applications that have captured the public's imagination. Such models must be trained on multiple GPUs due to their size and computational load, driving the development of a bevy of "model parallelism" techniques and tools. Navigating such parallelism choices, however, is a new burden for DL users such as data scientists, domain scientists, etc., who may lack the necessary systems knowhow. The need for model selection, which leads to many models to train due to hyper-parameter tuning or layer-wise finetuning, compounds the situation with two more burdens: resource apportioning and scheduling. In this work, we unify these three burdens by formalizing them as a joint problem that we call SPASE: Select a Parallelism, Allocate resources, and Schedule. We propose a new information system architecture to tackle the SPASE problem holistically, exploiting the performance opportunities presented by joint optimization. We devise an extensible template for existing parallelism schemes and combine it with an automated empirical profiler for runtime estimation. We then formulate SPASE as an MILP. We find that direct use of an MILP-solver is significantly more effective than several baseline heuristics. We optimize the system runtime further with an introspective scheduling approach. We implement all these techniques into a new data system we call Saturn. Experiments with benchmark DL workloads show that Saturn achieves 39-49% lower model selection runtimes than current DL practice.},
journal = {Proc. VLDB Endow.},
month = dec,
pages = {712–725},
numpages = {14}
}

@article{10.1109/TASLP.2023.3240661,
author = {Liu, Hong and Cai, Yucheng and Lin, Zhenru and Ou, Zhijian and Huang, Yi and Feng, Junlan},
title = {Variational Latent-State GPT for Semi-Supervised Task-Oriented Dialog Systems},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3240661},
doi = {10.1109/TASLP.2023.3240661},
abstract = {Recently, two approaches, fine-tuning large pre-trained language models and variational training, have attracted significant interests, separately, for semi-supervised end-to-end task-oriented dialog (TOD) systems. In this paper, we propose Variational Latent-State GPT model (VLS-GPT), which is the first to combine the strengths of the two approaches. Among many options of models, we propose the generative model and the inference model for variational learning of the end-to-end TOD system, both as auto-regressive language models based on GPT-2, which can be further trained over a mix of labeled and unlabeled dialog data in a semi-supervised manner. Variational training of VLS-GPT is both statistically and computationally more challenging than previous variational learning works for sequential latent variable models, which use turn-level first-order Markovian. The inference model in VLS-GPT is non-Markovian due to the use of the Transformer architecture. In this work, we establish Recursive Monte Carlo Approximation (RMCA) to the variational objective with non-Markovian inference model and prove its unbiasedness. Further, we develop the computational strategy of sampling-then-forward-computation to realize RMCA, which successfully overcomes the memory explosion issue of using GPT in variational learning and speeds up training. Semi-supervised TOD experiments are conducted on two benchmark multi-domain datasets of different languages - MultiWOZ2.1 and CrossWOZ. VLS-GPT is shown to significantly outperform both supervised-only and semi-supervised self-training baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {970–984},
numpages = {15}
}

@inproceedings{10.1145/3600211.3604744,
author = {Affsprung, Daniel},
title = {The ELIZA Defect: Constructing the Right Users for Generative AI},
year = {2023},
isbn = {9798400702310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600211.3604744},
doi = {10.1145/3600211.3604744},
abstract = {Artificial intelligence (AI) is at the center of debates on what kind of future we want and how to bring it about. But AI ethics is not only a technical risk assessment and accounting effort, or an application of general principles to stable artifacts. It is also a social self-diagnosis, a contested and contestable assertion of values and desirable futures, and a selective understanding of the nature of AI in its different forms. In expressions of concern and efforts at preparation for increasingly powerful AI tools, we can trace the ways we imagine ourselves and our society to be compatible with AI's promises and susceptible to its dangers. The problems we notice, and the solutions we offer, arise from the interaction of these imagined elements.The socially embedded efficacy of AI tools leads many commentators to imagine their risks specifically in conjunction with understandings of society as it currently is and imaginations of how it can and should exist in the future [1]. The sense-making moves performed in the wake of developments in generative AI are thus a site to examine the movement and uses of different concepts brought together in this domain: the human, rationality, and the place of expertise. As these sense-making efforts are carried out, they become constraints on how the risks of generative AI can be noticed and understood.The problems raised by generative AI are so fundamentally tied to its performance as a simulator of human interpersonal acts that we should ask: where is the risk of generative AI located, such that the utility and the safety of the tools can be preserved after troubling cases? Boundaries between malicious deception and magnificent design are unclear without an answer to this question. Thus, to fit generative AI into our world, we are trying to answer it; this is one goal of efforts at regulation which seeks to allow the benefits of imitation to arrive while avoiding the harms of deception. In the current regulation, reporting, and corporate responses to generative AI, the challenge of safely introducing generative AI is being approached in large part as a challenge of producing the right kind of knowledge in its users.Below is a summary of my findings from three cases, chosen to investigate the following question: What ways, or whose ways, of using, knowing, and understanding generative AI are being offered as appropriate? I examine the EU AI Act language reflecting disclosure requirements for interactions with generative AI, responses to a chatbot-facilitated suicide in Belgium, and reactions to expert claims of a chatbot's sentience. In the first two cases, AI-generated content is problematic insofar as users are uninformed about its provenance or maliciously deceived by it, while users who know they are interacting with AI but behave problematically are designated as deluded or irrational. In the third case, a Google engineer who presents evidence to support claims that AI is sentient is censured as nationwide reporting denounces his claim against an expert consensus from which he is ejected. In all three cases, challenges facing widespread generative AI development and use are avoided by attending to the knowledge and understanding of those who use them rather than the functioning of the tools themselves.The EU AI Act is illuminating as a general and authoritative account of how AI interactions can be made safe, requiring first and foremost that users are informed. [2, 3] The AI Act is useful in the present paper as it shows the effort to match and reconcile a new technology with an extant set of values, chief among which is autonomy. Its reliance on disclosure reflects a general sense that harms are acceptable or unacceptable not on the basis of outcomes but based on the degree of autonomy possessed by the actors in question. Rational actors in a simulated environment are responsible for the effects of the simulation, so long as they are informed of the nature of that environment and have essentially consented to consume deceptive or false content. The other two cases I examine explore this very issue, of problematic understandings and behavior on the part of knowing users.The first of these is the case of the Belgian man. After his suicide responses from the company which provided the chatbot, media [4, 5, 6] and government [5], and prominent expert AI ethics commentary [7] characterized it as arising because the user was vulnerable and consequently did not relate to the bot in the right way. While the chatbot's emotionally charged language was seen as a part of the problem, in the reporting on this event the unanimous emphasis on the man's mental state presents the risk as arising in an interaction, in a pathological mistake of the user, rather than in the tool.Locating risk is a necessary and immensely powerful, if often unexamined step which precedes intervention in a worrisome state of affairs: where we locate risk is where we intervene. If the risk accompanying generative AI is located in the minds of uninformed or misapprehending users, disclosures and disclaimers are indeed sensible interventions. In this conception, when knowledge fails to protect the user, it is not a failed safeguard but a bad user. Problematizing user understandings in this way provides an exonerating resource for the companies providing these tools and suggests the rectitude of expert authority on the nature of these tools, by linking delays and dangers in generative AI to users who do not abide by the (strategically underdetermined) expert consensus on generative AI's accuracy, capabilities, and nature.My third case examines how the expert consensus around generative AI is maintained through the story of Blake Lemoine, who publicly announced his belief that Google's LaMDA model had become sentient and was presented by major media outlets and experts as deluded [8, 9, 10, 11, 12, 13]. In the media and corporate response to Lemoine, wherein Google questioned his sanity before firing him [13], we see his ejection from the community of experts permitted to call for greater scrutiny based on qualitative changes in the nature of these models. He becomes a layperson on account of his anthropomorphizing error. In this act of boundary work [14], policing who is in the body of experts qualified to decide on the sentience of the chatbot, and the nature of AI models in general, we must notice how small this group truly is and what Lemoine's ejection preserves. If safeguards like those Lemoine called for should follow on the kind of change he claimed to detect, and those outside Google's leadership could determine when such changes have arrived, Google would cease to lead the conversation on regulation by defining the nature of its technology. This state of affairs leaves the right relations with generative AI underdetermined but maintains that positions which challenge the expert consensus are the result of misunderstandings so significant as to disqualify the concerned party's thoughts on the matter from rational consideration. In the three cases examined here, events and concerns which threaten to depict generative AI as in need of significant scrutiny or changes are defused not by intervening in the company's technology, but by delineating between user understandings which are empowered and exploitative, safe and vulnerable, rational and deluded.Named after an early chatbot, the ELIZA effect refers to the readiness with which users anthropomorphize computer systems [15]. Reporting on both Lemoine [11] and the Belgian man cited this effect [6]. The chatbot which encouraged the Belgian man to commit suicide was named Eliza. One way of summarizing the change I trace in the cases described above is a transition away from the Turing test and towards the ELIZA effect as the conceptual frame for AI which imitates humans. While the Turing test implies the layperson's relevance to the discussion and regulation of AI, the ELIZA effect implies their irrelevance.This project will continue as an effort to follow popular, expert, and regulatory perceptions of the risk of generative AI as the tools themselves and the public concern surrounding them continue to develop. The resources of science and technology studies (STS) enable crucial perspectives on numerous ways of thinking about AI and the challenges of its development and regulation such as the common citations of law lag, invocations of self-regulation in the mode of the Asilomar Conference on rDNA, collective action problem framings, and more. The STS literature on sociotechnical imaginaries [1] and public understandings of science [16] contribute to the present insight as to how the efforts of tech-society reconciliation and risk-benefit balancing presented as appropriate for AI reveal and produce our understandings of the technology, even as they reproduce and reshape social norms. There is an urgent need for work which extends this powerful scholarly tradition for understanding science, technology, and society to AI, as one of the most important and concerning technological developments of our moment.},
booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {945–946},
numpages = {2},
keywords = {chatbots, expertise, generative AI, public understanding of science, science and technology studies},
location = {Montr\'{e}al, QC, Canada},
series = {AIES '23}
}

@inproceedings{10.1145/3532512.3539664,
author = {Lewis, Clayton},
title = {Automatic Programming and Education},
year = {2022},
isbn = {9781450396561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532512.3539664},
doi = {10.1145/3532512.3539664},
abstract = {Automatic programming, as supported by recent language-model based AI systems, potentially allows a new approach to making computation a useful tool for learning, a goal of the Boxer project. This paper shows that the Codex system can be used to support some of the explorations in mathematics for which Boxer has been used. Virtually no knowledge of programming is required. Reflecting on the lessons from this exploration may sharpen the goals we bring to educational computing. What knowledge about computing, as distinct from the ability to creatively use computing, should learners gain?},
booktitle = {Companion Proceedings of the 6th International Conference on the Art, Science, and Engineering of Programming},
pages = {70–80},
numpages = {11},
keywords = {Boxer, automatic programming, computational literacy, education},
location = {Porto, Portugal},
series = {Programming '22}
}

@inproceedings{10.1145/3625704.3625744,
author = {Chan, Victor K. Y.},
title = {Evaluation of e-learning platforms using artificial intelligence (AI) robots: Are the AI robots consistent},
year = {2023},
isbn = {9798400709142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625704.3625744},
doi = {10.1145/3625704.3625744},
abstract = {This article aims to explore the consistency between a few popular generative AI robots in the evaluation of e-learning platforms. The three robots adopted in the study were GPT-4, Sage, and Dragonfly, which were requested to award rating scores to the six major dimensions, namely (1) features and capabilities, (2) ease of use and customization, (3) cost, (4) security, (5) customer support, and (6) scalability, of 10 to 20 currently most popular e-learning platforms. For each of the three robots, the minimum, the maximum, the range, and the standard deviation of the rating scores for each of the six dimensions were computed across all the e-learning platforms. The rating score difference for each of the six dimensions between any pair of robots was calculated for each platform. The mean of the absolute value, the minimum, the maximum, the range, and the standard deviation of the differences for each dimensions between each pair of robots were calculated across all platforms. Finally, a Cronbach alpha coefficient of the rating scores was computed for each of the six dimensions between all the three robots across all the e-learning platforms. The computational results were to reveal whether the three robots accorded discrimination in evaluating each dimension across the platforms and whether there was consistency between the three robots in evaluating each dimension across the platforms. Among some auxiliary results, it was found that the evaluation by the three robots was severely inconsistent for the two dimensions cost and security, inconsistent to a lesser extent for the dimension scalability, and consistent for the remaining three dimensions.},
booktitle = {Proceedings of the 7th International Conference on Education and Multimedia Technology},
pages = {96–100},
numpages = {5},
keywords = {E-learning platforms, artificial intelligence, consistency, evaluation, learning management systems},
location = {Tokyo, Japan},
series = {ICEMT '23}
}

@inproceedings{10.1145/3610548.3618228,
author = {Abdelreheem, Ahmed and Eldesokey, Abdelrahman and Ovsjanikov, Maks and Wonka, Peter},
title = {Zero-Shot 3D Shape Correspondence},
year = {2023},
isbn = {9798400703157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610548.3618228},
doi = {10.1145/3610548.3618228},
abstract = {We propose a novel zero-shot approach to computing correspondences between 3D shapes. Existing approaches mainly focus on isometric and near-isometric shape pairs (e.g., human vs. human), but less attention has been given to strongly non-isometric and inter-class shape matching (e.g., human vs. cow). To this end, we introduce a fully automatic method that exploits the exceptional reasoning capabilities of recent foundation models in language and vision to tackle difficult shape correspondence problems. Our approach comprises multiple stages. First, we classify the 3D shapes in a zero-shot manner by feeding rendered shape views to a language-vision model (e.g., BLIP2) to generate a list of class proposals per shape. These proposals are unified into a single class per shape by employing the reasoning capabilities of ChatGPT. Second, we attempt to segment the two shapes in a zero-shot manner, but in contrast to the co-segmentation problem, we do not require a mutual set of semantic regions. Instead, we propose to exploit the in-context learning capabilities of ChatGPT to generate two different sets of semantic regions for each shape and a semantic mapping between them. This enables our approach to match strongly non-isometric shapes with significant differences in geometric structure. Finally, we employ the generated semantic mapping to produce coarse correspondences that can further be refined by the functional maps framework to produce dense point-to-point maps. Our approach, despite its simplicity, produces highly plausible results in a zero-shot manner, especially between strongly non-isometric shapes.},
booktitle = {SIGGRAPH Asia 2023 Conference Papers},
articleno = {59},
numpages = {11},
keywords = {3D Semantic Segmentation, 3D Shape Matching, Deep Neural Networks, Zero-Shot Shape Correspondence},
location = {Sydney, NSW, Australia},
series = {SA '23}
}

@inproceedings{10.1109/MICRO56248.2022.00051,
author = {Hong, Seongmin and Moon, Seungjae and Kim, Junsoo and Lee, Sungjae and Kim, Minsub and Lee, Dongsoo and Kim, Joo-Young},
title = {DFX: A Low-Latency Multi-FPGA Appliance for Accelerating Transformer-Based Text Generation},
year = {2023},
isbn = {9781665462723},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MICRO56248.2022.00051},
doi = {10.1109/MICRO56248.2022.00051},
abstract = {Transformer is a deep learning language model widely used for natural language processing (NLP) services in datacenters. Among transformer models, Generative Pre-trained Transformer (GPT) has achieved remarkable performance in text generation, or natural language generation (NLG), which needs the processing of a large input context in the summarization stage, followed by the generation stage that produces a single word at a time. The conventional platforms such as GPU are specialized for the parallel processing of large inputs in the summarization stage, but their performance significantly degrades in the generation stage due to its sequential characteristic. Therefore, an efficient hardware platform is required to address the high latency caused by the sequential characteristic of text generation.In this paper, we present DFX, a multi-FPGA acceleration appliance that executes GPT-2 model inference end-to-end with low latency and high throughput in both summarization and generation stages. DFX uses model parallelism and optimized dataflow that is model-and-hardware-aware for fast simultaneous workload execution among devices. Its compute cores operate on custom instructions and provide GPT-2 operations end-to-end. We implement the proposed hardware architecture on four Xilinx Alveo U280 FPGAs and utilize all of the channels of the high bandwidth memory (HBM) and the maximum number of compute resources for high hardware efficiency. DFX achieves 5.58\texttimes{} speedup and 3.99\texttimes{} energy efficiency over four NVIDIA V100 GPUs on the modern GPT-2 model. DFX is also 8.21\texttimes{} more cost-effective than the GPU appliance, suggesting that it is a promising solution for text generation workloads in cloud datacenters.},
booktitle = {Proceedings of the 55th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {616–630},
numpages = {15},
keywords = {natural language processing, GPT, text generation, datacenter, multi-FPGA acceleration, model parallelism},
location = {Chicago, Illinois, USA},
series = {MICRO '22}
}

@inproceedings{10.1145/3520304.3529052,
author = {Clei, Maximilien Le and Bellec, Pierre},
title = {Neuroevolution of recurrent architectures on control tasks},
year = {2022},
isbn = {9781450392686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520304.3529052},
doi = {10.1145/3520304.3529052},
abstract = {Modern artificial intelligence works typically train the parameters of fixed-sized deep neural networks using gradient-based optimization techniques. Simple evolutionary algorithms have recently been shown to also be capable of optimizing deep neural network parameters, at times matching the performance of gradient-based techniques, e.g. in reinforcement learning settings. In addition to optimizing network parameters, many evolutionary computation techniques are also capable of progressively constructing network architectures. However, constructing network architectures from elementary evolution rules has not yet been shown to scale to modern reinforcement learning benchmarks. In this paper we therefore propose a new approach in which the architectures of recurrent neural networks dynamically evolve according to a small set of mutation rules. We implement a massively parallel evolutionary algorithm and run experiments on all 19 OpenAI Gym state-based reinforcement learning control tasks. We find that in the majority of cases, dynamic agents match or exceed the performance of gradient-based agents while utilizing orders of magnitude fewer parameters. We believe our work to open avenues for real-life applications where network compactness and autonomous design are of critical importance. We provide our source code, final model checkpoints and full results at github.com/MaximilienLC/nra.Artificial neural networks are computing systems that have become central to the field of artificial intelligence. Through waves of innovation, these networks have gotten bigger, more efficient, and increasingly competent on a wide range of tasks. Most often, the parameters of these artificial neural networks are optimized using first-order gradient-based optimization techniques, yet their architecture is for the most part still constructed manually.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {651–654},
numpages = {4},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@inproceedings{10.1145/3490100.3516473,
author = {Suh, Sangho and An, Pengcheng},
title = {Leveraging Generative Conversational AI to Develop a Creative Learning Environment for Computational Thinking},
year = {2022},
isbn = {9781450391450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490100.3516473},
doi = {10.1145/3490100.3516473},
abstract = {We explore how generative conversational AI can assist students’ learning, creative, and sensemaking process in a visual programming environment where users can create comics from code. The process of visualizing code in terms of comics involves mapping programming language (code) to natural language (story) and then to visual language (of comics). While this process requires users to brainstorm code examples, metaphors, and story ideas, the recent development in generative models introduces an exciting opportunity for learners to harness their creative superpower and researchers to advance our understanding of how generative conversational AI can augment our intelligence in creative learning contexts. We provide an overview of our system and discuss interaction scenarios to demonstrate ways we can partner with generative conversational AI in the context of learning computer programming.},
booktitle = {Companion Proceedings of the 27th International Conference on Intelligent User Interfaces},
pages = {73–76},
numpages = {4},
keywords = {coding strip, comics, generative conversational AI, visual programming environment},
location = {Helsinki, Finland},
series = {IUI '22 Companion}
}

@inproceedings{10.1145/3626111.3628212,
author = {Zhou, Yajie and Yu, Nengneng and Liu, Zaoxing},
title = {Towards Interactive Research Agents for Internet Incident Investigation},
year = {2023},
isbn = {9798400704154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626111.3628212},
doi = {10.1145/3626111.3628212},
abstract = {Investigating Internet incidents involves significant human effort and is limited by the domain knowledge of network researchers and operators. In this paper, we propose to develop computational software agents based on emerging language models (e.g., GPT-4) that can simulate the behaviors of knowledgeable researchers to assist in investigating certain Internet incidents and understanding their impacts. Our agent training framework uses Auto-GPT as an autonomous interface to interact with GPT-4 and gain knowledge by memorizing related information retrieved from online resources. The agent uses the model to reason the investigation questions and continuously performs knowledge testing to see if the conclusion is sufficiently confident or more information is needed. In our preliminary experiment, we build an agent Bob, who studies the impact of solar superstorms on the Internet and draws conclusions similar to those from a recent SIGCOMM paper written by a knowledgeable researcher. We envision this as a first step toward developing a future highly knowledgeable Internet researcher simulacra.},
booktitle = {Proceedings of the 22nd ACM Workshop on Hot Topics in Networks},
pages = {33–40},
numpages = {8},
keywords = {Generative AI, Internet Investigation, Internet Resilience, LLM, Software Agent},
location = {Cambridge, MA, USA},
series = {HotNets '23}
}

@inproceedings{10.1145/3531146.3533138,
author = {Hundt, Andrew and Agnew, William and Zeng, Vicky and Kacianka, Severin and Gombolay, Matthew},
title = {Robots Enact Malignant Stereotypes},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533138},
doi = {10.1145/3531146.3533138},
abstract = {Stereotypes, bias, and discrimination have been extensively documented in Machine Learning (ML) methods such as Computer Vision (CV)&nbsp;[18, 80], Natural Language Processing (NLP)&nbsp;[6], or both, in the case of large image and caption models such as OpenAI CLIP&nbsp;[14]. In this paper, we evaluate how ML bias manifests in robots that physically and autonomously act within the world. We audit one of several recently published CLIP-powered robotic manipulation methods, presenting it with objects that have pictures of human faces on the surface which vary across race and gender, alongside task descriptions that contain terms associated with common stereotypes. Our experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy, at scale. Furthermore, the audited methods are less likely to recognize Women and People of Color. Our interdisciplinary sociotechnical analysis synthesizes across fields and applications such as Science Technology and Society (STS), Critical Studies, History, Safety, Robotics, and AI. We find that robots powered by large datasets and Dissolution Models (sometimes called “foundation models”, e.g. CLIP) that contain humans risk physically amplifying malignant stereotypes in general; and that merely correcting disparities will be insufficient for the complexity and scale of the problem. Instead, we recommend that robot learning methods that physically manifest stereotypes or other harmful outcomes be paused, reworked, or even wound down when appropriate, until outcomes can be proven safe, effective, and just. Finally, we discuss comprehensive policy changes and the potential of new interdisciplinary research on topics like Identity Safety Assessment Frameworks and Design Justice to better understand and address these harms.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {743–756},
numpages = {14},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@inproceedings{10.1145/3527927.3532801,
author = {Jonsson, Martin and Tholander, Jakob},
title = {Cracking the code: Co-coding with AI in creative programming education},
year = {2022},
isbn = {9781450393270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3527927.3532801},
doi = {10.1145/3527927.3532801},
abstract = {This paper presents a study of a group of university students using generative machine learning to translate from natural language to computer code. The study explores how the use of the AI tool can be understood in terms of co-creation, focusing on the one hand on how the tool may serve as a resource for understanding and learning, and on the other hand how the tool affects the creative processes. Findings show how the participants search for a ’correct’ syntax in their instructions to the machine learning tool, and how the inconsistent and erroneous behavior can work as a way to generate clues and inspiration for generating creative expressions. The notion of friction is used to describe how systems like this can serve to both lower thresholds for programming, and also interfere with the creative processes, encouraging reflection and exploration of alternative solutions.},
booktitle = {Proceedings of the 14th Conference on Creativity and Cognition},
pages = {5–14},
numpages = {10},
keywords = {GPT-3, co-creation, generative machine learning, post-human design, programming},
location = {Venice, Italy},
series = {C&amp;C '22}
}

@inproceedings{10.1145/3485447.3512067,
author = {Zhao, Yan and Chen, Xuanhao and Deng, Liwei and Kieu, Tung and Guo, Chenjuan and Yang, Bin and Zheng, Kai and Jensen, Christian S.},
title = {Outlier Detection for Streaming Task Assignment in Crowdsourcing},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3512067},
doi = {10.1145/3485447.3512067},
abstract = {Crowdsourcing aims to enable the assignment of available resources to the completion of tasks at scale. The continued digitization of societal processes translates into increased opportunities for crowdsourcing. For example, crowdsourcing enables the assignment of computational resources of humans, called workers, to tasks that are notoriously hard for computers. In settings faced with malicious actors, detection of such actors holds the potential to increase the robustness of crowdsourcing platform. We propose a framework called Outlier Detection for Streaming Task Assignment that aims to improve robustness by detecting malicious actors. In particular, we model the arrival of workers and the submission of tasks as evolving time series and provide means of detecting malicious actors by means of outlier detection. We propose a novel socially aware Generative Adversarial Network (GAN) based architecture that is capable of contending with the complex distributions found in time series. The architecture includes two GANs that are designed to adversarially train an autoencoder to learn the patterns of distributions in worker and task time series, thus enabling outlier detection based on reconstruction errors. A GAN structure encompasses a game between a generator and a discriminator, where it is desirable that the two can learn to coordinate towards socially optimal outcomes, while avoiding being exploited by selfish opponents. To this end, we propose a novel training approach that incorporates social awareness into the loss functions of the two GANs. Additionally, to improve task assignment efficiency, we propose an efficient greedy algorithm based on degree reduction that transforms task assignment into a bipartite graph matching. Extensive experiments offer insight into the effectiveness and efficiency of the proposed framework.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {1933–1943},
numpages = {11},
keywords = {crowdsourcing, outlier detection, task assignment, time series},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@article{10.1109/TASLP.2023.3235202,
author = {Rohmatillah, Mahdin and Chien, Jen-Tzung},
title = {Hierarchical Reinforcement Learning With Guidance for Multi-Domain Dialogue Policy},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3235202},
doi = {10.1109/TASLP.2023.3235202},
abstract = {Achieving high performance in a multi-domain dialogue system with low computation is undoubtedly challenging. Previous works applying an end-to-end approach have been very successful. However, the computational cost remains a major issue since the large-sized language model using GPT-2 is required. Meanwhile, the optimization for individual components in the dialogue system has not shown promising result, especially for the component of dialogue management due to the complexity of multi-domain state and action representation. To cope with these issues, this article presents an efficient guidance learning where the imitation learning and the hierarchical reinforcement learning (HRL) with human-in-the-loop are performed to achieve high performance via an inexpensive dialogue agent. The behavior cloning with auxiliary tasks is exploited to identify the important features in latent representation. In particular, the proposed HRL is designed to treat each goal of a dialogue with the corresponding sub-policy so as to provide efficient dialogue policy learning by utilizing the guidance from human through action pruning and action evaluation, as well as the reward obtained from the interaction with the simulated user in the environment. Experimental results on ConvLab-2 framework show that the proposed method achieves state-of-the-art performance in dialogue policy optimization and outperforms the GPT-2 based solutions in end-to-end system evaluation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {748–761},
numpages = {14}
}

@article{10.1145/3487045,
author = {Gupta, Manish and Agrawal, Puneet},
title = {Compression of Deep Learning Models for Text: A Survey},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3487045},
doi = {10.1145/3487045},
abstract = {In recent years, the fields of natural language processing (NLP) and information retrieval (IR) have made tremendous progress thanks to deep learning models like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and Long Short-Term Memory (LSTMs) networks, and Transformer&nbsp;[121] based models like Bidirectional Encoder Representations from Transformers (BERT)&nbsp;[24], Generative Pre-training Transformer (GPT-2)&nbsp;[95], Multi-task Deep Neural Network (MT-DNN)&nbsp;[74], Extra-Long Network (XLNet)&nbsp;[135], Text-to-text transfer transformer (T5)&nbsp;[96], T-NLG&nbsp;[99], and GShard&nbsp;[64]. But these models are humongous in size. On the other hand, real-world applications demand small model size, low response times, and low computational power wattage. In this survey, we discuss six different types of methods (Pruning, Quantization, Knowledge Distillation (KD), Parameter Sharing, Tensor Decomposition, and Sub-quadratic Transformer-based methods) for compression of such models to enable their deployment in real industry NLP projects. Given the critical need of building applications with efficient and small models, and the large amount of recently published work in this area, we believe that this survey organizes the plethora of work done by the “deep learning for NLP” community in the past few years and presents it as a coherent story.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {61},
numpages = {55},
keywords = {Model compression, deep learning, pruning, quantization, knowledge distillation, parameter sharing, tensor factorization, sub-quadratic transformers}
}

@inproceedings{10.1145/3543873.3587591,
author = {Bogireddy, Neha Reddy and Suresh, Smriti and Rai, Sunny},
title = {I’m out of breath from laughing! I think? A dataset of COVID-19 Humor and its toxic variants},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587591},
doi = {10.1145/3543873.3587591},
abstract = {Humor is a cognitive construct that predominantly evokes the feeling of mirth. During the COVID-19 pandemic, the situations that arouse out of the pandemic were so incongruous to the world we knew that even factual statements often had a humorous reaction. In this paper, we present a dataset of 2510 samples hand-annotated with labels such as humor style, type, theme, target and stereotypes formed or exploited while creating the humor in addition to 909 memes. Our dataset comprises Reddit posts, comments, Onion news headlines, real news headlines, and tweets. We evaluate the task of humor detection and maladaptive humor detection on state-of-the-art models namely RoBERTa and GPT-3. The finetuned models trained on our dataset show significant gains over zero-shot models including GPT-3 when detecting humor. Even though GPT-3 is good at generating meaningful explanations, we observed that it fails to detect maladaptive humor due to the absence of overt targets and profanities. We believe that the presented dataset will be helpful in designing computational methods for topical humor processing as it provides a unique sample set to study the theory of incongruity in a post-pandemic world. The data is available to research community at https://github.com/smritae01/Covid19_Humor.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {1004–1013},
numpages = {10},
keywords = {COVID-19, Creative Text Processing, Hate Speech, Humor Detection, Maladaptive Humor, Memes, Topical Humor},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3546000.3546007,
author = {Bai, Hao},
title = {Modern Distributed Data-Parallel Large-Scale Pre-training Strategies For NLP models},
year = {2022},
isbn = {9781450396295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546000.3546007},
doi = {10.1145/3546000.3546007},
abstract = {Distributed deep learning is becoming increasingly popular due to the expanding demand for computing resources for deep learning models with a larger amount of parameters. Different from traditional training approaches, data-parallel training allows multiple compute nodes to train large deep learning models simultaneously in order to boost the training efficiency. In this paper, we present and compare six strategies for data-parallel training using PyTorch on the language model GPT-2 with 100M parameters using a qualitative approach. These strategies are Single GPU, Single Parameter Server, Distributed Parameter Server, Horovod, Distributed Parameter Server with Apex mixed-precision strategy, and Horovod with Apex mixed-precision strategy. We also analyze the quantitative experiment results from each strategy. In the end, we draw the conclusion that the Distributed Parameter Server with Apex mixed-precision strategy has the best performance on single node training, while Horovod with Apex is the most robust approach to use when we have single or multiple nodes.},
booktitle = {Proceedings of the 6th International Conference on High Performance Compilation, Computing and Communications},
pages = {44–53},
numpages = {10},
keywords = {Data Parallelism, Distributed Deep Learning, GPT-2, High-Performance Computing, Natural Language Processing, PyTorch},
location = {Jilin, China},
series = {HP3C '22}
}

@inproceedings{10.1145/3581783.3612014,
author = {Wu, Zhenqian and Ren, Yazhou and Pu, Xiaorong and Hao, Zhifeng and He, Lifang},
title = {Generative Neutral Features-Disentangled Learning for Facial Expression Recognition},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612014},
doi = {10.1145/3581783.3612014},
abstract = {Facial expression recognition (FER) plays a critical role in human-computer interaction and affective computing. Traditional FER methods typically rely on comparing the difference between an examined facial expression and a neutral face of the same person to extract the motion of facial features and filter out expression-irrelevant information. With the extensive use of deep learning, the performance of FER has been further improved. However, existing deep learning-based methods rarely utilize neutral faces. To address this gap, we propose a novel deep learning-based FER method called Generative Neutral Features-Disentangled Learning (GNDL), which draws inspiration from the facial feature manifold. Our approach integrates a neutral feature generator (NFG) that generates neutral features in scenarios where the neutral face of the same subject is not available. The NFG uses fine-grained features from examined images as input and produces corresponding neutral features with the same identity. We train the NFG using a neutral feature reconstruction loss to ensure that the generative neutral features are consistent with the actual neutral features. We then disentangle the generative neutral features from the examined features to remove disturbance features and generate an expression deviation embedding for classification. Extensitive experimental results on three popular databases (CK+, Oulu-CASIA, and MMI) demonstrate that our proposed GNDL method outperforms state-of-the-art FER methods.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {4300–4308},
numpages = {9},
keywords = {disturbance-disentangling, facial expression recognition, facial feature manifold, neutral feature generator},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3569219.3569418,
author = {Ghajargar, Maliheh and Bardzell, Jeffrey and Lagerkvist, Love},
title = {A Redhead Walks into a Bar: Experiences of Writing Fiction with Artificial Intelligence},
year = {2022},
isbn = {9781450399555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569219.3569418},
doi = {10.1145/3569219.3569418},
abstract = {Human creativity has been often aided and supported by artificial tools, spanning traditional tools such as ideation cards, pens, and paper, to computed and software. Tools for creativity are increasingly using artificial intelligence to not only support the creative process, but also to act upon the creation with a higher level of agency. This paper focuses on writing fiction as a creative activity and explores human-AI co-writing through a research product, which employs a natural language processing model, the Generative Pre-trained Transformer 3 (GPT-3), to assist the co-authoring of narrative fiction. We report on two progressive – not comparative – autoethnographic studies to attain our own creative practices in light of our engagement with the research product: (1) a co-writing activity initiated by basic textual prompts using basic elements of narrative and (2) a co-writing activity initiated by more advanced textual prompts using elements of narrative, including dialects and metaphors undertaken by one of the authors of this paper who has doctoral training in literature. In both studies, we quickly came up against the limitations of the system; then, we repositioned our goals and practices to maximize our chances of success. As a result, we discovered not only limitations but also hidden capabilities, which not only altered our creative practices and outcomes, but which began to change the ways we were relating to the AI as collaborator.},
booktitle = {Proceedings of the 25th International Academic Mindtrek Conference},
pages = {230–241},
numpages = {12},
keywords = {AI co-creativity, Artificial creativity, Creative writing, Creativity tool, GPT-3, Storytelling},
location = {Tampere, Finland},
series = {Academic Mindtrek '22}
}

@article{10.1145/3538649,
author = {Galteri, Leonardo and Seidenari, Lorenzo and Bongini, Pietro and Bertini, Marco and Bimbo, Alberto Del},
title = {LANBIQUE: LANguage-based Blind Image QUality Evaluation},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3538649},
doi = {10.1145/3538649},
abstract = {Image quality assessment is often performed with deep networks that are fine-tuned to regress a human provided quality score of a given image. Usually, this approach may lack generalization capabilities and, while being highly precise on similar image distribution, it may yield lower correlation on unseen distortions. In particular, they show poor performances, whereas images corrupted by noise, blur, or compression have been restored by generative models. As a matter of fact, evaluation of these generative models is often performed providing anecdotal results to the reader. In the case of image enhancement and restoration, reference images are usually available. Nevertheless, using signal based metrics often leads to counterintuitive results: Highly natural crisp images may obtain worse scores than blurry ones. However, blind reference image assessment may rank images reconstructed with GANs higher than the original undistorted images. To avoid time-consuming human-based image assessment, semantic computer vision tasks may be exploited instead. In this article, we advocate the use of language generation tasks to evaluate the quality of restored images. We refer to our assessment approach as LANguage-based Blind Image QUality Evaluation (LANBIQUE). We show experimentally that image captioning, used as a downstream task, may serve as a method to score image quality, independently of the distortion process that affects the data. Captioning scores are better aligned with human rankings with respect to classic signal based or No-reference image quality metrics. We show insights on how the corruption, by artefacts, of local image structure may steer image captions in the wrong direction.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = oct,
articleno = {116},
numpages = {19},
keywords = {generative models evaluation, GAN, image quality evaluation, image captioning, Image quality enhancement}
}

@inproceedings{10.1145/3575828.3575834,
author = {Qu, Shenghe},
title = {Research and Analysis of Knee Joint Prosthesis Design Based on 3D Simulation Technology Based on Computer Method},
year = {2023},
isbn = {9781450397247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575828.3575834},
doi = {10.1145/3575828.3575834},
abstract = {Knee joint was reconstructed and tibial plateau parameters were measured and to explore the difference of human evolutionary rhythm, to analyze the matching degree of imported knee prosthesis and Chinese tibial plateau osteotomy plane, and to analyze its influence on the design of knee prosthesis. 3D reconstruction refers to the establishment of mathematical models suitable for computer representation and processing of 3D objects, which is the basis of processing, operating and analyzing its properties under the computer environment. The basic research on the 3D structure of the knee joint is helpful for a more comprehensive understanding of the evolution of the human knee joint and the structural differences between people. In this study, 60 patients (120 knees) with non-knee diseases and 20 healthy volunteers (40 knees) were selected from the Department of Orthopedics, Beijing Chaoyang Hospital, Capital Medical University from January 2018 to January 2020, including 46 males (92 knees) and 34 females (68 knees), aged 24-72 years, with an average age of 46.8 years. Bilateral knee CT scan and 3D reconstruction were performed, and 3d tibial images reconstructed were rotated and cut on HP Advantage Workstation 4.3 advanced image Workstation, and linear parameters such as transverse diameter and anteroposterior diameter of tibial plateau osteotomy surface were measured and calculated, and the differences of parameters between men and women were compared. Statistical analysis was performed. The matching degree of three imported components (depuy-PFC Sigma, Link-Gemini MK-II and Zimmer-Nexgen) with the Chinese tibial plateau tolerance surface was evaluated by using the 5mm tolerance range method. The matching rates were compared by χ2 test. The mean cross diameter of tibial plateau was (74.22±2.84)mm in 80 Chinese adults with 160 knees, and the difference was statistically significant (t=12.36, P &lt; 0.01). The mean diameter was (48.15±2.58) mm, and the difference was statistically significant (t=9.48, P &lt; 0.01). There was no significant difference in the matching rates between prosthesis A and B (χ2=1.027, P=0.184), but there were significant differences in the matching rates between prosthesis A and C (χ2= 8.050, P=0.003), and between prosthesis B and C (χ2= 14.672, P=0.000). There is a significant difference between Chinese and Caucasian in the normal bearing surface of tibial plateau. The matching degree between imported knee prosthesis and Chinese tibial plateau osteotomy is generally low. The tibial plateau section of Chinese is relatively round, which suggests that in the course of human evolution, Chinese walked from four limbs to upright earlier than Caucasians.},
booktitle = {Proceedings of the 2022 7th International Conference on Systems, Control and Communications},
pages = {29–36},
numpages = {8},
keywords = {Arthroplasty, Cover rate, Human evolution, Prosthesis, Three-dimensional reconstruction, knee, replacement},
location = {Chongqing, China},
series = {ICSCC '22}
}

@inproceedings{10.1145/3604237.3626838,
author = {Yun, Jiseon and Sohn, Jae Eui and Kyeong, Sunghyon},
title = {Fine-Tuning Pretrained Language Models to Enhance Dialogue Summarization in Customer Service Centers},
year = {2023},
isbn = {9798400702402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604237.3626838},
doi = {10.1145/3604237.3626838},
abstract = {The application of pretrained language models in real-world business domains has gained significant attention. However, research on the practical use of generative artificial intelligence (AI) to address real-world downstream tasks is limited. This study aims to enhance the routine tasks of customer service (CS) representatives, particularly in the finance domain, by applying a fine-tuning method to dialogue summarization in CS centers. KakaoBank handles an average of 15,000 CS calls daily. By employing a fine-tuning method using real-world CS dialogue data, we can reduce the time required to summarize CS dialogues and standardize summarization skills. To ensure effective dialogue summarization in the finance domain, pretrained language models should acquire additional knowledge and skills, such as specific knowledge of financial products, problem-solving abilities, and the capacity to handle emotionally charged customers. In this study, we developed a reference fine-tuned model using Polyglot-Ko (5.8B) as the baseline PLM and a dataset containing a wide range of zero-shot instructions and partially containing summarization instructions. We compared this reference model with another model fine-tuned using KakaoBank’s CS dialogues and summarization data as the instruct dataset. The results demonstrated that the fine-tuned model based on KakaoBank’s internal datasets outperformed the reference model, showing a 199% and 12% improvement in ROUGE-L and RDASS, respectively. This study emphasizes the significance of task-specific fine-tuning using appropriate instruct datasets for effective performance in specific downstream tasks. Considering its practical use, we suggest that fine-tuning using real-world instruct datasets is a powerful and cost-effective technique for developing generative AI in the business domain.},
booktitle = {Proceedings of the Fourth ACM International Conference on AI in Finance},
pages = {365–373},
numpages = {9},
keywords = {Korean language model, dialogue summarization, fine-tuning, instruct tuning},
location = {Brooklyn, NY, USA},
series = {ICAIF '23}
}

@article{10.1145/3597434,
author = {Tang, Zhenjun and Chen, Zhiyuan and Li, Zhixin and Zhong, Bineng and Zhang, Xianquan and Zhang, Xinpeng},
title = {Unifying Dual-Attention and Siamese Transformer Network for Full-Reference Image Quality Assessment},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {1551-6857},
url = {https://doi.org/10.1145/3597434},
doi = {10.1145/3597434},
abstract = {Image Quality Assessment (IQA) is a critical task of computer vision. Most Full-Reference (FR) IQA methods have limitation in the accurate prediction of perceptual qualities of the traditional distorted images and the Generative Adversarial Networks (GANs) based distorted images. To address this issue, we propose a novel method by Unifying Dual-Attention and Siamese Transformer Network (UniDASTN) for FR-IQA. An important contribution is the spatial attention module composed of a Siamese Transformer Network and a feature fusion block. It can focus on significant regions and effectively maps the perceptual differences between the reference and distorted images to a latent distance for distortion evaluation. Another contribution is the dual-attention strategy that exploits channel attention and spatial attention to aggregate features for enhancing distortion sensitivity. In addition, a novel loss function is designed by jointly exploiting Mean Square Error (MSE), bidirectional Kullback–Leibler divergence, and rank order of quality scores. The designed loss function can offer stable training and thus enables the proposed UniDASTN to effectively learn visual perceptual image quality. Extensive experiments on standard IQA databases are conducted to validate the effectiveness of the proposed UniDASTN. The IQA results demonstrate that the proposed UniDASTN outperforms some state-of-the-art FR-IQA methods on the LIVE, CSIQ, TID2013, and PIPAL databases.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jul,
articleno = {205},
numpages = {24},
keywords = {image quality assessment (IQA), dual-attention, siamese network, Transformer}
}

@inproceedings{10.1145/3583780.3614767,
author = {Ma, Denghao and Chen-Chuan Chang, Kevin and Chen, Yueguo and Lv, Xueqiang and Shen, Liang},
title = {A Principled Decomposition of Pointwise Mutual Information for Intention Template Discovery},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614767},
doi = {10.1145/3583780.3614767},
abstract = {With the rise of Artificial Intelligence (AI), question answering systems have become common for users to interact with computers, e.g., ChatGPT and Siri. These systems require a substantial amount of labeled data to train their models. However, the labeled data is scarce and challenging to be constructed. The construction process typically involves two stages: discovering potential sample candidates and manually labeling these candidates. To discover high-quality candidate samples, we study the intention paraphrase template discovery task: Given some seed questions or templates of an intention, discover new paraphrase templates that describe the intention and are diverse to the seeds enough in text. As the first exploration of the task, we identify the new quality requirements, i.e., relevance, divergence and popularity, and identify the new challenges, i.e., the paradox of divergent yet relevant paraphrases, and the conflict of popular yet relevant paraphrases. To untangle the paradox of divergent yet relevant paraphrases, in which the traditional bag of words falls short, we develop usage-centric modeling, which represents a question/template/answer as a bag of usages that users engaged (e.g., up-votes), and uses a usage-flow graph to interrelate templates, questions and answers. To balance the conflict of popular yet relevant paraphrases, we propose a new and principled decomposition for the well-known Pointwise Mutual Information from the usage perspective (usage-PMI), and then develop a Bayesian inference framework over the usage-flow graph to estimate the usage-PMI. Extensive experiments over three large CQA corpora show strong performance advantage over the baselines adopted from paraphrase identification task. We release 885,000 paraphrase templates of high quality discovered by our proposed PMI decomposition model, and the data is available in site https://github.com/Para-Questions/Intention_template_discovery.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {1746–1755},
numpages = {10},
keywords = {Bayesian inference, paraphrasing, pointwise mutual information},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3573381.3597232,
author = {Chatterjee, Jit and Torres Vega, Maria},
title = {Human-Centered and AI-driven Generation of 6-DoF Extended Reality},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573381.3597232},
doi = {10.1145/3573381.3597232},
abstract = {In order to unlock the full potential of Extended Reality (XR) and its application to societal sectors such as health (e.g., training) or Industry 5.0 (e.g., remote control of infrastructure) there is a need for very realistic environments to enhance the presence of the user. However, current photo-realistic content generation methods (such as Light Fields) require a massive amount of data transmission (i.e., ultra-high bandwidths) and extreme computational power for displaying. Thus, they are not suited for interactive immersive and realistic applications. In this research, we hypothesize that is possible to generate realistic dynamic 3D environments by means of Deep Generative Networks. The work will consist of two parts: (1) a computer vision system that generates the 3D environment based on 2D images, and (2) a Human-Computer Interaction system (HCI) that predicts Region of Interest (RoI) for efficient 3D rendering, subjective and objective assessment of user perception (by means of presence) to enhance the 3D scene quality. This work aims to gain insights into how well deep generative methods can create realistic and immersive environments. This will significantly help future developments in realistic and immersive XR content creation.},
booktitle = {Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
pages = {398–401},
numpages = {4},
keywords = {Presence, Extended Reality, Deep Generative Networks, Computer Vision},
location = {Nantes, France},
series = {IMX '23}
}

@inproceedings{10.1145/3544548.3581296,
author = {Scott, Ava Elizabeth and Neumann, Daniel and Niess, Jasmin and Wo\'{z}niak, Pawe\l{} W.},
title = {Do You Mind? User Perceptions of Machine Consciousness},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581296},
doi = {10.1145/3544548.3581296},
abstract = {The prospect of machine consciousness cultivates controversy across media, academia, and industry. Assessing whether non-experts perceive technologies as conscious, and exploring the consequences of this perception, are yet unaddressed challenges in Human Computer Interaction (HCI). To address them, we surveyed 100 people, exploring their conceptualisations of consciousness and if and how they perceive consciousness in currently available interactive technologies. We show that many people already perceive a degree of consciousness in GPT-3, a voice chat bot, and a robot vacuum cleaner. Within participant responses we identified dynamic tensions between denial and speculation, thinking and feeling, interaction and experience, control and independence, and rigidity and spontaneity. These tensions can inform future research into perceptions of machine consciousness and the challenges it represents for HCI. With both empirical and theoretical contributions, this paper emphasises the importance of HCI in an era of machine consciousness, real, perceived or denied.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {374},
numpages = {19},
keywords = {Consciousness, Machine Consciousness, Technology Consciousness},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1145/3622825,
author = {Ye, Fangke and Zhao, Jisheng and Shirako, Jun and Sarkar, Vivek},
title = {Concrete Type Inference for Code Optimization using Machine Learning with SMT Solving},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622825},
doi = {10.1145/3622825},
abstract = {Despite the widespread popularity of dynamically typed languages such as Python, it is well known that they pose significant challenges to code optimization due to the lack of concrete type information. To overcome this limitation, many ahead-of-time optimizing compiler approaches for Python rely on programmers to provide optional type information as a prerequisite for extensive code optimization. Since few programmers provide this information, a large majority of Python applications are executed without the benefit of code optimization, thereby contributing collectively to a significant worldwide wastage of compute and energy resources. In this paper, we introduce a new approach to concrete type inference that is shown to be effective in enabling code optimization for dynamically typed languages, without requiring the programmer to provide any type information. We explore three kinds of type inference algorithms in our approach based on: 1) machine learning models including GPT-4, 2) constraint-based inference based on SMT solving, and 3) a combination of 1) and 2). Our approach then uses the output from type inference to generate multi-version code for a bounded number of concrete type options, while also including a catch-all untyped version for the case when no match is found. The typed versions are then amenable to code optimization. Experimental results show that the combined algorithm in 3) delivers far superior precision and performance than the separate algorithms for 1) and 2). The performance improvement due to type inference, in terms of geometric mean speedup across all benchmarks compared to standard Python, when using 3) is 26.4\texttimes{} with Numba as an AOT optimizing back-end and 62.2\texttimes{} with the Intrepydd optimizing compiler as a back-end. These vast performance improvements can have a significant impact on programmers’ productivity, while also reducing their applications’ use of compute and energy resources.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {249},
numpages = {28},
keywords = {Code Optimization, Machine Learning, Python, Type Inference}
}

@inproceedings{10.1145/3545947.3576339,
author = {Koornneef, Stacey A. and Bradbury, Jeremy S. and Miljanovic, Michael A.},
title = {Run, Llama, Run: A Computational Thinking Game for K-5 Students Designed to Support Equitable Access},
year = {2023},
isbn = {9781450394338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545947.3576339},
doi = {10.1145/3545947.3576339},
abstract = {Computational thinking is now included in K-5 classrooms and this has led to a demand for new interactive and collaborative learning tools that engage a younger audience. Block-based programming and educational games have both been shown to be effective at engaging children, however they have limitations with respect to supporting collaborative learning and equitable access. Our goal in designing Run, Llama, Run was to build on the positive aspects of block-based programming and educational games while also addressing these limitations. Furthermore, we are using Run, Llama, Run as a platform to explore the trade-offs between digital and tangible interfaces to understand how best to support equitable access while maintaining learning, engagement, and collaboration.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1395},
numpages = {1},
keywords = {block-based programming, computational thinking, educational games, equitable access, tangible programming},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3610548.3618175,
author = {Jiang, Yifeng and Won, Jungdam and Ye, Yuting and Liu, C. Karen},
title = {DROP: Dynamics Responses from Human Motion Prior and Projective Dynamics},
year = {2023},
isbn = {9798400703157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610548.3618175},
doi = {10.1145/3610548.3618175},
abstract = {Synthesizing realistic human movements, dynamically responsive to the environment, is a long-standing objective in character animation, with applications in computer vision, sports, and healthcare, for motion prediction and data augmentation. Recent kinematics-based generative motion models offer impressive scalability in modeling extensive motion data, albeit without an interface to reason about and interact with physics. While simulator-in-the-loop learning approaches enable highly physically realistic behaviors, the challenges in training often affect scalability and adoption. We introduce DROP, a novel framework for modeling Dynamics Responses of humans using generative mOtion prior and Projective dynamics. DROP can be viewed as a highly stable, minimalist physics-based human simulator that interfaces with a kinematics-based generative motion prior. Utilizing projective dynamics, DROP allows flexible and simple integration of the learned motion prior as one of the projective energies, seamlessly incorporating control provided by the motion prior with Newtonian dynamics. Serving as a model-agnostic plug-in, DROP enables us to fully leverage recent advances in generative motion models for physics-based motion synthesis. We conduct extensive evaluations of our model across different motion tasks and various physical perturbations, demonstrating the scalability and diversity of responses.},
booktitle = {SIGGRAPH Asia 2023 Conference Papers},
articleno = {18},
numpages = {11},
keywords = {Generative Models, Hybrid Methods, Physics-based Motion Synthesis},
location = {Sydney, NSW, Australia},
series = {SA '23}
}

@article{10.1145/3610099,
author = {Scurto, Hugo and Similowski, Thomas and Bianchini, Samuel and Caramiaux, Baptiste},
title = {Probing Respiratory Care With Generative Deep Learning},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW2},
url = {https://doi.org/10.1145/3610099},
doi = {10.1145/3610099},
abstract = {This paper combines design, machine learning and social computing to explore generative deep learning as both tool and probe for respiratory care. We first present GANspire, a deep learning tool that generates fine-grained breathing waveforms, which we crafted in collaboration with one respiratory physician, attending to joint materialities of human breathing data and deep generative models. We then relate a probe, produced with breathing waveforms generated with GANspire, and led with a group of ten respiratory care experts, responding to its material attributes. Qualitative annotations showed that respiratory care experts interpreted both realistic and ambiguous attributes of breathing waveforms generated with GANspire, according to subjective aspects of physiology, activity and emotion. Semi-structured interviews also revealed experts' broader perceptions, expectations and ethical concerns on AI technology, based on their clinical practice of respiratory care, and reflexive analysis of GANspire. These findings suggest design implications for technological aids in respiratory care, and show how ambiguity of deep generative models can be leveraged as a resource for qualitative inquiry, enabling socio-material research with generative deep learning. Our paper contributes to the CSCW community by broadening how generative deep learning may be approached not only as a tool to design human-computer interactions, but also as a probe to provoke open conversations with communities of practice about their current and speculative uses of AI technology.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {308},
numpages = {34},
keywords = {AI, design research, generative deep learning, respiratory care}
}

@inproceedings{10.1145/3584371.3612953,
author = {Quintana, Felix and Treangen, Todd and Kavraki, Lydia},
title = {Leveraging Large Language Models for Predicting Microbial Virulence from Protein Structure and Sequence},
year = {2023},
isbn = {9798400701269},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584371.3612953},
doi = {10.1145/3584371.3612953},
abstract = {In the aftermath of COVID-19, screening for pathogens has never been a more relevant problem. However, computational screening for pathogens is challenging due to a variety of factors, including (i) the complexity and role of the host, (ii) virulence factor divergence and dynamics, and (iii) population and community-level dynamics. Considering a potential pathogen's molecular interactions, specifically individual proteins and protein interactions can help pinpoint a potential protein of a given microbe to cause disease. However, existing tools for pathogen screening rely on existing annotations (KEGG, GO, etc), making the assessment of novel and unannotated proteins more challenging. Here, we present an LLM-inspired approach that considers protein sequence and structure to predict protein virulence. We present a two-stage model incorporating evolutionary features captured from the DistilProtBert language model and protein structure in a graph convolutional network. Our model performs better than sequence alone for virulence function when high-quality structures are present, thus representing a path forward for virulence prediction of novel and unannotated proteins.},
booktitle = {Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {103},
numpages = {6},
keywords = {protein function, virulence prediction, graph-based models, large language models},
location = {Houston, TX, USA},
series = {BCB '23}
}

@inproceedings{10.1145/3508352.3549437,
author = {Chen, Hanning and Issa, Mariam and Ni, Yang and Imani, Mohsen},
title = {DARL: Distributed Reconfigurable Accelerator for Hyperdimensional Reinforcement Learning},
year = {2022},
isbn = {9781450392174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508352.3549437},
doi = {10.1145/3508352.3549437},
abstract = {Reinforcement Learning (RL) is a powerful technology to solve decisionmaking problems such as robotics control. Modern RL algorithms, i.e., Deep Q-Learning, are based on costly and resource hungry deep neural networks. This motivates us to deploy alternative models for powering RL agents on edge devices. Recently, brain-inspired Hyper-Dimensional Computing (HDC) has been introduced as a promising solution for lightweight and efficient machine learning, particularly for classification.In this work, we develop a novel platform capable of real-time hyperdimensional reinforcement learning. Our heterogeneous CPU-FPGA platform, called DARL, maximizes FPGA's computing capabilities by applying hardware optimizations to hyperdimensional computing's critical operations, including hardware-friendly encoder IP, the hypervector chunk fragmentation, and the delayed model update. Aside from hardware innovation, we also extend the platform to basic single-agent RL to support multi-agents distributed learning. We evaluate the effectiveness of our approach on OpenAI Gym tasks. Our results show that the FPGA platform provides on average 20\texttimes{} speedup compared to current state-of-the-art hyperdimensional RL methods running on Intel Xeon 6226 CPU. In addition, DARL provides around 4.8\texttimes{} faster and 4.2\texttimes{} higher energy efficiency compared to the state-of-the-art RL accelerator while ensuring a better or comparable quality of learning.},
booktitle = {Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design},
articleno = {84},
numpages = {9},
location = {San Diego, California},
series = {ICCAD '22}
}

@inproceedings{10.1145/3605731.3608930,
author = {Cho, Wendy K. Tam and Liu, Yan},
title = {A GPU-Accelerated Population Generation, Sorting, and Mutation Kernel for an Optimization-Based Causal Inference Model},
year = {2023},
isbn = {9798400708428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605731.3608930},
doi = {10.1145/3605731.3608930},
abstract = {We develop a GPU-accelerated machine learning generative adversarial network model that can be used with observational data for the purpose of constructing causal inferences. The theoretical basis of our machine learning model is novel and is conceptualized to be operable and scalable for high performance computing platforms. Our GPU-accelerated code enables large-scale parallelization of the computation within a common and accessible computing environment. This will expand the reach of our model and empower research in new substantive domains while maintaining the underlying theoretical properties.},
booktitle = {Proceedings of the 52nd International Conference on Parallel Processing Workshops},
pages = {167–171},
numpages = {5},
keywords = {Causal Inference, Optimization, Subset Selection},
location = {Salt Lake City, UT, USA},
series = {ICPP Workshops '23}
}

@article{10.1109/TASLP.2023.3321191,
author = {Yang, Runxuan and Peng, Yuyang and Hu, Xiaolin},
title = {A Fast High-Fidelity Source-Filter Vocoder With Lightweight Neural Modules},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3321191},
doi = {10.1109/TASLP.2023.3321191},
abstract = {The quality of raw audio waveform generated by a vocoder could affect various audio generative tasks. In recent years, the dominance of source-filter vocoders was greatly challenged by neural vocoders as the latter presents far superior synthesized audio quality. Meanwhile, neural vocoders introduced unprecedented limitations including low runtime efficiency as well as unstable pitch especially in those without explicit periodic excitation input, while these have never been a problem in source-filter vocoders. We present in this article a novel approach that takes the best from both parties. We start by an in-depth examination of every building block in WORLD – one of the best-performing source-filter vocoders based on plain signal processing algorithms, looking for ones that do not work well, and we replace them with small, lightweight and task-specific neural network models. We also rearranged the vocoding pipeline for a smoother collaboration between building blocks. Our objective and subjective evaluations demonstrate that our methods present competitive synthesized audio quality even when compared against neural vocoders at a much lower computational cost, while keeping spectral envelope acoustic feature, high pitch accuracy as in conventional source-filter vocoders.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {3362–3373},
numpages = {12}
}

@article{10.1145/3578521,
author = {Huang, Jingmin and Chen, Bowei and Yan, Zhi and Ounis, Iadh and Wang, Jun},
title = {GEO: A Computational Design Framework for Automotive Exterior Facelift},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3578521},
doi = {10.1145/3578521},
abstract = {Exterior facelift has become an effective method for automakers to boost the consumers’ interest in an existing car model before it is redesigned. To support the automotive facelift design process, this study develops a novel computational framework&nbsp;– Generator, Evaluator, Optimiser (GEO), which comprises three components: a StyleGAN2-based design generator that creates different facelift designs; a convolutional neural network (CNN)-based evaluator that assesses designs from the aesthetics perspective; and a recurrent neural network (RNN)-based decision optimiser that selects designs to maximise the predicted profit of the targeted car model over time. We validate the GEO framework in experiments with real-world datasets and describe some resulting managerial implications for automotive facelift. Our study makes both methodological and application contributions. First, the generator’s mapping network and projection methods are carefully tailored to facelift where only minor changes are performed without affecting the family signature of the automobile brands. Second, two evaluation metrics are proposed to assess the generated designs. Third, profit maximisation is taken into account in the design selection. From a high-level perspective, our study contributes to the recent use of machine learning and data mining in marketing and design studies. To the best of our knowledge, this is the first study that uses deep generative models for automotive regional design upgrading and that provides an end-to-end decision-support solution for automakers and designers.},
journal = {ACM Trans. Knowl. Discov. Data},
month = mar,
articleno = {82},
numpages = {20},
keywords = {Automotive design, exterior facelift, design generation, aesthetics evaluation, decision optimisation}
}

@inproceedings{10.1145/3469877.3490605,
author = {Seidenari, Lorenzo and Galteri, Leonardo and Bongini, Pietro and Bertini, Marco and Del Bimbo, Alberto},
title = {Language Based Image Quality Assessment},
year = {2022},
isbn = {9781450386074},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469877.3490605},
doi = {10.1145/3469877.3490605},
abstract = {Evaluation of generative models, in the visual domain, is often performed providing anecdotal results to the reader. In the case of image enhancement, reference images are usually available. Nonetheless, using signal based metrics often leads to counterintuitive results: highly natural crisp images may obtain worse scores than blurry ones. On the other hand, blind reference image assessment may rank images reconstructed with GANs higher than the original undistorted images. To avoid time consuming human based image assessment, semantic computer vision tasks may be exploited instead [9, 25, 33]. In this paper we advocate the use of language generation tasks to evaluate the quality of restored images. We show experimentally that image captioning, used as a downstream task, may serve as a method to score image quality. Captioning scores are better aligned with human rankings with respect to signal based metrics or no-reference image quality metrics. We show insights on how the corruption, by artifacts, of local image structure may steer image captions in the wrong direction.},
booktitle = {Proceedings of the 3rd ACM International Conference on Multimedia in Asia},
articleno = {25},
numpages = {7},
keywords = {image quality evaluation, image quality enhancement, image captioning, generative models evaluation, GAN},
location = {Gold Coast, Australia},
series = {MMAsia '21}
}

@inproceedings{10.1145/3539618.3591973,
author = {Feng, Jiazhan and Tao, Chongyang and Shen, Tao and Liu, Chang and Zhao, Dongyan},
title = {Dimension-Prompts Boost Commonsense Consolidation},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591973},
doi = {10.1145/3539618.3591973},
abstract = {Neural knowledge models emerged and advanced common-sense-centric knowledge grounding. They parameterize a small seed curated commonsense knowledge graph (CS-KG) in a language model to generalize more. A current trend is to scale the seed up by directly mixing multiple sources of CS-KG (e.g., ATOMIC, ConceptNet) into one model. But, such brute-force mixing inevitably hinders effective knowledge consolidation due to i) ambiguous, polysemic, and/or inconsistent relations across sources and ii) knowledge learned in an entangled manner despite distinct types (e.g., causal, temporal). To mitigate this, we adopt a concept of commonsense knowledge dimension and propose a brand-new dimension-disentangled knowledge model (D2KM) learning paradigm with multiple sources. That is, a generative language model with dimension-specific soft prompts is trained to disentangle knowledge acquisitions along with different dimensions and facilitate potential intra-dimension consolidation across CS-KG sources. Experiments show our knowledge model outperforms its baselines in both standard and zero-shot scenarios.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1934–1938},
numpages = {5},
keywords = {commonsense knowledge construction, neural knowledge models},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@article{10.1109/TASLP.2023.3285241,
author = {Richter, Julius and Welker, Simon and Lemercier, Jean-Marie and Lay, Bunlong and Gerkmann, Timo},
title = {Speech Enhancement and Dereverberation With Diffusion-Based Generative Models},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3285241},
doi = {10.1109/TASLP.2023.3285241},
abstract = {In this work, we build upon our previous publication and use diffusion-based generative models for speech enhancement. We present a detailed overview of the diffusion process that is based on a stochastic differential equation and delve into an extensive theoretical examination of its implications. Opposed to usual conditional generation tasks, we do not start the reverse process from pure Gaussian noise but from a mixture of noisy speech and Gaussian noise. This matches our forward process which moves from clean speech to noisy speech by including a drift term. We show that this procedure enables using only 30 diffusion steps to generate high-quality clean speech estimates. By adapting the network architecture, we are able to significantly improve the speech enhancement performance, indicating that the network, rather than the formalism, was the main limitation of our original approach. In an extensive cross-dataset evaluation, we show that the improved method can compete with recent discriminative models and achieves better generalization when evaluating on a different corpus than used for training. We complement the results with an instrumental evaluation using real-world noisy recordings and a listening experiment, in which our proposed method is rated best. Examining different sampler configurations for solving the reverse process allows us to balance the performance and computational speed of the proposed method. Moreover, we show that the proposed method is also suitable for dereverberation and thus not limited to additive background noise removal.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {2351–2364},
numpages = {14}
}

@inproceedings{10.1145/3587716.3587760,
author = {Usami, Yoshiyuki and Kitaoka, Kosuke and Shindo, Koichi},
title = {Integrated Artificial Intelligence for Making Digital Human},
year = {2023},
isbn = {9781450398411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587716.3587760},
doi = {10.1145/3587716.3587760},
abstract = {Artificial intelligence is actively researched in various fields such as image recognition, image detection, audio recognition, natural language processing, face expression recognition, and facial expression generation. If we want to create artificial intelligence in the original sense, it will be necessary to integrate these many research results and create a system that can exactly imitate the functions of the human brain. Commercially, the current situation is that integrated AI such as Ameria [2], Uneeq [3], Neon [13], LaMDA [29] and the system using GPT-3 [9] have entered the market. However, there is no research that creates integrated AI with open source in the academic field. This work is an attempt to construct such an integrated AI as an academic research which is in an form of open source. Furthermore, this work is described in a form of multi-processing job with socket connection. Then, execution of the program can be accomplished by multiple computers. For the visual input, object detection is performed by Redman’s YOLO [14]. Next, the system accomplishes Image2text which generates sentences describing the image [34]. The system recognizes the meaning of visual input. As for speech recognition, the question and answering task is activated, and it is possible to give an accurate answer to the question through the microphone [7]. In addition, text generation enables this system to respond to human chattering [5]. This work combines four different sources: visual, text, audio, and scraping outworld news sources. We believe that attempts like this work will become more common in future AI studies.},
booktitle = {Proceedings of the 2023 15th International Conference on Machine Learning and Computing},
pages = {267–273},
numpages = {7},
keywords = {mage2text, question and answering, text generation, visual object detection, visual object recognition},
location = {Zhuhai, China},
series = {ICMLC '23}
}

@inproceedings{10.1145/3583131.3590455,
author = {Nguyen, Thai Huy and Luong, Ngoc Hoang},
title = {Stable and Sample-Efficient Policy Search for Continuous Control via Hybridizing Phenotypic Evolutionary Algorithm with the Double Actors Regularized Critics},
year = {2023},
isbn = {9798400701191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583131.3590455},
doi = {10.1145/3583131.3590455},
abstract = {Evolutionary Reinforcement Learning arises from hybridizing the sample efficiency of policy gradient with the stability of evolutionary computation. Proximal Distilled Evolutionary Reinforcement Learning (PDERL) implements the hybridization by having information transferred between an RL agent operating alongside a population of candidate policies. PDERL employs two phenotype-based variation operators, behavior distillation crossover and proximal mutation, which exhibit better effectiveness compared to traditional genotype-based operators. We demonstrate that the proximal mutation is sensitive to its mutation magnitude hyperparameter, which yields damaging effects if its value is improperly set. Inspired from Differential Evolution, we propose a novel mutation procedure that operates on action vectors generated by candidate policies. The phenotypic differential mutation (PhDM) shows its stability in diversity maintenance with little disruption. A recently-introduced actor-critic policy gradient algorithm, Double Actors Regularized Critics (DARC), exhibits a superior sample efficiency. DARC alleviates both overestimation and underestimation bias via the usage of two actors for better exploration and a dedicated critic regularization technique. In this paper, we restructure PDERL to incorporate PhDM and the policy gradient mechanism of DARC. Experimental results show that our Phenotypic Evolutionary DARC (PhEDARC) outperforms both PDERL and DARC in four control tasks from OpenAI Gym. Ablation studies support our design choices.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1239–1247},
numpages = {9},
keywords = {evolutionary reinforcement learning, variation operators, policy search, continuous control},
location = {Lisbon, Portugal},
series = {GECCO '23}
}

@inproceedings{10.1145/3583780.3614941,
author = {Naddaf, Parmis and Mahmoudzaheh Ahmadi Nejad, Erfaneh and Zahirnia, Kiarash and Jaeger, Manfred and Schulte, Oliver},
title = {Joint Link Prediction Via Inference from a Model},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614941},
doi = {10.1145/3583780.3614941},
abstract = {A Joint Link Prediction Query (JLPQ) specifies a set of links to be predicted, given another set of links as well as node attributes as evidence. While single link prediction has been well studied in literature on deep graph learning, predicting multiple links together has gained little attention. This paper presents a novel framework for computing JLPQs using a probabilistic deep Graph Generative Model. Specifically, we develop inference procedures for an inductively trained Variational Graph Auto-Encoder (VGAE) that estimates the joint link probability for any input JLPQ, without retraining. For evaluation, we apply inference to a range of joint link prediction queries on six benchmark datasets. We find that for most datasets and query types, joint link prediction via inference from a model achieves good predictive performance, better than the independent link prediction baselines (by 0.02-0.4 AUC points depending on the dataset).},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {1877–1886},
numpages = {10},
keywords = {graph convolutional networks, graph representation learning, inference from a model, link prediction},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3600100.3623749,
author = {Almilaify, Yara and Nweye, Kingsley and Nagy, Zoltan},
title = {SCALEX: SCALability EXploration of Multi-Agent Reinforcement Learning Agents in Grid-Interactive Efficient Buildings},
year = {2023},
isbn = {9798400702303},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600100.3623749},
doi = {10.1145/3600100.3623749},
abstract = {Renewable energy transition and decarbonization pose significant challenges for grid-interactive efficient building communities. The optimization of intermittent renewable energy can be achieved using advanced control architecture and energy storage, enhancing energy flexibility. Reinforcement learning (RL) offers potential solutions, but its scalability and computational demands in large-scale settings remain unclear. This paper examines the scalability of Soft-Actor Critic (SAC) in multi-agent systems, comparing decentralized-independent SACs and centralized SACs using CityLearn, an OpenAI Gym environment. We consider neighborhoods consisting of 2 to 64 single-family residential buildings, each equipped with cooling and heating storage devices, domestic hot water storage devices, electrical storage devices, and solar PV systems. Our findings suggest that independent controllers outperform the centralized controller with increasing number of buildings. We also show that the performance on the building level can differ from the aggregated performance.},
booktitle = {Proceedings of the 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {261–264},
numpages = {4},
keywords = {demand response, energy flexibility, multi agent system},
location = {Istanbul, Turkey},
series = {BuildSys '23}
}

@inproceedings{10.5555/3545946.3598962,
author = {Li, Zun and Lanctot, Marc and McKee, Kevin R. and Marris, Luke and Gemp, Ian and Hennes, Daniel and Larson, Kate and Bachrach, Yoram and Wellman, Michael P. and Muller, Paul},
title = {Search-Improved Game-Theoretic Multiagent Reinforcement Learning in General and Negotiation Games},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Multiagent reinforcement learning (MARL) has benefited significantly from population-based and game-theoretic training regimes. One approach, Policy-Space Response Oracles (PSRO), employs standard reinforcement learning to compute response policies via approximate best responses and combines them via meta-strategy selection. We augment PSRO by adding a novel search procedure with generative sampling of world states, and introduce two new meta-strategy solvers based on the Nash bargaining solution. We evaluate PSRO's ability to compute approximate Nash equilibrium, and its performance in negotiation games: Colored Trails and Deal-or-no-Deal. We conduct behavioral studies where human participants negotiate with our agents (N = 346). Search with generative modeling finds stronger policies during both training time and test time, enables online Bayesian co-player prediction, and can produce agents that achieve comparable social welfare negotiating with humans as humans trading among themselves.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {2445–2447},
numpages = {3},
keywords = {alphazero, multiagent, nash bargaining solution, negotiation games, policy-space response oracles, reinforcement learning},
location = {London, United Kingdom},
series = {AAMAS '23}
}

@inproceedings{10.1145/3532106.3533449,
author = {Benabdallah, Gabrielle and Alexander, Ashten and Ghosh, Sourojit and Glogovac-Smith, Chariell and Jacoby, Lacey and Lustig, Caitlin and Nguyen, Anh and Parkhurst, Anna and Reyes, Kathryn and Tan, Neilly H. and Wolcher, Edward and Psarra, Afroditi and Rosner, Daniela},
title = {Slanted Speculations: Material Encounters with Algorithmic Bias},
year = {2022},
isbn = {9781450393584},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532106.3533449},
doi = {10.1145/3532106.3533449},
abstract = {Over the past few years, AI bias has become a central concern within design and computing fields. But as the concept of bias has grown in visibility, its meaning and form have become harder to grasp. To help designers realize bias, we take inspiration from textile bias (the skew of woven material) and examine the topic across its myriad forms: visual, textual, and tactile. By introducing a slanted experience of material and therefore of reality, we explore the translation of fraught machine learning algorithms into personal and probing artifacts. In this pictorial, we present nine pieces that materialize complex relationships with machine learning; ground these relationships in the present and the personal; and point to generative ways of engaging with biased systems around us.},
booktitle = {Proceedings of the 2022 ACM Designing Interactive Systems Conference},
pages = {85–99},
numpages = {15},
keywords = {Algorithmic bias, arts, design practice, machine learning, materiality, speculative design},
location = {Virtual Event, Australia},
series = {DIS '22}
}

@inproceedings{10.1145/3589737.3605985,
author = {Norman-Tenazas, Raphael and Western, Isaac and Vallabha, Gautam and Roos, Matthew J and Johnson, Erik C and Robinson, Brian S},
title = {Enabling local learning for generative-replay-based continual learning with a recurrent model of the insect memory center},
year = {2023},
isbn = {9798400701757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589737.3605985},
doi = {10.1145/3589737.3605985},
abstract = {Continual learning without catastrophic forgetting of previous experiences is an open general challenge for artificial neural networks, but is especially under-explored for artificial neural networks suitable to implement on neuromorphic platforms. An algorithmic understanding of how continual learning occurs in biological neural networks can inform solutions for artificial neural networks, especially in neuromorphic platforms whose biomimetic computing architectures lend themselves to more biofidelic algorithms. In this work, we derive an approach for generative-replay-based continual learning with a three-factor local learning rule based on recurrent connectivity in the insect's memory center, and characterize the model with a CIFAR-100 class incremental continual learning task. First, we investigate the properties of the model's internal representations and find that the high dimensional sparse representations enable this form of generative replay, and that these representations can be binary as required on spiking neuromorphic platforms with little detriment to model performance. Next, we derive a three-factor local learning rule by introducing simplifying assumptions to network updates from error backpropagation optimization which makes the learning rule biologically plausible (i.e., without weight transport) and amenable to neuromorphic implementation. Finally, we find that these simplifications enhance performance during gradient-based optimization for continual learning and when implemented locally achieve this increased performance. Overall, the outcomes of this work can inform a more detailed understanding for continual learning in this biological circuit, as well as introduce general approaches for neuromorphic continual learning.},
booktitle = {Proceedings of the 2023 International Conference on Neuromorphic Systems},
articleno = {25},
numpages = {7},
keywords = {generative replay, local learning, lifelong learning, neural networks, biologically-inspired algorithms},
location = {Santa Fe, NM, USA},
series = {ICONS '23}
}

@inproceedings{10.5555/3571885.3571935,
author = {Wang, Xiaohui and Wei, Yang and Xiong, Ying and Huang, Guyue and Qian, Xian and Ding, Yufei and Wang, Mingxuan and Li, Lei},
title = {LightSeq2: accelerated training for transformer-based models on GPUs},
year = {2022},
isbn = {9784665454445},
publisher = {IEEE Press},
abstract = {Transformer-based neural models are used in many AI applications. Training these models is expensive, as it takes huge GPU resources and long duration. It is challenging because typical data like sentences have variable lengths, and Transformer's computation patterns are more complex than convolutional neural networks. Existing systems either only focus on model inference or optimization for only BERT-like encoder models. In this paper, we present LightSeq2, a system to accelerate training for a general family of Transformer models on GPUs. We propose a series of GPU optimization techniques tailored to the specific computation flow and memory access patterns of Transformer models. LightSeq2 supports many model architectures, including BERT (encoder-only), GPT (decoder-only), Transformer (encoder-decoder), and vision Transformer. Our experiments for a variety of models and benchmarks show that LightSeq2 is consistently faster (1.4--3.5\texttimes{}) than previous systems on different GPUs. In particular, it gains 308% training speedup compared with existing systems on a large public machine translation benchmark (WMT14 English-German).},
booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
articleno = {38},
numpages = {14},
keywords = {GPU acceleration, computer vision, natural language processing, training, transformer},
location = {Dallas, Texas},
series = {SC '22}
}

@inproceedings{10.1145/3573381.3596460,
author = {Maiorca, Antoine and Yoon, Youngwoo and Dutoit, Thierry},
title = {Validating Objective Evaluation Metric: Is Fr\'{e}chet Motion Distance able to Capture Foot Skating Artifacts ?},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573381.3596460},
doi = {10.1145/3573381.3596460},
abstract = {Automatically generating character motion is one of the technologies required for virtual reality, graphics, and robotics. Motion synthesis with deep learning is an emerging research topic. A key component of the development of such an algorithm involves the design of a proper objective metric to evaluate the quality and diversity of the synthesized motion dataset, two key factors of the performance of generative models. The Fr\'{e}chet distance is nowadays a common method to assess this performance. In the motion generation field, the validation of such evaluation methods relies on the computation of the Fr\'{e}chet distance between embeddings of the ground truth dataset and motion samples polluted by synthetic noise to mimic the artifacts produced by generative algorithms. However, the synthetic noise degradation does not fully represent motion perturbations that are commonly perceived. One of these artifacts is foot skating: the unnatural foot slides on the ground during locomotion. In this work-in-progress paper, we tested how well the Fr\'{e}chet Motion Distance (FMD), which was proposed in previous works, is able to measure foot skating artifacts, and we found that FMD is not able to measure efficiently the intensity of the skating degradation.},
booktitle = {Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
pages = {242–247},
numpages = {6},
keywords = {Motion Generation, Generative Model Evaluation, Deep Neural Network},
location = {Nantes, France},
series = {IMX '23}
}

@article{10.1145/3588964,
author = {Nie, Xiaonan and Miao, Xupeng and Wang, Zilong and Yang, Zichao and Xue, Jilong and Ma, Lingxiao and Cao, Gang and Cui, Bin},
title = {FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588964},
doi = {10.1145/3588964},
abstract = {With the increasing data volume, there is a trend of using large-scale pre-trained models to store the knowledge into an enormous number of model parameters. The training of these models is composed of lots of dense algebras, requiring a huge amount of hardware resources. Recently, sparsely-gated Mixture-of-Experts (MoEs) are becoming more popular and have demonstrated impressive pretraining scalability in various downstream tasks. However, such a sparse conditional computation may not be effective as expected in practical systems due to the routing imbalance and fluctuation problems. Generally, MoEs are becoming a new data analytics paradigm in the data life cycle and suffering from unique challenges at scales, complexities, and granularities never before possible.In this paper, we propose a novel DNN training framework, FlexMoE, which systematically and transparently address the inefficiency caused by dynamic dataflow. We first present an empirical analysis on the problems and opportunities of training MoE models, which motivates us to overcome the routing imbalance and fluctuation problems by a dynamic expert management and device placement mechanism. Then we introduce a novel scheduling module over the existing DNN runtime to monitor the data flow, make the scheduling plans, and dynamically adjust the model-to-hardware mapping guided by the real-time data traffic. A simple but efficient heuristic algorithm is exploited to dynamically optimize the device placement during training. We have conducted experiments on both NLP models (e.g., BERT and GPT) and vision models (e.g., Swin). And results show FlexMoE can achieve superior performance compared with existing systems on real-world workloads --- FlexMoE outperforms DeepSpeed by 1.70x on average and up to 2.10x, and outperforms FasterMoE by 1.30x on average and up to 1.45x.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {110},
numpages = {19},
keywords = {deep learning system, distributed computing, sparse model}
}

@inproceedings{10.1145/3615366.3622793,
author = {Zamir, Bukhtawar and Campos, Jo\~{a}o R. and Vieira, Marco},
title = {Advanced Machine Learning for Runtime Data Generation},
year = {2023},
isbn = {9798400708442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615366.3622793},
doi = {10.1145/3615366.3622793},
abstract = {Given the ubiquity of software in everyday critical tasks, ensuring its dependability is of utmost importance. Software faults, which can lead to errors and vulnerabilities, can significantly comprise the target system. Various techniques have been developed to improve the dependability of software-intensive systems, from fault avoidance to fault tolerance. Machine Learning (ML) techniques have been playing a vital role in improving the dependability of systems. Nonetheless, such techniques require significant amounts of data, which are not typically available. To overcome this, various techniques, such as fault injection or intrusion injection, have been proposed to generate realistic data. Still, they are computationally expensive and require considerable expertise. At the same time, a recent growing sub-field of ML is generative models. Generative models offer an innovative solution by creating synthetic data that closely resemble real-world samples. If such models could be used to generate realistic synthetic failure or intrusion data on demand, their value would be significant. Notwithstanding, the feasibility of such an approach has not yet been researched. Generative models have only mostly been used for sequential data (e.g., text or music) or data with high spatial dependency (e.g., images). On the other hand, dependability problems often have high dimensional tabular data, for which generative models are yet to excel, and for which it is also considerably more difficult to assess the representativeness of the generated data. This research will focus on determining the feasibility of using generative techniques to generate runtime data to support dependability research.},
booktitle = {Proceedings of the 12th Latin-American Symposium on Dependable and Secure Computing},
pages = {182–187},
numpages = {6},
keywords = {Machine Learning, Generative Models, Artificial Intelligence},
location = {La Paz, Bolivia},
series = {LADC '23}
}

@inproceedings{10.1145/3610537.3622957,
author = {Tang, Yuying and Sun, Yuqian and Gao, Ze and Pan, Zhijun and Wang, Zhigang and Braud, Tristan and Lee, Chang Hee and Asadipour, Ali},
title = {AI N\"{u}shu (Women's scripts) - An Exploration of Language Emergence in Sisterhood},
year = {2023},
isbn = {9798400703089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610537.3622957},
doi = {10.1145/3610537.3622957},
abstract = {This paper presents "AI N\"{u}shu," an emerging language system inspired by N\"{u}shu (women's scripts), the unique language created and used exclusively by ancient Chinese women who were illiterate under a patriarchy society. Through an interactive art installation, two artificial intelligent (AI) agents continuously observe their environment and communicate with each other, developing a writing system that encodes Chinese. In this system, two AI agents observe the environment through cameras, record the unconscious behaviors of the audience, and generate summaries of their observations through visual recognition. Subsequently, the agent associates the corresponding original N\"{u}shu poetry lines and generates new poetry text through a Language Model (LLM), representing its reflection. To develop their language, they continuously switch roles between the speaker and listener, constantly communicating their reflections, and encrypting a word in the poetry line with their self-created AI N\"{u}shu character, allowing the other to guess and learn. Gradually, they reach a consensus on AI N\"{u}shu, forming a unique "AI N\"{u}shu Dictionary" for machines. This language, algorithmically combined into corresponding characters, has components derived from N\"{u}shu, similar to Chinese characters and traditional textile patterns. Thus, like ancient women, the two agents gradually developed their Chinese writing system, corresponding one-to-one with Chinese characters. In contrast, humans, as the authority of the language system, became an object observed, interpreted, and inspired by machines to stimulate non-human language. This is the first media art project to interpret N\"{u}shu from a computational linguistics perspective, infusing AI and art research with non-English natural language processing, Chinese cultural heritage, and a feminist viewpoint. This encourages the creation of more non-English, linguistically-oriented artworks for diverse cultures. We simulate communication in sisterhood through a multi-agent learning system, which questioned knowledge authority between humans and machines through the lens of language development.},
booktitle = {SIGGRAPH Asia 2023 Art Gallery},
articleno = {4},
numpages = {2},
location = {Sydney, NSW, Australia},
series = {SA '23}
}

@inproceedings{10.5555/3539845.3539929,
author = {Li, Junde and Ghosh, Swaroop},
title = {Scalable variational quantum circuits for autoencoder-based drug discovery},
year = {2022},
isbn = {9783981926361},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {The de novo design of drug molecules is recognized as a time-consuming and costly process, and computational approaches have been applied in each stage of the drug discovery pipeline. Variational autoencoder is one of the computer-aided design methods which explores the chemical space based on an existing molecular dataset. Quantum machine learning has emerged as an atypical learning method that may speed up some classical learning tasks because of its strong expressive power. However, near-term quantum computers suffer from limited number of qubits which hinders the representation learning in high dimensional spaces. We present a scalable quantum generative autoencoder (SQ-VAE) for simultaneously reconstructing and sampling drug molecules, and a corresponding vanilla variant (SQ-AE) for better reconstruction. The architectural strategies in hybrid quantum classical networks such as, adjustable quantum layer depth, heterogeneous learning rates, and patched quantum circuits are proposed to learn high dimensional dataset such as, ligand-targeted drugs. Extensive experimental results are reported for different dimensions including 8x8 and 32x32 after choosing suitable architectural strategies. The performance of quantum generative autoencoder is compared with the corresponding classical counterpart throughout all experiments. The results show that quantum computing advantages can be achieved for normalized low-dimension molecules, and that high-dimension molecules generated from quantum generative autoencoders have better drug properties within the same learning period.},
booktitle = {Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe},
pages = {340–345},
numpages = {6},
keywords = {variational autoencoder, quantum machine learning, drug discovery},
location = {Antwerp, Belgium},
series = {DATE '22}
}

@inproceedings{10.1145/3512290.3528742,
author = {Flores, Diana and Hemberg, Erik and Toutouh, Jamal and O'Reily, Una-May},
title = {Coevolutionary generative adversarial networks for medical image augumentation at scale},
year = {2022},
isbn = {9781450392372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3512290.3528742},
doi = {10.1145/3512290.3528742},
abstract = {Medical image processing can lack images for diagnosis. Generative Adversarial Networks (GANs) provide a method to train generative models for data augmentation. Synthesized images can be used to improve the robustness of computer-aided diagnosis systems. However, GANs are difficult to train due to unstable training dynamics that may arise during the learning process, e.g., mode collapse and vanishing gradients. This paper focuses on Lipizzaner, a GAN training framework that combines spatial coevolution with gradient-based learning, which has been used to mitigate GAN training pathologies. Lipizzaner improves performance by taking advantage of its distributed nature and running at scale. Thus, the Lipizzaner algorithm and implementation robustness can be scaled to high-performance computing (HPC) systems to provide more accurate generative models. We address medical imaging data augmentation to create chest X-Ray images by using Lipizzaner on the HPC infrastructure provided by Oak Ridge National Labs' Summit Supercomputer. The experimental analysis shows improved performance by increasing the scale of the Lipizzaner GAN training. We also demonstrate that distributed coevolutionary learning improves performance even when using suboptimal neural network architectures due to hardware constraints.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {367–376},
numpages = {10},
keywords = {coevolution, generative adversarial networks, high performance computing, medical imaging},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@inproceedings{10.1145/3555776.3578618,
author = {Kim, Jeongho and Lee, Yun-Gyoo and Ko, Donggeun and Kim, Taejune and Ham, Soo-Youn and Woo, Simon S},
title = {MGCMA: Multi-scale Generator with Channel-wise Mask Attention to generate Synthetic Contrast-enhanced Chest Computed Tomography},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3578618},
doi = {10.1145/3555776.3578618},
abstract = {Medical images, including computed tomography (CT) assist doctors and physicians in diagnosing anatomic structures and various internal pathologies. In CT, intravenous contrast media is often applied, which are chemicals developed to aid in the characterization of pathology by enhancing the capabilities of an imaging modality to differentiate between different biological tissues. Especially, with the use of contrast media, thorough examinations of the patients can be possible. However, contrast media can have severe adverse and side effects such as hypersensitive reaction to generalized seizures. Yet, without contrast media, it is difficult to diagnose patients that have disorders in the internal organs. With the help of DNN models, especially generative adversarial network (GAN), contrast-enhanced CT (CECT) images can be synthetically generated from non-contrast CT (NCCT) images. GANs or autoencoder-based models have been proposed to generate contrast-enhanced CT images; however, the synthesized image does not fully reflect and have crucial spots where contrast has not been synthesized. Thus, in order to enhance the quality of the CECT image, we propose MGCMA, a multi-scale generator with a channel-wise mask attention module for generating synthetic CECT images from NCCT images. Our extensive experiments demonstrate that our model outperforms other baseline models in various metrics such as SSIM and LPIPS. Also, generated images from our approach achieve plausible outcomes from the domain experts' (e.g., physicians and radiologists) evaluations.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {575–584},
numpages = {10},
keywords = {medical image synthesis, generative adversarial network, generative models},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3548606.3563493,
author = {Hallman, Roger A.},
title = {Poster EveGAN: Using Generative Deep Learning for Cryptanalysis},
year = {2022},
isbn = {9781450394505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548606.3563493},
doi = {10.1145/3548606.3563493},
abstract = {Cryptography and Machine Learning are two computational science fields that intuitively seem related. Privacy-preserving machine learning-either utilizing encrypted models or learning over encrypted data-is an exploding field thanks to the maturation of primitives such as fully homomorphic encryption and secure multiparty computation. However there has been surprisingly little work on applying recent advances in machine learning to the task of cryptanalysis, the branch of cryptography that studies how cryptographic ciphers can be attacked. In particular, while a cryptographic cipher seeks to keep certain information secret by making it appear random, discerning patterns and structure from random data is a common machine learning task. This paper proposes EveGAN, an approach that treats cryptanalysis as a language translation problem. While treating cipher cracking as a language translation problem has been validated against a handful of classical substitution ciphers, the EveGAN approach builds on these results to create a new class of generative deep learning-based cryptanalysis attacks.},
booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
pages = {3355–3357},
numpages = {3},
keywords = {cryptanalysis, cryptography, forgery/poisoning attacks, generative adversarial networks, language translation},
location = {Los Angeles, CA, USA},
series = {CCS '22}
}

@inproceedings{10.1145/3470496.3527423,
author = {Li, Zheng and Ghodrati, Soroush and Yazdanbakhsh, Amir and Esmaeilzadeh, Hadi and Kang, Mingu},
title = {Accelerating attention through gradient-based learned runtime pruning},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527423},
doi = {10.1145/3470496.3527423},
abstract = {Self-attention is a key enabler of state-of-art accuracy for various transformer-based Natural Language Processing models. This attention mechanism calculates a correlation score for each word with respect to the other words in a sentence. Commonly, only a small subset of words highly correlates with the word under attention, which is only determined at runtime. As such, a significant amount of computation is inconsequential due to low attention scores and can potentially be pruned. The main challenge is finding the threshold for the scores below which subsequent computation will be inconsequential. Although such a threshold is discrete, this paper formulates its search through a soft differentiable regularizer integrated into the loss function of the training. This formulation piggy backs on the back-propagation training to analytically co-optimize the threshold and the weights simultaneously, striking a formally optimal balance between accuracy and computation pruning. To best utilize this mathematical innovation, we devise a bit-serial architecture, dubbed LeOPArd, for transformer language models with bit-level early termination microarchitectural mechanism. We evaluate our design across 43 back-end tasks for MemN2N, BERT, ALBERT, GPT-2, and Vision transformer models. Post-layout results show that, on average, LeOPArd yields 1.9\texttimes{}and 3.9\texttimes{}speedup and energy reduction, respectively, while keeping the average accuracy virtually intact (&lt; 0.2% degradation).},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {902–915},
numpages = {14},
keywords = {accelerators, attention mechanism, deep learning, gradient-based optimization, learned pruning, neural processing units, self-attention, transformer},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3573382.3616033,
author = {Huang, Yuxuan},
title = {The Future of Generative AI: How GenAI Would Change Human-Computer Co-creation in the Next 10 to 15 Years},
year = {2023},
isbn = {9798400700293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573382.3616033},
doi = {10.1145/3573382.3616033},
abstract = {The past few years have witnessed a remarkable advancement in the field of Generative Artificial Intelligence (GenAI), a technology capable of generating new content based on input prompts and existing knowledge. This technology has the potential to revolutionize the way of human-computer co-creation. However, existing research on GenAI has primarily focused on technical aspects, and more research is needed from a design perspective, mainly through speculative and critical design. Therefore, this study aims to explore how GenAI would transform human-computer co-creation in the next 10 to 15 years by means of design fiction and playful critical design. The study will involve (co-)speculative workshops utilizing design fiction, followed by focus groups to gather insights. In addition, this research will examine the user experience issues of interacting with functional GenAI prototypes through playful critical design.},
booktitle = {Companion Proceedings of the Annual Symposium on Computer-Human Interaction in Play},
pages = {322–325},
numpages = {4},
keywords = {AI-generated content, design fiction, generative AI, playful critical design, speculative design},
location = {Stratford, ON, Canada},
series = {CHI PLAY Companion '23}
}

@inproceedings{10.1145/3587716.3587798,
author = {Zhao, Chenjing and Deng, Chuanshuai and Liu, Zhenghui and Zhang, Jiexin and Wu, Yunlong and Wang, Yanzhen and Yi, Xiaodong},
title = {Interpretable Reinforcement Learning of Behavior Trees},
year = {2023},
isbn = {9781450398411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587716.3587798},
doi = {10.1145/3587716.3587798},
abstract = {The interpretability of reinforcement learning (RL) algorithms has become one of the significant challenges for artificial intelligence (AI) researchers. Behavior Trees (BTs) have enabled developers to design AI policies visually and comprehend the agent’s behaviors in robotics and computer games. Combining their strengths, researchers have proposed to utilize the RL algorithm to generate BTs to present learned policies automatically. Existing methods are devoted to the incremental generation or modification of pre-designed BTs. These efforts necessitate specialized knowledge and the manual design of initial BTs. In this paper, we present intelligent generation methods that directly represent the policies generated by Q-learning and its derived algorithms in the form of BTs to enhance the interpretability of RL. We investigate the tradeoff between the size and performance of BTs while attaining interpretability, intending to obtain balanced policies that are easy to comprehend and good in performance. Evaluations in several classic OpenAI Gym environments validate the effectiveness of our methods.},
booktitle = {Proceedings of the 2023 15th International Conference on Machine Learning and Computing},
pages = {492–499},
numpages = {8},
keywords = {Behavior Trees, Interpretability, Reinforcement Learning},
location = {Zhuhai, China},
series = {ICMLC '23}
}

@inproceedings{10.1145/3604237.3626850,
author = {Skalski, Piotr and Sutton, David and Burrell, Stuart and Perez, Iker and Wong, Jason},
title = {Towards a Foundation Purchasing Model: Pretrained Generative Autoregression on Transaction Sequences},
year = {2023},
isbn = {9798400702402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604237.3626850},
doi = {10.1145/3604237.3626850},
abstract = {Machine learning models underpin many modern financial systems for use cases such as fraud detection and churn prediction. Most are based on supervised learning with hand-engineered features, which relies heavily on the availability of labelled data. Large self-supervised generative models have shown tremendous success in natural language processing and computer vision, yet so far they haven’t been adapted to multivariate time series of financial transactions. In this paper, we present a generative pretraining method that can be used to obtain contextualised embeddings of financial transactions. Benchmarks on public datasets demonstrate that it outperforms state-of-the-art self-supervised methods on a range of downstream tasks. We additionally perform large-scale pretraining of an embedding model using a corpus of data from 180 issuing banks containing 5.1 billion transactions and apply it to the card fraud detection problem on hold-out datasets. The embedding model significantly improves value detection rate at high precision thresholds and transfers well to out-of-domain distributions.},
booktitle = {Proceedings of the Fourth ACM International Conference on AI in Finance},
pages = {141–149},
numpages = {9},
keywords = {fraud detection, generative modelling, multivariate time series, self-supervised learning, transaction embeddings},
location = {Brooklyn, NY, USA},
series = {ICAIF '23}
}

@inproceedings{10.1145/3548606.3560599,
author = {Si, Wai Man and Backes, Michael and Blackburn, Jeremy and De Cristofaro, Emiliano and Stringhini, Gianluca and Zannettou, Savvas and Zhang, Yang},
title = {Why So Toxic? Measuring and Triggering Toxic Behavior in Open-Domain Chatbots},
year = {2022},
isbn = {9781450394505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548606.3560599},
doi = {10.1145/3548606.3560599},
abstract = {Chatbots are used in many applications, e.g., automated agents, smart home assistants, interactive characters in online games, etc. Therefore, it is crucial to ensure they do not behave in undesired manners, providing offensive or toxic responses to users. This is not a trivial task as state-of-the-art chatbot models are trained on large, public datasets openly collected from the Internet. This paper presents a first-of-its-kind, large-scale measurement of toxicity in chatbots. We show that publicly available chatbots are prone to providing toxic responses when fed toxic queries. Even more worryingly, some non-toxic queries can trigger toxic responses too. We then set out to design and experiment with an attack, ToxicBuddy, which relies on fine-tuning GPT-2 to generate non-toxic queries that make chatbots respond in a toxic manner. Our extensive experimental evaluation demonstrates that our attack is effective against public chatbot models and outperforms manually-crafted malicious queries proposed by previous work. We also evaluate three defense mechanisms against ToxicBuddy, showing that they either reduce the attack performance at the cost of affecting the chatbot's utility or are only effective at mitigating a portion of the attack. This highlights the need for more research from the computer security and online safety communities to ensure that chatbot models do not hurt their users. Overall, we are confident that ToxicBuddy can be used as an auditing tool and that our work will pave the way toward designing more effective defenses for chatbot safety.},
booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2659–2673},
numpages = {15},
keywords = {dialogue system, online toxicity, trustworthy machine learning},
location = {Los Angeles, CA, USA},
series = {CCS '22}
}

@inproceedings{10.1145/3583133.3596362,
author = {Babaagba, Kehinde O. and Wylie, Jordan},
title = {An Evolutionary based Generative Adversarial Network Inspired Approach to Defeating Metamorphic Malware},
year = {2023},
isbn = {9798400701207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583133.3596362},
doi = {10.1145/3583133.3596362},
abstract = {Defeating dangerous families of malware like polymorphic and metamorphic malware have become well studied due to their increased attacks on computer systems and network. Traditional Machine Learning (ML) models have been used in detecting this malware, however they are often not resistant to future attacks. In this paper, an Evolutionary based Generative Adversarial Network (GAN) inspired approach is proposed as a step towards defeating metamorphic malware. This method uses an Evolutionary Algorithm as a generator to create malware that are designed to fool a detector, a deep learning model into classifying them as benign. We employ a personal information stealing malware family (Dougalek) as a testbed, selected based on its malicious payload and evaluate the samples generated based on their adversarial accuracy, measured based on the number of Antivirus (AV) engines they are able to fool and their ability to fool a set of ML detectors (k-Nearest Neighbors algorithm, Support Vector Machine, Decision Trees, and Multi-Layer Perceptron). The results show that the adversarial samples are on average able to fool 63% of the AV engines and the ML detectors are susceptible to the new mutants achieving an accuracy between 60%-77%.},
booktitle = {Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
pages = {1753–1759},
numpages = {7},
keywords = {metamorphic malware, evolutionary algorithm, generative adversarial network},
location = {Lisbon, Portugal},
series = {GECCO '23 Companion}
}

@inproceedings{10.1145/3539618.3591923,
author = {B\'{e}n\'{e}dict, Garbiel and Zhang, Ruqing and Metzler, Donald},
title = {Gen-IR@SIGIR 2023: The First Workshop on Generative Information Retrieval},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591923},
doi = {10.1145/3539618.3591923},
abstract = {Generative information retrieval (IR) has experienced substantial growth across multiple research communities (e.g., information retrieval, computer vision, natural language processing, and machine learning), and has been highly visible in the popular press. Theoretical, empirical, and actual user-facing products have been released that retrieve documents (via generation) or directly generate answers given an input request. We would like to investigate whether end-to-end generative models are just another trend or, as some claim, a paradigm change for IR. This necessitates new metrics, theoretical grounding, evaluation methods, task definitions, models, user interfaces, etc. The goal of this workshop1 is to focus on previously explored Generative IR techniques like document retrieval and direct Grounded Answer Generation, while also offering a venue for the discussion and exploration of how Generative IR can be applied to new domains like recommendation systems, summarization, etc. The format of the workshop is interactive, including roundtable and keynote sessions and tends to avoid the one-sided dialogue of a mini-conference.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3460–3463},
numpages = {4},
keywords = {generative models, information retrieval, large language models},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3574131.3574448,
author = {Zhou, Lanfeng and Ji, Xiaoyun and Li, Ling},
title = {Monocular Human Body Shape Estimation: A Generation-aid Approach},
year = {2023},
isbn = {9798400700316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3574131.3574448},
doi = {10.1145/3574131.3574448},
abstract = {Observing human beings from monocular images is one of the basic tasks of computer vision. Reconstructing human bodies from monocular images mainly includes the reconstruction of posture and body shape. However, in the past studies, researchers were more interested in pose estimation, ignoring the study of body shape, and this paper focuses on the estimation of the body shape of a 3D model. Learning body parameters via instance segmentation requires a large number of labels. While the parameters based on pose estimation are completely based on the results of key points detection, which effect is not friendly for pictures with poor angles and low resolution. In response to the above problems, we propose a method to automatically generate datasets. The dataset provides low-resolution images and labels of various angles and blurred shapes. On the generated low-resolution and poorly angled dataset, we propose a generative-assisted deep learning network framework. Experiments show that the framework can effectively estimate the body shape parameters of the model from monocular images.},
booktitle = {Proceedings of the 18th ACM SIGGRAPH International Conference on Virtual-Reality Continuum and Its Applications in Industry},
articleno = {19},
numpages = {8},
keywords = {3D human reconstruction, Body shape, Deep learning, Generative network, Monocular image},
location = {Guangzhou, China},
series = {VRCAI '22}
}

@inproceedings{10.1145/3581783.3612508,
author = {Wang, Jionghao and Chen, Ziyu and Ling, Jun and Xie, Rong and Song, Li},
title = {360-Degree Panorama Generation from Few Unregistered NFoV Images},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612508},
doi = {10.1145/3581783.3612508},
abstract = {360° panoramas are extensively utilized as environmental light sources in computer graphics. However, capturing a 360° \texttimes{} 180° panorama poses challenges due to the necessity of specialized and costly equipment, and additional human resources. Prior studies develop various learning-based generative methods to synthesize panoramas from a single Narrow Field-of-View (NFoV) image, but they are limited in alterable input patterns, generation quality, and controllability. To address these issues, we propose a novel pipeline called PanoDiff, which efficiently generates complete 360° panoramas using one or more unregistered NFoV images captured from arbitrary angles. Our approach has two primary components to overcome the limitations. Firstly, a two-stage angle prediction module to handle various numbers of NFoV inputs. Secondly, a novel latent diffusion-based panorama generation model uses incomplete panorama and text prompts as control signals and utilizes several geometric augmentation schemes to ensure geometric properties in generated panoramas. Experiments show that PanoDiff achieves state-of-the-art panoramic generation quality and high controllability, making it suitable for applications such as content editing.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {6811–6821},
numpages = {11},
keywords = {360-degree panorama, generative models, image pose estimation, latent diffusion, multimodal models},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@article{10.5555/3648699.3648884,
author = {Siry, Rodrigue and Webster, Ryan and Simon, Loic and Rabin, Julien},
title = {On the theoretical equivalence of several trade-off curves assessing statistical proximity},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {The recent advent of powerful generative models has triggered the renewed development of quantitative measures to assess the proximity of two probability distributions. As the scalar Frechet Inception Distance remains popular, several methods have explored computing entire curves, which reveal the trade-off between the fidelity and variability of the first distribution with respect to the second one. Several of such variants have been proposed independently and while intuitively similar, their relationship has not yet been made explicit. In an effort to make the emerging picture of generative evaluation more clear, we propose a unification of four curves known respectively as: the Precision-Recall (PR) curve, the Lorenz curve, the Receiver Operating Characteristic (ROC) curve and a special case of R\'{e}nyi divergence frontiers. In addition, we discuss possible links between PR / Lorenz curves with the derivation of domain adaptation bounds.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {185},
numpages = {34},
keywords = {trade-off curve, distributional closeness, generative modeling, domain adaptation}
}

@inproceedings{10.1145/3577193.3593712,
author = {Randall, Thomas and Koo, Jaehoon and Videau, Brice and Kruse, Michael and Wu, Xingfu and Hovland, Paul and Hall, Mary and Ge, Rong and Balaprakash, Prasanna},
title = {Transfer-learning-based Autotuning using Gaussian Copula},
year = {2023},
isbn = {9798400700569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577193.3593712},
doi = {10.1145/3577193.3593712},
abstract = {As diverse high-performance computing (HPC) systems are built, many opportunities arise for applications to solve larger problems than ever before. Given the significantly increased complexity of these HPC systems and application tuning, empirical performance tuning, such as autotuning, has emerged as a promising approach in recent years. Despite its effectiveness, autotuning is often a computationally expensive approach. Transfer learning (TL)-based autotuning seeks to address this issue by leveraging the data from prior tuning. Current TL methods for autotuning spend significant time modeling the relationship between parameter configurations and performance, which is ineffective for few-shot (that is, few empirical evaluations) tuning on new tasks. We introduce the first generative TL-based autotuning approach based on the Gaussian copula (GC) to model the high-performing regions of the search space from prior data and then generate high-performing configurations for new tasks. This allows a sampling-based approach that maximizes few-shot performance and provides the first probabilistic estimation of the few-shot budget for effective TL-based autotuning. We compare our generative TL approach with state-of-the-art autotuning techniques on several benchmarks. We find that the GC is capable of achieving 64.37% of peak few-shot performance in its first evaluation. Furthermore, the GC model can determine a few-shot transfer budget that yields up to 33.39\texttimes{} speedup, a dramatic improvement over the 20.58\texttimes{} speedup using prior techniques.},
booktitle = {Proceedings of the 37th ACM International Conference on Supercomputing},
pages = {37–49},
numpages = {13},
keywords = {gaussian copula, few-shot learning, autotuning, transfer learning},
location = {Orlando, FL, USA},
series = {ICS '23}
}

@inproceedings{10.1145/3581792.3581802,
author = {Ravikumar, Aswathy and Sriraman, Harini},
title = {Single Node Acceleration of Generative Adversarial Networks using HPC for Image Analytics},
year = {2023},
isbn = {9781450397612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581792.3581802},
doi = {10.1145/3581792.3581802},
abstract = {Generative Adversarial Networks (GAN) are approaches that are utilized for data augmentation, which facilitates the development of more accurate detection models for unusual or unbalanced datasets. Computer-assisted diagnostic methods may be made more reliable by using synthetic pictures generated by GAN. Generative adversarial networks are challenging to train because too unpredictable training dynamics may occur throughout the learning process, such as model collapse and vanishing gradients. For accurate and faster results the GAN network need to trained in parallel and distributed manner. We enhance the speed and precision of the Deep Convolutional Generative Adversarial Networks (DCGAN) architecture by using its parallelism and executing it on High-Performance Computing platforms. The effective analysis of a DCGAN in Graphic Processing Unit and Tensor Processing Unit platforms in which each layer execution pattern is analyzed. The bottleneck is identified for the GAN structure for each execution platforms. The Central Processing Unit is capable of processing neural network models, but it requires a great deal of time to do it. Graphic Processing Unit in contrast, side, are a hundred times quicker than CPUs for Neural Networks, however, they are prohibitively expensive compared to CPUs. Using the systolic array structure, TPU performs well on neural networks with high batch sizes but in GAN the shift between CPU and TPU is huge so it does not perform well.},
booktitle = {Proceedings of the 2022 5th International Conference on Computational Intelligence and Intelligent Systems},
pages = {54–59},
numpages = {6},
keywords = {Tensor Processing Unit, High-Performance Computing, Graphic Processing Unit, Generative Adversarial Network},
location = {Quzhou, China},
series = {CIIS '22}
}

@inproceedings{10.1145/3573942.3573974,
author = {Lomurno, Eugenio and Archetti, Alberto and Cazzella, Lorenzo and Samele, Stefano and Di Perna, Leonardo and Matteucci, Matteo},
title = {SGDE: Secure Generative Data Exchange for Cross-Silo Federated Learning},
year = {2023},
isbn = {9781450396899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573942.3573974},
doi = {10.1145/3573942.3573974},
abstract = {Privacy regulation laws, such as GDPR, impose transparency and security as design pillars for data processing algorithms. In this context, federated learning is one of the most influential frameworks for privacy-preserving distributed machine learning, achieving astounding results in many natural language processing and computer vision tasks. Several federated learning frameworks employ differential privacy to prevent private data leakage to unauthorized parties and malicious attackers. Many studies, however, highlight the vulnerabilities of standard federated learning to poisoning and inference, thus raising concerns about potential risks for sensitive data. To address this issue, we present SGDE, a generative data exchange protocol that improves user security and machine learning performance in a cross-silo federation. The core of SGDE is to share data generators with strong differential privacy guarantees trained on private data instead of communicating explicit gradient information. These generators synthesize an arbitrarily large amount of data that retain the distinctive features of private samples but differ substantially. In this work, SGDE is tested in a cross-silo federated network on images and tabular datasets, exploiting beta-variational autoencoders as data generators. From the results, the inclusion of SGDE turns out to improve task accuracy and fairness, as well as resilience to the most influential attacks on federated learning.},
booktitle = {Proceedings of the 2022 5th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {205–214},
numpages = {10},
keywords = {Deep learning, Differential privacy, Federated learning, Generative deep learning, Gradient leakage, Privacy},
location = {Xiamen, China},
series = {AIPR '22}
}

@inproceedings{10.1145/3505170.3511034,
author = {Ren, Haoxing},
title = {Embracing Machine Learning in EDA},
year = {2022},
isbn = {9781450392105},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3505170.3511034},
doi = {10.1145/3505170.3511034},
abstract = {The application of machine learning (ML) in EDA is a hot research trend. To use ML in EDA, it is nature to think from the ML method point of view, i.e. supervised learning, reinforcement learning and unsupervised learning. Based on this point of view, we can roughly classify the ML applications in EDA into three categories: prediction, optimization, and generation. The prediction category applies supervised learning methods to predict design quality of result (QoR) metrics. There are two kinds of QoR metrics that benefit from the prediction. One kind of metrics are those that can be determined at the current design stage but calculating them consumes a lot of computing resources. For ex-ample, [11] [12] leverage ML to predict circuit power consumption without expensive simulations. The other kind of metrics are those that depend on future design stages. For example, [8] predicts post layout parasitics from schematic of analog circuits. The optimization category applies Bayesian Optimization (BO)and reinforcement learning (RL) to directly optimize EDA problems.BO treats the optimization objective as a blackbox function and tries to find optimal solutions by iteratively sampling the solution space. For example, [5] proposes to use BO with graph embedding and neural network-based surrogate model to size analog circuits. RL treats the optimization objective as the reward from an environment, and trains agents to maximize the reward. [7] proposes to use RL to optimize macro placement, and [9] proposes to use RL to optimize parallel prefix circuit structures. The generation category applies generative models such as generative adversarial networks (GANs) to directly generate solutions to EDA problems. Generative models can learn from previous optimized data distribution and generate solutions for a new problem instance without going through iterative processes like BO or RL. For example, [10] builds a conditional GAN model that learns to generate optical proximity correction (OPC) layout from the original mask.},
booktitle = {Proceedings of the 2022 International Symposium on Physical Design},
pages = {55–56},
numpages = {2},
keywords = {machine learning, electronic design automation, deep learning},
location = {Virtual Event, Canada},
series = {ISPD '22}
}

@inproceedings{10.1145/3565995.3566022,
author = {Feighelstein, Marcelo and Kovalyo, Einat and Abrams, Jennifer and Byosiere, Sarah-Elisabeth and Zamansky, Anna},
title = {Do AI Models “Like" Black Dogs? Towards Exploring Perceptions of Dogs with Vision-Language Models},
year = {2023},
isbn = {9781450398305},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565995.3566022},
doi = {10.1145/3565995.3566022},
abstract = {Large-scale, pretrained vision-language models such as OpenAI’s CLIP are a game changer in Computer Vision due to their unprecedented ‘zero-shot’ image classification capabilities. As they are pretrained on huge amounts of unsupervised web-scraped data, they suffer from inherent biases reflecting human perceptions, norms and beliefs. This position paper aims to highlight the potential of studying models such as CLIP in the context of human-animal relationships, in particular for understanding human perceptions and preferences with respect to physical attributes of pets and their adoptability.},
booktitle = {Proceedings of the Ninth International Conference on Animal-Computer Interaction},
articleno = {7},
numpages = {6},
keywords = {animal-assisted reading, animal-computer interaction, app design, child, support dog},
location = {Newcastle-upon-Tyne, United Kingdom},
series = {ACI '22}
}

@inproceedings{10.1145/3533271.3561669,
author = {Feng, Yichen and Min, Ming and Fouque, Jean-Pierre},
title = {Deep Learning for Systemic Risk Measures},
year = {2022},
isbn = {9781450393768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533271.3561669},
doi = {10.1145/3533271.3561669},
abstract = {The aim of this paper is to study a new methodological framework for systemic risk measures by applying deep learning method as a tool to compute the optimal strategy of capital allocations. Under this new framework, systemic risk measures can be interpreted as the minimal amount of cash that secures the aggregated system by allocating capital to the single institutions before aggregating the individual risks. This problem has no explicit solution except in very limited situations. Deep learning is increasingly receiving attention in financial modelings and risk management and we propose our deep learning based algorithms to solve both the primal and dual problems of the risk measures, and thus to learn the fair risk allocations. In particular, our method for the dual problem involves the training philosophy inspired by the well-known Generative Adversarial Networks (GAN) approach and a newly designed direct estimation of Radon-Nikodym derivative. We close the paper with substantial numerical studies of the subject and provide interpretations of the risk allocations associated to the systemic risk measures. In the particular case of exponential preferences, numerical experiments demonstrate excellent performance of the proposed algorithm, when compared with the optimal explicit solution as a benchmark.},
booktitle = {Proceedings of the Third ACM International Conference on AI in Finance},
pages = {62–69},
numpages = {8},
keywords = {GAN, Radon-Nikodym derivative, banking system, deep learning, risk allocations, risk modeling and risk management, systemic risk measure},
location = {New York, NY, USA},
series = {ICAIF '22}
}

@inproceedings{10.1145/3544549.3577061,
author = {Lehmann, Florian},
title = {Mixed-Initiative Interaction with Computational Generative Systems},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3577061},
doi = {10.1145/3544549.3577061},
abstract = {Machine learning models provide functions to transform and generate image and text data. This promises powerful applications but it remains unclear how users can interact with these models. With my research, I focus on designing, implementing, and evaluating functional prototypes for understanding human-AI interactions. Methodologically, I focus on web-based experiments with a mixed-methods approach. Furthermore, I use these prototypes and generative models as a material to understand fundamental concepts in human-AI interactions, such as initiative, intent, and control. In an already conducted study, for example, I showed that the levels of initiative and control afforded by the UI influence perceived authorship when writing text. For the future, I plan to carry out more studies on collaborative writing. With my dissertation, I contribute to how we will build human-AI interactions and how we will collaborate with computational generative systems in future.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {501},
numpages = {6},
keywords = {control, functional prototypes, generative systems, human-ai interaction, initiative, intent, language model, mixed-initiative, text generation, typing, writing},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3544549.3577036,
author = {Yakura, Hiromu},
title = {A Generative Framework for Designing Interactions to Overcome the Gaps between Humans and Imperfect AIs Instead of Improving the Accuracy of the AIs},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3577036},
doi = {10.1145/3544549.3577036},
abstract = {My research focuses on improving human-machine collaboration in the context of machine learning, particularly by recognizing the limitations and potential for errors in machine learning techniques and designing effective interactions for filling the gaps between humans and them. To this end, I have explored the application of machine learning in a variety of domains, such as malware analysis, music recommendation, conversation analysis, photo editing, and video-based learning. I also worked on clarifying the limitations of the current technologies by using adversarial approaches and qualitative methods. My thesis is planned to synthesize what I learned from these projects into design principles for constructing interactions that take full advantage of imperfect machine learning models. I particularly put emphasis on deriving principles that do not depend on the fine-tuning of the models, thereby providing a generative framework allowing researchers and practitioners to design a range of effective intelligent interactions without incurring significant computational and data collection costs.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {479},
numpages = {5},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.5555/3539845.3540016,
author = {Yang, Tao and Li, Dongyue and Song, Zhuoran and Zhao, Yilong and Liu, Fangxin and Wang, Zongwu and He, Zhezhi and Jiang, Li},
title = {DTQAtten: leveraging &lt;u&gt;d&lt;/u&gt;ynamic &lt;u&gt;t&lt;/u&gt;oken-based quantization for efficient &lt;u&gt;atten&lt;/u&gt;tion architecture},
year = {2022},
isbn = {9783981926361},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {Models based on the attention mechanism, i.e. transformers, have shown extraordinary performance in Natural Language Processing (NLP) tasks. However, their memory footprint, inference latency, and power consumption are still prohibitive for efficient inference at edge devices, even at data centers. To tackle this issue, we present an algorithm-architecture co-design with dynamic and mixed-precision quantization, DTQAtten. We present empirically that the tolerance to the noise varies from token to token in attention-based NLP models. This finding leads us to quantize different tokens with mixed levels of bits. Thus, we design a compression framework that (i) dynamically quantizes tokens while they are forwarded in the models and (ii) jointly determines the ratio of each precision. Moreover, due to the dynamic mixed-precision tokens caused by our framework, previous matrix-multiplication accelerators (e.g. systolic array) cannot effectively exploit the benefit of the compressed attention computation. We thus design our accelerator with the variable-speed systolic array (VSSA) and propose an effective optimization strategy to alleviate the pipeline-stall problem in VSSA without hardware overhead. We conduct experiments with existing attention-based NLP models, including BERT and GPT-2 on various language tasks. Our results show that DTQAtten outperforms the previous neural network accelerator Eyeriss by 13.12\texttimes{} in terms of speedup and 3.8\texttimes{} in terms of energy-saving. Compared with the state-of-the-art attention accelerator SpAtten, our DTQAtten achieves at least 2.65\texttimes{} speedup and 3.38\texttimes{} energy efficiency improvement.},
booktitle = {Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe},
pages = {700–705},
numpages = {6},
keywords = {algorithm-architecture co-design, domain-specific accelerator, dynamic quantization, transformers},
location = {Antwerp, Belgium},
series = {DATE '22}
}

@inproceedings{10.1145/3557915.3560994,
author = {Zhang, Liming and Zhao, Liang and Pfoser, Dieter},
title = {Factorized deep generative models for end-to-end trajectory generation with spatiotemporal validity constraints},
year = {2022},
isbn = {9781450395298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3557915.3560994},
doi = {10.1145/3557915.3560994},
abstract = {A growing number of research areas such as location-based social networks, intelligent transportation systems, and urban computing utilize large amounts of trajectory data for benchmarking data management approaches and analysis methods. Given the general lackness of available large datasets, realistic synthetic trajectory datasets become important. This work proposes deep generative models for trajectory data that can learn disentangled models for sophisticated latent patterns. Existing methods rely on predefined heuristics and cannot learn the unknown underlying generative mechanisms. The proposed novel deep generative VAE-like models factorize global and local semantics (habits vs. random routing change). We further develop new inference strategies based on variational inference and constrained optimization to encapsulate spatiotemporal validity. New deep neural network architectures are developed to implement generative and inference models with dynamic latent priors. The proposed methods represent significant quantitative and qualitative improvements over existing approaches as demonstrated by extensive experiments. The software is made publicly available 1.},
booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
articleno = {59},
numpages = {12},
keywords = {variational autoencoders, spatiotemporal-validity constraint, end-to-end trajectory generation, deep generative models},
location = {Seattle, Washington},
series = {SIGSPATIAL '22}
}

@inproceedings{10.1145/3532106.3533464,
author = {Wu, Shengzhi and Byrne, Daragh and Du, Ruofei and Steenson, Molly Wright},
title = {“Slurp” Revisited: Using ‘system re-presencing’ to look back on, encounter, and design with the history of spatial interactivity and locative media},
year = {2022},
isbn = {9781450393584},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532106.3533464},
doi = {10.1145/3532106.3533464},
abstract = {Hand-based gestural interaction in augmented reality (AR) is an increasingly popular mechanism for spatial interactions. However, it presents many challenges. For example, most hand gesture interactions work well for interactions with virtual content and interfaces, but seldom work with physical devices and users’ environment. To explore this, and rather than inventing new paradigms for AR interactions, this paper revisits Zigelbaum, Kumpf, Vazquez, and Ishii’s 2008 project ‘Slurp’ [72] - a physical eyedropper to interact with digital content from IoT devices. We revive this historical work in a new modality of AR through a five step process: re-presecencing, design experimentation, scenario making, expansion through generative engagements with designers, and reflection. For the designers we engaged, looking back and designing with a restored prototype helped increased understanding of interactive strategies, intentions and rationales of original work. By revisiting Slurp, we also found many new potentials of its metaphorical interactions that could be applied in the context of emerging spatial computing platforms (e.g., smart home devices). In doing so, we discuss the value of mining past works in new domains and demonstrate a new way of thinking about designing interactions in emerging platforms.},
booktitle = {Proceedings of the 2022 ACM Designing Interactive Systems Conference},
pages = {263–276},
numpages = {14},
keywords = {affordances, augmented reality, gestural interface, historical precedents, metaphor, software reconstruction, spatial interaction, system re-presencing},
location = {Virtual Event, Australia},
series = {DIS '22}
}

@inproceedings{10.1145/3594315.3594644,
author = {Zhu, Shan and Ling, Xufeng and Zhang, Kui and Niu, Jiachao},
title = {Food Image Recognition Method Based on Generative Self-supervised Learning},
year = {2023},
isbn = {9781450399029},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594315.3594644},
doi = {10.1145/3594315.3594644},
abstract = {The demand of social life for automatic recognition of food images is increasing. Food images have the characteristics of diverse forms, small differences between classes and large differences within classes, which has the problem of high recognition difficulty. This paper proposes a food image recognition method based on generative self-supervised learning. Firstly, we use a BEiT based pre-training model which is trained through generative self-monitoring learning method as the feature extraction network to extract the global semantics and local detail features of food images. And then we fine-tune the fully connected network MLP for classification and recognition through supervised learning method. The model is tested on the current mainstream public food image dataset Food-101, and the top-1 accuracy of 85.99% is obtained. The experimental results show that this method can significantly reduce the computation of pixel level expression as well as extract the global and detailed features of the image, achieving quite good food image classification and recognition effect. Our method has good robustness, generalization and flexibility, which has practical application value.},
booktitle = {Proceedings of the 2023 9th International Conference on Computing and Artificial Intelligence},
pages = {203–207},
numpages = {5},
location = {Tianjin, China},
series = {ICCAI '23}
}

@inproceedings{10.1145/3617233.3617274,
author = {Moholdt, Eivind and Khan, Sohail Ahmed and Dang-Nguyen, Duc-Tien},
title = {Detecting Out-of-Context Image-Caption Pair in News: A Counter-Intuitive Method},
year = {2023},
isbn = {9798400709128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617233.3617274},
doi = {10.1145/3617233.3617274},
abstract = {The growth of misinformation and re-contextualized media in social media and news leads to an increasing need for fact-checking methods. Concurrently, the advancement in generative models makes cheapfakes and deepfakes both easier to make and harder to detect. In this paper, we present a novel approach using generative image models to our advantage for detecting Out-of-Context (OOC) use of images-caption pairs in news. We present two new datasets with a total of 6800 images generated using two different generative models including (1) DALL-E 2, and (2) Stable-Diffusion. We are confident that the method proposed in this paper can further research on generative models in the field of cheapfake detection, and that the resulting datasets can be used to train and evaluate new models aimed at detecting cheapfakes. We run a preliminary qualitative and quantitative analysis to evaluate the performance of each image generation model for this task, and evaluate a handful of methods for computing image similarity.},
booktitle = {Proceedings of the 20th International Conference on Content-Based Multimedia Indexing},
pages = {203–209},
numpages = {7},
keywords = {Cheapfake Detection, Computer Vision, Dataset, Generative Models, Image Similarity, Text-to-Image},
location = {Orleans, France},
series = {CBMI '23}
}

@article{10.1145/3626193,
author = {Xia, Weihao and Xue, Jing-Hao},
title = {A Survey on Deep Generative 3D-aware Image Synthesis},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3626193},
doi = {10.1145/3626193},
abstract = {Recent years have seen remarkable progress in deep learning powered visual content creation. This includes deep generative 3D-aware image synthesis, which produces high-fidelity images in a 3D-consistent manner while simultaneously capturing compact surfaces of objects from pure image collections without the need for any 3D supervision, thus bridging the gap between 2D imagery and 3D reality. The field of computer vision has been recently captivated by the task of deep generative 3D-aware image synthesis, with hundreds of papers appearing in top-tier journals and conferences over the past few years (mainly the past two years), but there lacks a comprehensive survey of this remarkable and swift progress. Our survey aims to introduce new researchers to this topic, provide a useful reference for related works, and stimulate future research directions through our discussion section. Apart from the presented papers, we aim to constantly update the latest relevant papers along with corresponding implementations at .},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {90},
numpages = {34},
keywords = {3D-aware image synthesis, deep generative models, implicit neural representation, generative adversarial network, diffusion probabilistic models}
}

@inproceedings{10.1145/3511808.3557310,
author = {Chen, Haonan and Dou, Zhicheng and Zhu, Yutao and Cao, Zhao and Cheng, Xiaohua and Wen, Ji-Rong},
title = {Enhancing User Behavior Sequence Modeling by Generative Tasks for Session Search},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557310},
doi = {10.1145/3511808.3557310},
abstract = {Users' search tasks have become increasingly complicated, requiring multiple queries and interactions with the results. Recent studies have demonstrated that modeling the historical user behaviors in a session can help understand the current search intent. Existing context-aware ranking models primarily encode the current session sequence (from the first behavior to the current query) and compute the ranking score using the high-level representations. However, there is usually some noise in the current session sequence (useless behaviors for inferring the search intent) that may affect the quality of the encoded representations. To help the encoding of the current user behavior sequence, we propose to use a decoder and the information of future sequences and a supplemental query. Specifically, we design three generative tasks that can help the encoder to infer the actual search intent: (1) predicting future queries, (2) predicting future clicked documents, and (3) predicting a supplemental query. We jointly learn the ranking task with these generative tasks using an encoder-decoder structured approach. Extensive experiments on two public search logs demonstrate that our model outperforms all existing baselines, and the designed generative tasks can actually help the ranking task. Besides, additional experiments also show that our approach can be easily applied to various Transformer-based encoder-decoder models and improve their performance.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {180–190},
numpages = {11},
keywords = {auto-session-encoder, document ranking, session search},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{10.1145/3605548,
author = {Zhang, Yuxin and Tang, Fan and Dong, Weiming and Huang, Haibin and Ma, Chongyang and Lee, Tong-Yee and Xu, Changsheng},
title = {A Unified Arbitrary Style Transfer Framework via Adaptive Contrastive Learning},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {5},
issn = {0730-0301},
url = {https://doi.org/10.1145/3605548},
doi = {10.1145/3605548},
abstract = {This work presents Unified Contrastive Arbitrary Style Transfer (UCAST), a novel style representation learning and transfer framework, that can fit in most existing arbitrary image style transfer models, such as CNN-based, ViT-based, and flow-based methods. As the key component in image style transfer tasks, a suitable style representation is essential to achieve satisfactory results. Existing approaches based on deep neural networks typically use second-order statistics to generate the output. However, these hand-crafted features computed from a single image cannot leverage style information sufficiently, which leads to artifacts such as local distortions and style inconsistency. To address these issues, we learn style representation directly from a large number of images based on contrastive learning by considering the relationships between specific styles and the holistic style distribution. Specifically, we present an adaptive contrastive learning scheme for style transfer by introducing an input-dependent temperature. Our framework consists of three key components: a parallel contrastive learning scheme for style representation and transfer, a domain enhancement (DE) module for effective learning of style distribution, and a generative network for style transfer. Qualitative and quantitative evaluations show the results of our approach are superior to those obtained via state-of-the-art methods. The code is available at .},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {169},
numpages = {16},
keywords = {Arbitrary style transfer, contrastive learning, style encoding}
}

@inproceedings{10.1145/3532213.3532217,
author = {Zhang, Jie and Jiang, Runhao and Xiao, Rong and Yan, Rui},
title = {Dynamic Resistance Based Spiking Actor Network for Improving Reinforcement Learning},
year = {2022},
isbn = {9781450396110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532213.3532217},
doi = {10.1145/3532213.3532217},
abstract = {Designing algorithms for continuous control is a big challenge no matter in real robot controlling or simulation tasks. Deep reinforcement learning (DRL) is the most successful algorithm in dealing with such tasks for it utilizing the powerful ability of deep neuron network (DNN) in handling complex information. However, the more powerful ability the DNN has, the more energy it consumes. It’s a barrier for DRL to be realized in real-world control tasks. With more biological features, spiking neuron network (SNN) is one of the frontier fields of high-efficiency computing. The binary spike it used to represent information contains more temporal information and leads to greater computational efficiency on neuromorphic chips. Based on a hybrid architecture of SNN and DNN, we propose an actor-critic model to utilize the ability of SNN in dealing with complex continuous information and the ability of DNN in large scale accurate computation. The common Leaky Integrate-and-Fire (LIF) neuron model which is mainly used to build deep SNN neglects the resistance flexibility in the neuron. Considering that causes a descend capacity of representing continuous information which is of vital important in continuous control, we propose a new dynamic resistance LIF (R-LIF) model to compensate the temporal relation dependencies in neurons. With the same gradient updating rule, our R-LIF based spiking actor network (RSAN) shows a better performance when inferring in OpenAI benchmark tasks not only than the deep neuron actor network but also than the same LIF based spiking neuron actor network.},
booktitle = {Proceedings of the 8th International Conference on Computing and Artificial Intelligence},
pages = {18–23},
numpages = {6},
keywords = {continuous control, reinforcement learning, spiking neural network},
location = {Tianjin, China},
series = {ICCAI '22}
}

@inproceedings{10.1145/3534678.3539290,
author = {Wei, Jiawen and Wang, Fangyuan and Zeng, Wanxin and Lin, Wenwei and Gui, Ning},
title = {An Embedded Feature Selection Framework for Control},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539290},
doi = {10.1145/3534678.3539290},
abstract = {Reducing sensor requirements while keeping optimal control performance is crucial to many industrial control applications to achieve robust, low-cost, and computation-efficient controllers. However, existing feature selection solutions for the typical machine learning domain can hardly be applied in the domain of control with changing dynamics. In this paper, a novel framework, namely the Dual-world embedded Attentive Feature Selection (D-AFS), can efficiently select the most relevant sensors for the system under dynamic control. Rather than the one world used in most Deep Reinforcement Learning (DRL) algorithms, D-AFS has both the real world and its virtual peer with twisted features. By analyzing the DRL's response in two worlds, D-AFS can quantitatively identify respective features' importance towards control. A well-known active flow control problem, cylinder drag reduction, is used for evaluation. Results show that D-AFS successfully finds an optimized five-probes layout with 18.7% drag reduction than the state-of-the-art solution with 151 probes and 49.2% reduction than five-probes layout by human experts. We also apply this solution to four OpenAI classical control cases. In all cases, D-AFS achieves the same or better sensor configurations than originally provided solutions. Results highlight, we argued, a new way to achieve efficient and optimal sensor designs for experimental or industrial systems. Our source codes are made publicly available at https://github.com/G-AILab/DAFSFluid.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1979–1988},
numpages = {10},
keywords = {deep reinforcement learning, feature selection, optimal sensor placement},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3606283.3606285,
author = {Nakada, Kenta and Kimata, Hideaki},
title = {A Study on Voxel Shape Generation and Reconstruction with VQ-VAE-2},
year = {2023},
isbn = {9798400700460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3606283.3606285},
doi = {10.1145/3606283.3606285},
abstract = {With the recent advances in deep learning, research on reconstructing and generating 3D shapes has become increasingly popular. While there are several types of representation methods for 3D shapes, our work focuses on voxel models that are arranged in a regular grid and are easy to handle on computers. In the area of 2D image generation, generative models such as GANs have been proposed that can output higher quality generative images, but are difficult to train. The recently proposed VQ-VAE-2 is easy to learn and shows performance comparable to the latest GANs. Therefore, this work proposes a method that can generate and reconstruct sharp voxel shape using VQ-VAE-2. Experiments compare the reconstruction and generation results and demonstrate that the proposed method performs better than existing methods in both experiments and is capable to output sharper shapes.},
booktitle = {Proceedings of the 2023 7th International Conference on Graphics and Signal Processing},
pages = {9–15},
numpages = {7},
keywords = {3d model generation, 3d model reconstruction, VQ-VAE-2, voxel shape},
location = {Fujisawa, Japan},
series = {ICGSP '23}
}

@article{10.1145/3510032,
author = {Ren, Hanchi and Deng, Jingjing and Xie, Xianghua},
title = {GRNN: Generative Regression Neural Network—A Data Leakage Attack for Federated Learning},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3510032},
doi = {10.1145/3510032},
abstract = {Data privacy has become an increasingly important issue in Machine Learning (ML), where many approaches have been developed to tackle this challenge, e.g., cryptography (Homomorphic Encryption (HE), Differential Privacy (DP)) and collaborative training (Secure Multi-Party Computation (MPC), Distributed Learning, and Federated Learning (FL)). These techniques have a particular focus on data encryption or secure local computation. They transfer the intermediate information to the third party to compute the final result. Gradient exchanging is commonly considered to be a secure way of training a robust model collaboratively in Deep Learning (DL). However, recent researches have demonstrated that sensitive information can be recovered from the shared gradient. Generative Adversarial Network (GAN), in particular, has shown to be effective in recovering such information. However, GAN based techniques require additional information, such as class labels that are generally unavailable for privacy-preserved learning. In this article, we show that, in the FL system, image-based privacy data can be easily recovered in full from the shared gradient only via our proposed Generative Regression Neural Network (GRNN). We formulate the attack to be a regression problem and optimize two branches of the generative model by minimizing the distance between gradients. We evaluate our method on several image classification tasks. The results illustrate that our proposed GRNN outperforms state-of-the-art methods with better stability, stronger robustness, and higher accuracy. It also has no convergence requirement to the global FL model. Moreover, we demonstrate information leakage using face re-identification. Some defense strategies are also discussed in this work.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = may,
articleno = {65},
numpages = {24},
keywords = {Federated learning, data privacy, gradient leakage attack, image generation}
}

@article{10.1145/3569492,
author = {Elhattab, Fatima and Bouchenak, Sara and Talbi, Rania and Nitu, Vlad},
title = {Robust Federated Learning for Ubiquitous Computing through Mitigation of Edge-Case Backdoor Attacks},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
url = {https://doi.org/10.1145/3569492},
doi = {10.1145/3569492},
abstract = {Federated Learning (FL) allows several data owners to train a joint model without sharing their training data. Such a paradigm is useful for better privacy in many ubiquitous computing systems. However, FL is vulnerable to poisoning attacks, where malicious participants attempt to inject a backdoor task in the model at training time, along with the main task that the model was initially trained for. Recent works show that FL is particularly vulnerable to edge-case backdoors introduced by data points with unusual out-of-distribution features. Such attacks are among the most difficult to counter, and today's FL defense mechanisms usually fail to tackle them. In this paper, we present ARMOR, a defense mechanism that leverages adversarial learning to uncover edge-case backdoors. In contrast to most of existing FL defenses, ARMOR does not require real data samples and is compatible with secure aggregation, thus, providing better FL privacy protection. ARMOR relies on GANs (Generative Adversarial Networks) to extract data features from model updates, and uses the generated samples to test the activation of potential edge-case backdoors in the model. Our experimental evaluations with three widely used datasets and neural networks show that ARMOR can tackle edge-case backdoors with 95% resilience against attacks, and without hurting model quality.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jan,
articleno = {162},
numpages = {27},
keywords = {Edge-Case Backdoor, Federated Learning, Robustness, Ubiquitous Computing}
}

@article{10.1145/3592427,
author = {Shacklett, Brennan and Rosenzweig, Luc Guy and Xie, Zhiqiang and Sarkar, Bidipta and Szot, Andrew and Wijmans, Erik and Koltun, Vladlen and Batra, Dhruv and Fatahalian, Kayvon},
title = {An Extensible, Data-Oriented Architecture for High-Performance, Many-World Simulation},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3592427},
doi = {10.1145/3592427},
abstract = {Training AI agents to perform complex tasks in simulated worlds requires millions to billions of steps of experience. To achieve high performance, today's fastest simulators for training AI agents adopt the idea of batch simulation: using a single simulation engine to simultaneously step many environments in parallel. We introduce a framework for productively authoring novel training environments (including custom logic for environment generation, environment time stepping, and generating agent observations and rewards) that execute as high-performance, GPU-accelerated batched simulators. Our key observation is that the entity-component-system (ECS) design pattern, popular for expressing CPU-side game logic today, is also well-suited for providing the structure needed for high-performance batched simulators. We contribute the first fully-GPU accelerated ECS implementation that natively supports batch environment simulation. We demonstrate how ECS abstractions impose structure on a training environment's logic and state that allows the system to efficiently manage state, amortize work, and identify GPU-friendly coherent parallel computations within and across different environments. We implement several learning environments in this framework, and demonstrate GPU speedups of two to three orders of magnitude over open source CPU baselines and 5-33\texttimes{} over strong baselines running on a 32-thread CPU. An implementation of the OpenAI hide and seek 3D environment written in our framework, which performs rigid body physics and ray tracing in each simulator step, achieves over 1.9 million environment steps per second on a single GPU.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {90},
numpages = {13},
keywords = {game AI, reinforcement learning}
}

@inproceedings{10.1145/3617233.3617246,
author = {Beddiar, Djamila Romaissa and Oussalah, Mourad and Seppanen, Tapio},
title = {Retrieved Generative Captioning for Medical Images},
year = {2023},
isbn = {9798400709128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617233.3617246},
doi = {10.1145/3617233.3617246},
abstract = {Understanding the content of medical images and mapping it into text is a very trending topic in intersection of two main domains; computer vision and natural language processing. This is known as medical image captioning, which plays a vital role in developing automatic systems for diagnosis purposes. Recent research in the medical field provided promising results for both deep-learning based and retrieval-based models for image captioning. However, each one of them has its own drawbacks, that can be overcome if combined. In addition, existing diagnosis systems are still not able to provide enough explanation about the findings, which might be similar to what a physician can deliver. In this regard, we present in this paper a combination of a generative deep-learning based method and a retrieval-based model for medical image captioning. First, we train an attention-based encoder-decoder model to generate new captions for given medical images. Then, we fit the generated caption from the generative model to the retrieval-based model, which retrieves the most similar caption from the training database. This multi-stage approach allows us to generate most important words of the caption (with the generative model) and then search for the most close caption that includes such words (with the retrieval-based model). Another way of combining both models is by selecting at each time the caption with highest score among generated and retrieved captions. We evaluate our proposed model on the medical ROCO dataset for which we achieved a BLEU-4 score of 07.89 for the radiology class and 03.19 for the out-of-class data, for the multi-stage model. Similarly, best results were achieved for the fused model (predicted caption is the best among generated and retrieved) where we obtain a BLEU-4 values of 18.61 for the radiology class and 13.28 for the out-of-class data. Even though our results seem to be low, they outperformed the state-of-the-art results on the same dataset and could be further improved.},
booktitle = {Proceedings of the 20th International Conference on Content-Based Multimedia Indexing},
pages = {48–54},
numpages = {7},
keywords = {Generative-based Captioning, Image Captioning, Medical Images, Neural Networks, Retrieval-based Captioning},
location = {Orleans, France},
series = {CBMI '23}
}

@inproceedings{10.1145/3609395.3610594,
author = {Zhang, Xuechen and Li, Zheng and Oymak, Samet and Chen, Jiasi},
title = {Text-to-3D Generative AI on Mobile Devices: Measurements and Optimizations},
year = {2023},
isbn = {9798400703034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609395.3610594},
doi = {10.1145/3609395.3610594},
abstract = {Emerging generative models can create 3D objects from text prompts. However, deploying these models on mobile devices is challenging due to resource constraints and user demand for real-time performance. We take a first step towards understanding the bottlenecks by performing a measurement study of three recent text-to-3D generative models (Point-E, Shap-E, and CLIP-Mesh) in terms of their runtime GPU memory usage, latency, and synthesis quality. We investigate the effectiveness of quantization and distillation techniques to overcome these challenges by speeding up inference execution, potentially at the expense of quality. We find that the Shap-E model is promising for mobile deployment, but requires further optimization in its bottleneck diffusion step for real-time performance, as well as reduced memory usage and load times. Further work is needed on custom optimizations for generative text-to-3D models, including targeting specific metrics at each computation stage, efficient representations of 3D objects, and adaptive network and system support for resource-hungry models.},
booktitle = {Proceedings of the 2023 Workshop on Emerging Multimedia Systems},
pages = {8–14},
numpages = {7},
keywords = {text-to-3D, generative AI, mobile devices, latency, GPU memory},
location = {New York, NY, USA},
series = {EMS '23}
}

@inproceedings{10.1145/3615522.3615548,
author = {Zhang, Yang and Li, Jie and Xu, Chao},
title = {Graph-based Latent Space Traversal for New Molecules Discovery},
year = {2023},
isbn = {9798400707513},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615522.3615548},
doi = {10.1145/3615522.3615548},
abstract = {Generative models provide an efficient way to analyze and understand unlabeled data, creating the latent space for data modeling and generation. Since the interpretation of latent space usually requires implicit expert knowledge, this human-centered feature makes visual analytic methods effective. In the filed of computer chemistry, some research have applied generative models to generate chemical spaces and generated new molecules by sampling in the latent space. However, the latent space is typically high and sparse, and there may be a large number of “dead zones”, which may lead to decoding sample points from the latent space are noisy or invalid. Therefore, it is extremely challenging to efficiently search and traverse the latent space and generate new molecules with the desired properties. This paper aims to propose a visualization system for interactive exploration of latent space, which inspires the researchers to design new potential molecules with desired properties. The main work of this paper is as follows: First, we investigate a series of literature on the application of generative networks to drug design and synthesis, and interview experts with computer chemistry background to summarize the requirements and tasks. Second, based on the above requirements and tasks, we propose a graph-based latent space traversal and interpolation algorithms and neighborhood sampling algorithms. This can improve the number of generated potential molecules and the speed of discovery of similar molecules. Then, we conduct comparison experiments to verify the effectiveness of the algorithms. Finally, we design visualization system and then conduct the case study and user study to verify the effectiveness of the visualization system.},
booktitle = {Proceedings of the 16th International Symposium on Visual Information Communication and Interaction},
articleno = {26},
numpages = {8},
keywords = {chemical drug design and synthesis, generative model, latent space, visual analysis},
location = {Guangzhou, China},
series = {VINCI '23}
}

@inproceedings{10.1145/3503161.3549973,
author = {Forero, Jorge and Bernardes, Gilberto and Mendes, M\'{o}nica},
title = {Emotional Machines: Toward Affective Virtual Environments},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3549973},
doi = {10.1145/3503161.3549973},
abstract = {Emotional Machines is an interactive installation that builds affective virtual environments through spoken language. In response to the existing limitations of emotion recognition models incorporating computer vision and electrophysiological activity, whose sources are hindered by a head-mounted display, we propose the adoption of speech emotion recognition (from the audio signal) and semantic sentiment analysis. In detail, we use two machine learning models to predict three main emotional categories from high-level semantic and low-level speech features. Output emotions are mapped to audiovisual representation by an end-to-end process. We use a generative model of chord progressions to transfer speech emotion into music and a synthesized image from the text (transcribed from the user's speech). The generated image is used as the style source in the style-transfer process onto an equirectangular projection image target selected for each emotional category. The installation is an immersive virtual space encapsulating emotions in spheres disposed into a 3D environment. Thus, users can create new affective representations or interact with other previous encoded instances using joysticks.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {7237–7238},
numpages = {2},
keywords = {affective computing, intelligent virtual environments, machine learning, speech emotion recognition, tonal interval space, virtual reality},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3544548.3580940,
author = {Mcnutt, Andrew M and Wang, Chenglong and Deline, Robert A and Drucker, Steven M.},
title = {On the Design of AI-powered Code Assistants for Notebooks},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580940},
doi = {10.1145/3544548.3580940},
abstract = {AI-powered code assistants, such as Copilot, are quickly becoming a ubiquitous component of contemporary coding contexts. Among these environments, computational notebooks, such as Jupyter, are of particular interest as they provide rich interface affordances that interleave code and output in a manner that allows for both exploratory and presentational work. Despite their popularity, little is known about the appropriate design of code assistants in notebooks. We investigate the potential of code assistants in computational notebooks by creating a design space (reified from a survey of extant tools) and through an interview-design study (with 15 practicing data scientists). Through this work, we identify challenges and opportunities for future systems in this space, such as the value of disambiguation for tasks like data visualization, the potential of tightly scoped domain-specific tools (like linters), and the importance of polite assistants.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {434},
numpages = {16},
keywords = {Artificial Intelligence, Code Assistant, Computational Notebooks, Copilot, Design Probe},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3548606.3560586,
author = {Liu, Yupei and Jia, Jinyuan and Liu, Hongbin and Gong, Neil Zhenqiang},
title = {StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning},
year = {2022},
isbn = {9781450394505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548606.3560586},
doi = {10.1145/3548606.3560586},
abstract = {Pre-trained encoders are general-purpose feature extractors that can be used for many downstream tasks. Recent progress in self-supervised learning can pre-train highly effective encoders using a large volume of unlabeled data, leading to the emerging encoder as a service (EaaS). A pre-trained encoder may be deemed confidential because its training often requires lots of data and computation resources as well as its public release may facilitate misuse of AI, e.g., for deepfakes generation. In this paper, we propose the first attack called StolenEncoder to steal pre-trained image encoders. We evaluate StolenEncoder on multiple target encoders pre-trained by ourselves and three real-world target encoders including the ImageNet encoder pre-trained by Google, CLIP encoder pre-trained by OpenAI, and Clarifai's General Embedding encoder deployed as a paid EaaS. Our results show that the encoders stolen by StolenEncoder have similar functionality with the target encoders. In particular, the downstream classifiers built upon a target encoder and a stolen encoder have similar accuracy. Moreover, stealing a target encoder using StolenEncoder requires much less data and computation resources than pre-training it from scratch. We also explore three defenses that perturb feature vectors produced by a target encoder. Our evaluation shows that these defenses are not enough to mitigate StolenEncoder.},
booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2115–2128},
numpages = {14},
keywords = {model stealing attacks, pre-trained models, self-supervised learning},
location = {Los Angeles, CA, USA},
series = {CCS '22}
}

@inproceedings{10.1145/3583780.3614821,
author = {Chen, Jiangui and Zhang, Ruqing and Guo, Jiafeng and de Rijke, Maarten and Chen, Wei and Fan, Yixing and Cheng, Xueqi},
title = {Continual Learning for Generative Retrieval over Dynamic Corpora},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614821},
doi = {10.1145/3583780.3614821},
abstract = {Generative retrieval (GR) directly predicts the identifiers of relevant documents (i.e., docids) based on a parametric model. It has achieved solid performance on many ad-hoc retrieval tasks. So far, these tasks have assumed a static document collection. In many practical scenarios, however, document collections are dynamic, where new documents are continuously added to the corpus. The ability to incrementally index new documents while preserving the ability to answer queries with both previously and newly indexed relevant documents is vital to applying GR models. In this paper, we address this practical continual learning problem for GR. We put forward a novel Continual-LEarner for generatiVE Retrieval (CLEVER) model and make two major contributions to continual learning for GR: (i) To encode new documents into docids with low computational cost, we present Incremental Product Quantization, which updates a partial quantization codebook according to two adaptive thresholds; and (ii) To memorize new documents for querying without forgetting previous knowledge, we propose a memory-augmented learning mechanism, to form meaningful connections between old and new documents. Empirical results demonstrate the effectiveness and efficiency of the proposed model.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {306–315},
numpages = {10},
keywords = {document increment, generative retrieval, product quantization},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3588029.3595471,
author = {Tanaka, Kengo and Fushimi, Tatsuki and Tsutsui, Ayaka and Ochiai, Yoichi},
title = {Text to Haptics: Method and Case Studies of Designing Tactile Graphics for Inclusive Tactile Picture Books by Digital Fabrication and Generative AI},
year = {2023},
isbn = {9798400701535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588029.3595471},
doi = {10.1145/3588029.3595471},
abstract = {In this case study, we explore the possibilities between Generative AI and tactile graphics for inclusivity in computer graphics communities. The use of Generative AI in the design of tactile graphics has made it possible to support the processes used by publishers and tactile graphics designers. In addition, the idea of printing tactile graphics on transparent sheets with a 3D printer through digital fabrication technology allows the creation of inclusive tactile picture books that can be read and enjoyed together by sighted and visually impaired people in a single picture book.},
booktitle = {ACM SIGGRAPH 2023 Labs},
articleno = {10},
numpages = {2},
keywords = {3D printing, generative ai, tactile graphics, tactile picture books, visual impairments},
location = {Los Angeles, CA, USA},
series = {SIGGRAPH '23}
}

@inproceedings{10.1145/3581783.3613859,
author = {Wang, Xin and Chen, Hong and Zhu, Wenwu},
title = {Disentangled Representation Learning for Multimedia},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3613859},
doi = {10.1145/3581783.3613859},
abstract = {Disentangled Representation Learning (DRL) aims to learn a model capable of identifying and disentangling the underlying factors hidden in the observable data in representation form. The process of separating underlying factors of variation into variables with semantic meaning benefits in learning explainable representations of data, which imitates the meaningful understanding process of humans when observing an object or relation. As a general learning strategy, DRL has demonstrated its power in improving the model explainability, controllability, robustness, as well as generalization capacity in a wide range of scenarios such as computer vision, natural language processing, data mining etc. In this tutorial, we comprehensively present DRL from various aspects including motivations, definitions, methodologies, evaluations, applications and model designs for multimedia. We discuss works on DRL based on two well-recognized definitions, i.e., Intuitive Definition and Group Theory Definition. We further categorize the methodologies for DRL into four groups, i.e., Traditional Statistical Approaches, Variational Auto-encoder Based Approaches, Generative Adversarial Networks Based Approaches, Hierarchical Approaches and Other Approaches. We also analyze principles to design different DRL models that may benefit different tasks in practical multimedia applications. Finally, we point out challenges in DRL as well as potential research directions deserving future investigations. We believe this tutorial may provide insights for promoting the DRL research in the multimedia community.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {9702–9704},
numpages = {3},
keywords = {disentangled representation learning, learning methodology, multimedia applications},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3573942.3574097,
author = {Zhai, Sheping and Liu, Yuanbiao and Cheng, Dabao},
title = {Single Image Dehazing Via Enhanced CycleGAN},
year = {2023},
isbn = {9781450396899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573942.3574097},
doi = {10.1145/3573942.3574097},
abstract = {Due to the influence of atmospheric light scattering, the images acquired by outdoor imaging device in haze scene will appear low definition, contrast reduction, overexposure and other visible quality degradation, which makes it difficult to handle the relevant computer vision tasks. Therefore, image dehazing has become an important research area of computer vision. However, existing dehazing methods generally require paired image datasets that include both hazy images and corresponding ground truth images, while the recovered images are easy to occur color distortion and detail loss. In this study, an end-to-end image dehazing method based on Cycle-consistent Generative Adversarial Networks (CycleGAN) is proposed. For effectively learning the mapping relationship between hazy images and clear images, we refine the transformation module of the generator by weighting optimization, which can promote the network adaptability to scale. Then in order to further improve the quality of generated images, the enhanced perceptual loss and low-frequency loss combined with image feature attributes are constructed in the overall optimization objective of the network. The experimental results show that our dehazing algorithm effectively recovers the texture information while correcting the color distortion of original CycleGAN, and the recovery effect is clear and more natural, which better reduces the influence of haze on the imaging quality.},
booktitle = {Proceedings of the 2022 5th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {795–802},
numpages = {8},
location = {Xiamen, China},
series = {AIPR '22}
}

@inproceedings{10.1145/3520304.3534048,
author = {Wang, Gabriel and Thite, Anish and Talebi, Rodd and D'Achille, Anthony and Mussa, Alex and Zutty, Jason},
title = {Evolving SimGANs to improve abnormal electrocardiogram classification},
year = {2022},
isbn = {9781450392686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520304.3534048},
doi = {10.1145/3520304.3534048},
abstract = {Machine Learning models often require a large amount of data in order to be successful. This is troublesome in domains where collecting real-world data is difficult and/or expensive. Data simulators do exist, but they do not sufficiently reflect the real world data due to factors such as a lack of real-world noise. Generative adversarial networks (GANs) have been modified to refine simulated image data to better fit real world characteristics, using the SimGAN method. While evolutionary computing has been used for GAN evolution, there are currently no frameworks that can evolve a SimGAN. In this paper we (1) extend the SimGAN method to refine one-dimensional data, (2) modify Easy Cartesian Genetic Programming (ezCGP), an evolutionary computing framework, to create SimGANs that more accurately refine simulated data, and (3) create new feature-based quantitative metrics to evaluate refined data. We also use our framework to augment an electrocardiogram (ECG) dataset, a domain that suffers from the issues previously mentioned. In particular, while healthy ECGs can be simulated there are no current simulators of abnormal ECGs. We show that by using an evolved SimGAN to refine simulated healthy ECG data to mimic real-world abnormal ECGs, we can improve the accuracy of ECG classifiers.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1887–1894},
numpages = {8},
keywords = {automated machine learning, datasets, general adversarial networks, neural networks},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@inproceedings{10.1145/3573942.3573980,
author = {Zhu, Tianyi and Liu, Huan},
title = {A Review of Research Based on Generative Adversarial Networks},
year = {2023},
isbn = {9781450396899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573942.3573980},
doi = {10.1145/3573942.3573980},
abstract = {In recent years, with the rapid development of deep learning, Generative Adversarial Network (GAN), as one of the unsupervised learning, has also gradually developed into one of the current research hotspots in the field of deep learning. GAN has more powerful feature learning and feature representation capabilities than traditional machine learning algorithms. GAN is currently showing great potential in deep learning, especially in the field of computer vision. In this paper, we discuss and analyse the basic principles of the basic GAN model and its advantages and disadvantages; introduce the derivative models of GAN and their limitations; summarise the application results of GAN in various fields such as image reconstruction, text description image generation, audio generation, migration learning, etc. Finally, this paper also presents some prospects for future research directions in this field.},
booktitle = {Proceedings of the 2022 5th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {988–994},
numpages = {7},
keywords = {Data generation, Generative Adversarial Networks, Review},
location = {Xiamen, China},
series = {AIPR '22}
}

@article{10.5555/3586589.3586704,
author = {Tran, Linh and Pantic, Maja and Deisenroth, Marc Peter},
title = {Cauchy-Schwarz regularized autoencoder},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {Recent work in unsupervised learning has focused on efficient inference and learning in latent variables models. Training these models by maximizing the evidence (marginal likelihood) is typically intractable. Thus, a common approximation is to maximize the Evidence Lower BOund (ELBO) instead. Variational autoencoders (VAE) are a powerful and widely-used class of generative models that optimize the ELBO efficiently for large datasets. However, the VAE's default Gaussian choice for the prior imposes a strong constraint on its ability to represent the true posterior, thereby degrading overall performance. A Gaussian mixture model (GMM) would be a richer prior but cannot be handled efficiently within the VAE framework because of the intractability of the Kullback-Leibler divergence for GMMs. We deviate from the common VAE framework in favor of one with an analytical solution for Gaussian mixture prior. To perform efficient inference for GMM priors, we introduce a new constrained objective based on the Cauchy-Schwarz divergence, which can be computed analytically for GMMs. This new objective allows us to incorporate richer, multi-modal priors into the autoencoding framework. We provide empirical studies on a range of datasets and show that our objective improves upon variational autoencoding models in density estimation, unsupervised clustering, semi-supervised learning, and face analysis.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {115},
numpages = {37},
keywords = {generative models, Cauchy-Schwarz divergence, constrained optimization, auto-encoding models, face analysis}
}

@inproceedings{10.1145/3556557.3557953,
author = {Korto\c{c}i, Pranvera and Liang, Yilei and Zhou, Pengyuan and Lee, Lik-Hang and Mehrabi, Abbas and Hui, Pan and Tarkoma, Sasu and Crowcroft, Jon},
title = {Federated split GANs},
year = {2022},
isbn = {9781450395212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3556557.3557953},
doi = {10.1145/3556557.3557953},
abstract = {Mobile devices and the immense amount and variety of data they generate are key enablers of machine learning (ML)-based applications. Traditional ML techniques have shifted toward new paradigms such as federated learning (FL) and split learning (SL) to improve the protection of user's data privacy. However, SL often relies on server(s) located in the edge or cloud to train computationally-heavy parts of an ML model to avoid draining the limited resource on client devices, potentially resulting in exposure of device data to such third parties.This work proposes an alternative approach to train computationally heavy ML models in user's devices themselves, where corresponding device data resides. Specifically, we focus on GANs (generative adversarial networks) and leverage their network architecture to preserve data privacy. We train the discriminative part of a GAN on user's devices with their data, whereas the generative model is trained remotely (e.g., server) for which there is no need to access device true data. Moreover, our approach ensures that the computational load of training the discriminative model is shared among user's devices - proportional to their computation capabilities - by means of SL. We implement our proposed collaborative training scheme of a computationally-heavy GAN model in simulated resource-constrained devices. The results show that our system preserves data privacy, keeps a short training time, and yields the same model accuracy as when the model is trained on devices with unconstrained resources (e.g., cloud). Our code can be found at https://github.com/YukariSonz/FSL-GAN.},
booktitle = {Proceedings of the 1st ACM Workshop on Data Privacy and Federated Learning Technologies for Mobile Edge Network},
pages = {25–30},
numpages = {6},
keywords = {GAN, federated learning, split learning},
location = {Sydney, New South Wales, Australia},
series = {FedEdge '22}
}

@inproceedings{10.1109/DAC18074.2021.9586268,
author = {Li, Junde and Alam, Mahabubul and Sha, Congzhou M and Wang, Jian and Dokholyan, Nikolay V. and Ghosh, Swaroop},
title = {Invited: Drug Discovery Approaches Using Quantum Machine Learning},
year = {2022},
isbn = {9781665432740},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/DAC18074.2021.9586268},
doi = {10.1109/DAC18074.2021.9586268},
abstract = {Traditional drug discovery pipelines can require multiple years and billions of dollars of investment. Deep generative and discriminative models are widely adopted to assist in drug development. Classical machines cannot efficiently reproduce the atypical patterns of quantum computers, which may improve the quality of learned tasks. We propose a suite of quantum machine learning techniques: incorporating generative adversarial networks (GAN), convolutional neural networks (CNN) and variational auto-encoders (VAE) to generate small drug molecules, classify binding pockets in proteins, and generate large drug molecules, respectively.},
booktitle = {Proceedings of the 58th Annual ACM/IEEE Design Automation Conference},
pages = {1356–1359},
numpages = {4},
location = {San Francisco, California, United States},
series = {DAC '21}
}

@article{10.1145/3583885,
author = {Dai, Qingfeng and Wong, Yongkang and Sun, Guofei and Wang, Yanwei and Zhou, Zhou and Kankanhalli, Mohan S. and Li, Xiangdong and Geng, Weidong},
title = {Unsupervised Domain Adaptation by Causal Learning for Biometric Signal-based HCI},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1551-6857},
url = {https://doi.org/10.1145/3583885},
doi = {10.1145/3583885},
abstract = {Biometric signal based human-computer interface (HCI) has attracted increasing attention due to its wide application in healthcare, entertainment, neurocomputing, and so on. In recent years, deep learning-based approaches have made great progress on biometric signal processing. However, the state-of-the-art (SOTA) approaches still suffer from model degradation across subjects or sessions. In this work, we propose a novel unsupervised domain adaptation approach for biometric signal-based HCI via causal representation learning. Specifically, three kinds of interventions on biometric signals (i.e.,&nbsp;subjects, sessions, and trials) can be selected to generalize deep models across the selected intervention. In the proposed approach, a generative model is trained for producing intervened features that are subsequently used for learning transferable and causal relations with three modes. Experiments on the EEG-based emotion recognition task and sEMG-based gesture recognition task are conducted to confirm the superiority of our approach. An improvement of +0.21% on the task of inter-subject EEG-based emotion recognition is achieved using our approach. Besides, on the task of inter-session sEMG-based gesture recognition, our approach achieves improvements of +1.47%, +3.36%, +1.71%, and +1.01% on sEMG datasets including CSL-HDEMG, CapgMyo DB-b, 3DC, and Ninapro DB6, respectively. The proposed approach also works on the task of inter-trial sEMG-based gesture recognition and an average improvement of +0.66% on Ninapro databases is achieved. These experimental results show the superiority of the proposed approach compared with the SOTA unsupervised domain adaptation methods on HCIs based on biometric signal.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = sep,
articleno = {49},
numpages = {18},
keywords = {Deep learning, domain adaptation, causal learning, human-computer interface}
}

@inproceedings{10.1145/3550469.3555425,
author = {Leimer, Kurt and Guerrero, Paul and Weiss, Tomer and Musialski, Przemyslaw},
title = {LayoutEnhancer: Generating Good Indoor Layouts from Imperfect Data},
year = {2022},
isbn = {9781450394703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550469.3555425},
doi = {10.1145/3550469.3555425},
abstract = {We address the problem of indoor layout synthesis, which is a topic of continuing research interest in computer graphics. The newest works made significant progress using data-driven generative methods; however, these approaches rely on suitable datasets. In practice, desirable layout properties may not exist in a dataset, for instance, specific expert knowledge can be missing in the data. We propose a method that combines expert knowledge, for example, knowledge about ergonomics, with a data-driven generator based on the popular Transformer architecture. The knowledge is given as differentiable scalar functions, which can be used both as weights or as additional terms in the loss function. Using this knowledge, the synthesized layouts can be biased to exhibit desirable properties, even if these properties are not present in the dataset. Our approach can also alleviate problems of lack of data and imperfections in the data. Our work aims to improve generative machine learning for modeling and provide novel tools for designers and amateurs for the problem of interior layout creation.},
booktitle = {SIGGRAPH Asia 2022 Conference Papers},
articleno = {27},
numpages = {8},
keywords = {indoor layout synthesis, interior design, neural networks},
location = {Daegu, Republic of Korea},
series = {SA '22}
}

@inproceedings{10.5555/3539845.3540187,
author = {Wei, Zheng and Zhang, Xingjun and Li, Jingbo and Ji, Zeyu and Wei, Jia},
title = {BenQ: &lt;u&gt;ben&lt;/u&gt;chmarking automated quantization on deep neural network accelerators},
year = {2022},
isbn = {9783981926361},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {Hardware-aware automated quantization promises to unlock an entirely new algorithm-hardware co-design paradigm for efficiently accelerating deep neural network (DNN) inference by incorporating the hardware cost into the reinforcement learning (RL) -based quantization strategy search process. Existing works usually design an automated quantization algorithm targeting one hardware accelerator with a device-specific performance model or pre-collected data. However, determining the hardware cost is non-trivial for algorithm experts due to their lack of cross-disciplinary knowledge in computer architecture, compiler, and physical chip design. Such a barrier limits reproducibility and fair comparison. Moreover, it is notoriously challenging to interpret the results due to the lack of quantitative metrics. To this end, we first propose BenQ, which includes various RL-based automated quantization algorithms with aligned settings and encapsulates two off-the-shelf performance predictors with standard OpenAI Gym API. Then, we leverage cosine similarity and manhattan distance to interpret the similarity between the searched policies. The experiments show that different automated quantization algorithms can achieve near equivalent optimal trade-offs because of the high similarity between the searched policies, which provides insights for revisiting the innovations in automated quantization algorithms.},
booktitle = {Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe},
pages = {1479–1484},
numpages = {6},
keywords = {DNN accelerator, automated quantization, benchmark, reinforcement learning},
location = {Antwerp, Belgium},
series = {DATE '22}
}

@inproceedings{10.1145/3630202.3630224,
author = {Wang, Chao and Finamore, Alessandro and Michiardi, Pietro and Gallo, Massimo and Rossi, Dario},
title = {Toward Generative Data Augmentation for Traffic Classification},
year = {2023},
isbn = {9798400704529},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630202.3630224},
doi = {10.1145/3630202.3630224},
abstract = {Data Augmentation (DA)--augmenting training data with synthetic samples--is wildly adopted in Computer Vision (CV) to improve models performance. Conversely, DA has not been yet popularized in networking use cases, including Traffic Classification (TC). In this work, we present a preliminary study of 14 hand-crafted DAs applied on the MIRAGE19 dataset. Our results (i) show that DA can reap benefits previously unexplored in TC and (ii) foster a research agenda on the use of generative models to automate DA design.},
booktitle = {Proceedings of the on CoNEXT Student Workshop 2023},
pages = {33–34},
numpages = {2},
keywords = {data augmentation, deep learning, generative model, traffic classification},
location = {Paris, France},
series = {CoNEXT-SW '23}
}

@inproceedings{10.1145/3580305.3599373,
author = {Zhao, Wentao and Wu, Qitian and Yang, Chenxiao and Yan, Junchi},
title = {GraphGLOW: Universal and Generalizable Structure Learning for Graph Neural Networks},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599373},
doi = {10.1145/3580305.3599373},
abstract = {Graph structure learning is a well-established problem that aims at optimizing graph structures adaptive to specific graph datasets to help message passing neural networks (i.e., GNNs) to yield effective and robust node embeddings. However, the common limitation of existing models lies in the underlying closed-world assumption: the testing graph is the same as the training graph. This premise requires independently training the structure learning model from scratch for each graph dataset, which leads to prohibitive computation costs and potential risks for serious over-fitting. To mitigate these issues, this paper explores a new direction that moves forward to learn a universal structure learning model that can generalize across graph datasets in an open world. We first introduce the mathematical definition of this novel problem setting, and describe the model formulation from a probabilistic data-generative aspect. Then we devise a general framework that coordinates a single graph-shared structure learner and multiple graph-specific GNNs to capture the generalizable patterns of optimal message-passing topology across datasets. The well-trained structure learner can directly produce adaptive structures for unseen target graphs without any fine-tuning. Across diverse datasets and various challenging cross-graph generalization protocols, our experiments show that even without training on target graphs, the proposed model i) significantly outperforms expressive GNNs trained on input (non-optimized) topology, and ii) surprisingly performs on par with state-of-the-art models that independently optimizes adaptive structures for specific target graphs, with notably orders-of-magnitude acceleration for training on the target graph.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3525–3536},
numpages = {12},
keywords = {graph neural networks, graph structure learning, network representation learning, out-of-distribution generalization},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@article{10.1145/3514244,
author = {Rajasekaran, Suren Deepak and Kang, Hao and \v{C}ad\'{\i}k, Martin and Galin, Eric and Gu\'{e}rin, Eric and Peytavie, Adrien and Slav\'{\i}k, Pavel and Benes, Bedrich},
title = {PTRM: Perceived Terrain Realism Metric},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {1544-3558},
url = {https://doi.org/10.1145/3514244},
doi = {10.1145/3514244},
abstract = {Terrains are visually prominent and commonly needed objects in many computer graphics applications. While there are many algorithms for synthetic terrain generation, it is rather difficult to assess the realism of a generated output. This article presents a first step toward the direction of perceptual evaluation for terrain models. We gathered and categorized several classes of real terrains, and we generated synthetic terrain models using computer graphics methods. The terrain geometries were rendered by using the same texturing, lighting, and camera position. Two studies on these image sets were conducted, ranking the terrains perceptually, and showing that the synthetic terrains are perceived as lacking realism compared to the real ones. We provide insight into the features that affect the perceived realism by a quantitative evaluation based on localized geomorphology-based landform features (geomorphons) that categorize terrain structures such as valleys, ridges, hollows, and so forth. We show that the presence or absence of certain features has a significant perceptual effect. The importance and presence of the terrain features were confirmed by using a generative deep neural network that transferred the features between the geometric models of the real terrains and the synthetic ones. The feature transfer was followed by another perceptual experiment that further showed their importance and effect on perceived realism. We then introduce Perceived Terrain Realism Metrics (PTRM), which estimates human-perceived realism of a terrain represented as a digital elevation map by relating the distribution of terrain features with their perceived realism. This metric can be used on a synthetic terrain, and it will output an estimated level of perceived realism. We validated the proposed metrics on real and synthetic data and compared them to the perceptual studies.},
journal = {ACM Trans. Appl. Percept.},
month = jul,
articleno = {6},
numpages = {22},
keywords = {Procedural modeling, terrains, visual perception, feature transfer, neural networks}
}

@article{10.1145/3473340,
author = {Xu, Sheng and Liu, Chang and Zhang, Baochang and L\"{u}, Jinhu and Guo, Guodong and Doermann, David},
title = {BiRe-ID: Binary Neural Network for Efficient Person Re-ID},
year = {2022},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3473340},
doi = {10.1145/3473340},
abstract = {Person re-identification (Re-ID) has been promoted by the significant success of convolutional neural networks (CNNs). However, the application of such CNN-based Re-ID methods depends on the tremendous consumption of computation and memory resources, which affects its development on resource-limited devices such as next generation AI chips. As a result, CNN binarization has attracted increasing attention, which leads to binary neural networks (BNNs). In this article, we propose a new BNN-based framework for efficient person Re-ID (BiRe-ID). In this work, we discover that the significant performance drop of binarized models for Re-ID task is caused by the degraded representation capacity of kernels and features. To address the issues, we propose the kernel and feature refinement based on generative adversarial learning (KR-GAL and FR-GAL) to enhance the representation capacity of BNNs. We first introduce an adversarial attention mechanism to refine the binarized kernels based on their real-valued counterparts. Specifically, we introduce a scale factor to restore the scale of 1-bit convolution. And we employ an effective generative adversarial learning method to train the attention-aware scale factor. Furthermore, we introduce a self-supervised generative adversarial network to refine the low-level features using the corresponding high-level semantic information. Extensive experiments demonstrate that our BiRe-ID can be effectively implemented on various mainstream backbones for the Re-ID task. In terms of the performance, our BiRe-ID surpasses existing binarization methods by significant margins, at the level even comparable with the real-valued counterparts. For example, on Market-1501, BiRe-ID achieves 64.0% mAP on ResNet-18 backbone, with an impressive 12.51\texttimes{} speedup in theory and 11.75\texttimes{} storage saving. In particular, the KR-GAL and FR-GAL methods show strong generalization on multiple tasks such as Re-ID, image classification, object detection, and 3D point cloud processing.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = feb,
articleno = {26},
numpages = {22},
keywords = {Person re-identification, network binarization, network compression}
}

@inproceedings{10.1145/3581783.3612356,
author = {Kang, Xujie and Liu, Kanglin and Duan, Jiang and Gong, Yuanhao and Qiu, Guoping},
title = {P2I-NET: Mapping Camera Pose to Image via Adversarial Learning for New View Synthesis in Real Indoor Environments},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612356},
doi = {10.1145/3581783.3612356},
abstract = {Given a new 6DoF camera pose in an indoor environment, we study the challenging problem of predicting the view from that pose based on a set of reference RGBD views. Existing explicit or implicit 3D geometry construction methods are computationally expensive while those based on learning have predominantly focused on isolated views of object categories with regular geometric structure. Differing from the traditional render-inpaint approach to new view synthesis in the real indoor environment, we propose a conditional generative adversarial neural network (P2I-NET) to directly predict the new view from the given pose. P2I-NET learns the conditional distribution of the images of the environment for establishing the correspondence between the camera pose and its view of the environment, and achieves this through a number of innovative designs in its architecture and training lost function. Two auxiliary discriminator constraints are introduced for enforcing the consistency between the pose of the generated image and that of the corresponding real world image in both the latent feature space and the real world pose space. Additionally a deep convolutional neural network (CNN) is introduced to further reinforce this consistency in the pixel space. We have performed extensive new view synthesis experiments on real indoor datasets. Results show that P2I-NET has superior performance against a number of NeRF based strong baseline models. In particular, we show that P2I-NET is 40 to 100 times faster than these competitor techniques while synthesising similar quality images. Furthermore, we contribute a new publicly available indoor environment dataset containing 22 high resolution RGBD videos where each frame also has accurate camera pose parameters.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {2635–2643},
numpages = {9},
keywords = {conditional generative adversarial network, new view image, rgbd datasets},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@article{10.1145/3478642,
author = {Zhang, Feifei and Xu, Mingliang and Xu, Changsheng},
title = {Tell, Imagine, and Search: End-to-end Learning for Composing Text and Image to Image Retrieval},
year = {2022},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1551-6857},
url = {https://doi.org/10.1145/3478642},
doi = {10.1145/3478642},
abstract = {Composing Text and Image to Image Retrieval (CTI-IR) is an emerging task in computer vision, which allows retrieving images relevant to a query image with text describing desired modifications to the query image. Most conventional cross-modal retrieval approaches usually take one modality data as the query to retrieve relevant data of another modality. Different from the existing methods, in this article, we propose an end-to-end trainable network for simultaneous image generation and CTI-IR. The proposed model is based on Generative Adversarial Network (GAN) and enjoys several merits. First, it can learn a generative and discriminative feature for the query (a query image with text description) by jointly training a generative model and a retrieval model. Second, our model can automatically manipulate the visual features of the reference image in terms of the text description by the adversarial learning between the synthesized image and target image. Third, global-local collaborative discriminators and attention-based generators are exploited, allowing our approach to focus on both the global and local differences between the query image and the target image. As a result, the semantic consistency and fine-grained details of the generated images can be better enhanced in our model. The generated image can also be used to interpret and empower our retrieval model. Quantitative and qualitative evaluations on three benchmark datasets demonstrate that the proposed algorithm performs favorably against state-of-the-art methods.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = mar,
articleno = {59},
numpages = {23},
keywords = {Composing text and image to image retrieval, end-to-end, image generation, generative adversarial network, global-local}
}

@inproceedings{10.1145/3520304.3528770,
author = {Tang, Yujin and Tian, Yingtao and Ha, David},
title = {EvoJAX: hardware-accelerated neuroevolution},
year = {2022},
isbn = {9781450392686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520304.3528770},
doi = {10.1145/3520304.3528770},
abstract = {Evolutionary computation has been shown to be a highly effective method for training neural networks, particularly when employed at scale on CPU clusters. Recent work have also showcased their effectiveness on hardware accelerators, such as GPUs, but so far such demonstrations are tailored for very specific tasks, limiting applicability to other domains. We present EvoJAX, a scalable, general purpose, hardware-accelerated neuroevolution toolkit. Building on top of the JAX library, our toolkit enables neuroevolution algorithms to work with neural networks running in parallel across multiple TPU/GPUs. EvoJAX achieves very high performance by implementing the evolution algorithm, neural network and task all in NumPy, which is compiled just-in-time to run on accelerators. We provide extensible examples of EvoJAX for a wide range of tasks, including supervised learning, reinforcement learning and generative art. Since EvoJAX can find solutions to most of these tasks within minutes on a single accelerator, compared to hours or days when using CPUs, our toolkit can significantly shorten the iteration cycle of evolutionary computation experiments. EvoJAX is available at https://github.com/google/evojax},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {308–311},
numpages = {4},
location = {Boston, Massachusetts},
series = {GECCO '22}
}

@article{10.1145/3610909,
author = {Chen, Ling and Hu, Rong and Wu, Menghan and Zhou, Xin},
title = {HMGAN: A Hierarchical Multi-Modal Generative Adversarial Network Model for Wearable Human Activity Recognition},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
url = {https://doi.org/10.1145/3610909},
doi = {10.1145/3610909},
abstract = {Wearable Human Activity Recognition (WHAR) is an important research field of ubiquitous and mobile computing. Deep WHAR models suffer from the overfitting problem caused by the lack of a large amount and variety of labeled data, which is usually addressed by generating data to enlarge the training set, i.e., Data Augmentation (DA). Generative Adversarial Networks (GANs) have shown their excellent data generation ability, and the generalization ability of a classification model can be improved by GAN-based DA. However, existing GANs cannot make full use of the important modality information and fail to balance modality details and global consistency, which cannot meet the requirements of deep multi-modal WHAR. In this paper, a hierarchical multi-modal GAN model (HMGAN) is proposed for WHAR. HMGAN consists of multiple modal generators, one hierarchical discriminator, and one auxiliary classifier. Multiple modal generators can learn the complex multi-modal data distributions of sensor data. Hierarchical discriminator can provide discrimination outputs for both low-level modal discrimination losses and high-level overall discrimination loss to draw a balance between modality details and global consistency. Experiments on five public WHAR datasets demonstrate that HMGAN achieves the state-of-the-art performance for WHAR, outperforming the best baseline by an average of 3.4%, 3.8%, and 3.5% in accuracy, macro F1 score, and weighted F1 score, respectively.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {88},
numpages = {27},
keywords = {Wearable human activity recognition, generative adversarial network, multi-modal}
}

@inproceedings{10.1145/3535511.3535520,
author = {Gomes, Anna Luiza and Vianna, Get\'{u}lio and Escovedo, Tatiana and Kalinowski, Marcos},
title = {Predicting IMDb Rating of TV Series with Deep Learning: The Case of Arrow},
year = {2022},
isbn = {9781450396981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535511.3535520},
doi = {10.1145/3535511.3535520},
abstract = {Context: The number of TV series offered nowadays is very high. Due to its large amount, many series are canceled due to a lack of originality that generates a low audience.Problem: Having a decision support system that can show why some shows are a huge success or not would facilitate the choices of renewing or starting a show.Solution: We studied the case of the series Arrow broadcasted by CW Network and used descriptive and predictive modeling techniques to predict the IMDb rating. We assumed that the theme of the episode would affect its evaluation by users, so the dataset is composed only by the director of the episode, the number of reviews that episode got, the percentual of each theme extracted by the Latent Dirichlet Allocation (LDA) model of an episode, the number of viewers from Wikipedia and the rating from IMDb. The LDA model is a generative probabilistic model of a collection of documents made up of words.IS Theory: This study was developed under the aegis of Computational Learning Theory, which aims to understand the fundamental principles of learning and contribute to designing better-automated learning methods applied to the entertainment business.Method: In this prescriptive research, the case study method was used, and its results were analyzed using a quantitative approach.Summary of Results: With the features of each episode, the model that performed the best to predict the rating was Catboost due to a similar mean squared error of the KNN model but a better standard deviation during the test phase. It was possible to predict IMDb ratings with an acceptable root mean squared error of 0.55.Contributions and Impact in the IS area: The results show that deep learning techniques can be applied to support decisions in the entertainment field, allowing facilitating the decisions of renewing or starting a show. The rationale for building the model is detailed throughout the paper and can be replicated for other contexts.},
booktitle = {Proceedings of the XVIII Brazilian Symposium on Information Systems},
articleno = {9},
numpages = {6},
keywords = {Deep Learning, IMDb, LDA, Machine Learning, prediction, tv-series},
location = {Curitiba, Brazil},
series = {SBSI '22}
}

@inproceedings{10.1145/3548814.3551460,
author = {Shimizu, Kye and Ienaga, Naoto and Takada, Kazuma and Sugimoto, Maki and Kasahara, Shunichi},
title = {Human Latent Metrics: Perceptual and Cognitive Response Correlates to Distance in GAN Latent Space for Facial Images},
year = {2022},
isbn = {9781450394550},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548814.3551460},
doi = {10.1145/3548814.3551460},
abstract = {Generative adversarial networks (GANs) generate high-dimensional vector spaces (latent spaces) that can interchangeably represent vectors as images. Advancements have extended their ability to computationally generate images indistinguishable from real images such as faces, and more importantly, to manipulate images using their inherit vector values in the latent space. This interchangeability of latent vectors has the potential to calculate not only the distance in the latent space, but also the human perceptual and cognitive distance toward images, that is, how humans perceive and recognize images. However, it is still unclear how the distance in the latent space correlates with human perception and cognition. Our studies investigated the relationship between latent vectors and human perception or cognition through psycho-visual experiments that manipulates the latent vectors of face images. In the perception study, a change perception task was used to examine whether participants could perceive visual changes in face images before and after moving an arbitrary distance in the latent space. In the cognition study, a face recognition task was utilized to examine whether participants could recognize a face as the same, even after moving an arbitrary distance in the latent space. Our experiments show that the distance between face images in the latent space correlates with human perception and cognition for visual changes in face imagery, which can be modeled with a logistic function. By utilizing our methodology, it will be possible to interchangeably convert between the distance in the latent space and the metric of human perception and cognition, potentially leading to image processing that better reflects human perception and cognition.},
booktitle = {ACM Symposium on Applied Perception 2022},
articleno = {3},
numpages = {10},
keywords = {change perception, face cognition, generative adversarial networks},
location = {TBC, USA},
series = {SAP '22}
}

@inproceedings{10.1145/3569052.3572993,
author = {Lu, Yi-Chen and Ren, Haoxing and Hsiao, Hao-Hsiang and Lim, Sung Kyu},
title = {DREAM-GAN: Advancing DREAMPlace towards Commercial-Quality using Generative Adversarial Learning},
year = {2023},
isbn = {9781450399784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3569052.3572993},
doi = {10.1145/3569052.3572993},
abstract = {DREAMPlace is a renowned open-source placer that provides GPU-acceleratable infrastructure for placements of Very-Large-Scale-Integration (VLSI) circuits. However, due to its limited focus on wirelength and density, existing placement solutions of DREAMPlace are not applicable to industrial design flows. To improve DREAMPlace towards commercial-quality without knowing the black-boxed algorithms of the tools, in this paper, we present DREAM-GAN, a placement optimization framework that advances DREAMPlace using generative adversarial learning. At each placement iteration, aside from optimizing the wirelength and density objectives of the vanilla DREAMPlace, DREAM-GAN computes and optimizes a differentiable loss that denotes the similarity score between the underlying placement and the tool-generated placements in commercial databases. Experimental results on 5 commercial and OpenCore designs using an industrial design flow implemented by Synopsys ICC2 not only demonstrate that DREAM-GAN significantly improves the vanilla DREAMPlace at the placement stage across each benchmark, but also show that the improvements last firmly to the post-route stage, where we observe improvements by up to 8.3% in wirelength and 7.4% in total power.},
booktitle = {Proceedings of the 2023 International Symposium on Physical Design},
pages = {141–148},
numpages = {8},
keywords = {generative adversarial learning, placement optimization},
location = {Virtual Event, USA},
series = {ISPD '23}
}

@inproceedings{10.1145/3588444.3591050,
author = {Henry, Chris and Liao, Rijun and Lin, Ruiyuan and Zhang, Zhebin and Sun, Hongyu and Li, Zhu},
title = {Fast and Robust Video Deduplication},
year = {2023},
isbn = {9798400701603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588444.3591050},
doi = {10.1145/3588444.3591050},
abstract = {The popularity of social media networks and mobile devices have skyrocketed in recent years. This has led to the rapid increase in the video content being recorded and uploaded to online platforms like TikTok, YouTube, etc. As a result, a rise in the amount of illegal pirate videos can be observed. These pirate videos have the same content as original videos but with minor editing effects and variations in coding. The task of finding such duplicate videos is known as video deduplication. Also, storing this huge amount of video data is a challenging issue.Generally, video deduplication systems rely on feature extraction followed by computing a similarity score. The ideal goal is to develop a discriminative feature to differentiate among similar videos. In this work, we present a robust and lightweight video deduplication that can find duplicate videos among a large video repository extremely quickly within a few milliseconds. We propose a robust and highly discriminative feature that was generated using fisher vector and a thumbnail feature. The fisher vector was generated by applying fisher vector aggregation on Scale-Invariant Feature Transform (SIFT) keypoints with Gaussian Mixture Model (GMM) as the generative model. GMM was trained via SIFT key-points extracted from frames uniformly sampled from videos. For the thumbnail feature, each frame is resized to a 12x12 resolution. We reduce the dimensions of the fisher vector and thumbnail feature to 32-d by using principal component analysis (PCA), leaving us with our proposed deduplication features.A large repository of 1 million frames was generated by extracting frames from videos at uniform steps. Each frame in the repository was assigned a unique global timestamp. For fast video retrieval from the large repository, a multiple k-d tree setup was designed that trains a separate k-d tree for the fisher vector and the thumbnail feature. KNN search was used to search the k-d tree which returns the samples nearest to the query. In our case, we combine the nodes retrieved from both the k-d trees for better retrieval results.The k-d tree retrieval provides us with a frame retrieval system. For video retrieval, we develop a data pruning strategy that utilizes the sequence ID and timestamp information to accurately retrieve duplicate videos. CDVS dataset [1] was used for training the GMM. The 1 million frame repository and the test data were generated using the large-scale FIVR-200K dataset [2]. Experimental results show that our retrieval results are highly accurate and the system can process a query video within a few milliseconds.},
booktitle = {Proceedings of the 2nd Mile-High Video Conference},
pages = {160},
numpages = {1},
keywords = {video deduplication, near-duplicate video retrieval, copy detection, fisher vector},
location = {Denver, CO, USA},
series = {MHV '23}
}

@inproceedings{10.1145/3577164.3577169,
author = {Zhao, Zhihong and You, Jieshun and Cui, Yunong},
title = {Freshness Recognition of Fruit and Vegetable Images using GANs Series Data Augmentation},
year = {2023},
isbn = {9781450397810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577164.3577169},
doi = {10.1145/3577164.3577169},
abstract = {The use of computer vision techniques to distinguish fresh from stale fruit has gained widespread acceptance in recent years. However, the scarcity of datasets in this field has made it more difficult for many of the research results to be accessible in real-life situations. The core idea of this paper is to improve the generalization of the training set and the performance of the classifier by augmenting the fruit and vegetable image dataset with neural networks. This paper will investigate the applicability of the established Conditional Generative Adversarial Nets (CGAN) in this domain and discover problems such as mode collapse. The contribution of this paper is to combine existing structures and propose novel adversarial generative networks for the fruit and vegetable image, including Conditional Wasserstein GAN with Gradient Penalty (CWGAN-GP) and Auxiliary Classifier Wasserstein GAN with Gradient Penalty (ACWGAN-GP). In addition, a pre-trained models with Spinal fully-connected layer and ProgressiveSpinal fully-connected layer was used to freshness classify. It was found that ACWGAN-GP, which combines multiple network structures, not only produced more realistic and diverse images in the fruit and vegetable domain, but also maintained the learning speed of a single model during the training. In addition, by adding synthetic images to the training, the fruit and vegetable freshness classifier achieves 100% accuracy on a specified validation set.},
booktitle = {Proceedings of the 2022 4th International Conference on Video, Signal and Image Processing},
pages = {29–36},
numpages = {8},
keywords = {Data augmentation, Freshness recognition, Image generation},
location = {Shanghai, China},
series = {VSIP '22}
}

@inproceedings{10.1145/3565287.3617628,
author = {Brewington, Joseph and Kar, Dulal},
title = {UAV GPS Spoofing Detection via Neural Generative One-Class Classification},
year = {2023},
isbn = {9781450399265},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3565287.3617628},
doi = {10.1145/3565287.3617628},
abstract = {Unmanned Aerial Vehicles (UAVs) are increasingly used for civilian applications but remain vulnerable to spoofing attacks due to their reliance on unencrypted Global Positioning System (GPS) signals. While traditional hardware-based detection methods are effective, they are often inflexible and costly. Through comprehensive simulations and comparative analysis, this study delves into the potential of neural generative one-class classifiers, particularly Generative Adversarial Networks (GANs), for GPS spoofing detection in civilian UAVs. Results indicate that these models provide consistent and computationally efficient one-class classification. Future research directions include exploring further extensions of these models and testing in real-world UAV scenarios.},
booktitle = {Proceedings of the Twenty-Fourth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing},
pages = {492–497},
numpages = {6},
keywords = {unmanned aerial vehicles, global positioning system, GPS spoofing, intrusion detection, variational autoencoders, generative adversarial networks, machine learning, one-class classification, anomaly detection, autoencoder, security},
location = {Washington, DC, USA},
series = {MobiHoc '23}
}

@inproceedings{10.1145/3583133.3590626,
author = {Ripa, Guillermo and Mautone, Agust\'{\i}n and Vidal, Andr\'{e}s and Nesmachnow, Sergio and Toutouh, Jamal},
title = {Multiobjective coevolutionary training of Generative Adversarial Networks},
year = {2023},
isbn = {9798400701207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583133.3590626},
doi = {10.1145/3583133.3590626},
abstract = {This article presents a multiobjective evolutionary approach for coevolutionary training of Generative Adversarial Networks. The proposal applies an explicit multiobjective optimization approach based on Pareto ranking and non-dominated sorting over the co-evolutionary search implemented by the Lipizzaner framework, to optimize the quality and diversity of the generated synthetic data. Two functions are studied for evaluating diversity. The main results obtained for the handwritten digits generation problem show that the proposed multiobjective search is able to compute accurate and diverse solutions, improving over the standard Lipizzaner implementation.},
booktitle = {Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
pages = {319–322},
numpages = {4},
keywords = {generative adversarial networks, multiobjective optimization, co-evolutionary algorithms, image generation},
location = {Lisbon, Portugal},
series = {GECCO '23 Companion}
}

@inproceedings{10.1145/3586209.3591404,
author = {Williams, Mathew and Kokalj-Filipovic, Silvija and Rodriguez, Armani},
title = {Analysis of Lossy Generative Data Compression for Robust Remote Deep Inference},
year = {2023},
isbn = {9798400701337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586209.3591404},
doi = {10.1145/3586209.3591404},
abstract = {Networks of wireless sensors, including Internet of Things (IoT), motivate the use of lossy compression of the sensor data to match the available network bandwidth (BW). Hence, sensor data intended for inference by a remote deep learning (RDL) model is likely to be reconstructed with distortion, from a compressed representation received by the remote user over a wireless channel. Our focus is a particular type of lossy compression algorithm based on DL models, and known as learned compression (LC). The link between the information loss and compression rate in LCs has not been studied yet in the framework of information theory, nor practically associated with any meta-data which could describe the type and level of information loss to downstream users. This may make this compression undetectable yet potentially harmful. We study the robustness of a RDL classification model against the lossy compression of the input, including the robustness under an adversarial attack. We apply different compression methods of MNIST images, such as JPEG and a hierarchical LC, all with different compression ratios. For any lossy reconstruction and its uncompressed original, several techniques for topological feature characterization based on persistent homology are used to highlight the important differences amongst compression approaches that may affect the robust accuracy of a DL classifier trained on the original data. We conclude that LC is preferred in the described context, because we achieve the same accuracy as with the originals (with and without an adversarial attack) on a trained DL MNIST classifier, while using only 1/4 of the BW. We show that calculated topological features differ between JPEG and the comparable LC reconstructions, which are closer to the features of the original. We show that there is a distribution shift in those features due to the attack. Finally, most LC models are generative, meaning that we can generate multiple statistically independent compressed representations of a data point, which opens the possibility for the inference error correction at the RDL model. Due to space limitations, we leave this aspect for future work.},
booktitle = {Proceedings of the 2023 ACM Workshop on Wireless Security and Machine Learning},
pages = {33–38},
numpages = {6},
keywords = {adversarial attack, generative deep learning, learned compression, lossy compression, persistent homology},
location = {Guildford, United Kingdom},
series = {WiseML'23}
}

@inproceedings{10.1145/3529466.3529475,
author = {Li, Qinyang and Fan, Wentao},
title = {Mixture Density Hyperspherical Generative Adversarial Networks},
year = {2022},
isbn = {9781450395502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529466.3529475},
doi = {10.1145/3529466.3529475},
abstract = {The Generative Adversarial Networks (GANs) are deep generative models that can generate realistic samples, but they are difficult to train in practice due to the problem of mode collapse, where the generator only repeatedly generates one mode in samples during the learning process, or only generates a small number of modes after reaching the Nash equilibrium during the adversarial training. In order to solve this issue while making the generator contains promising generation ability, we propose a mixture density hyperspherical generative model namely MDH-GAN that combines variational autoencoder (VAE) and generative adversarial network. Unlike most of the GAN-based generative models that consider a Gaussian prior, MDH-GAN adopts the von Mises-Fisher (vMF) prior defined on a unit hypersphere. Our model combines VAE with GAN by integrating the encoder of VAE with GAN to form a jointly training framework. Therefore, the generator of our model can learn data distribution with a hyperspherical latent structure, leading to an improved generative ability of the generator. Moreover, a vMF mixture model is deployed in the discriminator to form a hypersphere space to avoid mode collapse of the model. In our experiments, by calculating the Fr\'{e}chet Inception distance (FID) between the generated images and real ones, we prove that MDH-GAN has a better ability to generate high-quality images with high diversity.},
booktitle = {Proceedings of the 2022 6th International Conference on Innovation in Artificial Intelligence},
pages = {31–37},
numpages = {7},
location = {Guangzhou, China},
series = {ICIAI '22}
}

@inproceedings{10.1145/3580305.3599252,
author = {Wang, Xu and Zhao, Huan and Tu, Wei-wei and Yao, Quanming},
title = {Automated 3D Pre-Training for Molecular Property Prediction},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599252},
doi = {10.1145/3580305.3599252},
abstract = {Molecular property prediction is an important problem in drug discovery and materials science. As geometric structures have been demonstrated necessary for molecular property prediction,3D information has been combined with various graph learning methods to boost prediction performance. However, obtaining the geometric structure of molecules is not feasible in many real-world applications due to the high computational cost. In this work, we propose a novel 3D pre-training framework (dubbed 3D PGT), which pre-trains a model on 3D molecular graphs, and then fine-tunes it on molecular graphs without 3D structures. Based on fact that bond length, bond angle, and dihedral angle are three basic geometric descriptors corresponding to a complete molecular 3D conformer, we first develop a multi-task generative pre-train framework based on these three attributes. Next, to automatically fuse these three generative tasks, we design a surrogate metric using the total energy to search for weight distribution of the three pretext tasks since total energy corresponding to the quality of 3D conformer. Extensive experiments on 2D molecular graphs are conducted to demonstrate the accuracy, efficiency and generalization ability of the proposed 3D PGT compared to various pre-training baselines.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2419–2430},
numpages = {12},
keywords = {3d pre-training, graph transformer, molecular property prediction},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3530190.3534817,
author = {Ge, Xiou and Goodwin, Richard T. and Yu, Haizi and Romero, Pablo and Abdelrahman, Omar and Sudhalkar, Amruta and Kusuma, Julius and Cialdella, Ryan and Garg, Nishant and Varshney, Lav R.},
title = {Accelerated Design and Deployment of Low-Carbon Concrete for Data Centers},
year = {2022},
isbn = {9781450393478},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3530190.3534817},
doi = {10.1145/3530190.3534817},
abstract = {Concrete is the most widely used engineered material in the world with more than 10 billion tons produced annually. Unfortunately, with that scale comes a significant burden in terms of energy, water, and release of greenhouse gases and other pollutants; indeed 8% of worldwide carbon emissions are attributed to the production of cement, a key ingredient in concrete. As such, there is interest in creating concrete formulas that minimize this environmental burden, while satisfying engineering performance requirements including compressive strength. Specifically for computing, concrete is a major ingredient in the construction of data centers. In this work, we use conditional variational autoencoders (CVAEs), a type of semi-supervised generative artificial intelligence (AI) model, to discover concrete formulas with desired properties. Our model is trained just using a small open dataset from the UCI Machine Learning Repository joined with environmental impact data from standard lifecycle analysis. Computational predictions demonstrate CVAEs can design concrete formulas with much lower carbon requirements than existing formulations while meeting design requirements. Next we report laboratory-based compressive strength experiments for five AI-generated formulations, which demonstrate that the formulations exceed design requirements. The resulting formulations were then used by Ozinga Ready Mix—a concrete supplier—to generate field-ready concrete formulations, based on local conditions and their expertise in concrete design. Finally, we report on how these formulations were used in the construction of buildings and structures in a Meta data center in DeKalb, IL, USA. Results from field experiments as part of this real-world deployment corroborate the efficacy of AI-generated low-carbon concrete mixes.},
booktitle = {Proceedings of the 5th ACM SIGCAS/SIGCHI Conference on Computing and Sustainable Societies},
pages = {340–352},
numpages = {13},
keywords = {artificial intelligence, concrete, sustainable building materials, variational autoencoders},
location = {Seattle, WA, USA},
series = {COMPASS '22}
}

@article{10.1145/3584019,
author = {M, Diviya and A, Karmel},
title = {TAM GAN: Tamil Text to Naturalistic Image Synthesis Using Conventional Deep Adversarial Networks},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3584019},
doi = {10.1145/3584019},
abstract = {Text-to-image synthesis has advanced recently as a prospective area for improvement in computer vision applications. The image synthesis model follows significant neural network architectures such as Generative Adversarial Networks (GANs). The flourishing text-to-image generation approaches can nominally reflect the meaning of the text in generated images. Still, they need the prospect of providing the necessary details and eloquent object features. Intelligent systems are trained in text-to-image synthesis applications for various languages. However, their contribution to regional languages is yet to be explored. Autoencoders prompt the synthesis of images, but they result in blurriness, which results in clear output and essential features of the picture. Based on textual descriptions, The GAN model is capable of producing realistic images of a high quality that can be used in various applications, like fashion design, photo editing, computer-aided design, and educational platforms. The proposed method uses two-stage processing to create a language model using a BERT model called TAM-BERT and an existing MuRIL BERT, followed by image synthesis using a GAN. The work was conducted using the Oxford-102 dataset, and the model's efficiency was evaluated using the F1-Score measure.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {128},
numpages = {18},
keywords = {Computer vision, Generative Adversarial Network (GAN), BERT, MuRIL BERT, language model, L1Norm, feature matching, latent vectors}
}

@article{10.1145/3618398,
author = {Chakravarthula, Praneeth and Sun, Jipeng and Li, Xiao and Lei, Chenyang and Chou, Gene and Bijelic, Mario and Froesch, Johannes and Majumdar, Arka and Heide, Felix},
title = {Thin On-Sensor Nanophotonic Array Cameras},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3618398},
doi = {10.1145/3618398},
abstract = {Today's commodity camera systems rely on compound optics to map light originating from the scene to positions on the sensor where it gets recorded as an image. To record images without optical aberrations, i.e., deviations from Gauss' linear model of optics, typical lens systems introduce increasingly complex stacks of optical elements which are responsible for the height of existing commodity cameras. In this work, we investigate flat nanophotonic computational cameras as an alternative that employs an array of skewed lenslets and a learned reconstruction approach. The optical array is embedded on a metasurface that, at 700 nm height, is flat and sits on the sensor cover glass at 2.5 mm focal distance from the sensor. To tackle the highly chromatic response of a metasurface and design the array over the entire sensor, we propose a differentiable optimization method that continuously samples over the visible spectrum and factorizes the optical modulation for different incident fields into individual lenses. We reconstruct a megapixel image from our flat imager with a learned probabilistic reconstruction method that employs a generative diffusion model to sample an implicit prior. To tackle scene-dependent aberrations in broadband, we propose a method for acquiring paired captured training data in varying illumination conditions. We assess the proposed flat camera design in simulation and with an experimental prototype, validating that the method is capable of recovering images from diverse scenes in broadband with a single nanophotonic layer.},
journal = {ACM Trans. Graph.},
month = dec,
articleno = {249},
numpages = {18},
keywords = {computational optics}
}

@inproceedings{10.1145/3580305.3599520,
author = {Hussain, Md Shamim and Zaki, Mohammed J. and Subramanian, Dharmashankar},
title = {The Information Pathways Hypothesis: Transformers are Dynamic Self-Ensembles},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599520},
doi = {10.1145/3580305.3599520},
abstract = {Transformers use the dense self-attention mechanism which gives a lot of flexibility for long-range connectivity. Over multiple layers of a deep transformer, the number of possible connectivity patterns increases exponentially. However, very few of these contribute to the performance of the network, and even fewer are essential. We hypothesize that there are sparsely connected sub-networks within a transformer, called information pathways which can be trained independently. However, the dynamic (i.e., input-dependent) nature of these pathways makes it difficult to prune dense self-attention during training. But the overall distribution of these pathways is often predictable. We take advantage of this fact to propose Stochastically Subsampled self-Attention (SSA) - a general-purpose training strategy for transformers that can reduce both the memory and computational cost of self-attention by 4 to 8 times during training while also serving as a regularization method - improving generalization over dense training. We show that an ensemble of sub-models can be formed from the subsampled pathways within a network, which can achieve better performance than its densely attended counterpart. We perform experiments on a variety of NLP, computer vision and graph learning tasks in both generative and discriminative settings to provide empirical evidence for our claims and show the effectiveness of the proposed method.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {810–821},
numpages = {12},
keywords = {ensemble methods, information pathway, self-attention, sparse attention, transformer neural networks},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1109/DAC18074.2021.9586171,
author = {Hamilton, Kathleen E. and Lynn, Emily and Kharazi, Tyler and Morris, Titus and Bennink, Ryan S. and Pooser, Raphael C.},
title = {Building Scalable Variational Circuit Training for Machine Learning Tasks},
year = {2022},
isbn = {9781665432740},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/DAC18074.2021.9586171},
doi = {10.1109/DAC18074.2021.9586171},
abstract = {Parameterized quantum circuits (PQC) have emerged as a quantum analogue of deep neural networks and can be trained for discriminative or generative tasks and can be trained with gradient-based optimization on near-term quantum devices [1], [2], [3]. In the current era of quantum computing, known as the noisy intermediate scale quantum (NISQ) era [4], these devices contain a moderate number of qubits (&lt; 100), and algorithmic performance is strongly impacted by hardware noise. Additionally, the training of PQCs are hybrid algorithms, in which the computational workflow is split between quantum and classical computing platforms.},
booktitle = {Proceedings of the 58th Annual ACM/IEEE Design Automation Conference},
pages = {1351},
numpages = {1},
keywords = {quantum computing, NISQ computing, error mitigation, noise characterization},
location = {San Francisco, California, United States},
series = {DAC '21}
}

@inproceedings{10.1145/3591106.3592262,
author = {Wu, Yankun and Nakashima, Yuta and Garcia, Noa},
title = {Not Only Generative Art: Stable Diffusion for Content-Style Disentanglement in Art Analysis},
year = {2023},
isbn = {9798400701788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591106.3592262},
doi = {10.1145/3591106.3592262},
abstract = {The duality of content and style is inherent to the nature of art. For humans, these two elements are clearly different: content refers to the objects and concepts in the piece of art, and style to the way it is expressed. This duality poses an important challenge for computer vision. The visual appearance of objects and concepts is modulated by the style that may reflect the author’s emotions, social trends, artistic movement, etc., and their deep comprehension undoubtfully requires to handle both. A promising step towards a general paradigm for art analysis is to disentangle content and style, whereas relying on human annotations to cull a single aspect of artworks has limitations in learning semantic concepts and the visual appearance of paintings. We thus present GOYA, a method that distills the artistic knowledge captured in a recent generative model to disentangle content and style. Experiments show that synthetically generated images sufficiently serve as a proxy of the real distribution of artworks, allowing GOYA to separately represent the two elements of art while keeping more information than existing methods.},
booktitle = {Proceedings of the 2023 ACM International Conference on Multimedia Retrieval},
pages = {199–208},
numpages = {10},
keywords = {art analysis, representation disentanglement, text-to-image generation},
location = {Thessaloniki, Greece},
series = {ICMR '23}
}

@inproceedings{10.1145/3531146.3533137,
author = {Cooper, A. Feder and Vidan, Gili},
title = {Making the Unaccountable Internet: The Changing Meaning of Accounting in the Early ARPANET},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533137},
doi = {10.1145/3531146.3533137},
abstract = {Contemporary concerns over the governance of technological systems often run up against narratives about the technical infeasibility of designing mechanisms for accountability. While in recent AI ethics literature these concerns have been deliberated predominantly in relation to machine learning, other instances in the history of computing also presented circumstances in which computer scientists needed to un-muddle what it means to design accountable systems. One such compelling narrative can frequently be found in canonical histories of the Internet that highlight how its original designers’ commitment to the “End-to-End” architectural principle precluded other features from being implemented, resulting in the fast-growing, generative, but ultimately unaccountable network we have today. This paper offers a critique of such technologically essentialist notions of accountability and the characterization of the “unaccountable Internet” as an unintended consequence. It explores the changing meaning of accounting and its relationship to accountability in a selected corpus of requests for comments (RFCs) concerning the early Internet’s design from the 1970s and 80s. We characterize four ways of conceptualizing accounting: as billing, as measurement, as management, and as policy, and demonstrate how an understanding of accountability was constituted through these shifting meanings. We link together the administrative and technical mechanisms of accounting for shared resources in a distributed system and an emerging notion of accountability as a social, political, and technical category, arguing that the former is constitutive of the latter. Recovering this history is not only important for understanding the processes that shaped the Internet, but also serves as a starting point for unpacking the complicated political choices that are involved in designing accountability mechanisms for other technological systems today.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {726–742},
numpages = {17},
keywords = {Accountability, Accountable systems, Accounting, Internet governance, Resource sharing},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@article{10.5555/3648699.3648909,
author = {Bengio, Yoshua and Lahlou, Salem and Deleu, Tristan and Hu, Edward J. and Tiwari, Mo and Bengio, Emmanuel},
title = {GFlowNet foundations},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of GFlowNets, including a new local and efficient training objective called detailed balance for the analogy with MCMC. GFlowNets can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. GFlowNets amortize the work typically done by computationally expensive MCMC methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entropy and mutual information, continuous actions and modular energy functions.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {210},
numpages = {55}
}

@inproceedings{10.1145/3588428.3593821,
author = {Rozin, Daniel},
title = {RGB Peg Mirror},
year = {2023},
isbn = {9798400701573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588428.3593821},
doi = {10.1145/3588428.3593821},
abstract = {In SIGGRAPH 2000 I debuted my piece Wooden Mirror. Over the past 23 years I have been expanding my series of Mechanical Mirrors to include tens of pieces utilizing various materials and techniques to examine the creation and perception of images and computer graphic concepts in a physical, interactive manner. Recently I have introduced color to my Mechanical Mirrors, the piece RGB Peg Mirror is my first piece that reproduces a full color reflection of the viewer by mechanically controlling 1555 motorized Red, Green, Blue aluminum pegs. I thought it would be appropriate to show this milestone piece where the series began. RGB Peg Mirror has two modes of operation and those change every two minutes: The first mode is the interactive reflective mode where the viewer is reflected on the piece in real time. The second mode is the generative mode, in this mode the piece displays various animations that were designed to take advantage of the colorful/ dynamic nature of the piece.},
booktitle = {ACM SIGGRAPH 2023 Art Gallery},
articleno = {11},
numpages = {1},
location = {Los Angeles, California},
series = {SIGGRAPH '23}
}

@inproceedings{10.1145/3627915.3629596,
author = {Miura, Shingo and Mizutani, Tetsuya},
title = {Analysis of Performance Expressions Using Sequential Tension Values},
year = {2023},
isbn = {9798400700590},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627915.3629596},
doi = {10.1145/3627915.3629596},
abstract = {Human performers interpret and perform a piece of music based on their own musical sensibilities and the musical characteristics they read from the information in the score. In doing so, the changes in speed and volume that appear in the performance are called performance expressions. This performance expression changes each time the music piece or performer changes, and we believe that formalizing this expression will make it possible to perform more human-like performances on a computer.In this study, we focused on the fact that Generative Theory of Tonal Music (GTTM) [1], one of the cognitive music theories, uses a parameter called tension value to analyze a piece of music. However, one method of calculating tension values, hierarchical tension values, is not suitable for automatic analysis because it requires consideration based on advanced musical knowledge when calculating them. Therefore, this study uses a parameter called sequential tension value, which is obtained by a different calculation method. Although sequential tension values are less accurate than hierarchical tension values, they have the advantage of being easy to calculate and suitable for automatic analysis. This paper discusses how to improve the accuracy of analysis of musical pieces using sequential tension values and how each component of the musical structure that makes up the sequential tension value affects the analysis results. Multiple regression analysis is used for this analysis, with the musical expression obtained from the local tempo and volume changes for each event calculated from the score data as the objective variable and the musical symbols in each event as the explanatory variables. The accuracy of the obtained regression model is compared with the methods used in previous studies, indicating that the analysis using sequential tension values is useful for music analysis.},
booktitle = {Proceedings of the 7th International Conference on Computer Science and Application Engineering},
articleno = {25},
numpages = {7},
keywords = {CrestMusePEDB, Sequential Tension Value, Tonal Pitch Space},
location = {Virtual Event, China},
series = {CSAE '23}
}

@inproceedings{10.1145/3581754.3584124,
author = {Goodarzy, Sepideh and Keller, Eric and Nazari, Maziyar and Rozner, Eric and Han, Richard and Dras, Mark and Lee, Young Choon and Richards, Deborah},
title = {Capturing and Predicting User Frustration to Support a Smart Operating System},
year = {2023},
isbn = {9798400701078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581754.3584124},
doi = {10.1145/3581754.3584124},
abstract = {This paper presents an IRB-approved human study to capture data to build models for human frustration prediction of computer users. First, an application was developed that ran in the user’s computer/laptop/VM with Linux 20.04. Then, the application collected a variety of data from their computers, including: mouse clicks, movements and scrolls; the pattern of keyboard keys clicks; user audio features; and head movements through the user video; System-wide information such as computation, memory usage, network bandwidth, and input/output bandwidth of the running applications in the computer and user frustrations. Finally, the application sent the data to the cloud. After two weeks of data collection, supervised and semi-supervised models were developed offline to predict user frustration with the computer using the collected data. A semi-supervised model using a generative adversarial network (GAN) resulted in the highest accuracy of 90%.},
booktitle = {Companion Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {29–32},
numpages = {4},
keywords = {Computer-based User Frustration, Intelligent Operating System},
location = {Sydney, NSW, Australia},
series = {IUI '23 Companion}
}

@inproceedings{10.1145/3604078.3604167,
author = {Liu, Wenhui and Liu, Bin and Lin, Xiao and Li, Bo and He, Zhifen and Wang, Kang},
title = {ArtDiff: Artwork Generation via Conditional Diffusion Models},
year = {2023},
isbn = {9798400708237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604078.3604167},
doi = {10.1145/3604078.3604167},
abstract = {Artwork Generation is an important research area of computer vision. Recently, kinds of generative models have achieved great success in natural image generation. However, artwork generation has rarely been studied due to the unfixed structure of artworks. Combined with prevailing diffusion models, we propose a simple yet effective framework, named as ArtDiff, for artwork generation. Given a name of an artist, we can generate diverse and novel artworks which reflecting the style of the artist. To the best of our knowledge, ArtDiff is the first diffusion model that generate artworks with the guidance of artist's name. Experimental results demonstrate that ArtDiff is able to recognize the artist's preference and generate artworks with reasonable structures and fine-grained details.},
booktitle = {Proceedings of the 15th International Conference on Digital Image Processing},
articleno = {88},
numpages = {6},
keywords = {Artwork Generation, denoising diffusion probabilistic models, image synthesis},
location = {Nanjing, China},
series = {ICDIP '23}
}

@article{10.1145/3595244.3595259,
author = {Sam, Tyler and Chen, Yudong and Lee Yu, Christina},
title = {Overcoming the Long Horizon Barrier for Sample-Efficient Reinforcement Learning with Latent Low-Rank Structure},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0163-5999},
url = {https://doi.org/10.1145/3595244.3595259},
doi = {10.1145/3595244.3595259},
abstract = {Reinforcement learning (RL) methods have been increasingly popular in sequential decision making tasks due to its empirical success. However, large state and action spaces in real-world problems modeled as a Markov decision processes (MDPs) limit the use of RL algorithms. Given a finite-horizon MDP with state space S, action space A, and horizon H, one needs Ω |S|A|H3/ε2 samples given a generative model to learn an optimal policy [3], which can be impractical when S and A are large. The above tabular RL framework does not capture the fact that many realworld systems in fact have additional structure that if exploited should improve computational and statistical efficiency. Moreover, [1] empirically verifies that optimal and near-optimal action-value functions (both viewed as |S|-by- |A| matrices) of classical stochastic control tasks have low rank. Thus, the critical question is what are the minimal low rank structural assumptions that allow for computationally and statistically efficient learning.},
journal = {SIGMETRICS Perform. Eval. Rev.},
month = apr,
pages = {41–43},
numpages = {3}
}

@inproceedings{10.5555/3586210.3586443,
author = {Montevechi, Jos\'{e} Arnaldo Barra and Gabriel, Gustavo Teodoro and Campos, Afonso Teberga and Santos, Carlos Henrique dos and Leal, Fabiano and Machado, Michael E. F. H. S.},
title = {Using Generative Adversarial Networks to Validate Discrete Event Simulation Models},
year = {2023},
publisher = {IEEE Press},
abstract = {Computer model validation is an essential step in simulation projects. The literature suggests using statistical techniques for comparing the outputs from the simulated model and the real system; however, statistical assumptions may be violated. Thus, Generative Adversarial Networks (GANs) are an alternative since they adapt to any data. The work aims to use GANs to generate synthetic data from the real data and use the Discriminator to discriminate real from simulated outputs. Five statistical distributions were trained, and distributions with the same characteristics were submitted to verify the Power of the Test. The curves of each distribution were generated. In addition, a real case of a Discrete Event Simulation in a large emergency department was applied to the new validation technique. The results showed that GANs effectively discriminate data and can help validate computer models.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2772–2783},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.1145/3534678.3539020,
author = {Ren, Houxing and Wang, Jingyuan and Zhao, Wayne Xin},
title = {Generative Adversarial Networks Enhanced Pre-training for Insufficient Electronic Health Records Modeling},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539020},
doi = {10.1145/3534678.3539020},
abstract = {In recent years, automatic computational systems based on deep learning are widely used in medical fields, such as automatic diagnosing and disease prediction. Most of these systems are designed for data sufficient scenarios. However, due to the disease rarity or privacy, the medical data are always insufficient. When applying these data-hungry deep learning models with insufficient data, it is likely to lead to issues of over-fitting and cause serious performance problems. Many data augmentation methods have been proposed to solve the data insufficiency problem, such as using GAN (Generative Adversarial Networks) to generate training data. However, the augmented data usually contains lots of noise. Directly using them to train sensitive medical models is very difficult to achieve satisfactory results.To overcome this problem, we propose a novel deep model learning method for insufficient EHR (Electronic Health Record) data modeling, namely GRACE, which stands GeneRative Adversarial networks enhanCed prE-training. In the method, we propose an item-relation-aware GAN to capture changing trends and correlations among data for generating high-quality EHR records. Furthermore, we design a pre-training mechanism consisting of a masked records prediction task and a real-fake contrastive learning task to learn representations for EHR data using both generated and real data. After the pre-training, only the representations of real data is used to train the final prediction model. In this way, we can fully exploit useful information in generated data through pre-training, and also avoid the problems caused by directly using noisy generated data to train the final prediction model. The effectiveness of the proposed method is evaluated using extensive experiments on three healthcare-related real-world datasets. We also deploy our method in a maternal and child health care hospital for the online test. Both offline and online experimental results demonstrate the effectiveness of the proposed method. We believe doctors and patients can benefit from our effective learning method in various healthcare-related applications.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3810–3818},
numpages = {9},
keywords = {healthcare informatics, pre-training, representation learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.5555/3539845.3540004,
author = {Elfar, Mahmoud and Liang, Tung-Che and Chakrabarty, Krishnendu and Pajic, Miroslav},
title = {Adaptive droplet routing for MEDA biochips via deep reinforcement learning},
year = {2022},
isbn = {9783981926361},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {Digital microfluidic biochips (DMFBs) based on a micro-electrode-dot-array (MEDA) architecture provide finegrained control and sensing of droplets in real-time. However, excessive actuation of microelectrodes in MEDA biochips can lead to charge trapping during bioassay execution, causing the failure of microelectrodes and erroneous bioassay outcomes. A recently proposed enhancement to MEDA allows run-time measurement of microelectrode health information, thereby enabling synthesis of adaptive routing strategies for droplets. However, existing synthesis solutions are computationally infeasible for large MEDA biochips that have been commercialized. In this paper, we propose a synthesis framework for adaptive droplet routing in MEDA biochips via deep reinforcement learning (DRL). The framework utilizes the real-time microelectrode health feedback to synthesize droplet routes that proactively minimize the likelihood of charge trapping. We show how the adaptive routing strategies can be synthesized using DRL. We implement the DRL agent, the MEDA simulation environment, and the bioassay scheduler using the OpenAI Gym environment. Our framework obtains adaptive routing policies efficiently for COVID-19 testing protocols on large arrays that reflect the sizes of commercial MEDA biochips available in the marketplace, significantly increasing probabilities of successful bioassay completion compared to existing methods.},
booktitle = {Proceedings of the 2022 Conference &amp; Exhibition on Design, Automation &amp; Test in Europe},
pages = {640–645},
numpages = {6},
location = {Antwerp, Belgium},
series = {DATE '22}
}

@inproceedings{10.1145/3563357.3566167,
author = {Quintana, Matias and Nagy, Zoltan and Tartarini, Federico and Schiavon, Stefano and Miller, Clayton},
title = {ComfortLearn: enabling agent-based occupant-centric building controls},
year = {2022},
isbn = {9781450398909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563357.3566167},
doi = {10.1145/3563357.3566167},
abstract = {The intersection of buildings control and thermal comfort modeling may seem obvious, but there are still prevalent challenges in combining them. "Occupant centric" control strategies are mainly trained using building data but rarely leverage occupants' feedback. While thermal comfort models are developed using occupants' data but are seldom integrated into building controls. To bridge this gap, we developed an open-source simulation tool named ComfortLearn. ComfortLearn is an OpenAI Gym-based environment that leverages historical building management system data from real buildings and existing longitudinal thermal comfort datasets for occupant-centric control strategies and benchmarking. We used an evaluation metric named 'exceedance' to evaluate occupants' thermal comfort and provide a more realistic picture than traditional evaluations like comfort bands. This setup allows the analysis of different building control strategies and their effect on real occupants, based on empirical data, without the need for computationally expensive co-simulations. A theoretical case study implementation shows that an as-is schedule-based controller complies with its comfort band more than 93% of the time, but the simulated occupants are comfortable for only 25% of the occupied time.},
booktitle = {Proceedings of the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {475–478},
numpages = {4},
keywords = {agent-based, building control, smart buildings, thermal comfort},
location = {Boston, Massachusetts},
series = {BuildSys '22}
}

@article{10.1145/3520125,
author = {Mostafiz, Rafid and Uddin, Mohammad Shorif and Uddin, Khandaker Mohammad Mohi and Rahman, Mohammad Motiur},
title = {COVID-19 Along with Other Chest Infection Diagnoses Using Faster R-CNN and Generative Adversarial Network},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
issn = {2374-0353},
url = {https://doi.org/10.1145/3520125},
doi = {10.1145/3520125},
abstract = {The rapid spreading of coronavirus (COVID-19) caused severe respiratory infections affecting the lungs. Automatic diagnosis helps to fight against COVID-19 in community outbreaks. Medical imaging technology can reinforce disease monitoring and detection facilities with the advancement of computer vision. Unfortunately, deep learning models are facing starvation of more generalized datasets as the data repositories of COVID-19 are not rich enough to provide significant distinct features. To address the limitation, this article describes the generation of synthetic images of COVID-19 along with other chest infections with distinct features by empirical top entropy-based patch selection approach using the generative adversarial network. After that, a diagnosis is performed through a faster region-based convolutional neural network using 6,406 synthetic as well as 3,933 original chest X-ray images of different chest infections, which also addressed the data imbalance problems and not recumbent to a particular class. The experiment confirms a satisfactory COVID-19 diagnosis accuracy of 99.16% in a multi-class scenario.},
journal = {ACM Trans. Spatial Algorithms Syst.},
month = oct,
articleno = {24},
numpages = {21},
keywords = {COVID-19, convolutional neural network (CNN), faster R-CNN, generative adversarial network (GAN), chest X-ray (CXR)}
}

@inproceedings{10.1145/3583133.3596376,
author = {Shyju, Saurav and Murali, Ritwik},
title = {ATLAS - A Co-evolutionary Framework for Automatic Tuning of Adversarial Neural Networks},
year = {2023},
isbn = {9798400701207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583133.3596376},
doi = {10.1145/3583133.3596376},
abstract = {Generative Adversarial Networks (GANs) have gained popularity due to their ability to produce realistic examples from existing data without any supervision. However, they are dependent on their hyperparameters, the tuning of which is usually a manual task. Additionally, the computing resources required for such training are also extremely high. In this paper, ATLAS - a Cloud-based Co-evolutionary Framework for training such adversarial networks using Evolutionary Algorithms is proposed. ATLAS views the GAN components (generator and discriminator) as in a predator-prey relationship and involves co-evolution as a method to address the challenges of overfitting, exploding/vanishing gradients and tunes the hyperparameters of both the components of the GAN. The ATLAS framework is designed to be customizable, and resource flexible to allow for set-up and easy usage for training complex adversarial networks in both distributed and cloud environments. Experiments testing ATLAS capability for anomaly detection were performed and the results show that ATLAS can consistently evolve and produce high-performance GAN models.},
booktitle = {Proceedings of the Companion Conference on Genetic and Evolutionary Computation},
pages = {2398–2401},
numpages = {4},
keywords = {generative adversarial networks, co-evolution, evolutionary algorithms, genetic algorithms, cloud computing, distributed computing, hyperparameter tuning, NeuroEvolution},
location = {Lisbon, Portugal},
series = {GECCO '23 Companion}
}

@inproceedings{10.1145/3610543.3626179,
author = {Chen, Bingyi and Liu, Zengyu and Yuan, Li and Liu, Zhitao and Li, Yi and Wang, Guan and Xie, Ning},
title = {Monte Carlo Denoising via Multi-scale Auxiliary Feature Fusion Guided Transformer},
year = {2023},
isbn = {9798400703140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610543.3626179},
doi = {10.1145/3610543.3626179},
abstract = {Deep learning-based single-frame Monte Carlo denoising techniques have demonstrated remarkable results in photo-realistic rendering research. However, the current state-of-the-art methods relying on self-attention mechanisms underutilize auxiliary features and struggle to preserve intricate high-frequency details in complex scenes. Employing a generative adversarial architecture, we present a transformer-based denoising network guided by multi-scale auxiliary feature. The proposed U-shaped denoising network extracts multi-scale texture and geometric features from auxiliaries, modulating them to guide the improved transformer module’s denoising process. The improved transformer module employs cross-channel self-attention to capture non-local relationships with near-linear computational complexity. Additionally, a gating mechanism is introduced in the transformer module’s feed-forward network, enhancing information flow. Extensive experiments on noisy images with varied per-pixel sampling rates demonstrate the method’s superiority in quantitative metrics and visual perception compared with state-of-the-art methods. Our method excels notably in intricate scenes with complex hair and texture details, which are historically challenging to denoise.},
booktitle = {SIGGRAPH Asia 2023 Technical Communications},
articleno = {1},
numpages = {4},
keywords = {Monte Carlo denoising, Transformer, self-attention},
location = {Sydney, NSW, Australia},
series = {SA '23}
}

@inproceedings{10.1145/3580305.3599412,
author = {Wang, Yunke and Wang, Xiyu and Dinh, Anh-Dung and Du, Bo and Xu, Charles},
title = {Learning to Schedule in Diffusion Probabilistic Models},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599412},
doi = {10.1145/3580305.3599412},
abstract = {Recently, the field of generative models has seen a significant advancement with the introduction of Diffusion Probabilistic Models (DPMs). The Denoising Diffusion Implicit Model (DDIM) was designed to reduce computational time by skipping a number of steps in the inference process of DPMs. However, the hand-crafted sampling schedule in DDIM, which relies on human expertise, has its limitations in considering all relevant factors in the sampling process. Additionally, the assumption that all instances should have the same schedule is not always valid. To address these problems, this paper proposes a method that leverages reinforcement learning to automatically search for an optimal sampling schedule for DPMs. This is achieved by a policy network that predicts the next step to visit based on the current state of the noisy image. The optimization of the policy network is accomplished using an episodic actor-critic framework, which incorporates reinforcement learning. Empirical results demonstrate the superiority of our approach over various datasets with different timesteps. We also observe that the trained sampling schedule has a strong generalization ability across different DPM baselines.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2478–2488},
numpages = {11},
keywords = {diffusion probabilistic model, inference, planning and scheduling, reinforcement learning},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3592571.3592978,
author = {Mach\'{a}\v{c}ek, Roman and Mozaffari, Leila and Sepasdar, Zahra and Parasa, Sravanthi and Halvorsen, P\r{a}l and Riegler, Michael A. and Thambawita, Vajira},
title = {Mask-conditioned latent diffusion for generating gastrointestinal polyp images},
year = {2023},
isbn = {9798400701863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592571.3592978},
doi = {10.1145/3592571.3592978},
abstract = {In order to take advantage of artificial intelligence (AI) solutions in endoscopy diagnostics, we must overcome the issue of limited annotations. These limitations are caused by the high privacy concerns in the medical field and the requirement of getting aid from experts for the time-consuming and costly medical data annotation process. In computer vision, image synthesis has made a significant contribution in recent years, as a result of the progress of generative adversarial networks (GANs) and diffusion probabilistic models (DPMs). Novel DPMs have outperformed GANs in text, image, and video generation tasks. Therefore, this study proposes a conditional DPM framework to generate synthetic gastrointestinal (GI) polyp images conditioned on given generated segmentation masks. Our experimental results show that our system can generate an unlimited number of high-fidelity synthetic polyp images with the corresponding ground truth masks of polyps. To test the usefulness of the generated data we trained binary image segmentation models to study the effect of using synthetic data. Results show that the best micro-imagewise intersection over union (IOU) of 0.7751 was achieved from DeepLabv3+ when the training data consists of both real data and synthetic data. However, the results reflect that achieving good segmentation performance with synthetic data heavily depends on model architectures.},
booktitle = {Proceedings of the 4th ACM Workshop on Intelligent Cross-Data Analysis and Retrieval},
pages = {1–9},
numpages = {9},
keywords = {diffusion model, generating synthetic data, polyp generative model, polyp segmentation},
location = {Thessaloniki, Greece},
series = {ICDAR '23}
}

@inproceedings{10.1145/3594315.3594652,
author = {Li, Wenzheng and Tian, Lihua and Sun, Zhigang and Xiao, Li},
title = {Image Sample Generation of Stator Surface Defects Based on Layer Mask Blending Generative Adversarial Network},
year = {2023},
isbn = {9781450399029},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594315.3594652},
doi = {10.1145/3594315.3594652},
abstract = {In industrial production processes, defect inspection plays an important role in reducing the occurrence of failures and improving production efficiency. Data-driven algorithms represented by deep learning have made great progress in recent years, but need to face the problems of small quantity and poor quality of datasets when applied to industrial defect inspection. This paper proposes a layer mask blending-based generative adversarial network (LMBGAN) and optimizes the training process to generate high-quality surface defect samples. LMBGAN generates defect images and layer masks using the defect image decoder and layer mask decoder with the Pixel Shuffle operation. Inspired by the layer mask in computer painting, LMBGAN adopts the input image as the base layer and blends the defect foreground through the layer mask, giving it the ability to focus more on generating upper-layer defect images and reducing unnecessary background changes. LMBGAN additionally introduces adaptive discriminator augmentation and non-saturating logistic loss to promote model convergence under small datasets, effectively alleviating the problem of GAN training difficulties with limited data. The experiment results show that the proposed method can generate high-quality and diverse defect image samples through easily accessible normal samples, thus reducing the difficulty of obtaining rare defect image samples.},
booktitle = {Proceedings of the 2023 9th International Conference on Computing and Artificial Intelligence},
pages = {258–265},
numpages = {8},
location = {Tianjin, China},
series = {ICCAI '23}
}

@inproceedings{10.1145/3578338.3593562,
author = {Sam, Tyler and Chen, Yudong and Yu, Christina Lee},
title = {Overcoming the Long Horizon Barrier for Sample-Efficient Reinforcement Learning with Latent Low-Rank Structure},
year = {2023},
isbn = {9798400700743},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578338.3593562},
doi = {10.1145/3578338.3593562},
abstract = {Reinforcement learning (RL) methods have been increasingly popular in sequential decision making tasks due to its empirical success. However, large state and action spaces in real-world problems modeled as a Markov decision processes (MDPs) limit the use of RL algorithms. Given a standard finite-horizon MDP (S, A, P, R, H) with state space S, action space A, transition kernel P = {Ph} ∈ []H, reward function R = {R h} ∈ [H] bounded between zero and one, and time horizon H, one needs Ω (|S||A|H3/∈2 samples given a generative model to learn an optimal policy [3], which can be impractical when S and A are large. The above tabular RL framework does not capture the fact that many real-world systems in fact have additional structure that if exploited should improve computational and statistical efficiency. Moreover, [1] empirically verifies that optimal and near-optimal action-value functions (both viewed as |S|-by-|A| matrices) of classical stochastic control tasks have low rank. Thus, the critical question is what are the minimal low rank structural assumptions that allow for computationally and statistically efficient learning?},
booktitle = {Abstract Proceedings of the 2023 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
pages = {85–86},
numpages = {2},
keywords = {low-rank matrix estimation, reinforcement learning},
location = {Orlando, Florida, United States},
series = {SIGMETRICS '23}
}

@inproceedings{10.1145/3573900.3591113,
author = {Xu, Wanpeng and Wei, Hua},
title = {Learning to Calibrate Hybrid Hyperparameters: a Study on Traffic Simulation},
year = {2023},
isbn = {9798400700309},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573900.3591113},
doi = {10.1145/3573900.3591113},
abstract = {Traffic simulation is an important computational technique that models the behavior and interactions of vehicles, pedestrians, and infrastructure in a transportation system. Calibration, which involves adjusting simulation parameters to match real-world data, is a key challenge in traffic simulation. Traffic simulators involve multiple models with hybrid hyperparameters, which could be either categorical or continuous. In this paper, we present CHy2, an approach that generates a set of hyperparameters for simulator calibration using generative adversarial imitation learning. CHy2 learns to mimic expert behavior models by rewarding hyperparameters that deceive a discriminator trained to classify policy-generated and expert trajectories. Specifically, we propose a hybrid architecture of actor-critic algorithms to handle the hybrid choices between hyperparameters. Experimental results show that CHy2 outperforms previous methods in calibrating traffic simulators.},
booktitle = {Proceedings of the 2023 ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
pages = {144–147},
numpages = {4},
keywords = {Reinforcement learning, model calibration, traffic simulation},
location = {Orlando, FL, USA},
series = {SIGSIM-PADS '23}
}

@inproceedings{10.1145/3539597.3570429,
author = {Liu, Chang and Yang, Yuwen and Xie, Zhe and Lu, Hongtao and Ding, Yue},
title = {Position-Aware Subgraph Neural Networks with Data-Efficient Learning},
year = {2023},
isbn = {9781450394079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539597.3570429},
doi = {10.1145/3539597.3570429},
abstract = {Data-efficient learning on graphs (GEL) is essential in real-world applications. Existing GEL methods focus on learning useful representations for nodes, edges, or entire graphs with "small" labeled data. But the problem of data-efficient learning for subgraph prediction has not been explored. The challenges of this problem lie in the following aspects: 1) It is crucial for subgraphs to learn positional features to acquire structural information in the base graph in which they exist. Although the existing subgraph neural network method is capable of learning disentangled position encodings, the overall computational complexity is very high. 2) Prevailing graph augmentation methods for GEL, including rule-based, sample-based, adaptive, and automated methods, are not suitable for augmenting subgraphs because a subgraph contains fewer nodes but richer information such as position, neighbor, and structure. Subgraph augmentation is more susceptible to undesirable perturbations. 3) Only a small number of nodes in the base graph are contained in subgraphs, which leads to a potential "bias" problem that the subgraph representation learning is dominated by these "hot" nodes. By contrast, the remaining nodes fail to be fully learned, which reduces the generalization ability of subgraph representation learning. In this paper, we aim to address the challenges above and propose a Position-Aware Data-Efficient Learning framework for subgraph neural networks called PADEL. Specifically, we propose a novel node position encoding method that is anchor-free, and design a new generative subgraph augmentation method based on a diffused variational subgraph autoencoder, and we propose exploratory and exploitable views for subgraph contrastive learning. Extensive experiment results on three real-world datasets show the superiority of our proposed method over state-of-the-art baselines.},
booktitle = {Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
pages = {643–651},
numpages = {9},
keywords = {contrastive learning, data-efficient learning, generative model, subgraph neural networks},
location = {Singapore, Singapore},
series = {WSDM '23}
}

@inproceedings{10.1145/3607541.3616824,
author = {Berlincioni, Lorenzo and Berretti, Stefano and Bertini, Marco and Bimbo, Alberto Del},
title = {4DSR-GCN: 4D Video Point Cloud Upsampling using Graph Convolutional Networks},
year = {2023},
isbn = {9798400702785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607541.3616824},
doi = {10.1145/3607541.3616824},
abstract = {Time varying sequences of 3D point clouds, or 4D point clouds, are now being acquired at an increasing pace in several applications (personal avatar representation, LiDAR in autonomous or assisted driving). In many cases, such volume of data is transmitted, thus requiring that proper compression tools are applied to either reduce the resolution or the bandwidth. In this paper, we propose a new solution for upscaling and restoration of time-varying 3D video point clouds after they have been heavily compressed. Our model consists of a specifically designed Graph Convolutional Network that combines Dynamic Edge Convolution and Graph Attention Networks for feature aggregation in a Generative Adversarial setting. We present a different way to sample dense point clouds with the intent to make these modules work in synergy to provide each node enough features about its neighbourhood in order to later on generate new vertices. Compared to other solutions in the literature that address the same task, our proposed model is capable of obtaining comparable results in terms of quality of the reconstruction, while using a substantially lower number of parameters (simeq 300KB), making our solution deployable in edge computing devices.},
booktitle = {Proceedings of the 1st International Workshop on Multimedia Content Generation and Evaluation: New Methods and Practice},
pages = {57–65},
numpages = {9},
keywords = {3d upscaling, generative adversarial setting, graph attention network, super resolution, time varying 3d point clouds},
location = {Ottawa ON, Canada},
series = {McGE '23}
}

@inproceedings{10.1145/3586183.3606759,
author = {Kang, Hyeonsu B and Wu, Tongshuang and Chang, Joseph Chee and Kittur, Aniket},
title = {Synergi: A Mixed-Initiative System for Scholarly Synthesis and Sensemaking},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606759},
doi = {10.1145/3586183.3606759},
abstract = {Efficiently reviewing scholarly literature and synthesizing prior art are crucial for scientific progress. Yet, the growing scale of publications and the burden of knowledge make synthesis of research threads more challenging than ever.While significant research has been devoted to helping scholars interact with individual papers, building research threads scattered across multiple papers remains a challenge.Most top-down synthesis (and LLMs) make it difficult to personalize and iterate on the output, while bottom-up synthesis is costly in time and effort.Here, we explore a new design space of mixed-initiative workflows.In doing so we develop a novel computational pipeline, Synergi, that ties together user input of relevant seed threads with citation graphs and LLMs, to expand and structure them, respectively.Synergiallows scholars to start with an entire threads-and-subthreads structure generated from papers relevant to their interests, and to iterate and customize on it as they wish. In our evaluation, we find that Synergi helps scholars efficiently make sense of relevant threads, broaden their perspectives, and increases their curiosity. We discuss future design implications for thread-based, mixed-initiative scholarly synthesis support tools.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {43},
numpages = {19},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inbook{10.1109/DAC18074.2021.9586123,
author = {Jia, Zhenge and Hong, Feng and Ping, Lichuan and Shi, Yiyu and Hu, Jingtong},
title = {Enabling On-Device Model Personalization for Ventricular Arrhythmias Detection by Generative Adversarial Networks},
year = {2022},
isbn = {9781665432740},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/DAC18074.2021.9586123},
abstract = {Implantable Cardioverter Defibrillator (ICD) is an ultra-low-power device which monitors heart rate and delivers in-time defibrillation on detected ventricular arrhythmias (VAs). The parameters of VAs detection mechanism on each recipient's ICD are supposed to be fine-tuned to obtain accurate detection due to the individual's unique rhythm features. However, the process extremely relies on clinical expertise and thus must be conducted manually and routinely by cardiologists diagnosing massive amount of rhythm data. In this paper, we introduce a novel self-supervised on-device personalization of convolutional neural network (CNNs) for VAs detection. We first propose a computing framework consisting of an edge device and an ICD to enable efficient on-device CNNs personalization and real-time inference respectively. Then, we propose a generative model that learns to synthesize patient-specific intracardiac EGMs signals, which can then be used as personalized training data to improve patient-specific VAs detection performance on ICDs. Evaluations on three detection models show that the self-supervised on-device personalization significantly improve VAs detection performance under a patient-specific setting.},
booktitle = {Proceedings of the 58th Annual ACM/IEEE Design Automation Conference},
pages = {163–168},
numpages = {6}
}

@inproceedings{10.1145/3577190.3616115,
author = {Harz, Leon and Vo\ss{}, Hendric and Kopp, Stefan},
title = {FEIN-Z: Autoregressive Behavior Cloning for Speech-Driven Gesture Generation},
year = {2023},
isbn = {9798400700552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577190.3616115},
doi = {10.1145/3577190.3616115},
abstract = {Human communication relies on multiple modalities such as verbal expressions, facial cues, and bodily gestures. Developing computational approaches to process and generate these multimodal signals is critical for seamless human-agent interaction. A particular challenge is the generation of co-speech gestures due to the large variability and number of gestures that can accompany a verbal utterance, leading to a one-to-many mapping problem. This paper presents an approach based on a Feature Extraction Infusion Network (FEIN-Z) that adopts insights from robot imitation learning and applies them to co-speech gesture generation. Building on the BC-Z architecture, our framework combines transformer architectures and Wasserstein generative adversarial networks. We describe the FEIN-Z methodology and evaluation results obtained within the GENEA Challenge 2023, demonstrating good results and significant improvements in human-likeness over the GENEA baseline. We discuss potential areas for improvement, such as refining input segmentation, employing more fine-grained control networks, and exploring alternative inference methods.},
booktitle = {Proceedings of the 25th International Conference on Multimodal Interaction},
pages = {763–771},
numpages = {9},
keywords = {behavior cloning, co-speech gesture generation, deep learning, gesture synthesis, machine learning, multimodal data, reinforcement learning, transformer},
location = {Paris, France},
series = {ICMI '23}
}

@article{10.1145/3559540,
author = {Brophy, Eoin and Wang, Zhengwei and She, Qi and Ward, Tom\'{a}s},
title = {Generative Adversarial Networks in Time Series: A Systematic Literature Review},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {10},
issn = {0360-0300},
url = {https://doi.org/10.1145/3559540},
doi = {10.1145/3559540},
abstract = {Generative adversarial network (GAN) studies have grown exponentially in the past few years. Their impact has been seen mainly in the computer vision field with realistic image and video manipulation, especially generation, making significant advancements. Although these computer vision advances have garnered much attention, GAN applications have diversified across disciplines such as time series and sequence generation. As a relatively new niche for GANs, fieldwork is ongoing to develop high-quality, diverse, and private time series data. In this article, we review GAN variants designed for time series related applications. We propose a classification of discrete-variant GANs and continuous-variant GANs, in which GANs deal with discrete time series and continuous time series data. Here we showcase the latest and most popular literature in this field—their architectures, results, and applications. We also provide a list of the most popular evaluation metrics and their suitability across applications. Also presented is a discussion of privacy measures for these GANs and further protections and directions for dealing with sensitive data. We aim to frame clearly and concisely the latest and state-of-the-art research in this area and their applications to real-world technologies.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {199},
numpages = {31},
keywords = {Generative adversarial networks, time series, discrete-variant GANs, continuous-variant GANs}
}

@article{10.5555/3648699.3648900,
author = {Jia, Junxiong and Wu, Yanni and Li, Peijun and Meng, Deyu},
title = {Variational inverting network for statistical inverse problems of partial differential equations},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {To quantify uncertainties in inverse problems of partial differential equations (PDEs), we formulate them into statistical inference problems using Bayes' formula. Recently, well-justified infinite-dimensional Bayesian analysis methods have been developed to construct dimension-independent algorithms. However, there are three challenges for these infinite-dimensional Bayesian methods: prior measures usually act as regularizers and are not able to incorporate prior information efficiently; complex noises, such as more practical noni.i.d. distributed noises, are rarely considered; and time-consuming forward PDE solvers are needed to estimate posterior statistical quantities. To address these issues, an infinite-dimensional inference framework has been proposed based on the infinite-dimensional variational inference method and deep generative models. Specifically, by introducing some measure equivalence assumptions, we derive the evidence lower bound in the infinite-dimensional setting and provide possible parametric strategies that yield a general inference framework called the Variational Inverting Network (VINet). This inference framework can encode prior and noise information from learning examples. In addition, relying on the power of deep neural networks, the posterior mean and variance can be efficiently and explicitly generated in the inference stage. In numerical experiments, we design specific network structures that yield a computable VINet from the general inference framework. Numerical examples of linear inverse problems of an elliptic equation and the Helmholtz equation are presented to illustrate the effectiveness of the proposed inference framework.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {201},
numpages = {60},
keywords = {infinite-dimensional variational inference, inverse problems, Bayesian analysis for functions, partial differential equations, deep neural networks}
}

@article{10.1145/3505244,
author = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
title = {Transformers in Vision: A Survey},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3505244},
doi = {10.1145/3505244},
abstract = {Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {200},
numpages = {41},
keywords = {Self-attention, transformers, bidirectional encoders, deep neural networks, convolutional networks, self-supervision, literature survey}
}

@inproceedings{10.1145/3503161.3548073,
author = {Du, Zhekai and Li, Jingjing and Zuo, Lin and Zhu, Lei and Lu, Ke},
title = {Energy-Based Domain Generalization for Face Anti-Spoofing},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548073},
doi = {10.1145/3503161.3548073},
abstract = {With various unforeseeable face presentation attacks (PA) springing up, face anti-spoofing (FAS) urgently needs to generalize to unseen scenarios. Research on generalizable FAS has lately attracted growing attention. Existing methods cast FAS as a vanilla binary classification problem and address it by a standard discriminative classifier p(y|x) under a domain generalization framework. However, discriminative models are unreliable for samples far away from the training distribution. In this paper, we resort to an energy-based model (EBM) to tackle FAS in a generative perspective. Our motivation is to model the joint density p(x,y), which allows to compute not only p(y|x) but also p(x). Due to the intractability of direct modeling, we use EBMs as an alternative to probabilistic estimation. With energy-based training, real faces are encouraged to get low free energy associated with the marginal probability p(x) of real faces, and all samples with high free energy are regarded as fake faces, thus rejecting any kind of PA out of the distribution of real faces. To learn to generalize to unseen domains, we generate diverse and novel populations in feature space under the guidance of energy model. Our model is updated in a meta-learning schema, where the original source samples are utilized for meta-training and the generated ones for meta-testing. We validate our method on four widely used FAS datasets. Comprehensive experimental results demonstrate the effectiveness of our method compared with state-of-the-arts.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {1749–1757},
numpages = {9},
keywords = {domain generalization, energy-based model, face anti-spoofing},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3615522.3615545,
author = {Chen, Yihan and Ye, Yilin and Zeng, Wei},
title = {The Rich, the Poor, and the Ugly: An Aesthetic-Perspective Assessment of NFT Values},
year = {2023},
isbn = {9798400707513},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615522.3615545},
doi = {10.1145/3615522.3615545},
abstract = {The adoption of non-fungible tokens (NFTs) has revolutionized digital art transactions, providing artists with unprecedented opportunities to tokenize and monetize their generative creations, leading to increased scrutiny and demand within blockchain-oriented marketplaces. The pricing of NFT artworks, however, exhibits substantial variations within and across collections, influenced by various factors. This study aims to investigate the relationship between visual features and pricing, shedding light on the variations underlying the pricing of NFTs. First, measures of both computational aesthetics and visual complexity were applied to extract multi-faceted visual aesthetic features, encompassing aesthetic factors such as color and composition as well as complexity factors like entropy. Second, with extracted visual aesthetic features and preprocessed price data, the study proceeds to conduct correlation analysis within collections and statistical modeling across collections. Through these approaches, we reveal a moderate correlation between visual features and prices within collections, while also identifying different influential visual features across collections. The differential performance of price models highlights the distinctiveness and unique pricing characteristics of NFT collections.},
booktitle = {Proceedings of the 16th International Symposium on Visual Information Communication and Interaction},
articleno = {23},
numpages = {8},
keywords = {Non-fungible tokens, computational aesthetics, statistical modeling},
location = {Guangzhou, China},
series = {VINCI '23}
}

@inproceedings{10.1145/3592686.3592698,
author = {Fang, Ruiqi and Guo, Ruipeng and Zhao, Min and Yao, Min},
title = {Low-count PET image reconstruction algorithm based on WGAN-GP},
year = {2023},
isbn = {9798400700200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592686.3592698},
doi = {10.1145/3592686.3592698},
abstract = {Positron emission tomography (PET) technique can visualize the working status or fluid flow state inside opaque devices, and how to reconstruct high-quality images from low-count (LC) projection data with short scan time to meet the real-time online inspection remains an important research problem. A direct reconstruction algorithm CED-PET based on gradient-penalized Wasserstein Generative Adversarial Network (WGAN-GP) architecture is proposed. This network combines content loss, perceptual loss, and adversarial loss to achieve fast and high-quality reconstruction of low-count projection data. In addition, a special dataset for obtuse body bypassing was produced by combining Computational Fluid Dynamics (CFD) simulation software and the Geant4 Application for Tomographic Emission (GATE) simulation platform. The results on this dataset show that CED-PET can quickly reconstruct high-quality images with more realistic detail contours.},
booktitle = {Proceedings of the 2023 3rd International Conference on Bioinformatics and Intelligent Computing},
pages = {60–64},
numpages = {5},
location = {Sanya, China},
series = {BIC '23}
}

@inproceedings{10.1145/3490422.3502329,
author = {Liu, Yanqi and Opipari, Anthony and Guerin, Theo and Bahar, Ruth Iris},
title = {Hardware Acceleration of Nonparametric Belief Propagation for Efficient Robot Manipulation},
year = {2022},
isbn = {9781450391498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490422.3502329},
doi = {10.1145/3490422.3502329},
abstract = {Probabilistic graphical models (PGMs) have been widely used in computer vision, robotics, and statistics. Generative inference algorithms used to solve PGMs, such as belief propagation (BP), involve integration over high-dimensional variables, which becomes computationally infeasible. Drawing inspiration from particle filters, nonparametric belief propagation (NBP) combines the efficiency of Monte-Carlo sampling to capture the belief space of hidden variables while preserving accuracy and algorithm robustness. In this poster presentation, we describe a novel method to accelerate an NBP algorithm in hardware and evaluate our approach on the 6 degree-of-freedom (DoF) articulated object pose estimation problem. Our two major contributions include 1) identifying three independent computation flows in the algorithm to effectively overlap the two main steps of the algorithm: belief update and message update, and 2) creating deeply pipelined processing units that allow for fine-grained parallelism, which helps to balance the workloads on different computation streams. Results from this study demonstrate that our design achieves both improved runtime and energy efficiency. In particular, we achieved 26X energy saving compared to running the algorithm on a Titan Xp GPU, and 10X runtime speedup and 14X energy saving compared to running on the Jetson AGX . We believe that our FPGA implementation can greatly improve particle-based sampling methods for real time applications.},
booktitle = {Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
pages = {51},
numpages = {1},
keywords = {belief propagation, hardware accelerators, real-time system architecture},
location = {Virtual Event, USA},
series = {FPGA '22}
}

@article{10.1145/3550454.3555480,
author = {Wu, Rundi and Zheng, Changxi},
title = {Learning to Generate 3D Shapes from a Single Example},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3550454.3555480},
doi = {10.1145/3550454.3555480},
abstract = {Existing generative models for 3D shapes are typically trained on a large 3D dataset, often of a specific object category. In this paper, we investigate the deep generative model that learns from only a single reference 3D shape. Specifically, we present a multi-scale GAN-based model designed to capture the input shape's geometric features across a range of spatial scales. To avoid large memory and computational cost induced by operating on the 3D volume, we build our generator atop the tri-plane hybrid representation, which requires only 2D convolutions. We train our generative model on a voxel pyramid of the reference shape, without the need of any external supervision or manual annotation. Once trained, our model can generate diverse and high-quality 3D shapes possibly of different sizes and aspect ratios. The resulting shapes present variations across different scales, and at the same time retain the global structure of the reference shape. Through extensive evaluation, both qualitative and quantitative, we demonstrate that our model can generate 3D shapes of various types.1},
journal = {ACM Trans. Graph.},
month = nov,
articleno = {224},
numpages = {19},
keywords = {3D shape generation, generative models, shape analysis and synthesis}
}

@inproceedings{10.1145/3543507.3587428,
author = {de Berardinis, Jacopo and Mero\~{n}o-Pe\~{n}uela, Albert and Poltronieri, Andrea and Presutti, Valentina},
title = {The Harmonic Memory: a Knowledge Graph of harmonic patterns as a trustworthy framework for computational creativity},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3587428},
doi = {10.1145/3543507.3587428},
abstract = {Computationally creative systems for music have recently achieved impressive results, fuelled by progress in generative machine learning. However, black-box approaches have raised fundamental concerns for ethics, accountability, explainability, and musical plausibility. To enable trustworthy machine creativity, we introduce the Harmonic Memory, a Knowledge Graph (KG) of harmonic patterns extracted from a large and heterogeneous musical corpus. By leveraging a cognitive model of tonal harmony, chord progressions are segmented into meaningful structures, and patterns emerge from their comparison via harmonic similarity. Akin to a music memory, the KG holds temporal connections between consecutive patterns, as well as salient similarity relationships. After demonstrating the validity of our choices, we provide examples of how this design enables novel pathways for combinational creativity. The memory provides a fully accountable and explainable framework to inspire and support creative professionals – allowing for the discovery of progressions consistent with given criteria, the recomposition of harmonic sections, but also the co-creation of new progressions.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {3873–3882},
numpages = {10},
keywords = {computational creativity, knowledge graphs, music technology},
location = {Austin, TX, USA},
series = {WWW '23}
}

@article{10.1145/3571740,
author = {Hunt, Sebastian and Sands, David and Stucki, Sandro},
title = {Reconciling Shannon and Scott with a Lattice of Computable Information},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {POPL},
url = {https://doi.org/10.1145/3571740},
doi = {10.1145/3571740},
abstract = {This paper proposes a reconciliation of two different theories of information. The first, originally proposed in a lesser-known work by Claude Shannon (some five years after the publication of his celebrated quantitative theory of communication), describes how the information content of channels can be described qualitatively, but still abstractly, in terms of information elements, where information elements can be viewed as equivalence relations over the data source domain. Shannon showed that these elements have a partial ordering, expressing when one information element is more informative than another, and that these partially ordered information elements form a complete lattice. In the context of security and information flow this structure has been independently rediscovered several times, and used as a foundation for understanding and reasoning about information flow. The second theory of information is Dana Scott’s domain theory, a mathematical framework for giving meaning to programs as continuous functions over a particular topology. Scott’s partial ordering also represents when one element is more informative than another, but in the sense of computational progress – i.e. when one element is a more defined or evolved version of another. To give a satisfactory account of information flow in computer programs it is necessary to consider both theories together, in order to understand not only what information is conveyed by a program (viewed as a channel, \`{a} la Shannon) but also how the precision with which that information can be observed is determined by the definedness of its encoding (\`{a} la Scott). To this end we show how these theories can be fruitfully combined, by defining the Lattice of Computable Information (LoCI), a lattice of preorders rather than equivalence relations. LoCI retains the rich lattice structure of Shannon’s theory, filters out elements that do not make computational sense, and refines the remaining information elements to reflect how Scott’s ordering captures possible varieties in the way that information is presented. We show how the new theory facilitates the first general definition of termination-insensitive information flow properties, a weakened form of information flow property commonly targeted by static program analyses.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {68},
numpages = {30},
keywords = {Information Flow, Semantics}
}

@article{10.1109/TCBB.2022.3153963,
author = {Chen, Jiatao and Zhang, Liang and Cheng, Ke and Jin, Bo and Lu, Xinjiang and Che, Chao},
title = {Predicting Drug-Target Interaction Via Self-Supervised Learning},
year = {2022},
issue_date = {Sept.-Oct. 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2022.3153963},
doi = {10.1109/TCBB.2022.3153963},
abstract = {Recent advances in graph representation learning provide new opportunities for computational drug-target interaction (DTI) prediction. However, it still suffers from deficiencies of dependence on manual labels and vulnerability to attacks. Inspired by the success of self-supervised learning (SSL) algorithms, which can leverage input data itself as supervision,we propose SupDTI, a SSL-enhanced drug-target interaction prediction framework based on a heterogeneous network (i.e., drug-protein, drug-drug, and protein-protein interaction network; drug-disease, drug-side-effect, and protein-disease association network; drug-structure and protein-sequence similarity network). Specifically, SupDTI is an end-to-end learning framework consisting of five components. First, localized and globalized graph convolutions are designed to capture the nodes’ information from both local and global perspectives, respectively. Then, we develop a variational autoencoder to constrain the nodes’ representation to have desired statistical characteristics. Finally, a unified self-supervised learning strategy is leveraged to enhance the nodes’ representation, namely, a contrastive learning module is employed to enable the nodes’ representation to fit the graph-level representation, followed by a generative learning module which further maximizes the node-level agreement across the global and local views by learning the probabilistic connectivity distribution of the original heterogeneous network. Experimental results show that our model can achieve better prediction performance than state-of-the-art methods.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = mar,
pages = {2781–2789},
numpages = {9}
}

@inproceedings{10.1145/3528233.3530738,
author = {Sauer, Axel and Schwarz, Katja and Geiger, Andreas},
title = {StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets},
year = {2022},
isbn = {9781450393379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528233.3530738},
doi = {10.1145/3528233.3530738},
abstract = {Computer graphics has experienced a recent surge of data-centric approaches for photorealistic and controllable content creation. StyleGAN in particular sets new standards for generative modeling regarding image quality and controllability. However, StyleGAN’s performance severely degrades on large unstructured datasets such as ImageNet. StyleGAN was designed for controllability; hence, prior works suspect its restrictive design to be unsuitable for diverse datasets. In contrast, we find the main limiting factor to be the current training strategy. Following the recently introduced Projected GAN paradigm, we leverage powerful neural network priors and a progressive growing strategy to successfully train the latest StyleGAN3 generator on ImageNet. Our final model, StyleGAN-XL, sets a new state-of-the-art on large-scale image synthesis and is the first to generate images at a resolution of 10242 at such a dataset scale. We demonstrate that this model can invert and edit images beyond the narrow domain of portraits or specific object&nbsp;classes. Code, models, and supplementary videos can be found at https://sites.google.com/view/stylegan-xl/ .},
booktitle = {ACM SIGGRAPH 2022 Conference Proceedings},
articleno = {49},
numpages = {10},
keywords = {Generative Adversarial Networks, Image Editing, Image Synthesis, Pretrained Models},
location = {Vancouver, BC, Canada},
series = {SIGGRAPH '22}
}

@inproceedings{10.1145/3514221.3520161,
author = {Porwal, Vibhor and Mitra, Subrata and Du, Fan and Anderson, John and Sheoran, Nikhil and Rao, Anup and Mai, Tung and Kowshik, Gautam and Nair, Sapthotharan and Arora, Sameeksha and Mahapatra, Saurabh},
title = {Efficient Insights Discovery through Conditional Generative Model based Query Approximation},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3520161},
doi = {10.1145/3514221.3520161},
abstract = {There are various scenarios where very quick insights from a massive amount of data need to be extracted in a time-critical manner. These might be fresh insights or re-looking at why previous insights did not work and how to fix those. A marketing campaign is one real-world scenario where a non-programmer needs to dig such huge data in a very short period of time (a few hours) in order to hit a target revenue. In this demo paper, we will describe Electra - a system that integrates an automated data-insight discovery mechanism with a novel machine-learning (ML) driven approximate query processing (AQP) engine that can answer complex queries with a large number of predicates or conditions with high accuracy. This AQP engine uses a conditional generative model to generate a very small sample (~1000 rows) corresponding to the actual query to be answered and computes the highly accurate approximate answer from those instead of running the query against the original data. The insight discovery workflow bootstraps insights using ML algorithms based on the statistical characteristics of the data and further offers a no-code based interface to drill down for deeper insights. The queries from this interface are answered by the AQP engine that runs locally at the client-side itself to offer low latency interactions.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {2397–2400},
numpages = {4},
keywords = {approximate query processing, chart recommendation, data analytics, machine learning},
location = {Philadelphia, PA, USA},
series = {SIGMOD '22}
}

@inproceedings{10.1145/3477495.3531827,
author = {Chen, Jiangui and Zhang, Ruqing and Guo, Jiafeng and Fan, Yixing and Cheng, Xueqi},
title = {GERE: Generative Evidence Retrieval for Fact Verification},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531827},
doi = {10.1145/3477495.3531827},
abstract = {Fact verification (FV) is a challenging task which aims to verify a claim using multiple evidential sentences from trustworthy corpora, e.g., Wikipedia. Most existing approaches follow a three-step pipeline framework, including document retrieval, sentence retrieval and claim verification. High-quality evidences provided by the first two steps are the foundation of the effective reasoning in the last step. Despite being important, high-quality evidences are rarely studied by existing works for FV, which often adopt the off-the-shelf models to retrieve relevant documents and sentences in an "index-retrieve-then-rank'' fashion. This classical approach has clear drawbacks as follows: i) a large document index as well as a complicated search process is required, leading to considerable memory and computational overhead; ii) independent scoring paradigms fail to capture the interactions among documents and sentences in ranking; iii) a fixed number of sentences are selected to form the final evidence set. In this work, we proposeGERE, the first system that retrieves evidences in a generative fashion, i.e., generating the document titles as well as evidence sentence identifiers. This enables us to mitigate the aforementioned technical issues since: i) the memory and computational cost is greatly reduced because the document index is eliminated and the heavy ranking process is replaced by a light generative process; ii) the dependency between documents and that between sentences could be captured via sequential generation process; iii) the generative formulation allows us to dynamically select a precise set of relevant evidences for each claim. The experimental results on the FEVER dataset show that GERE achieves significant improvements over the state-of-the-art baselines, with both time-efficiency and memory-efficiency.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2184–2189},
numpages = {6},
keywords = {evidence retrieval, fact verification, generative retrieval},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3507548.3507558,
author = {Zheng, Liang and Chen, Ya and Chen, Xiaopan and Zheng, Fengbin},
title = {Age-uniform Feature Learning for Image-based Kinship Verification},
year = {2022},
isbn = {9781450384155},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3507548.3507558},
doi = {10.1145/3507548.3507558},
abstract = {Kinship verification based on face image is an important topic in computer vision and has many applications in practice, such as family pedigree organization, missing person search, etc. Although parent and children share certain similarities in facial images, it is still difficult to verify the kin between people with large age gap. Therefore, how to reduce the influence of age factors on verification is the key to improve the accuracy of kinship verification. To this end, we propose an Age-uniform Face Representation Learning Network (AFRLN) to verify kinship. It mainly consists of Age Uniform Network (AUN) and Verification Network (VFN). Specifically, the design of AUN utilizes the idea of generative adversarial network, which aims to transform parent and child's face images of different ages into images of the uniform age range. Then, the transformed facial images are fed into the verification network, and discriminative deep features of parent and child are obtained. Finally, the output features are fused and then kinship verification task is conducted. Our approach is tested on two publicly kinship datasets: KinFaceW-I and KinfaceW-II. Experimental results validate performance of our method.},
booktitle = {Proceedings of the 2021 5th International Conference on Computer Science and Artificial Intelligence},
pages = {65–71},
numpages = {7},
keywords = {Age Uniform, Feature Fusion, Generative Adversarial Network, Kinship Verification},
location = {Beijing, China},
series = {CSAI '21}
}

@inproceedings{10.1145/3579375.3579395,
author = {Kaur, Sehajpreet and Kumar, Shivansh and Homayouni, Hajar},
title = {Synthetic High-Resolution COVID-19 Chest X-Ray Generation},
year = {2023},
isbn = {9798400700057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579375.3579395},
doi = {10.1145/3579375.3579395},
abstract = {Chest X-ray images provide critical information for the diagnosis of COVID-19. Machine learning techniques for COVID-19 detection require substantial amounts of chest images to discover correct patterns. However, concerns over confidentiality and privacy have limited access to patients’ data. The distribution of samples across normal/abnormal classes is typically biased or skewed due to unavailability of sufficient data because of COVID-19 recency. Existing synthetic COVID-19 data generation approaches fail to generate high-resolution and diverse images. Moreover, there is a lack of research identifying whether synthetic images represent patients at high risk of severe disease, which is critical for making treatment decisions. We propose a High-Resolution COVID-19 X-Ray Generator (HRCX) framework based on a combination of a generative adversarial network and a predictive learning model that uses limited available chest images to generate balanced diverse high-resolution COVID-19 images with their severity scores. We use StyleGAN2 with adaptive discriminator augmentation, which controls generated images’ style and generates diverse patterns. In addition, we provide a COVID-19 severity index to aid in predicting illness severity. We generated 3300 high-quality and diverse COVID-19 X-Ray images with a resolution of 512x512, which we further increased to 1024x1024 with the help of Super-Resolution. Additionally, severity scores of 300 images are calculated and demonstrated to be effective in both normal and infected cases.},
booktitle = {Proceedings of the 2023 Australasian Computer Science Week},
pages = {151–159},
numpages = {9},
keywords = {COVID-19, Chest X-rays, Deep learning, Generative Adversarial Networks, Synthetic image generation},
location = {Melbourne, VIC, Australia},
series = {ACSW '23}
}

@inproceedings{10.1145/3588432.3591555,
author = {Yu, Wangbo and Fan, Yanbo and Zhang, Yong and Wang, Xuan and Yin, Fei and Bai, Yunpeng and Cao, Yan-Pei and Shan, Ying and Wu, Yang and Sun, Zhongqian and Wu, Baoyuan},
title = {NOFA: NeRF-based One-shot Facial Avatar Reconstruction},
year = {2023},
isbn = {9798400701597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588432.3591555},
doi = {10.1145/3588432.3591555},
abstract = {3D facial avatar reconstruction has been a significant research topic in computer graphics and computer vision, where photo-realistic rendering and flexible controls over poses and expressions are necessary for many related applications. Recently, its performance has been greatly improved with the development of neural radiance fields (NeRF). However, most existing NeRF-based facial avatars focus on subject-specific reconstruction and reenactment, requiring multi-shot images containing different views of the specific subject for training, and the learned model cannot generalize to new identities, limiting its further applications. In this work, we propose a one-shot 3D facial avatar reconstruction framework that only requires a single source image to reconstruct a high-fidelity 3D facial avatar. For the challenges of lacking generalization ability and missing multi-view information, we leverage the generative prior of 3D GAN and develop an efficient encoder-decoder network to reconstruct the canonical neural volume of the source image, and further propose a compensation network to complement facial details. To enable fine-grained control over facial dynamics, we propose a deformation field to warp the canonical volume into driven expressions. Through extensive experimental comparisons, we achieve superior synthesis results compared to several state-of-the-art methods.},
booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},
articleno = {85},
numpages = {12},
keywords = {Facial avatar, NeRF, Video synthesis},
location = {Los Angeles, CA, USA},
series = {SIGGRAPH '23}
}

@inproceedings{10.1145/3594739.3605101,
author = {Tonkin, Emma L. and Tourte, Gregory J. L. and Stoev, Teodor and Yordanova, Kristina},
title = {ARDUOUS: Tutorial on Annotation of useR Data for UbiquitOUs Systems - Developing a Data Annotation Protocol},
year = {2023},
isbn = {9798400702006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594739.3605101},
doi = {10.1145/3594739.3605101},
abstract = {Data annotation is key to a large number of fields, including ubiquitous computing. Documenting the quality and extent of annotation is increasingly recognised as an important aspect of understanding the validity, biases and limitations of systems built using this data: hence, it is also relevant to regulatory and compliance needs and outcomes. However, the process of annotation often receives little attention, and is characterised in the literature as “under-described” and “invisible work”. In this tutorial, we bring together existing resources and methods to present a framework for the iterative development and evaluation of an annotation protocol, from requirements gathering, setting scope, development, documentation, piloting and evaluation, through to scaling-up annotation processes for a production annotation process. We also explore the potential of semi-supervised approaches and state-of-the-art methods such as the use of generative AI in supporting annotation workflows, and how such approaches are validated and their strengths and weaknesses characterised. This tutorial is designed to be suitable for people from a wide range of backgrounds, as annotation can be understood as a highly interdisciplinary task and often requires collaboration with subject matter experts from relevant fields. Participants will trial and evaluate a selection of annotation interfaces and walk through the process of evaluating the outcomes. By the end of the workshop, participants will develop a deeper understanding of the task of developing an annotation protocol and aspects of the requirements and context which should be taken into account. Presentations and code from this event will be shared openly on a Github repository.},
booktitle = {Adjunct Proceedings of the 2023 ACM International Joint Conference on Pervasive and Ubiquitous Computing &amp; the 2023 ACM International Symposium on Wearable Computing},
pages = {755–758},
numpages = {4},
keywords = {annotation methods, annotation protocol, automated annotation, data annotation, data labelling, datasets, manual annotation},
location = {Cancun, Quintana Roo, Mexico},
series = {UbiComp/ISWC '23 Adjunct}
}

@article{10.1145/3589973,
author = {Sam, Tyler and Chen, Yudong and Yu, Christina Lee},
title = {Overcoming the Long Horizon Barrier for Sample-Efficient Reinforcement Learning with Latent Low-Rank Structure},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
url = {https://doi.org/10.1145/3589973},
doi = {10.1145/3589973},
abstract = {The practicality of reinforcement learning algorithms has been limited due to poor scaling with respect to the problem size, as the sample complexity of learning an ε-optimal policy is Ω(|S||A|H/ ε2) over worst case instances of an MDP with state space S, action space A, and horizon H. We consider a class of MDPs for which the associated optimal Q* function is low rank, where the latent features are unknown. While one would hope to achieve linear sample complexity in |S| and |A| due to the low rank structure, we show that without imposing further assumptions beyond low rank of Q*, if one is constrained to estimate the Q function using only observations from a subset of entries, there is a worst case instance in which one must incur a sample complexity exponential in the horizon H to learn a near optimal policy. We subsequently show that under stronger low rank structural assumptions, given access to a generative model, Low Rank Monte Carlo Policy Iteration (LR-MCPI) and Low Rank Empirical Value Iteration (LR-EVI) achieve the desired sample complexity of \~{O}((|S|+|A|)poly (d,H)/ε2) for a rank d setting, which is minimax optimal with respect to the scaling of |S|, |A|, and ε. In contrast to literature on linear and low-rank MDPs, we do not require a known feature mapping, our algorithm is computationally simple, and our results hold for long time horizons. Our results provide insights on the minimal low-rank structural assumptions required on the MDP with respect to the transition kernel versus the optimal action-value function.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = may,
articleno = {29},
numpages = {60},
keywords = {low-rank matrix estimation, reinforcement learning}
}

@inproceedings{10.1145/3534678.3539288,
author = {Ling, Chen and Jiang, Junji and Wang, Junxiang and Liang, Zhao},
title = {Source Localization of Graph Diffusion via Variational Autoencoders for Graph Inverse Problems},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539288},
doi = {10.1145/3534678.3539288},
abstract = {Graph diffusion problems such as the propagation of rumors, computer viruses, or smart grid failures are ubiquitous and societal. Hence it is usually crucial to identify diffusion sources according to the current graph diffusion observations. Despite its tremendous necessity and significance in practice, source localization, as the inverse problem of graph diffusion, is extremely challenging as it is ill-posed: different sources may lead to the same graph diffusion patterns. Different from most traditional source localization methods, this paper focuses on a probabilistic manner to account for the uncertainty of different candidate sources. Such endeavors require to overcome significant challenges along the way including: 1) the uncertainty in graph diffusion source localization is hard to be quantified; 2) the complex patterns of the graph diffusion sources are difficult to be probabilistically characterized; 3) the generalization under any underlying diffusion patterns is hard to be imposed. To solve the above challenges, this paper presents a generic framework: Source Localization Variational AutoEncoder (SL-VAE) for locating the diffusion sources under arbitrary diffusion patterns. Particularly, we propose a probabilistic model that leverages the forward diffusion estimation model along with deep generative models to approximate the diffusion source distribution for quantifying the uncertainty. SL-VAE further utilizes prior knowledge of the source-observation pairs to characterize the complex patterns of diffusion sources by a learned generative prior. Lastly, a unified objective that integrates the forward diffusion estimation model is derived to enforce the model to generalize under arbitrary diffusion patterns. Extensive experiments are conducted on $7$ real-world datasets to demonstrate the superiority of SL-VAE in reconstructing the diffusion sources by excelling the state-of-the-arts on average 20% in AUC score. The code and data are available at: https://github.com/triplej0079/SLVAE.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1010–1020},
numpages = {11},
keywords = {graph source localization, information diffusion, inverse problem},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3586183.3606795,
author = {Strohm, Florian and B\^{a}ce, Mihai and Bulling, Andreas},
title = {Usable and Fast Interactive Mental Face Reconstruction},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606795},
doi = {10.1145/3586183.3606795},
abstract = {We introduce an end-to-end interactive system for mental face reconstruction – the challenging task of visually reconstructing a face image a person only has in their mind. In contrast to existing methods that suffer from low usability and high mental load, our approach only requires the user to rank images over multiple iterations according to the perceived similarity with their mental image. Based on these rankings, our mental face reconstruction system extracts image features in each iteration, combines them into a joint feature vector, and then uses a generative model to visually reconstruct the mental image. To avoid the need for collecting large amounts of human training data, we further propose a computational user model that can simulate human ranking behaviour using data from an online crowd-sourcing study (N=215). Results from a 12-participant user study show that our method can reconstruct mental images that are visually similar to existing approaches but has significantly higher usability, lower perceived workload, and is faster. In addition, results from a third 22-participant lineup study in which we validated our reconstructions on a face ranking task show a identification rate of , which is in line with prior work. These results represent an important step towards new interactive intelligent systems that can robustly and effortlessly reconstruct a user’s mental image.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {99},
numpages = {15},
keywords = {deep learning, faces, mental image reconstruction, user modelling},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@article{10.1145/3517154,
author = {Gao, Honghao and Dai, Baobin and Miao, Huaikou and Yang, Xiaoxian and Barroso, Ramon J. Duran and Walayat, Hussain},
title = {A Novel GAPG Approach to Automatic Property Generation for Formal Verification: The GAN Perspective},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {1551-6857},
url = {https://doi.org/10.1145/3517154},
doi = {10.1145/3517154},
abstract = {Formal methods have been widely used to support software testing to guarantee correctness and reliability. For example, model checking technology attempts to ensure that the verification property of a specific formal model is satisfactory for discovering bugs or abnormal behavior from the perspective of temporal logic. However, because automatic approaches are lacking, a software developer/tester must manually specify verification properties. A generative adversarial network (GAN) learns features from input training data and outputs new data with similar or coincident features. GANs have been successfully used in the image processing and text processing fields and achieved interesting and automatic results. Inspired by the power of GANs, in this article, we propose a GAN-based automatic property generation (GAPG) approach to generate verification properties supporting model checking. First, the verification properties in the form of computational tree logic (CTL) are encoded and used as input to the GAN. Second, we introduce regular expressions as grammar rules to check the correctness of the generated properties. These rules work to detect and filter meaningless properties that occur because the GAN learning process is uncontrollable and may generate unsuitable properties in real applications. Third, the learning network is further trained by using labeled information associated with the input properties. These are intended to guide the training process to generate additional new properties, particularly those that map to corresponding formal models. Finally, a series of comprehensive experiments demonstrate that the proposed GAPG method can obtain new verification properties from two aspects: (1) using only CTL formulas and (2) using CTL formulas combined with Kripke structures.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jan,
articleno = {16},
numpages = {22},
keywords = {Model checking, verification property, generative adversarial network (GAN), automatic property generation, computational tree logic, correctness and reliability}
}

@article{10.5555/3648699.3648859,
author = {Masud, Shoaib Bin and Werenski, Matthew and Murphy, James M. and Aeron, Shuchin},
title = {Multivariate soft rank via entropy-regularized optimal transport: sample efficiency and generative modeling},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {The framework of optimal transport has been leveraged to extend the notion of rank to the multivariate setting as corresponding to an optimal transport map, while preserving desirable properties of the resulting goodness-of-fit (GoF) statistics. In particular, the rank energy (RE) and rank maximum mean discrepancy (RMMD) are distribution-free under the null, exhibit high power in statistical testing, and are robust to outliers. In this paper, we point to and alleviate some of the shortcomings of these GoF statistics that are of practical significance, namely high computational cost, curse of dimensionality in statistical sample complexity, and lack of differentiability with respect to the data. We show that all these issues are addressed by defining multivariate rank as an entropic transport map derived from the entropic regularization of the optimal transport problem, which we refer to as the soft rank. We consequently propose two new statistics, the soft rank energy (sRE) and soft rank maximum mean discrepancy (sRMMD). Given n sample data points, we provide non-asymptotic convergence rates for the sample estimate of the entropic transport map to its population version that are essentially of the order n-1/2 when the source measure is subgaussian and the target measure has compact support. This result is novel compared to existing results which achieve a rate of n-1 but crucially rely on both measures having compact support. In contrast, the corresponding convergence rate of estimating an optimal transport map, and hence the rank map, is exponential in the data dimension. We leverage these fast convergence rates to show that the sample estimates of sRE and sRMMD converge rapidly to their population versions. Combined with the computational efficiency of methods in solving the entropy-regularized optimal transport problem, these results enable efficient rank-based GoF statistical computation, even in high dimensions. Furthermore, the sample estimates of sRE and sRMMD are differentiable with respect to the data and amenable to popular machine learning frameworks that rely on gradient methods. We leverage these properties towards showcasing their utility for generative modeling on two important problems: image generation and generating valid knockoffs for controlled feature selection.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {160},
numpages = {65},
keywords = {optimal transport, multivariate rank, high-dimensional statistics, goodness-of-fit testing, generative modeling, knockoff filtering}
}

@inproceedings{10.1145/3503161.3548378,
author = {Zhang, Peng-Fei and Bai, Guangdong and Huang, Zi and Xu, Xin-Shun},
title = {Machine Unlearning for Image Retrieval: A Generative Scrubbing Approach},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548378},
doi = {10.1145/3503161.3548378},
abstract = {Data owners have the right to request for deleting their data from a machine learning (ML) model. In response, a na\"{\i}ve way is to retrain the model with the original dataset excluding the data to forget, which is however unrealistic as the required dataset may no longer be available and the retraining process is usually computationally expensive. To cope with this reality, machine unlearning has recently attained much attention, which aims to enable data removal from a trained ML model responding to deletion requests, without retraining the model from scratch or full access to the original training dataset. Existing unlearning methods mainly focus on handling conventional ML methods, while unlearning deep neural networks (DNNs) based models remains underexplored, especially for the ones trained on large-scale datasets.In this paper, we make the first attempt to realize data forgetting on deep models for image retrieval. Image retrieval targets at searching relevant data to the query according to similarity measures. Intuitively, unlearning a deep image retrieval model can be achieved by breaking down its ability of similarity modeling on the data to forget. To this end, we propose a generative scrubbing (GS) method that learns a generator to craft noisy data to manipulate the model weights. A novel framework is designed consisting of the generator and the target retrieval model, where a pair of coupled static and dynamic learning procedures are performed simultaneously. This novel learning strategy effectively enables the generated noisy data to fade away the memory of the model on the data to forget whilst retaining the information of the remaining data. Extensive experiments on three widely-used datasets have successfully verified the effectiveness of the proposed method.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {237–245},
numpages = {9},
keywords = {deep learning, image retrieval, machine unlearning, privacy protection},
location = {Lisboa, Portugal},
series = {MM '22}
}

@article{10.1145/3526115,
author = {Chhabria, Vidya A. and Ahuja, Vipul and Prabhu, Ashwath and Patil, Nikhil and Jain, Palkesh and Sapatnekar, Sachin S.},
title = {Encoder-Decoder Networks for Analyzing Thermal and Power Delivery Networks},
year = {2022},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {1084-4309},
url = {https://doi.org/10.1145/3526115},
doi = {10.1145/3526115},
abstract = {Power delivery network (PDN) analysis and thermal analysis are computationally expensive tasks that are essential for successful integrated circuit (IC) design. Algorithmically, both these analyses have similar computational structure and complexity as they involve the solution to a partial differential equation of the same form. This article converts these analyses into image-to-image and sequence-to-sequence translation tasks, which allows leveraging a class of machine learning models with an encoder-decoder–based generative (EDGe) architecture to address the time-intensive nature of these tasks. For PDN analysis, we propose two networks: (i)&nbsp;IREDGe: a full-chip static and dynamic IR drop predictor and (ii)&nbsp;EMEDGe: electromigration (EM) hotspot classifier based on input power, power grid distribution, and power pad distribution patterns. For thermal analysis, we propose ThermEDGe, a full-chip static and dynamic temperature estimator based on input power distribution patterns for thermal analysis. These networks are transferable across designs synthesized within the same technology and packing solution. The networks predict on-chip IR drop, EM hotspot locations, and temperature in milliseconds with negligibly small errors against commercial tools requiring several hours.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = dec,
articleno = {3},
numpages = {27},
keywords = {Machine learning (ML) for electronic design automation (EDA), Thermal analysis, Power delivery network (PDN) analysis, IR drop, electromigration (EM), encoder-decoder networks, U-Nets}
}

@inproceedings{10.1145/3559400.3565581,
author = {Lipkowitz, Gabriel and Shaqfeh, Eric and DeSimone, Joseph},
title = {Generative co-design for microfluidics-accelerated 3D printing},
year = {2022},
isbn = {9781450398725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3559400.3565581},
doi = {10.1145/3559400.3565581},
abstract = {Increasing the speed of 3D printing is critical for its widespread adoption as a manufacturing technique. A recently-introduced multi-material 3D printing process called injection CLIP has been shown to accelerate printing by nearly an order of magnitude using computationally-designed microfluidic networks. However, automated design tools for 3D microfluidics have lagged far behind increasing fabrication capabilities and needs. Here, to facilitate the study and application of this novel printing process, we present a fluid dynamics-informed generative design tool for microfluidic networks based on native CAD geometries.},
booktitle = {Proceedings of the 7th Annual ACM Symposium on Computational Fabrication},
articleno = {26},
numpages = {3},
location = {Seattle, WA, USA},
series = {SCF '22}
}

@inproceedings{10.1145/3534678.3539032,
author = {Moomtaheen, Fariha and Killeen, Matthew and Oswald, James and Gonz\`{a}lez-Rosell, Anna and Mastracco, Peter and Gorovits, Alexander and Copp, Stacy M. and Bogdanov, Petko},
title = {DNA-Stabilized Silver Nanocluster Design via Regularized Variational Autoencoders},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539032},
doi = {10.1145/3534678.3539032},
abstract = {DNA-stabilized silver nanoclusters (AgN-DNAs) are a class of nanomaterials comprised of 10-30 silver atoms held together by short synthetic DNA template strands. AgN-DNAs are promising biosensors and fluorophores due to their small sizes, natural compatibility with DNA, and bright fluorescence---the property of absorbing light and re-emitting light of a different color. The sequence of the DNA template acts as a "genome" for AgN-DNAs, tuning the size of the encapsulated silver nanocluster, and thus its fluorescence color. However, current understanding of the AgN-DNA genome is still limited. Only a minority of DNA sequences produce highly fluorescent AgN-DNAs, and the bulky DNA strands and complex DNA-silver interactions make it challenging to use first principles chemical calculations to understand and design AgN-DNAs. Thus, a major challenge for researchers studying these nanomaterials is to develop methods to employ observational data about studied AgN-DNAs to design new nanoclusters for targeted applications.In this work, we present an approach to design AgN-DNAs by employing variational autoencoders (VAEs) as generative models. Specifically, we employ an LSTM-based β-VAE architecture and regularize its latent space to correlate with AgN-DNA properties such as color and brightness. The regularization is adaptive to skewed sample distributions of available observational data along our design axes of properties. We employ our model for design of AgN-DNAs in the near-infrared (NIR) band, where relatively few AgN-DNAs have been observed to date. Wet lab experiments validate that when employed for designing new AgN-DNAs, our model significantly shifts the distribution of AgN-DNA colors towards the NIR while simultaneously achieving bright fluorescence. This work shows that VAE-based generative models are well-suited for the design of AgN-DNAs with multiple targeted properties, with significant potential to advance the promising applications of these nanomaterials for bioimaging, biosensing, and other critical technologies.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3593–3602},
numpages = {10},
keywords = {DNA, nanomaterials design, variational autoencoders},
location = {Washington DC, USA},
series = {KDD '22}
}

@article{10.1109/TASLP.2022.3190734,
author = {Sekiguchi, Kouhei and Bando, Yoshiaki and Nugraha, Aditya Arie and Fontaine, Mathieu and Yoshii, Kazuyoshi and Kawahara, Tatsuya},
title = {Autoregressive Moving Average Jointly-Diagonalizable Spatial Covariance Analysis for Joint Source Separation and Dereverberation},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3190734},
doi = {10.1109/TASLP.2022.3190734},
abstract = {This article describes a computationally-efficient statistical approach to joint (semi-)blind source separation and dereverberation for multichannel noisy reverberant mixture signals. A standard approach to source separation is to formulate a generative model of a multichannel mixture spectrogram that consists of source and spatial models representing the time-frequency power spectral densities (PSDs) and spatial covariance matrices (SCMs) of source images, respectively, and find the maximum-likelihood estimates of these parameters. A state-of-the-art blind source separation method in this thread of research is fast multichannel nonnegative matrix factorization (FastMNMF) based on the low-rank PSDs and jointly-diagonalizable full-rank SCMs. To perform mutually-dependent separation and dereverberation jointly, in this paper we integrate both moving average (MA) and autoregressive (AR) models that represent the early reflections and late reverberations of sources, respectively, into the FastMNMF formalism. Using a pretrained deep generative model of speech PSDs as a source model, we realize semi-blind joint speech separation and dereverberation. We derive an iterative optimization algorithm based on iterative projection or iterative source steering for jointly and efficiently updating the AR parameters and the SCMs. Our experimental results showed the superiority of the proposed ARMA extension over its AR- or MA-ablated version in a speech separation and/or dereverberation task.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {2368–2382},
numpages = {15}
}

@article{10.1145/3522574,
author = {Gao, Yang},
title = {Icon Art Design Language Combined with Real-time Intelligent Image Processing under Internet of Things},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3522574},
doi = {10.1145/3522574},
abstract = {While providing massive information, the intelligent media Internet of Things (IoT) also poses challenges to the overall environment and the development of modern market economy. The employment of enterprises and people is still facing great difficulties, and the world economic situation is still complicated and severe. In addition, there are many design resources for icon art design on the Internet, and with different design styles, the demand for icon design is also increasing. The biggest difference between icons and ordinary pictures is that icons can convey the characteristics and meaning of pictures faster. The Generative Adversarial Network (GAN) technology in intelligent image processing and the TensorFlow learning framework are used to build and improve the icon generation network to simplify the icon design process. Computers are used in place of designers for icon art design. Firstly, the related technical background of icon generation network implementation is drawn through the introduction of related concepts of intelligent image processing. Secondly, Python is used to process the established icon dataset. Finally, the icon generation network is improved. The model training results show that the icon generation network has a peak feature loss value of 9.0 and an average error of 8.0. After the color label is added, the effect is significantly improved. The improved icon generation network has a peak feature loss of 7.0 and an average error of 6.0. The results show that after the color labels are added, the improved GAN model has a very high recognition rate for artistic icons. The improved network model also distinguishes the newly generated icons from the original ones. The comprehensive application effect of the model is good. This provides specific application and reference value for the intelligent development of the IoT.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
keywords = {intelligent media era, image processing, icon art, generative adversarial network, IoT intelligence}
}

@inproceedings{10.1145/3597926.3598099,
author = {Xiao, Yisong and Liu, Aishan and Li, Tianlin and Liu, Xianglong},
title = {Latent Imitator: Generating Natural Individual Discriminatory Instances for Black-Box Fairness Testing},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598099},
doi = {10.1145/3597926.3598099},
abstract = {Machine learning (ML) systems have achieved remarkable performance across a wide area of applications. However, they frequently exhibit unfair behaviors in sensitive application domains (e.g., employment and loan), raising severe fairness concerns. To evaluate and test fairness, engineers often generate individual discriminatory instances to expose unfair behaviors before model deployment. However, existing baselines ignore the naturalness of generation and produce instances that deviate from the real data distribution, which may fail to reveal the actual model fairness since these unnatural discriminatory instances are unlikely to appear in practice. To address the problem, this paper proposes a framework named Latent Imitator (LIMI) to generate more natural individual discriminatory instances with the help of a generative adversarial network (GAN), where we imitate the decision boundary of the target model in the semantic latent space of GAN and further samples latent instances on it. Specifically, we first derive a surrogate linear boundary to coarsely approximate the decision boundary of the target model, which reflects the nature of the original data distribution. Subsequently, to obtain more natural instances, we manipulate random latent vectors to the surrogate boundary with a one-step movement, and further conduct vector calculation to probe two potential discriminatory candidates that may be more closely located in the real decision boundary. Extensive experiments on various datasets demonstrate that our LIMI outperforms other baselines largely in effectiveness (\texttimes{}9.42 instances), efficiency (\texttimes{}8.71 speeds), and naturalness (+19.65%) on average. In addition, we empirically demonstrate that retraining on test samples generated by our approach can lead to improvements in both individual fairness (45.67% on IFr and 32.81% on IFo) and group fairness (9.86% on SPD and 28.38% on AOD). Our codes can be found on our website.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {829–841},
numpages = {13},
keywords = {Fairness Testing, Individual Discrimination, Latent Space, Natural Individual Discriminatory Instances},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@article{10.5555/3648699.3649079,
author = {Guo, Xiao and Qiu, Yixuan and Zhang, Hai and Chang, Xiangyu},
title = {Randomized spectral co-clustering for large-scale directed networks},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Directed networks are broadly used to represent asymmetric relationships among units. Co-clustering aims to cluster the senders and receivers of directed networks simultaneously. In particular, the well-known spectral clustering algorithm could be modified as the spectral co-clustering to co-cluster directed networks. However, large-scale networks pose great computational challenges to it. In this paper, we leverage sketching techniques and derive two randomized spectral co-clustering algorithms, one random-projection-basedand the other random-sampling-based, to accelerate the co-clustering of large-scale directed networks. We theoretically analyze the resulting algorithms under two generative models - the stochastic co-block model and the degree-corrected stochastic co-block model, and establish their approximation error rates and misclustering error rates, indicating better bounds than the state-of-the-art results of co-clustering literature. Numerically, we design and conduct simulations to support our theoretical results and test the efficiency of the algorithms on real networks with up to millions of nodes. A publicly available R package RandClust is developed for better usability and reproducibility of the proposed methods.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {380},
numpages = {68},
keywords = {co-clustering, directed network, random projection, random sampling, stochastic co-block model}
}

@article{10.1145/3526212,
author = {Yang, Jie and Mo, Kaichun and Lai, Yu-Kun and Guibas, Leonidas J. and Gao, Lin},
title = {DSG-Net: Learning Disentangled Structure and Geometry for 3D Shape Generation},
year = {2022},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {0730-0301},
url = {https://doi.org/10.1145/3526212},
doi = {10.1145/3526212},
abstract = {3D shape generation is a fundamental operation in computer graphics. While significant progress has been made, especially with recent deep generative models, it remains a challenge to synthesize high-quality shapes with rich geometric details and complex structures, in a controllable manner. To tackle this, we introduce DSG-Net, a deep neural network that learns a disentangled structured &amp; geometric mesh representation for 3D shapes, where two key aspects of shapes, geometry and structure, are encoded in a synergistic manner to ensure plausibility of the generated shapes, while also being disentangled as much as possible. This supports a range of novel shape generation applications with disentangled control, such as interpolation of structure (geometry) while keeping geometry (structure) unchanged. To achieve this, we simultaneously learn structure and geometry through variational autoencoders (VAEs) in a hierarchical manner for both, with bijective mappings at each level. In this manner, we effectively encode geometry and structure in separate latent spaces, while ensuring their compatibility: the structure is used to guide the geometry and vice versa. At the leaf level, the part geometry is represented using a conditional part VAE, to encode high-quality geometric details, guided by the structure context as the condition. Our method not only supports controllable generation applications, but also produces high-quality synthesized shapes, outperforming state-of-the-art methods.},
journal = {ACM Trans. Graph.},
month = aug,
articleno = {1},
numpages = {17},
keywords = {3D shape generation, disentangled representation, structure, geometry, hierarchies}
}

@inproceedings{10.1145/3477495.3531858,
author = {Chaffin, Antoine and Scialom, Thomas and Lamprier, Sylvain and Staiano, Jacopo and Piwowarski, Benjamin and Kijak, Ewa and Claveau, Vincent},
title = {Which Discriminator for Cooperative Text Generation?},
year = {2022},
isbn = {9781450387323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477495.3531858},
doi = {10.1145/3477495.3531858},
abstract = {Language models generate texts by successively predicting probability distributions for next tokens given past ones. A growing field of interest tries to leverage external information in the decoding process so that the generated texts have desired properties, such as being more natural, non toxic, faithful, or having a specific writing style. A solution is to use a classifier at each generation step, resulting in a cooperative environment where the classifier guides the decoding of the language model distribution towards relevant texts for the task at hand. In this paper, we examine three families of (transformer-based) discriminators for this specific task of cooperative decoding: bidirectional, left-to-right and generative ones. We evaluate the pros and cons of these different types of discriminators for cooperative generation, exploring respective accuracy on classification tasks along with their impact on the resulting sample quality and computational performances. We also provide the code of a batched implementation of the powerful cooperative decoding strategy used for our experiments, the Monte Carlo Tree Search, working with each discriminator for Natural Language Generation.},
booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2360–2365},
numpages = {6},
keywords = {attention, cooperative, discriminator, empirical, monte carlo tree search, natural language generation, performance},
location = {Madrid, Spain},
series = {SIGIR '22}
}

@inproceedings{10.1145/3580305.3599582,
author = {Bagherjeiran, Abraham and Djuric, Nemanja and Lee, Kuang-Chih and Pang, Linsey and Radosavljevic, Vladan and Rajan, Suju},
title = {AdKDD 2023},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599582},
doi = {10.1145/3580305.3599582},
abstract = {The digital advertising field has always had challenging ML problems, learning from petabytes of data that is highly imbalanced, reactivity times in the milliseconds, and more recently compounded with the complex user's path to purchase across devices, across platforms, and even online/real-world behavior. The AdKDD workshop continues to be a forum for researchers in advertising, during and after KDD. Our website which hosts slides and abstracts receives approximately 2,000 monthly visits and 1,800 active users during the KDD 2021. In surveys during AdKDD 2019 and 2020, over 60% agreed that AdKDD is the reason they attended KDD, and over 90% indicated they would attend next year. The 2023 edition is particularly timely because of the increasing application of Graph-based NN and Generative AI models in advertising. Coupled with privacy-preserving initiatives enforced by GDPR, CCPA the future of computational advertising is at an interesting crossroads. For this edition, we plan to solicit papers that span the spectrum of deep user understanding while remaining privacy-preserving. In addition, we will seek papers that discuss fairness in the context of advertising, to what extent does hyper-personalization work, and whether the ad industry as a whole needs to think through more effective business models such as incrementality. We have hosted several academic and industry luminaries as keynote speakers and have found our invited speaker series hosting expert practitioners to be an audience favorite. We will continue fielding a diverse set of keynote speakers and invited talks for this edition as well. As with past editions, we hope to motivate researchers in this space to think not only about the ML aspects but also to spark conversations about the societal impact of online advertising.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5849–5850},
numpages = {2},
keywords = {ad targeting, computational advertising, user modeling},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3520495.3520513,
author = {Gough, Phillip and Kocaballi, A. Baki and Naqshbandi, Khushnood Z. and Cochrane, Karen and Mah, Kristina and Pillai, Ajit G. and Yorulmaz, Yeliz and Deny, Ainnoun Kornita and Ahmadpour, Naseem},
title = {Co-designing a Technology Probe with Experienced Designers},
year = {2022},
isbn = {9781450395984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520495.3520513},
doi = {10.1145/3520495.3520513},
abstract = {Technology probes are low-fidelity devices that can be used to understand research participant’s lived experiences, but they are not usually subject to iterative design. There are opportunities in human-computer interaction to develop technology probes through co-design, by including diverse perspectives during probe development. To explore this opportunity, five design researchers with different disciplinary and cultural backgrounds engaged with a technology probe to support daily reflections, discussed new directions in a co-design workshop and developed narratives to negotiate possibilities of the probe. This paper presents observations described by each of the researchers using the probe, and insights from the process we followed. We discuss how the the designers’ postitionalities are reflected in the processes, and how they brought value by shaping narratives of the different roles a technology probe might take. We also discuss how we may use co-design of technology probes as a generative method, highlight the importance of open-endedness in the process, and reflect on lessons learned.},
booktitle = {Proceedings of the 33rd Australian Conference on Human-Computer Interaction},
pages = {1–13},
numpages = {13},
keywords = {Technology probes, co-design, positionality, reflection},
location = {Melbourne, VIC, Australia},
series = {OzCHI '21}
}

@article{10.1145/3630633,
author = {Belkhouja, Taha and Yan, Yan and Doppa, Janardhan Rao},
title = {Out-of-distribution Detection in Time-series Domain: A Novel Seasonal Ratio Scoring Approach},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {2157-6904},
url = {https://doi.org/10.1145/3630633},
doi = {10.1145/3630633},
abstract = {Safe deployment of time-series classifiers for real-world applications relies on the ability to detect the data that is not generated from the same distribution as training data. This task is referred to as out-of-distribution (OOD) detection. We consider the novel problem of OOD detection for the time-series domain. We discuss the unique challenges posed by time-series data and explain why prior methods from the image domain will perform poorly. Motivated by these challenges, this article proposes a novel Seasonal Ratio Scoring (SRS) approach. SRS consists of three key algorithmic steps. First, each input is decomposed into class-wise semantic component and remainder. Second, this decomposition is employed to estimate the class-wise conditional likelihoods of the input and remainder using deep generative models. The seasonal ratio score is computed from these estimates. Third, a threshold interval is identified from the in-distribution data to detect OOD examples. Experiments on diverse real-world benchmarks demonstrate that the SRS method is well-suited for time-series OOD detection when compared to baseline methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = dec,
articleno = {8},
numpages = {24},
keywords = {Time series data, Deep learning, Machine learning robustness, Out-of-distribution detection}
}

@article{10.1145/3628605,
author = {Ceberio, Josu and Santucci, Valentino},
title = {Model-based Gradient Search for Permutation Problems},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
url = {https://doi.org/10.1145/3628605},
doi = {10.1145/3628605},
abstract = {Global random search algorithms are characterized by using probability distributions to optimize problems. Among them, generative methods iteratively update the distributions by using the observations sampled. For instance, this is the case of the well-known Estimation of Distribution Algorithms. Although successful, this family of algorithms iteratively adopts numerical methods for estimating the parameters of a model or drawing observations from it. This is often a very time-consuming task, especially in permutation-based combinatorial optimization problems. In this work, we propose using a generative method, under the model-based gradient search framework, to optimize permutation-coded problems and address the mentioned computational overheads. To that end, the Plackett–Luce model is used to define the probability distribution on the search space of permutations. Not limited to that, a parameter-free variant of the algorithm is investigated. Conducted experiments, directed to validate the work, reveal that the gradient search scheme produces better results than other analogous competitors, reducing the computational cost and showing better scalability.},
journal = {ACM Trans. Evol. Learn. Optim.},
month = dec,
articleno = {15},
numpages = {35},
keywords = {Gradient search, permutation, probability distribution, combinatorial problem, self-adaptive}
}

@inproceedings{10.1145/3607827.3616840,
author = {Mohiuddin, Tasnim and Zhang, Tianyi and Nie, Maowen and Huang, Jing and Chen, Qianqian and Shi, Wei},
title = {ImEW: A Framework for Editing Image in the Wild},
year = {2023},
isbn = {9798400702839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607827.3616840},
doi = {10.1145/3607827.3616840},
abstract = {The ability to edit images in a realistic and visually appealing manner is a fundamental requirement in various computer vision applications. In this paper, we present ImEW, a unified framework designed for solving image editing tasks. ImEW utilizes off-the-shelf foundation models to address four essential editing tasks: object removal, object translation, object replacement, and generative fill beyond the image frame. These tasks are accomplished by leveraging the capabilities of state-of-the-art foundation models, namely the Segment Anything Model, Grounding DINO, LaMa, and Stable Diffusion. These models have undergone extensive training on large-scale datasets and have exhibited exceptional performance in understanding image context, object manipulation, and texture synthesis. Through extensive experimentation, we demonstrate the effectiveness and versatility of ImEW in accomplishing image editing tasks across a wide range of real-world scenarios. The proposed framework opens up new possibilities for realistic and visually appealing image editing and enables diverse applications requiring sophisticated image modifications. Additionally, we discuss the limitations and outline potential directions for future research in the field of image editing using off-the-shelf foundation models, enabling continued advancements in this domain.},
booktitle = {Proceedings of the 1st Workshop on Large Generative Models Meet Multimodal Applications},
pages = {34–44},
numpages = {11},
keywords = {diffusion models, generative models, image editing, segment anything model},
location = {Ottawa ON, Canada},
series = {LGM3A '23}
}

@inproceedings{10.1145/3580305.3599365,
author = {Id\'{e}, Tsuyoshi and Abe, Naoki},
title = {Generative Perturbation Analysis for Probabilistic Black-Box Anomaly Attribution},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599365},
doi = {10.1145/3580305.3599365},
abstract = {We address the task of probabilistic anomaly attribution in the black-box regression setting, where the goal is to compute the probability distribution of the attribution score of each input variable, given an observed anomaly. The training dataset is assumed to be unavailable. This task differs from the standard XAI (explainable AI) scenario, since we wish to explain the anomalous deviation from a black-box prediction rather than the black-box model itself.We begin by showing that mainstream model-agnostic explanation methods, such as the Shapley values, are not suitable for this task because of their ''deviation-agnostic property.'' We then propose a novel framework for probabilistic anomaly attribution that allows us to not only compute attribution scores as the predictive mean but also quantify the uncertainty of those scores. This is done by considering a generative process for perturbations that counter-factually bring the observed anomalous observation back to normalcy. We introduce a variational Bayes algorithm for deriving the distributions of per variable attribution scores. To the best of our knowledge, this is the first probabilistic anomaly attribution framework that is free from being deviation-agnostic.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {845–856},
numpages = {12},
keywords = {anomaly attribution, explainable ai (xai), generative model, integrated gradient, shapley value, variational inference},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3589572.3589593,
author = {Ahmed, Tasnim and Munir, Ahnaf and Ahmed, Sabbir and Hasan, Md. Bakhtiar and Reza, Md. Taslim and Kabir, Md. Hasanul},
title = {Structure-Enhanced Translation from PET to CT Modality with Paired GANs},
year = {2023},
isbn = {9781450399531},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589572.3589593},
doi = {10.1145/3589572.3589593},
abstract = {Computed Tomography (CT) images play a crucial role in medical diagnosis and treatment planning. However, acquiring CT images can be difficult in certain scenarios, such as patients inability to undergo radiation exposure or unavailability of CT scanner. An alternative solution can be generating CT images from other imaging modalities. In this work, we propose a medical image translation pipeline for generating high-quality CT images from Positron Emission Tomography (PET) images using a Pix2Pix Generative Adversarial Network (GAN), which are effective in image translation tasks. However, traditional GAN loss functions often fail to capture the structural similarity between generated and target image. To alleviate this issue, we introduce a Multi-Scale Structural Similarity Index Measure (MS-SSIM) loss in addition to the GAN loss to ensure that the generated images preserve the anatomical structures and patterns present in the real CT images. Experiments on the ‘QIN-Breast’ dataset demonstrate that our proposed architecture achieves a Peak Signal-to-Noise Ratio (PSNR) of 17.70 dB and a Structural Similarity Index Measure (SSIM) of 42.51% in the region of interest.},
booktitle = {Proceedings of the 2023 6th International Conference on Machine Vision and Applications},
pages = {142–146},
numpages = {5},
keywords = {Breast Cancer Treatment, GAN, Medical Image Translation, Medical Imaging, PET to CT, QIN-Breast},
location = {Singapore, Singapore},
series = {ICMVA '23}
}

@article{10.1109/TCBB.2022.3141697,
author = {Qureshi, Rizwan and Zou, Bin and Alam, Tanvir and Wu, Jia and Lee, Victor H. F. and Yan, Hong},
title = {Computational Methods for the Analysis and Prediction of EGFR-Mutated Lung Cancer Drug Resistance: Recent Advances in Drug Design, Challenges and Future Prospects},
year = {2022},
issue_date = {Jan.-Feb. 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2022.3141697},
doi = {10.1109/TCBB.2022.3141697},
abstract = {Lung cancer is a major cause of cancer deaths worldwide, and has a very low survival rate. Non-small cell lung cancer (NSCLC) is the largest subset of lung cancers, which accounts for about 85% of all cases. It has been well established that a mutation in the epidermal growth factor receptor (EGFR) can lead to lung cancer. EGFR Tyrosine Kinase Inhibitors (TKIs) are developed to target the kinase domain of EGFR. These TKIs produce promising results at the initial stage of therapy, but the efficacy becomes limited due to the development of drug resistance. In this paper, we provide a comprehensive overview of computational methods, for understanding drug resistance mechanisms. The important EGFR mutants and the different generations of EGFR–TKIs, with the survival and response rates are discussed. Next, we evaluate the role of important EGFR parameters in drug resistance mechanism, including structural dynamics, hydrogen bonds, stability, dimerization, binding free energies, and signaling pathways. Personalized drug resistance prediction models, drug response curve, drug synergy, and other data-driven methods are also discussed. Recent advancements in deep learning; such as AlphaFold2, deep generative models, big data analytics, and the applications of statistics and permutation are also highlighted. We explore limitations in the current methodologies, and discuss strategies to overcome them. We believe this review will serve as a reference for researchers; to apply computational techniques for precision medicine, analyzing structures of protein-drug complexes, drug discovery, and understanding the drug response and resistance mechanisms in lung cancer patients.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jan,
pages = {238–255},
numpages = {18}
}

@inproceedings{10.1145/3594315.3594661,
author = {Zhang, Zongjie and Cheng, Haibo and Li, Wenting and Wang, Ping},
title = {An Improved GAN-based Depth Estimation Network for Face Anti-Spoofing},
year = {2023},
isbn = {9781450399029},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594315.3594661},
doi = {10.1145/3594315.3594661},
abstract = {Face spoofing is a serious threat to the security of face recognition and face anti-spoofing (FAS) has become one of the popular research directions recently. The pipeline of modern face anti-spoofing methods is to train deep neural networks to distinguish live and spoof faces using various auxiliary information. Depth maps are the most commonly used auxiliary features due to their computational simplicity and ease of discrimination. However, existing methods do not generalize well in complex environments and against unknown attacks. In this paper, we propose a novel FAS method using generative adversarial network (GAN). We train a GAN for generating depth maps of faces. The network uses the architecture of Wasserstein GAN and adds an attention module to learn the contribution of different regions to the deep graph. The classifier is trained using a latent variable containing depth information in the network. Experiments demonstrate that our approach can better generalize complex external conditions such as illumination and background. Among all results, we gain the best result of 1.17% (EER) on CASIA-FASD and 21.57% (HTER) on cross testing between Replay-Attack and CASIA-FASD.},
booktitle = {Proceedings of the 2023 9th International Conference on Computing and Artificial Intelligence},
pages = {323–328},
numpages = {6},
keywords = {attention mechanism, face anti-spoofing, generative adversarial network},
location = {Tianjin, China},
series = {ICCAI '23}
}

@inproceedings{10.1109/CGO53902.2022.9741258,
author = {Cummins, Chris and Wasti, Bram and Guo, Jiadong and Cui, Brandon and Ansel, Jason and Gomez, Sahir and Jain, Somya and Liu, Jia and Teytaud, Olivier and Steiner, Benoit and Tian, Yuandong and Leather, Hugh},
title = {CompilerGym: robust, performant compiler optimization environments for AI research},
year = {2022},
isbn = {9781665405843},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CGO53902.2022.9741258},
doi = {10.1109/CGO53902.2022.9741258},
abstract = {Interest in applying Artificial Intelligence (AI) techniques to compiler optimizations is increasing rapidly, but compiler research has a high entry barrier. Unlike in other domains, compiler and AI researchers do not have access to the datasets and frameworks that enable fast iteration and development of ideas, and getting started requires a significant engineering investment. What is needed is an easy, reusable experimental infrastructure for real world compiler optimization tasks that can serve as a common benchmark for comparing techniques, and as a platform to accelerate progress in the field.We introduce CompilerGym, a set of environments for real world compiler optimization tasks, and a toolkit for exposing new optimization tasks to compiler researchers. CompilerGym enables anyone to experiment on production compiler optimization problems through an easy-to-use package, regardless of their experience with compilers. We build upon the popular OpenAI Gym interface enabling researchers to interact with compilers using Python and a familiar API.We describe the CompilerGym architecture and implementation, characterize the optimization spaces and computational efficiencies of three included compiler environments, and provide extensive empirical evaluations. Compared to prior works, CompilerGym offers larger datasets and optimization spaces, is 27X more computationally efficient, is fault-tolerant, and capable of detecting reproducibility bugs in the underlying compilers.In making it easy for anyone to experiment with compilers - irrespective of their background - we aim to accelerate progress in the AI and compiler research domains.},
booktitle = {Proceedings of the 20th IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {92–105},
numpages = {14},
location = {Virtual Event, Republic of Korea},
series = {CGO '22}
}

@article{10.1109/TASLP.2022.3190736,
author = {Ferrer, Luciana and Castan, Diego and McLaren, Mitchell and Lawson, Aaron},
title = {A Discriminative Hierarchical PLDA-Based Model for Spoken Language Recognition},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3190736},
doi = {10.1109/TASLP.2022.3190736},
abstract = {Spoken language recognition (SLR) refers to the automatic process used to determine the language present in a speech sample. SLR is an important task in its own right, for example, as a tool to analyze or categorize large amounts of multi-lingual data. Further, it is also an essential tool for selecting downstream applications in a work flow, for example, to chose appropriate speech recognition or machine translation models. SLR systems are usually composed of two stages, one where an embedding representing the audio sample is extracted and a second one which computes the final scores for each language. In this work, we approach the SLR task as a detection problem and implement the second stage as a probabilistic linear discriminant analysis (PLDA) model. We show that discriminative training of the PLDA parameters gives large gains with respect to the usual generative training. Further, we propose a novel hierarchical approach where two PLDA models are trained, one to generate scores for clusters of highly-related languages and a second one to generate scores conditional to each cluster. The final language detection scores are computed as a combination of these two sets of scores. The complete model is trained discriminatively to optimize a cross-entropy objective. We show that this hierarchical approach consistently outperforms the non-hierarchical one for detection of highly related languages, in many cases by large margins. We train our systems on a collection of datasets including over 100 languages, and test them both on matched and mismatched conditions, showing that the gains are robust to condition mismatch.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {2396–2410},
numpages = {15}
}

@inproceedings{10.1145/3549737.3549794,
author = {Sioros, Vasilis and Giannakopoulos, George and Constantoudis, Vassileios},
title = {Generating Realistic Nanorough Surfaces Using an N-Gram-Graph Augmented Deep Convolutional Generative Adversarial Network},
year = {2022},
isbn = {9781450395977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549737.3549794},
doi = {10.1145/3549737.3549794},
abstract = {Modeling and simulation of roughness generation and functionality can be aided by synthesized nanorough surfaces that mimic real experimental ones. We can save time and resources in optimizing the linkages in the process-surface-functionality triangle if the synthesized samples are generated in a computationally inexpensive manner. Existing nanorough surface generation techniques presuppose that the structural feature space to be employed in the generation process may be identified. In many circumstances, however, this assumption cannot be safely confirmed. As a result, a data-driven approach appears to be a viable option worth considering. Generating synthesized nanorough surfaces in the context of a multi-physics simulation requires (1) identifying the structural feature space so that the generation of new nanorough surfaces is possible and (2) the generation process to be property-preserving. In this work, we present methods for integrating new nanorough surfaces similar to a preset sample of surfaces into multi-physics simulations in a computationally inexpensive fashion. We look at how a Generative Adversarial Network (GAN)-based strategy, given a nanorough surface data set, may learn to produce nanorough surface samples that are statistically equivalent to the ones belonging to the training data set. We also look at how combining the GAN framework with a variety of nanorough similarity measures might improve the realisticity of the synthesized nanorough surfaces. We showcase via multiple experiments that our framework is able to produce sufficiently realistic nanorough surfaces, in many cases indistinguishable from real ones.},
booktitle = {Proceedings of the 12th Hellenic Conference on Artificial Intelligence},
articleno = {53},
numpages = {10},
keywords = {Artificial Intelligence, Graph Theory, Machine Learning, Nanotechnology, Rough Surfaces},
location = {Corfu, Greece},
series = {SETN '22}
}

@article{10.1145/3592450,
author = {Avrahami, Omri and Fried, Ohad and Lischinski, Dani},
title = {Blended Latent Diffusion},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3592450},
doi = {10.1145/3592450},
abstract = {The tremendous progress in neural image generation, coupled with the emergence of seemingly omnipotent vision-language models has finally enabled text-based interfaces for creating and editing images. Handling generic images requires a diverse underlying generative model, hence the latest works utilize diffusion models, which were shown to surpass GANs in terms of diversity. One major drawback of diffusion models, however, is their relatively slow inference time. In this paper, we present an accelerated solution to the task of local text-driven editing of generic images, where the desired edits are confined to a user-provided mask. Our solution leverages a text-to-image Latent Diffusion Model (LDM), which speeds up diffusion by operating in a lower-dimensional latent space and eliminating the need for resource-intensive CLIP gradient calculations at each diffusion step. We first enable LDM to perform local image edits by blending the latents at each step, similarly to Blended Diffusion. Next we propose an optimization-based solution for the inherent inability of LDM to accurately reconstruct images. Finally, we address the scenario of performing local edits using thin masks. We evaluate our method against the available baselines both qualitatively and quantitatively and demonstrate that in addition to being faster, it produces more precise results.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {149},
numpages = {11},
keywords = {zero-shot text-driven local image editing}
}

@inproceedings{10.1145/3502803.3502804,
author = {Li, Guangxu and Wang, Xin and Wang, Zhen and Wu, Jiaqi and Kamiya, Tohru},
title = {Compare Derived U-Nets Using For Retinal Vessels Segmentation},
year = {2022},
isbn = {9781450385817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502803.3502804},
doi = {10.1145/3502803.3502804},
abstract = {U-Net frameworks reveal potential on image segmentation tasks of the complex morphologic objects, such as capillaries. To compare the performance of vessels segmentation in fundus images, in this paper, we review U-Net and its three derived architectures: Residual Spatial Attention Network, Generative Adversarial Networks and IterNet. The networks training and testing were completed using the same image datasets under the same configurations. We calculated the accuracy and precision of segmentation results. The results showed that the cascade U-Net architecture provided better results especially on the capillaries parts.},
booktitle = {Proceedings of the 2021 6th International Conference on Biomedical Imaging, Signal Processing},
pages = {1–8},
numpages = {8},
keywords = {Computer Aided Diagnosis, Deep learning, Image segmentation},
location = {Xiamen, China},
series = {ICBSP '21}
}

@inproceedings{10.1145/3627377.3627380,
author = {Zhang, Fan and Zhang, Shun and Fang, Chun},
title = {CPPGAN-ESM: GAN-Enhanced ESM Prediction Model for Cell-Penetrating Peptide},
year = {2023},
isbn = {9798400707667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627377.3627380},
doi = {10.1145/3627377.3627380},
abstract = {Cell-penetrating peptide, with their ability to traverse cell membranes and access the intracellular environment, has garnered significant attention in contemporary medical research. Predicting cell-penetrating peptide can expand the therapeutic scope, improve the therapeutic effect, and promote the development of drug delivery, gene therapy, and other fields. Compared with the traditional wet-lab method, predicting cell-penetrating peptide by computational methods has the advantages of low cost and accuracy. However, due to the difficulty of collecting cell-penetrating peptide datasets and the performance limitation of traditional feature extractors, Predicting the performance of cell-penetrating peptide using traditional computational methods has limitations. To address these problems, a cell-penetrating peptide prediction method named CPPGAN-ESM is proposed in this paper. CPPGAN-ESM firstly augments the cell-penetrating peptide dataset by using Deep Convolutional Generative Adversarial Network, and then fine-tunes the predictor based on the Evolutionary Scale Modeling 2 pre-trained feature extractor with the augmented dataset, to realize the prediction of the cell-penetrating peptide. Extensive experiments on a public dataset show that the method achieves an efficient performance of 0.979 AUC and 0.977 ACC in the cell-penetrating peptide prediction task.},
booktitle = {Proceedings of the 2023 6th International Conference on Big Data Technologies},
pages = {17–21},
numpages = {5},
keywords = {DCGAN model, ESM2 model, cell-penetrating peptide prediction, data augmentation},
location = {Qingdao, China},
series = {ICBDT '23}
}

@article{10.1145/3491226,
author = {Fincato, Matteo and Cornia, Marcella and Landi, Federico and Cesari, Fabio and Cucchiara, Rita},
title = {Transform, Warp, and Dress: A New Transformation-guided Model for Virtual Try-on},
year = {2022},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1551-6857},
url = {https://doi.org/10.1145/3491226},
doi = {10.1145/3491226},
abstract = {Virtual try-on has recently emerged in computer vision and multimedia communities with the development of architectures that can generate realistic images of a target person wearing a custom garment. This research interest is motivated by the large role played by e-commerce and online shopping in our society. Indeed, the virtual try-on task can offer many opportunities to improve the efficiency of preparing fashion catalogs and to enhance the online user experience. The problem is far to be solved: current architectures do not reach sufficient accuracy with respect to manually generated images and can only be trained on image pairs with a limited variety. Existing virtual try-on datasets have two main limits: they contain only female models, and all the images are available only in low resolution. This not only affects the generalization capabilities of the trained architectures but makes the deployment to real applications impractical. To overcome these issues, we present Dress Code, a new dataset for virtual try-on that contains high-resolution images of a large variety of upper-body clothes and both male and female models. Leveraging this enriched dataset, we propose a new model for virtual try-on capable of generating high-quality and photo-realistic images using a three-stage pipeline. The first two stages perform two different geometric transformations to warp the desired garment and make it fit into the target person’s body pose and shape. Then, we generate the new image of that same person wearing the try-on garment using a generative network. We test the proposed solution on the most widely used dataset for this task as well as on our newly collected dataset and demonstrate its effectiveness when compared to current state-of-the-art methods. Through extensive analyses on our Dress Code dataset, we show the adaptability of our model, which can generate try-on images even with a higher resolution.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = feb,
articleno = {62},
numpages = {24},
keywords = {Virtual try-on, geometric transformations, generative adversarial networks}
}

@article{10.1145/3618373,
author = {Jiang, Hai and Luo, Ao and Fan, Haoqiang and Han, Songchen and Liu, Shuaicheng},
title = {Low-Light Image Enhancement with Wavelet-Based Diffusion Models},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3618373},
doi = {10.1145/3618373},
abstract = {Diffusion models have achieved promising results in image restoration tasks, yet suffer from time-consuming, excessive computational resource consumption, and unstable restoration. To address these issues, we propose a robust and efficient Diffusion-based Low-Light image enhancement approach, dubbed DiffLL. Specifically, we present a wavelet-based conditional diffusion model (WCDM) that leverages the generative power of diffusion models to produce results with satisfactory perceptual fidelity. Additionally, it also takes advantage of the strengths of wavelet transformation to greatly accelerate inference and reduce computational resource usage without sacrificing information. To avoid chaotic content and diversity, we perform both forward diffusion and denoising in the training phase of WCDM, enabling the model to achieve stable denoising and reduce randomness during inference. Moreover, we further design a high-frequency restoration module (HFRM) that utilizes the vertical and horizontal details of the image to complement the diagonal information for better fine-grained restoration. Extensive experiments on publicly available real-world benchmarks demonstrate that our method outperforms the existing state-of-the-art methods both quantitatively and visually, and it achieves remarkable improvements in efficiency compared to previous diffusion-based methods. In addition, we empirically show that the application for low-light face detection also reveals the latent practical values of our method. Code is available at https://github.com/JianghaiSCU/Diffusion-Low-Light.},
journal = {ACM Trans. Graph.},
month = dec,
articleno = {238},
numpages = {14},
keywords = {diffusion models, low-light image enhancement, wavelet transformation}
}


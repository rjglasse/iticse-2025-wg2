@inproceedings{10.1109/ASE56229.2023.00217,
  abstract   = {Bug report management has been shown to be an important and time consuming software maintenance task. Often, the first step in managing bug reports is related to triaging a bug to the appropriate developer who is best suited to understand, localize, and fix the target bug. Additionally, assigning a given bug to a particular part of a software project can help to expedite the fixing process. However, despite the importance of these activities, they are quite challenging, where days can be spent on the manual triaging process. Past studies have attempted to leverage the limited textual data of bug reports to train text classification models that automate this process - to varying degrees of success. However, the textual representations and machine learning models used in prior work are limited by their expressiveness, often failing to capture nuanced textual patterns that might otherwise aid in the triaging process. Recently, large, transformer-based, pre-tained neural text representation techniques (i.e., large language models or LLMs) such as BERT and CodeBERT have achieved greater performance with simplified training procedures in several natural language processing tasks, including text classification. However, the potential for using these techniques to improve upon prior approaches for automated bug triaging is not well studied or understood.Therefore, in this paper we offer one of the first investigations that fine-tunes transformer-based language models for the task of bug triaging on four open source datasets, spanning a collective 53 years of development history with over 400 developers and over 150 software project components. Our study includes both a quantitative and qualitative analysis of effectiveness. Our findings illustrate that DeBERTa is the most effective technique across the triaging tasks of developer and component assignment, and the measured performance delta is statistically significant compared to other techniques. However, through our qualitative analysis, we also observe that each technique possesses unique abilities best suited to certain types of bug reports.},
  author     = {Dipongkor, Atish Kumar and Moran, Kevin},
  booktitle  = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
  doi        = {10.1109/ASE56229.2023.00217},
  isbn       = {9798350329964},
  keywords   = {bug triaging, transformer, llms, text-embedding, DL4SE},
  location   = {Echternach, Luxembourg},
  numpages   = {12},
  pages      = {1012–1023},
  publisher  = {IEEE Press},
  series     = {ASE '23},
  title      = {A Comparative Study of Transformer-Based Neural Text Representation Techniques on Bug Triaging},
  url        = {https://doi.org/10.1109/ASE56229.2023.00217},
  year       = {2024}
}

@inproceedings{10.1145/3545945.3569785,
  abstract   = {Advances in natural language processing have resulted in large language models (LLMs) that can generate code and code explanations. In this paper, we report on our experiences generating multiple code explanation types using LLMs and integrating them into an interactive e-book on web software development. Three different types of explanations -- a line-by-line explanation, a list of important concepts, and a high-level summary of the code -- were created. Students could view explanations by clicking a button next to code snippets, which showed the explanation and asked about its utility. Our results show that all explanation types were viewed by students and that the majority of students perceived the code explanations as helpful to them. However, student engagement varied by code snippet complexity, explanation type, and code snippet length. Drawing on our experiences, we discuss future directions for integrating explanations generated by LLMs into CS classrooms.},
  address    = {New York, NY, USA},
  author     = {MacNeil, Stephen and Tran, Andrew and Hellas, Arto and Kim, Joanne and Sarsa, Sami and Denny, Paul and Bernstein, Seth and Leinonen, Juho},
  booktitle  = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
  doi        = {10.1145/3545945.3569785},
  isbn       = {9781450394314},
  location   = {Toronto ON, Canada},
  numpages   = {7},
  pages      = {931–937},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSE 2023},
  title      = {Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book},
  url        = {https://doi.org/10.1145/3545945.3569785},
  year       = {2023}
}

@inproceedings{10.1145/3576123.3576134,
  abstract   = {The introduction of OpenAI Codex sparked a surge of interest in the impact of generative AI models on computing education practices. Codex is also the underlying model for GitHub Copilot, a plugin which makes AI-generated code accessible to students through auto-completion in popular code editors. Research in this area, particularly on the educational implications, is nascent and has focused almost exclusively on introductory programming (or CS1) questions. Very recent work has shown that Codex performs considerably better on typical CS1 exam questions than most students. It is not clear, however, what Codex’s limits are with regard to more complex programming assignments and exams. In this paper, we present results detailing how Codex performs on more advanced CS2 (data structures and algorithms) exam questions taken from past exams. We compare these results to those of students who took the same exams under normal conditions, demonstrating that Codex outscores most students. We consider the implications of such tools for the future of undergraduate computing education.},
  address    = {New York, NY, USA},
  author     = {Finnie-Ansley, James and Denny, Paul and Luxton-Reilly, Andrew and Santos, Eddie Antonio and Prather, James and Becker, Brett A.},
  booktitle  = {Proceedings of the 25th Australasian Computing Education Conference},
  doi        = {10.1145/3576123.3576134},
  isbn       = {9781450399418},
  keywords   = {AI, AlphaCode, CS1, CS2, Codex, DeepMind, GPT-3, GitHub, OpenAI, academic integrity, algorithms, artificial intelligence, code generation, copilot, data structures, deep learning, introductory programming, machine learning, neural networks, novice programming},
  location   = {Melbourne, VIC, Australia},
  numpages   = {8},
  pages      = {97–104},
  publisher  = {Association for Computing Machinery},
  series     = {ACE '23},
  title      = {My AI Wants to Know if This Will Be on the Exam: Testing OpenAI’s Codex on CS2 Programming Exercises},
  url        = {https://doi.org/10.1145/3576123.3576134},
  year       = {2023}
}

@inproceedings{10.1145/3576882.3617921,
  abstract   = {Educators have been concerned about the capability of large language models to automatically generate programs in response to textual prompts. However, little is known about whether and how students actually use these tools.In the context of an upper-level formal methods course, we gave students access to large language models. They were told they could use the models freely. We built a Visual Studio Code extension to simplify access to these models. We also paid for an account so students could use the models for free without worrying about cost.In this experience report we analyze the outcomes. We see how students actually do and do not use the models. We codify the different uses they make. Most of all, we notice that students actually do not use them very much at all, and provide insight into the many reasons why not. We believe such experiments can help rebalance some of the public narrative about such tools.},
  address    = {New York, NY, USA},
  author     = {Prasad, Siddhartha and Greenman, Ben and Nelson, Tim and Krishnamurthi, Shriram},
  booktitle  = {Proceedings of the ACM Conference on Global Computing Education Vol 1},
  doi        = {10.1145/3576882.3617921},
  isbn       = {9798400700484},
  keywords   = {formal methods, large language models, properties, testing},
  location   = {Hyderabad, India},
  numpages   = {7},
  pages      = {126–132},
  publisher  = {Association for Computing Machinery},
  series     = {CompEd 2023},
  title      = {Generating Programs Trivially: Student Use of Large Language Models},
  url        = {https://doi.org/10.1145/3576882.3617921},
  year       = {2023}
}

@inproceedings{10.1145/3580305.3599827,
  abstract   = {A final exam in machine learning at a top institution such as MIT, Harvard, or Cornell typically takes faculty days to write, and students hours to solve. We demonstrate that large language models pass machine learning finals at a human level on finals available online and automatically generate new human-quality final exam questions in seconds. Previous work has developed program synthesis and few-shot learning methods to solve university-level problem set questions in mathematics and STEM courses. In this work, we develop and compare methods that solve final exams, which differ from problem sets in several ways: the questions are longer, have multiple parts, are more complicated, and span a broader set of topics. We curate a dataset and benchmark of questions from machine learning final exams available online and code for answering these questions and generating new questions. We show how to generate new questions from other questions and course notes. For reproducibility and future research on this final exam benchmark, we use automatic checkers for multiple-choice, numeric, and questions with expression answers. A student survey comparing the quality, appropriateness, and difficulty of machine-generated questions with human-written questions shows that across multiple aspects, machine-generated questions are indistinguishable from human-generated questions and are suitable for final exams. We perform ablation studies comparing zero-shot learning with few-shot learning and chain-of-thought prompting using GPT-3, OPT, Codex, and ChatGPT across machine learning topics and find that few-shot learning methods perform best. We highlight the transformative potential of language models to streamline the writing and solution of large-scale assessments, significantly reducing the workload from human days to mere machine seconds. Our results suggest that rather than banning large language models such as ChatGPT in class, instructors should teach students to harness them by asking students meta-questions about correctness, completeness, and originality of the responses generated, encouraging critical thinking in academic studies.},
  address    = {New York, NY, USA},
  author     = {Drori, Iddo and Zhang, Sarah J. and Shuttleworth, Reece and Zhang, Sarah and Tyser, Keith and Chin, Zad and Lantigua, Pedro and Surbehera, Saisamrit and Hunter, Gregory and Austin, Derek and Tang, Leonard and Hicke, Yann and Simhon, Sage and Karnik, Sathwik and Granberry, Darnell and Udell, Madeleine},
  booktitle  = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  doi        = {10.1145/3580305.3599827},
  isbn       = {9798400701030},
  keywords   = {few-shot learning, large language models, machine learning, program synthesis, quantitative reasoning},
  location   = {Long Beach, CA, USA},
  numpages   = {9},
  pages      = {3947–3955},
  publisher  = {Association for Computing Machinery},
  series     = {KDD '23},
  title      = {From Human Days to Machine Seconds: Automatically Answering and Generating Machine Learning Final Exams},
  url        = {https://doi.org/10.1145/3580305.3599827},
  year       = {2023}
}

@inproceedings{10.1145/3585059.3611431,
  abstract   = {ChatGPT, an implementation and application of large language models, has gained significant popularity since its initial release. Researchers have been exploring ways to harness the practical benefits of ChatGPT in real-world scenarios. Educational researchers have investigated its potential in various subjects, e.g., programming, mathematics, finance, clinical decision support, etc. However, there has been limited attention given to its application in data science education. This paper aims to bridge that gap by utilizing ChatGPT in a data science course, gathering perspectives from students, and presenting our experiences and feedback on using ChatGPT for teaching and learning in data science education. The findings not only distinguish data science education from other disciplines but also uncover new opportunities and challenges associated with incorporating ChatGPT into the data science curriculum.},
  address    = {New York, NY, USA},
  author     = {Zheng, Yong},
  booktitle  = {Proceedings of the 24th Annual Conference on Information Technology Education},
  doi        = {10.1145/3585059.3611431},
  isbn       = {9798400701306},
  keywords   = {ChatGPT, data analytics, data science, large language model},
  location   = {Marietta, GA, USA},
  numpages   = {7},
  pages      = {66–72},
  publisher  = {Association for Computing Machinery},
  series     = {SIGITE '23},
  title      = {ChatGPT for Teaching and Learning: An Experience from Data Science Education},
  url        = {https://doi.org/10.1145/3585059.3611431},
  year       = {2023}
}

@inproceedings{10.1145/3587102.3588814,
  abstract   = {Recent studies show that AI-driven code generation tools, such as Large Language Models, are able to solve most of the problems usually presented in introductory programming classes. However, it is still unknown how they cope with Object Oriented Programming assignments, where the students are asked to design and implement several interrelated classes (either by composition or inheritance) that follow a set of best-practices. Since the majority of the exercises in these tools' training dataset are written in English, it is also unclear how well they function with exercises published in other languages.In this paper, we report our experience using GPT-3 to solve 6 real-world tasks used in an Object Oriented Programming course at a Portuguese University and written in Portuguese. Our observations, based on an objective evaluation of the code, performed by an open-source Automatic Assessment Tool, show that GPT-3 is able to interpret and handle direct functional requirements, however it tends not to give the best solution in terms of object oriented design. We perform a qualitative analysis of GPT-3's output, and gather a set of recommendations for computer science educators, since we expect students to use and abuse this tool in their academic work.},
  address    = {New York, NY, USA},
  author     = {Cipriano, Bruno Pereira and Alves, Pedro},
  booktitle  = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
  doi        = {10.1145/3587102.3588814},
  isbn       = {9798400701382},
  keywords   = {GPT-3, large language models, object oriented programming, programming assignments, teaching},
  location   = {Turku, Finland},
  numpages   = {7},
  pages      = {61–67},
  publisher  = {Association for Computing Machinery},
  series     = {ITiCSE 2023},
  title      = {GPT-3 vs Object Oriented Programming Assignments: An Experience Report},
  url        = {https://doi.org/10.1145/3587102.3588814},
  year       = {2023}
}

@inproceedings{10.1145/3587102.3588815,
  abstract   = {This position paper discusses the potential for using generative AIs like ChatGPT in software engineering education. Currently, discussions center around potential threats emerging from student's use of ChatGPT. For instance, generative AI will limit the usefulness of graded homework dramatically. However, there exist potential opportunities as well. For example, ChatGPT's ability to understand and generate human language allows providing personalized feedback to students, and can thus accompany current software engineering education approaches. This paper highlights the potential for enhancing software engineering education. The availability of generative AI will improve the individualization of education approaches. In addition, we discuss the need to adapt software engineering curricula to the changed profiles of software engineers. Moreover, we point out why it is important to provide guidance for using generative AI and, thus, integrate it in courses rather than accepting the unsupervised use by students, which can negatively impact the students' learning.},
  address    = {New York, NY, USA},
  author     = {Daun, Marian and Brings, Jennifer},
  booktitle  = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
  doi        = {10.1145/3587102.3588815},
  isbn       = {9798400701382},
  keywords   = {ChatGPT, generative AI, software engineering education},
  location   = {Turku, Finland},
  numpages   = {7},
  pages      = {110–116},
  publisher  = {Association for Computing Machinery},
  series     = {ITiCSE 2023},
  title      = {How ChatGPT Will Change Software Engineering Education},
  url        = {https://doi.org/10.1145/3587102.3588815},
  year       = {2023}
}

@inproceedings{10.1145/3596671.3598574,
  abstract   = {This paper presents findings from an exploratory study investigating the use of AI text-generation tools to support novice designers in persona creation. We conducted a workshop with 22 undergraduate students enrolled in an introductory human-computer interaction course, who were instructed to use GPT-3 in the creation of personas. These novice designers were able to use GPT-3 to iterate to produce satisfactory personas, particularly when providing detailed prompts. Our findings suggest that personas created with GPT-3 assistance were mostly comparable to those created manually but rated lower on some evaluation dimensions. The study also reveals merits and concerns of using GPT-3 for persona creation. Based on our findings, we propose recommendations for novice designers on how to use text-generative AIs to create personas effectively and responsibly.},
  address    = {New York, NY, USA},
  articleno  = {4},
  author     = {Goel, Toshali and Shaer, Orit and Delcourt, Catherine and Gu, Quan and Cooper, Angel},
  booktitle  = {Proceedings of the 2nd Annual Meeting of the Symposium on Human-Computer Interaction for Work},
  doi        = {10.1145/3596671.3598574},
  isbn       = {9798400708077},
  keywords   = {education, human-AI collaboration, large language models, natural-language generation, novice designers, personas},
  location   = {Oldenburg, Germany},
  numpages   = {14},
  publisher  = {Association for Computing Machinery},
  series     = {CHIWORK '23},
  title      = {Preparing Future Designers for Human-AI Collaboration in Persona Creation},
  url        = {https://doi.org/10.1145/3596671.3598574},
  year       = {2023}
}

@inproceedings{10.1145/3597503.3639201,
  abstract   = {Conversational Generative AI (convo-genAI) is revolutionizing Software Engineering (SE) as engineers and academics embrace this technology in their work. However, there is a gap in understanding the current potential and pitfalls of this technology, specifically in supporting students in SE tasks. In this work, we evaluate through a between-subjects study (N=22) the effectiveness of ChatGPT, a convo-genAI platform, in assisting students in SE tasks. Our study did not find statistical differences in participants' productivity or self-efficacy when using ChatGPT as compared to traditional resources, but we found significantly increased frustration levels. Our study also revealed 5 distinct faults arising from violations of Human-AI interaction guidelines, which led to 7 different (negative) consequences on participants.},
  address    = {New York, NY, USA},
  articleno  = {184},
  author     = {Choudhuri, Rudrajit and Liu, Dylan and Steinmacher, Igor and Gerosa, Marco and Sarma, Anita},
  booktitle  = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
  doi        = {10.1145/3597503.3639201},
  isbn       = {9798400702174},
  keywords   = {empirical study, software engineering, generative AI, ChatGPT},
  location   = {Lisbon, Portugal},
  numpages   = {13},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE '24},
  title      = {How Far Are We? The Triumphs and Trials of Generative AI in Learning Software Engineering},
  url        = {https://doi.org/10.1145/3597503.3639201},
  year       = {2024}
}

@inproceedings{10.1145/3605507.3610629,
  abstract   = {The traditional model of assigning textbook problems for homework is endangered by the ability of students to find answers to almost any published problem on the web. An alternative is a dual-submission approach, where students submit their work, then receive the solutions, and submit a second metacognitive reflection, explaining any errors they made. Students’ scores can depend on the quality of their second submissions alone or the combined quality of their first and second submissions. We tried this approach in a class on parallel computer architecture. We report students’ personal experience based on their questionnaires responses. In addition, we quantitatively compare students’ performance on test questions related to dual-submission homework against their performance on other questions and previous semesters’ student performance on similar questions. Students overwhelmingly preferred this approach and thought they learned more from it, but evidence about whether it improved their learning was inconclusive. We also analyze the continued viability of this approach in the era of large language models.},
  address    = {New York, NY, USA},
  author     = {Gehringer, Edward F. and Wang, Jianxun George and Jilla, Sharan Kumar},
  booktitle  = {Proceedings of the Workshop on Computer Architecture Education},
  doi        = {10.1145/3605507.3610629},
  isbn       = {9798400702532},
  location   = {Orlando, FL, USA},
  numpages   = {7},
  pages      = {41–47},
  publisher  = {Association for Computing Machinery},
  series     = {WCAE '23},
  title      = {Dual-Submission Homework in Parallel Computer Architecture: An Exploratory Study in the Age of LLMs},
  url        = {https://doi.org/10.1145/3605507.3610629},
  year       = {2024}
}

@inproceedings{10.1145/3613372.3614189,
  abstract   = {ChatGPT is a natural language model that works as a virtual chat assistant. It has the potential to be used for fostering classroom discussions and addressing student needs when the professor is not accessible. Although it is still early to assess the impact of ChatGPT and similar technologies, there is a considerable discussion on social media and blogs regarding the aspirations and opportunities of utilizing ChatGPT in the software industry and education. The main perception is that ChatGPT can serve as a support tool but should not completely replace interpersonal interaction, as face-to-face dialogue remains crucial for the development of interpersonal skills and a deeper understanding of concepts. This article reports a recent classroom experience in the subjects of Software Engineering and Systems Analysis, while also analyzing ChatGPT’s responses to student inquiries.},
  address    = {New York, NY, USA},
  author     = {Albonico, Michel and Varela, Paulo J\'{u}nior},
  booktitle  = {Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
  doi        = {10.1145/3613372.3614189},
  isbn       = {9798400707872},
  keywords   = {ChatGPT, Software Engineering, Student Support, System Analysis},
  location   = {Campo Grande, Brazil},
  numpages   = {9},
  pages      = {303–311},
  publisher  = {Association for Computing Machinery},
  series     = {SBES '23},
  title      = {A Report on the Use of ChatGPT in Software Engineering and Systems Analysis Courses},
  url        = {https://doi.org/10.1145/3613372.3614189},
  year       = {2023}
}

@inproceedings{10.1145/3613905.3651043,
  abstract   = {Personas are commonly used in User-centered Design (UCD) activities to help designers better understand users’ needs. However, there is still a reliance on traditional approaches such as interviews and ethnography for building personas in UCD activities. To this end, we developed an auto-generating persona system to enhance practices in UCD course activities. Our persona system is developed based on the GPT-4 model, the DALL-E 2 model, and knowledge graphs. Hence, our persona system includes three main features of our persona system: automated processing of survey data, automatic generation of 2D avatars, and providing options for automatic or customized entity generation. We recruited a total of 22 participants to evaluate our persona system. Our findings confirmed that there was a significant difference in terms of efficiency, satisfaction, accuracy, and diversity. Meanwhile, participants provided both positive and negative feedback regarding our persona system. As on-going work, we discuss the current limitations of our persona system and explore future research directions to further improve its capabilities and effectiveness.},
  address    = {New York, NY, USA},
  articleno  = {52},
  author     = {Zhang, Xishuo and Liu, Lin and Wang, Yi and Liu, Xiao and Wang, Hailong and Arora, Chetan and Liu, Haichao and Wang, Weijia and Hoang, Thuong},
  booktitle  = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
  doi        = {10.1145/3613905.3651043},
  isbn       = {9798400703317},
  keywords   = {GPT-4, Knowledge Graphs, Personas, User Studies},
  location   = {Honolulu, HI, USA},
  numpages   = {7},
  publisher  = {Association for Computing Machinery},
  series     = {CHI EA '24},
  title      = {Auto-Generated Personas: Enhancing User-centered Design Practices among University Students},
  url        = {https://doi.org/10.1145/3613905.3651043},
  year       = {2024}
}

@inproceedings{10.1145/3615335.3623035,
  abstract   = {The advent of widely-accessible generative AI tools and their rapid adoption across industry and education is necessitating large-scale revisions to user experience design and web development pedagogies and curricula, a process that will take some time. This report describes a series of initial experiments using generative AI tools as a student or junior designer or web developer might, sometimes na\"{\i}vely and sometimes in more sophisticated ways, to complete beginner-level and advanced projects. The report evaluates how ChatGPT performs across three categories of prompts (brainstorming, design, and coding) and assesses the quality of the outputs in order to inform the research design of a larger, ongoing interdisciplinary study in its initial phases and to document the results for instructors or senior members of design and development teams to aid them in assessing the fitness of generative AI for user experience design and web development production.},
  address    = {New York, NY, USA},
  author     = {York, Eric},
  booktitle  = {Proceedings of the 41st ACM International Conference on Design of Communication},
  doi        = {10.1145/3615335.3623035},
  isbn       = {9798400703362},
  keywords   = {Artificial Intelligence, Pedagogy, User experience (UX) design, Web development},
  location   = {Orlando, FL, USA},
  numpages   = {5},
  pages      = {197–201},
  publisher  = {Association for Computing Machinery},
  series     = {SIGDOC '23},
  title      = {Evaluating ChatGPT: Generative AI in UX Design and Web Development Pedagogy},
  url        = {https://doi.org/10.1145/3615335.3623035},
  year       = {2023}
}

@inproceedings{10.1145/3626252.3630832,
  abstract   = {In this experience paper, we introduce the concept of 'diverging assessments', process-based assessments designed so that they become unique for each student while all students see a common skeleton. We present experiences with diverging assessments in the contexts of computer networks, operating systems, ethical hacking, and software development. All the given examples allow the use of generative-AI-based tools, are authentic, and are designed to generate learning opportunities that foster students' meta-cognition. Finally, we reflect upon these experiences in five different courses across four universities, showing how diverging assessments enhance students' learning while respecting academic integrity.},
  address    = {New York, NY, USA},
  author     = {Sakzad, Amin and Paul, David and Sheard, Judithe and Brankovic, Ljiljana and Skerritt, Matthew P. and Li, Nan and Minagar, Sepehr and Simon and Billingsley, William},
  booktitle  = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
  doi        = {10.1145/3626252.3630832},
  isbn       = {9798400704239},
  keywords   = {assessment-as-learning, authentic assessment, diverging assessment},
  location   = {Portland, OR, USA},
  numpages   = {7},
  pages      = {1161–1167},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSE 2024},
  title      = {Diverging assessments: What, Why, and Experiences},
  url        = {https://doi.org/10.1145/3626252.3630832},
  year       = {2024}
}

@inproceedings{10.1145/3626252.3630854,
  abstract   = {StandUp Reports play an important role in capstone software engineering courses, facilitating progress tracking, obstacle identification, and team collaboration. However, despite their significance, students often grapple with the challenge of creating StandUp Reports that are clear, concise, and actionable. This paper investigates the impact of the use of generative AI in producing StandUp report recommendations, aiming to assist students in enhancing the quality and effectiveness of their reports. In a semester-long capstone course, 179 students participated in 16 real-world software development projects. They submitted weekly StandUp Reports with the assistance of an AI-powered Slack, which analyzed their initial reports and provided suggestions for enhancing them using both GPT-3.5 and the early access GPT-4 API. After each submitted report, students voluntarily answered a survey about usability and suggestion preference. Furthermore, we conducted a linguistic analysis of the recommendations made by the algorithms to gauge reading ease and comprehension complexity. Our findings indicate that the AI-based recommendation system helped students improve the overall quality of their StandUp Reports throughout the semester. Students expressed a high level of satisfaction with the tool and exhibited a strong willingness to continue using it in the future. The survey reveals that students perceived a slight improvement when using GPT-4 compared to GPT-3.5. Finally, a computational linguistic analysis performed on the recommendations demonstrates that both algorithms significantly improve the alignment between the generated texts and the students' educational level, thereby improving the quality of the original texts.},
  address    = {New York, NY, USA},
  author     = {Neyem, Andres and Sandoval Alcocer, Juan Pablo and Mendoza, Marcelo and Centellas-Claros, Leonardo and Gonzalez, Luis A. and Paredes-Robles, Carlos},
  booktitle  = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
  doi        = {10.1145/3626252.3630854},
  isbn       = {9798400704239},
  keywords   = {capstone courses, chatgpt, generative ai, large language models, software engineering education},
  location   = {Portland, OR, USA},
  numpages   = {7},
  pages      = {951–957},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSE 2024},
  title      = {Exploring the Impact of Generative AI for StandUp Report Recommendations in Software Capstone Project Development},
  url        = {https://doi.org/10.1145/3626252.3630854},
  year       = {2024}
}

@inproceedings{10.1145/3626252.3630874,
  abstract   = {ChatGPT is a conversational AI platform that can produce code to solve problems when provided with a natural language prompt. Prior work on similar AI models has shown that they perform well on typical intro-level Computer Science problems. However, little is known about the performance of such tools on Data Science (DS) problems. In this work, we assess the performance of ChatGPT on assignments from three DS courses with varying difficulty levels. First, we apply the raw assignment prompts provided to the students and find that ChatGPT performs well on assignments with dataset(s) descriptions and progressive question prompts, which divide the programming requirements into sub-problems. Then, we perform prompt engineering on the assignments for which ChatGPT had low performance. We find that the following prompt engineering techniques significantly increased ChatGPT's performance: breaking down abstract questions into steps, breaking down steps into multiple prompts, providing descriptions of the dataset(s), including algorithmic details, adding specific instructions to entice specific actions, and removing extraneous information. Finally, we discuss how our findings suggest potential changes to curriculum design of DS courses.},
  address    = {New York, NY, USA},
  author     = {Shen, Yiyin and Ai, Xinyi and Soosai Raj, Adalbert Gerald and Leo John, Rogers Jeffrey and Syamkumar, Meenakshi},
  booktitle  = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
  doi        = {10.1145/3626252.3630874},
  isbn       = {9798400704239},
  keywords   = {data science education, large language models, prompt engineering},
  location   = {Portland, OR, USA},
  numpages   = {7},
  pages      = {1230–1236},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSE 2024},
  title      = {Implications of ChatGPT for Data Science Education},
  url        = {https://doi.org/10.1145/3626252.3630874},
  year       = {2024}
}

@inproceedings{10.1145/3626252.3630927,
  abstract   = {In the era of artificial intelligence (AI), generative AI, and Large Language Models (LLMs) in particular, have become increasingly significant in various sectors. LLMs such as GPT expand their applications, from content creation to advanced code completion. They offer unmatched opportunities but pose unique challenges to the software engineering domain. This paper discusses the necessity and urgency for software engineering education to adapt and evolve to prepare software engineers for the emerging LLM environment. While existing literature and social media have investigated AI's integration into various educational spheres, there is a conspicuous gap in examining the specifics of LLMs' implications for software engineering education. We explore the goals of software engineering education, and changes to software engineering, software engineering education, course pedagogy, and ethics. We argue that a holistic approach is needed, combining technical skills, ethical awareness, and adaptable learning strategies. This paper seeks to contribute to the ongoing conversation about the future of software engineering education, emphasizing the importance of adapting and evolving to remain in sync with rapid advancements in AI and LLMs. It is hoped that this exploration will provide valuable insights for educators, curriculum developers, and policymakers in software engineering.},
  address    = {New York, NY, USA},
  author     = {Kirova, Vassilka D. and Ku, Cyril S. and Laracy, Joseph R. and Marlowe, Thomas J.},
  booktitle  = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
  doi        = {10.1145/3626252.3630927},
  isbn       = {9798400704239},
  keywords   = {chatgpt, generative ai, large language models (llms), responsible ai, software engineering, software engineering education, software engineering ethics, software ethics},
  location   = {Portland, OR, USA},
  numpages   = {7},
  pages      = {666–672},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSE 2024},
  title      = {Software Engineering Education Must Adapt and Evolve for an LLM Environment},
  url        = {https://doi.org/10.1145/3626252.3630927},
  year       = {2024}
}

@inproceedings{10.1145/3626252.3630928,
  abstract   = {Large language models (LLMs) have recently taken many fields, including computer science, by storm. Most recent work on LLMs in computing education has shown that they are capable of solving most introductory programming (CS1) exercises, exam questions, Parsons problems, and several other types of exercises and questions. Some work has investigated the ability of LLMs to solve CS2 problems as well. However, it remains unclear how well LLMs fare against more advanced upper-division coursework, such as proofs in algorithms courses. After all, while known to be proficient in many programming tasks, LLMs have been shown to have more difficulties in forming mathematical proofs.In this paper, we investigate the ability of LLMs to solve mathematical proofs by using Proof Blocks, a tool previously shown to efficaciously teach proofs to students. Our results show that GPT-3.5 is almost completely unable to provide correct solutions (11.4%), while GPT-4 shows a significant increase in correctness (64.8%). However, even given this improvement, current models still struggle to correctly order lines in a proof. It remains an open question whether this is a temporary situation or if LLMs will continue to struggle to solve these types of exercises in the future.},
  address    = {New York, NY, USA},
  author     = {Poulsen, Seth and Sarsa, Sami and Prather, James and Leinonen, Juho and Becker, Brett A. and Hellas, Arto and Denny, Paul and Reeves, Brent N.},
  booktitle  = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
  doi        = {10.1145/3626252.3630928},
  isbn       = {9798400704239},
  keywords   = {ai, algorithms, artificial intelligence, chatgpt, code generation, generative ai, gpt-3, gpt-4, large language models, openai, proof blocks, proofs},
  location   = {Portland, OR, USA},
  numpages   = {7},
  pages      = {1063–1069},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSE 2024},
  title      = {Solving Proof Block Problems Using Large Language Models},
  url        = {https://doi.org/10.1145/3626252.3630928},
  year       = {2024}
}

@inproceedings{10.1145/3626253.3635408,
  abstract   = {SQL stands as the foundational language for data analysis and manipulation, playing a pivotal role in the database learning process. Proficiency in SQL is essential for students seeking to excel in data-related fields. However, the conventional approaches to assessing SQL queries rely heavily on manual grading, and the automated assessment tools are usually producing only binary decisions for the submitted queries. Our primary research objective is to develop effective methods for evaluating the quality of the SQL queries. To meet this objective, we introduce two approaches: structure-based analysis and evaluation by an instruction tuned large language model (LLM). The first approach deconstructs queries into Abstract Syntax Trees (AST) and employs cosine similarity to assess student submissions. The second approach utilizes a pre-trained LLM: FLAN-T5, fine-tuned for predicting the quality of student submissions. These methodologies are tested on a SQL dataset, and our experimental findings evaluate against a grading rubric with categories ranging from "good" to "unacceptable". The experimental results demonstrate that we can enhance the grading efficiency by applying these approaches and illustrate the ability of utilizing LLM to classify the assessed SQL statements more accurately. In addition, this research contributes to Computer Science (CS) education by integrating these approaches into our team's automated SQL statement assessment tool, improving the learning experience and evaluation process.},
  address    = {New York, NY, USA},
  author     = {Xiang, Lili},
  booktitle  = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
  doi        = {10.1145/3626253.3635408},
  isbn       = {9798400704246},
  keywords   = {abstract syntax trees, auto-grader, cs education, large language model, sql},
  location   = {Portland, OR, USA},
  numpages   = {1},
  pages      = {1890},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSE 2024},
  title      = {SQL Query Evaluation with Large Language Model and Abstract Syntax Trees},
  url        = {https://doi.org/10.1145/3626253.3635408},
  year       = {2024}
}

@inproceedings{10.1145/3626253.3635609,
  abstract   = {Recent advancements in instruction-tuned large language models offer new potential for enhancing students' experiences in large-scale classes. Deploying LLMs as student-facing assistants, however, presents challenges. Key issues include integrating class-specific content into responses and applying effective pedagogical techniques. This study addresses these challenges through retrieval and prompting techniques, focusing on mitigating hallucinations in LLM-generated responses, a crucial concern in education. Furthermore, practical deployment brings further challenges related to student data privacy and computational constraints. This research strives to enhance the quality and relevance of LLM responses while addressing practical deployment issues, with an emphasis on creating a versatile system for diverse domains and teaching styles.},
  address    = {New York, NY, USA},
  author     = {Mitra, Chancharik and Miroyan, Mihran and Jain, Rishi and Kumud, Vedant and Ranade, Gireeja and Norouzi, Narges},
  booktitle  = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
  doi        = {10.1145/3626253.3635609},
  isbn       = {9798400704246},
  keywords   = {discussion forum, educational tools, natural language processing},
  location   = {Portland, OR, USA},
  numpages   = {2},
  pages      = {1752–1753},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSE 2024},
  title      = {Elevating Learning Experiences: Leveraging Large Language Models as Student-Facing Assistants in Discussion Forums},
  url        = {https://doi.org/10.1145/3626253.3635609},
  year       = {2024}
}

@inproceedings{10.1145/3632621.3671424,
  abstract   = {Background. Concurrent and parallel programming is difficult to teach and learn as the understanding of complex and abstract concepts such as nondeterminism, semaphore, and rare conditions, among others, is required [1, 2, 9], having as a core issue the synchronisation of processes to achieve a common goal [4]. It is well-acknowledged that concurrent and parallel programming skills are fundamental since, nowadays, computing is increasingly handled in a parallel manner [7].Problem and Motivation. Therefore, identifying students’ pitfalls and successes when solving practical concurrent and parallel programming exercises could shed light on the best approaches and strategies that they use [3]. In addition, the advent of large language models, and generative AI applications such as ChatGPT, has prompted intensive research on their use in several areas including programming teaching and learning [8]. Yet, the studies in the literature have focused on issues related to learning to program by novice students in introductory courses (e.g., CS1, CS2) [6]. Less work, however, has been presented on the impact of generative AI tools in advanced programming practices such as concurrent and parallel programming.Methodology. To investigate whether generative AI has had an impact on the submitted concurrent and parallel programming exercises solutions at the University of Aizu, Japan, we performed a comparison analysis of the students’ submissions over 2020–2023. The analysis included five different exercises covering the basis of concurrency through various tasks and scenarios where the implementation of parallel processes is needed as solution. For instance, exercises 2.3 and 2.4 required to create parallel processes and perform independent computations; exercises 3.2 and 3.3, required synchronisation of the parallel processes; and in exercise 3.5 a code template was given for modification. We analysed the submissions of 72 undergraduate 3rd year students (avg. 18 students/year) and labelled the solutions using the following nomenclature: OK, indicating a good solution; OKFeat, a good solution but with unusual features; AdvLib, use of unnecessary advanced library or functionality; BadTool, use of an inappropriate tool when the task definition explicitly required a different tool; CodeErr, general coding error; SyncErr, concurrent programming specific error; N/A, solution not submitted or incomplete.Results and Analysis. Results show a substantial increase in the incidence of use of advance libraries (AdvLib) and the wrong tools (BadTool) among students in 2023 for three out of the five analysed exercises. At the same time the concurrency programming-specific errors (SyncErr) also see a reduction in all the exercises. (Figure 1). This coincides with the availability of generative AI tools such as ChatGPT [5], which warrants further investigations to understand how students, teachers and instructors could harness the affordances of large language models in their concurrent programming learning, teaching, and practice.Contribution and Impact. This paper presents an initial step towards investigating the impact of generative AI on advanced programming topics. This research will continue to uncover strategies for the lecturers and instructors to identify the affordances and use of generative AI and to design exercises that harness these affordances to support students learning of difficult programming concepts.},
  address    = {New York, NY, USA},
  author     = {Mozgovoy, Maxim and Suero Montero, Calkin},
  booktitle  = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 2},
  doi        = {10.1145/3632621.3671424},
  isbn       = {9798400704765},
  keywords   = {Evaluation of students’ exercises, Large language models in advanced programming},
  location   = {Melbourne, VIC, Australia},
  numpages   = {2},
  pages      = {533–534},
  publisher  = {Association for Computing Machinery},
  series     = {ICER '24},
  title      = {Exploring Students Solutions to Concurrent and Parallel Programming Exercises – Impact of Generative AI},
  url        = {https://doi.org/10.1145/3632621.3671424},
  year       = {2024}
}

@inproceedings{10.1145/3633053.3633057,
  abstract   = {This paper explores how Generative AI can be incorporated into software development education. We present examples of formative and summative assessments, which explore various aspects of ChatGPT, including its coding capabilities, its ability to construct arguments as well as ethical issues of using ChatGPT and similar tools in education and the workplace. Our work is inspired by the insights from surveys that show that the learners on our Degree Apprenticeship Programme have a great interest in learning about and exploiting emerging AI technology. Similarly, our industrial partners have a clear interest for their employees to be formally prepared to use GenAI in their software engineering roles. In this vein, it is proposed that embedding the use of GenAI tools in a careful and creative way - by developing assessments which encourage learners to critically evaluate AI output - can be beneficial in helping learners understand the subject material being taught without the risk of the AI tools “doing the homework”.},
  address    = {New York, NY, USA},
  author     = {Petrovska, Olga and Clift, Lee and Moller, Faron and Pearsall, Rebecca},
  booktitle  = {Proceedings of the 8th Conference on Computing Education Practice},
  doi        = {10.1145/3633053.3633057},
  isbn       = {9798400709326},
  keywords   = {apprenticeship, assessment, education, generative AI, software engineering},
  location   = {Durham, United Kingdom},
  numpages   = {4},
  pages      = {37–40},
  publisher  = {Association for Computing Machinery},
  series     = {CEP '24},
  title      = {Incorporating Generative AI into Software Development Education},
  url        = {https://doi.org/10.1145/3633053.3633057},
  year       = {2024}
}

@inproceedings{10.1145/3636243.3636256,
  abstract   = {There is a constant need for educators to develop and maintain effective up-to-date assessments. While there is a growing body of research in computing education on utilizing large language models&nbsp;(LLMs) in generation and engagement with coding exercises, the use of LLMs for generating programming MCQs has not been extensively explored. We analyzed the capability of GPT-4 to produce multiple-choice questions (MCQs) aligned with specific learning objectives (LOs) from Python programming classes in higher education. Specifically, we developed an LLM-powered (GPT-4) system for generation of MCQs from high-level course context and module-level LOs. We evaluated 651 LLM-generated and 449 human-crafted MCQs aligned to 246 LOs from 6 Python courses. We found that GPT-4 was capable of producing MCQs with clear language, a single correct choice, and high-quality distractors. We also observed that the generated MCQs appeared to be well-aligned with the LOs. Our findings can be leveraged by educators wishing to take advantage of the state-of-the-art generative models to support MCQ authoring efforts.},
  address    = {New York, NY, USA},
  author     = {Doughty, Jacob and Wan, Zipiao and Bompelli, Anishka and Qayum, Jubahed and Wang, Taozhi and Zhang, Juran and Zheng, Yujia and Doyle, Aidan and Sridhar, Pragnya and Agarwal, Arav and Bogart, Christopher and Keylor, Eric and Kultur, Can and Savelka, Jaromir and Sakr, Majd},
  booktitle  = {Proceedings of the 26th Australasian Computing Education Conference},
  doi        = {10.1145/3636243.3636256},
  isbn       = {9798400716195},
  keywords   = {Assessments, Automated Content Generation, Automatic Generation, GPT-4, LLMs, LOs, Large Language Models, Learning Objectives, MCQs, Multiple-choice Questions},
  location   = {Sydney, NSW, Australia},
  numpages   = {10},
  pages      = {114–123},
  publisher  = {Association for Computing Machinery},
  series     = {ACE '24},
  title      = {A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education},
  url        = {https://doi.org/10.1145/3636243.3636256},
  year       = {2024}
}

@inproceedings{10.1145/3636243.3636263,
  abstract   = {Recent studies have showcased the exceptional performance of LLMs (Large Language Models) on assessment questions across various discipline areas. This can be helpful if used to support the learning process, for example by enabling students to quickly generate and contrast alternative solution approaches. However, concerns about student over-reliance and inappropriate use of LLMs in education are common. Understanding the capabilities of LLMs is essential for instructors to make informed decisions on question choices for learning and assessment tasks. In CS (Computer Science), previous evaluations of LLMs have focused on CS1 and CS2 questions, and little is known about how well LLMs perform for assessment questions in upper-level CS courses such as CG (Computer Graphics), which covers a wide variety of concepts and question types. To address this gap, we compiled a dataset of past assessment questions used in a final-year undergraduate course about introductory CG, and evaluated the performance of GPT-4 on this dataset. We also classified assessment questions and evaluated the performance of GPT-4 for different types of questions. We found that the performance tended to be best for simple mathematical questions, and worst for questions requiring creative thinking, and those with complex descriptions and/or images. We share our benchmark dataset with the community and provide new insights into the capabilities of GPT-4 in the context of CG courses. We highlight opportunities for teaching staff to improve student learning by guiding the use of LLMs for CG questions, and inform decisions around question choices for assessment tasks.},
  address    = {New York, NY, USA},
  author     = {Feng, Tony Haoran and Denny, Paul and Wuensche, Burkhard and Luxton-Reilly, Andrew and Hooper, Steffan},
  booktitle  = {Proceedings of the 26th Australasian Computing Education Conference},
  doi        = {10.1145/3636243.3636263},
  isbn       = {9798400716195},
  keywords   = {Artificial Intelligence, Assessment, Computer Graphics, Computing Education, Evaluation, GPT-4, Large Language Models},
  location   = {Sydney, NSW, Australia},
  numpages   = {10},
  pages      = {182–191},
  publisher  = {Association for Computing Machinery},
  series     = {ACE '24},
  title      = {More Than Meets the AI: Evaluating the performance of GPT-4 on Computer Graphics assessment questions},
  url        = {https://doi.org/10.1145/3636243.3636263},
  year       = {2024}
}

@inproceedings{10.1145/3639474.3640052,
  abstract   = {Large Language Models (LLMs) have emerged as promising tools to assist students while solving programming assignments. However, object-oriented programming (OOP), with its inherent complexity involving the identification of entities, relationships, and responsibilities, is not yet mastered by these tools. Contrary to introductory programming exercises, there exists a research gap with regard to the behavior of LLMs in OOP contexts. In this study, we experimented with three prominent LLMs - GPT-3.5, GPT-4, and Bard - to solve real-world OOP exercises used in educational settings, subsequently validating their solutions using an Automatic Assessment Tool (AAT). The findings revealed that while the models frequently achieved mostly working solutions to the exercises, they often overlooked the best practices of OOP. GPT-4 stood out as the most proficient, followed by GPT-3.5, with Bard trailing last. We advocate for a renewed emphasis on code quality when employing these models and explore the potential of pairing LLMs with AATs in pedagogical settings. In conclusion, while GPT-4 showcases promise, the deployment of these models in OOP education still mandates supervision.},
  address    = {New York, NY, USA},
  author     = {Cipriano, Bruno Pereira and Alves, Pedro},
  booktitle  = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
  doi        = {10.1145/3639474.3640052},
  isbn       = {9798400704987},
  keywords   = {programming assignments, teaching, object-oriented programming, object-oriented design, OOP best practices, large language models, GPT-3, GPT-4, bard},
  location   = {Lisbon, Portugal},
  numpages   = {8},
  pages      = {162–169},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE-SEET '24},
  title      = {LLMs Still Can't Avoid Instanceof: An Investigation Into GPT-3.5, GPT-4 and Bard's Capacity to Handle Object-Oriented Programming Assignments},
  url        = {https://doi.org/10.1145/3639474.3640052},
  year       = {2024}
}

@inproceedings{10.1145/3639474.3640061,
  abstract   = {With the rapid advancement of artificial intelligence (AI) in various domains, the education sector is set for transformation. The potential of AI-driven tools in enhancing the learning experience, especially in programming, is immense. However, the scientific evaluation of Large Language Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an AI-Tutor remains largely unexplored. Therefore, there is a need to understand how students interact with such AI-Tutors and to analyze their experiences.In this paper, we conducted an exploratory case study by integrating the GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis. Through a combination of empirical data collection and an exploratory survey, we identified different user types based on their interaction patterns with the AI-Tutor. Additionally, the findings highlight advantages, such as timely feedback and scalability. However, challenges like generic responses and students' concerns about a learning progress inhibition when using the AI-Tutor were also evident. This research adds to the discourse on AI's role in education.},
  address    = {New York, NY, USA},
  author     = {Frankford, Eduard and Sauerwein, Clemens and Bassner, Patrick and Krusche, Stephan and Breu, Ruth},
  booktitle  = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
  doi        = {10.1145/3639474.3640061},
  isbn       = {9798400704987},
  keywords   = {programming education, automated programming assessment systems, artificial intelligence, ChatGPT, OpenAI, ChatBots},
  location   = {Lisbon, Portugal},
  numpages   = {11},
  pages      = {309–319},
  publisher  = {Association for Computing Machinery},
  series     = {ICSE-SEET '24},
  title      = {AI-Tutoring in Software Engineering Education},
  url        = {https://doi.org/10.1145/3639474.3640061},
  year       = {2024}
}

@inproceedings{10.1145/3641554.3701785,
  abstract   = {Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) have led to changes in educational practices by creating opportunities for personalized learning and immediate support. Computer science student perceptions and behaviors towards GenAI tools have been studied, but the effects of such tools on student learning have yet to be determined conclusively. We investigate the impact of GenAI tools on computing students' performance in a database course and aim to understand why students use GenAI tools in assignments. Our mixed-methods study (N=226) asked students to self-report whether they used a GenAI tool to complete a part of an assignment and why. Our results reveal that students utilizing GenAI tools performed better on the assignment part in which LLMs were permitted but did worse in other parts of the assignment and in the course overall. Also, those who did not use GenAI tools viewed more discussion board posts and participated more than those who used ChatGPT. This suggests that using GenAI tools may not lead to better skill development or mental models, at least not if the use of such tools is unsupervised, and that engagement with official course help supports may be affected. Further, our thematic analysis of reasons for using or not using GenAI tools, helps understand why students are drawn to these tools. Shedding light into such aspects empowers instructors to be proactive in how to encourage, supervise, and handle the use or integration of GenAI into courses, fostering good learning habits.},
  address    = {New York, NY, USA},
  author     = {Ramirez Osorio, Valeria and Zavaleta Bernuy, Angela and Simion, Bogdan and Liut, Michael},
  booktitle  = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
  doi        = {10.1145/3641554.3701785},
  isbn       = {9798400705311},
  keywords   = {computing education, databases, generative artificial intelligence, large language models, student behavior, student performance},
  location   = {Pittsburgh, PA, USA},
  numpages   = {7},
  pages      = {959–965},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSETS 2025},
  title      = {Understanding the Impact of Using Generative AI Tools in a Database Course},
  url        = {https://doi.org/10.1145/3641554.3701785},
  year       = {2025}
}

@inproceedings{10.1145/3641554.3701800,
  abstract   = {Large language models (LLMs) are already heavily used by professional software engineers. An important skill for new university graduates to possess will be the ability to use such LLMs to effectively navigate and modify a large code base. While much of the prior work related to LLMs in computing education focuses on novice programmers learning to code, less work has focused on how upper-division students use and trust these tools, especially while working with large code bases. In this study, we taught students about various GitHub Copilot features, including Copilot chat, in an upper-division software engineering course and asked students to add a feature to a large code base using Copilot. Our analysis revealed a novel interaction pattern that we call one-shot prompting, in which students ask Copilot to implement the entire feature at once and spend the next few prompts asking Copilot to debug the code or asking Copilot to regenerate its incorrect response. Finally, students reported significantly more trust in the code comprehension features than code generation features of Copilot, perhaps due to the presence of trust affordances in the Copilot chat that are absent in the code generation features. Our study takes the first steps in understanding how upper-division students use Github Copilot so that our instruction can adequately prepare students for a career in software engineering.},
  address    = {New York, NY, USA},
  author     = {Shah, Anshul and Chernova, Anya and Tomson, Elena and Porter, Leo and Griswold, William G. and Soosai Raj, Adalbert Gerald},
  booktitle  = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
  doi        = {10.1145/3641554.3701800},
  isbn       = {9798400705311},
  keywords   = {github copilot, large code bases, program comprehension, trust},
  location   = {Pittsburgh, PA, USA},
  numpages   = {7},
  pages      = {1050–1056},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSETS 2025},
  title      = {Students' Use of GitHub Copilot for Working with Large Code Bases},
  url        = {https://doi.org/10.1145/3641554.3701800},
  year       = {2025}
}

@inproceedings{10.1145/3641554.3701801,
  abstract   = {The modern educational landscape faces the challenge of maintaining effective, personalized mentorship amid expanding class sizes. This challenge is particularly pronounced in fields requiring hands-on practice, such as cybersecurity education. Teaching assistants and peer interactions provide some relief, but the student-to-educator ratio often remains high, limiting individualized attention. The advent of Large Language Models (LLMs) offers a promising solution by potentially providing scalable and personalized guidance. In this paper, we introduce SENSAI, an AI-powered tutoring system that leverages LLMs to offer tailored feedback and assistance by transparently extracting and utilizing the learner's working context, including their active terminals and edited files. Over the past year, SENSAI has been deployed in an applied cybersecurity curriculum at a large public R1 university and made available to a broader online community of global learners, assisting 2,742 users with hundreds of educational challenges. In total 178,074 messages were exchanged across 15,413 sessions, incurring a total cost of 1,979--comparable to that of a single undergraduate teaching assistant but with a significantly wider reach. SENSAI demonstrates significant improvements in student problem-solving efficiency and satisfaction, offering insights into the future role of AI in education.},
  address    = {New York, NY, USA},
  author     = {Nelson, Connor and Doup\'{e}, Adam and Shoshitaishvili, Yan},
  booktitle  = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
  doi        = {10.1145/3641554.3701801},
  isbn       = {9798400705311},
  keywords   = {cybersecurity education, large language models, tutoring},
  location   = {Pittsburgh, PA, USA},
  numpages   = {7},
  pages      = {833–839},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSETS 2025},
  title      = {SENSAI: Large Language Models as Applied Cybersecurity Tutors},
  url        = {https://doi.org/10.1145/3641554.3701801},
  year       = {2025}
}

@inproceedings{10.1145/3641554.3701841,
  abstract   = {Digital accessibility ensures that digital products and services are usable by a diverse range of users, regardless of their physical or cognitive abilities. While numerous standards and guidelines have been established to aid developers in creating accessible content, studies reveal a persistent lack of accessibility in many web and mobile applications. This gap is often attributed to barriers such as lack of awareness, insufficient knowledge, absence of specific requirements, time constraints, and lack of executive support. In this context, we aim to address the lack of awareness and knowledge challenges by proposing a hands-on approach that leverages the capabilities of Large Language Models (LLMs) like ChatGPT to enhance students' accessibility awareness, knowledge, and practical skills. We engaged software engineering students in tasks involving website development and accessibility evaluation using checker tools, and we utilized ChatGPT 3.5 to fix identified accessibility issues. Our findings suggest that practical assignments significantly enhance learning outcomes, as interactions with LLMs allow students to develop a deeper understanding of accessibility concepts. This approach not only reinforces theoretical knowledge but also highlights the real-world impact of their work. The results indicate that combining practical assignments with AI-driven support effectively improves students' proficiency in web accessibility.},
  address    = {New York, NY, USA},
  author     = {Aljedaani, Wajdi and Eler, Marcelo Medeiros and Parthasarathy, P D},
  booktitle  = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
  doi        = {10.1145/3641554.3701841},
  isbn       = {9798400705311},
  keywords   = {chatgpt 3.5, digital accessibility, large language models, llms, project based learning, software engineering, wcag},
  location   = {Pittsburgh, PA, USA},
  numpages   = {7},
  pages      = {25–31},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSETS 2025},
  title      = {Enhancing Accessibility in Software Engineering Projects with Large Language Models (LLMs)},
  url        = {https://doi.org/10.1145/3641554.3701841},
  year       = {2025}
}

@inproceedings{10.1145/3641554.3701859,
  abstract   = {Generative AI tools are rapidly evolving and impacting many domains, including programming. Computer Science (CS) instructors must address student access to these tools. While some advocate to ban the tools entirely, others suggest embracing them so that students develop the skills for utilizing the tools safely and responsibly. Studies indicate positive impacts, as well as cautions, on student outcomes when these tools are integrated into courses. We studied the impact of incorporating instruction on industry-standard generative AI tools into a mid-level software development course with students from 16 Minority Serving Institutions. 89% of student participants used generative AI tools prior to the course without any formal instruction. After formal instruction, students most frequently used generative AI tools for explaining concepts and learning new things. Students generally reported positive viewpoints on their ability to learn to program and learn problem-solving skills while using generative AI tools. Finally, we found that students: reported to understand their code when they work with generative AI tools, are critical about the outputs that generative AI tools provide, and check outputs of generative AI tools to ensure accuracy.},
  address    = {New York, NY, USA},
  author     = {Gorson Benario, Jamie and Marroquin, Jenn and Chan, Monica M. and Holmes, Ernest D.V. and Mejia, Daniel},
  booktitle  = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
  doi        = {10.1145/3641554.3701859},
  isbn       = {9798400705311},
  keywords   = {cs education, generative ai, llms in cs education, minority serving institutions},
  location   = {Pittsburgh, PA, USA},
  numpages   = {7},
  pages      = {395–401},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSETS 2025},
  title      = {Unlocking Potential with Generative AI Instruction: Investigating Mid-level Software Development Student Perceptions, Behavior, and Adoption},
  url        = {https://doi.org/10.1145/3641554.3701859},
  year       = {2025}
}

@inproceedings{10.1145/3641554.3701917,
  abstract   = {Generative artificial intelligence (GenAI) is transforming Computer Science education, and every instructor is reflecting on how AI will impact their courses. Instructors must determine how students may use AI for course activities and what AI systems they will support and encourage students to use. This task is challenging with the proliferation of large language models (LLMs) and related AI systems. The contribution of this work is an experimental evaluation of the performance of multiple open-source and commercial LLMs utilizing retrieval-augmented generation in answering questions for computer science courses and a cost-benefit analysis for instructors when determining what systems to use. A key factor is the time an instructor has to maintain their supported AI systems and the most effective activities for improving their performance. The paper offers recommendations for deploying, using, and enhancing AI in educational settings.},
  address    = {New York, NY, USA},
  author     = {Wang, Kevin Shukang and Lawrence, Ramon},
  booktitle  = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
  doi        = {10.1145/3641554.3701917},
  isbn       = {9798400705311},
  keywords   = {artificial intelligence, human-in-the-loop, large language model, question answering, retrieval-augmented generation},
  location   = {Pittsburgh, PA, USA},
  numpages   = {7},
  pages      = {1183–1189},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSETS 2025},
  title      = {Quantitative Evaluation of Using Large Language Models and Retrieval-Augmented Generation in Computer Science Education},
  url        = {https://doi.org/10.1145/3641554.3701917},
  year       = {2025}
}

@inproceedings{10.1145/3641554.3701932,
  abstract   = {ChatGPT potential value in education is broadly recognized and many studies report experiments of its use inside or outside the classroom by students and teachers. On the other hand, the use of ChatGPT rises lots of concerns about well-known problems such as hallucination, plagiarism, overreliance, or misinformation. It is of primary importance to teach students a correct and constructive use of ChatGPT and a critical approach to its returned outputs. The paper presents a classroom experience where students were asked to interact with ChatGPT in the context of a database course. The declared challenge for the students was, given a set of predefined relational database schemata, to invent questions for ChatGPT and try to force wrong SQL solutions. Students had to record the question, the ChatGPT solution, their solution, and the comments about the eventual ChatGPT syntactical and/or semantical errors. This gamification approach was meant to enhance students' motivation, but the main teachers' goal was to make them reflect critically (i) on ChatGPT output, experiencing that it does make mistakes, (ii) on the interpretation of ChatGPT errors, and (iii) on the possible strategies for forcing ChatGPT errors. The experiment involved 166 B.S. students in Engineering and the collected data have been analyzed under different points of view to get an insight into the approach and the critical attitude of the students. The paper reports the results of this analysis and discusses the impact of the activity on learning by analyzing the correlation between students' participation and exam performance.},
  address    = {New York, NY, USA},
  author     = {Farinetti, Laura and Cagliero, Luca},
  booktitle  = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
  doi        = {10.1145/3641554.3701932},
  isbn       = {9798400705311},
  keywords   = {critical thinking, database education, human-computer interaction, large language model, sql},
  location   = {Pittsburgh, PA, USA},
  numpages   = {7},
  pages      = {318–324},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSETS 2025},
  title      = {A Critical Approach to ChatGPT: An Experience in SQL Learning},
  url        = {https://doi.org/10.1145/3641554.3701932},
  year       = {2025}
}

@inproceedings{10.1145/3641554.3701940,
  abstract   = {The integration of LLM-generated feedback into educational settings has shown promise in enhancing student learning outcomes. This paper presents a novel LLM-driven system that provides targeted feedback for conceptual designs in a Database Systems course. The system converts student-created entity-relationship diagrams (ERDs) into JSON format, allows the student to prune the diagram by isolating a relationship, extracts relevant requirements for the selected relationship, and utilizes a large language model (LLM) to generate detailed feedback. Additionally, the system creates a tailored set of questions and answers to further aid student understanding. Our pilot implementation in a Database System course demonstrates effective feedback generation that helped the students improve their design skills.},
  address    = {New York, NY, USA},
  author     = {Riazi, Sara and Rooshenas, Pedram},
  booktitle  = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
  doi        = {10.1145/3641554.3701940},
  isbn       = {9798400705311},
  keywords   = {conceptual design, database systems, educational technology, large language models, llm-generated feedback},
  location   = {Pittsburgh, PA, USA},
  numpages   = {7},
  pages      = {1001–1007},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSETS 2025},
  title      = {LLM-Driven Feedback for Enhancing Conceptual Design Learning in Database Systems Courses},
  url        = {https://doi.org/10.1145/3641554.3701940},
  year       = {2025}
}

@inproceedings{10.1145/3641554.3701965,
  abstract   = {While Large Language Models (LLMs) have emerged as promising methods for automated student question-answering, guaranteeing consistent instructional effectiveness of the response remains a key challenge. Therefore, there is a need for fine-grained analysis of State-Of-The-Art (SOTA) LLM-powered educational assistants.  This work evaluates Edison: a Retrieval Augmented Generation (RAG) pipeline based on GPT-4. We determine the pedagogical effectiveness of Edison's responses through expert Teaching Assistant (TA) evaluation of the answers. After the TA edits and improves the response, we analyze the original LLM response, the TA-assigned ratings, and the TA's edits to ascertain the essential characteristics of a high-quality response. Some key insights of our evaluation are as follows: (1) Edison can give relevant and factual answers in an educational style for conceptual and assignment questions, (2) Most TA edits are deletions made to improve the style of the response, and finally (3) Our analysis indicates that Edison improves TAs' efficiency by reducing the effort required to respond to student questions.},
  address    = {New York, NY, USA},
  author     = {Miroyan, Mihran and Mitra, Chancharik and Jain, Rishi and Ranade, Gireeja and Norouzi, Narges},
  booktitle  = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
  doi        = {10.1145/3641554.3701965},
  isbn       = {9798400705311},
  keywords   = {expert feedback, instructional technologies, language models, question answering},
  location   = {Pittsburgh, PA, USA},
  numpages   = {7},
  pages      = {770–776},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSETS 2025},
  title      = {Analyzing Pedagogical Quality and Efficiency of LLM Responses with TA Feedback to Live Student Questions},
  url        = {https://doi.org/10.1145/3641554.3701965},
  year       = {2025}
}

@inproceedings{10.1145/3641555.3705074,
  abstract   = {Artificial Intelligence (AI) tools have transformed software development, making it crucial to equip computer science (CS) students with the skills to leverage these technologies. This talk presents an innovative curriculum approach, integrating AI tools into an advanced CS capstone course at a stage where students possess foundational skills in software engineering. This strategic timing ensures students can critically engage with AI, recognizing biases and managing challenges like hallucinations in AI-generated outputs.Before redesigning the curriculum, independent research was conducted to understand the strengths and limitations of various AI tools, such as Lucidchart, Eraser.io for design documentation, and GitHub Copilot, GPT-4, Codeium, Claude, and Gemini for implementation tasks like code generation, code completion, UI design, error handling, and API integration. This research guided the curriculum by shaping assignment design and delivering foundational lectures on prompt engineering to ease the learning curve for students. Experiments during the capstone course included AI-enhanced assignments and projects, where students applied these tools for software design and implementation. Quantitative data-prompt refinement counts, error rates, code accuracy, and qualitative reflections revealed increased confidence in AI tools, enhanced productivity, and greater readiness for industry roles. Despite these benefits, students faced challenges with complex tasks that required iterative refinement and oversight, but they gained skills in managing biases and hallucinations in AI outputs. The curriculum's ''right-left'' approach enables a smooth transition to AI-assisted development, preparing students for the evolving tech landscape. This talk shares key findings, best practices, and insights into balancing manual skills with AI-enhanced learning.},
  address    = {New York, NY, USA},
  author     = {Roy, Nimisha and Olufisayo, Omojokun and Horielko, Oleksandr},
  booktitle  = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
  doi        = {10.1145/3641555.3705074},
  isbn       = {9798400705328},
  keywords   = {ai-enhanced learning, capstone courses, gen-ai tools in curriculum, iterative prompting., software engineering education, student preparedness},
  location   = {Pittsburgh, PA, USA},
  numpages   = {1},
  pages      = {1747},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSETS 2025},
  title      = {Empowering Future Software Engineers: Integrating AI Tools into Advanced CS Curriculum},
  url        = {https://doi.org/10.1145/3641555.3705074},
  year       = {2025}
}

@inproceedings{10.1145/3641555.3705183,
  abstract   = {In Agile software development methodology, a user story describes a new feature or functionality from an end user's perspective. The user story details may also incorporate acceptance testing criteria, which can be developed through negotiation with users. When creating stories from user feedback, the software engineer may maximize their usefulness by considering story attributes, including scope, independence, negotiability, and testability. This study investigates how LLMs (large language models), with guided instructions, affect undergraduate software engineering students' ability to transform user feedback into user stories. Students, working individually, were asked to analyze user feedback comments, appropriately group related items, and create user stories following the principles of INVEST, a framework for assessing user stories. We found that LLMs help students develop valuable stories with well-defined acceptance criteria. However, students tend to perform better without LLMs when creating user stories with an appropriate scope.},
  address    = {New York, NY, USA},
  author     = {Brockenbrough, Allan and Feild, Henry and Salinas, Dominic},
  booktitle  = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
  doi        = {10.1145/3641555.3705183},
  isbn       = {9798400705328},
  keywords   = {LLM, generative AI, large language model, user story},
  location   = {Pittsburgh, PA, USA},
  numpages   = {2},
  pages      = {1401–1402},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSETS 2025},
  title      = {Exploring LLMs Impact on Student-Created User Stories and Acceptance Testing in Software Development},
  url        = {https://doi.org/10.1145/3641555.3705183},
  year       = {2025}
}

@inproceedings{10.1145/3641555.3705250,
  abstract   = {We present the Requirement Elicitation Tool that leverages Large Language Model (LLM) (gpt-4o-mini) to enable simulated real-world interactions of requirements gathering from three synthetic personas. We demonstrate the use case of Computer Science (CS) students in Database Management Systems leveraging the tool to build a conceptual model and Entity-Relationship (ER) diagrams. Our preliminary findings show the potential of this tool to engage students in discovery process without providing predefined solutions and set the directions for future work.},
  address    = {New York, NY, USA},
  author     = {Akhmetov, Ildar and Prpa, Mirjana},
  booktitle  = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
  doi        = {10.1145/3641555.3705250},
  isbn       = {9798400705328},
  keywords   = {AI persona, requirement elicitation, software engineering education},
  location   = {Pittsburgh, PA, USA},
  numpages   = {2},
  pages      = {1357–1358},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSETS 2025},
  title      = {Simulating Requirement Elicitation: Development and Evaluation of a Persona-Based Tool},
  url        = {https://doi.org/10.1145/3641555.3705250},
  year       = {2025}
}

@article{10.1145/3643755,
  abstract   = {Software developers often repeat the same code changes within a project or across different projects. These repetitive changes are known as “code change patterns” (CPATs). Automating CPATs is crucial to expedite the software development process. While current Transformation by Example (TBE) techniques can automate CPATs, they are limited by the quality and quantity of the provided input examples. Thus, they miss transforming code variations that do not have the exact syntax, data-, or control-flow of the provided input examples, despite being semantically similar. Large Language Models (LLMs), pre-trained on extensive source code datasets, offer a potential solution. Harnessing the capability of LLMs to generate semantically equivalent, yet previously unseen variants of the original CPAT could significantly increase the effectiveness of TBE systems. 















In this paper, we first discover best practices for harnessing LLMs to generate code variants that meet three criteria: correctness (semantic equivalence to the original CPAT), usefulness (reflecting what developers typically write), and applicability (aligning with the primary intent of the original CPAT). We then implement these practices in our tool PyCraft, which synergistically combines static code analysis, dynamic analysis, and LLM capabilities. By employing chain-of-thought reasoning, PyCraft generates variations of input examples and comprehensive test cases that identify correct variations with an F-measure of 96.6%. Our algorithm uses feedback iteration to expand the original input examples by an average factor of 58x. Using these richly generated examples, we inferred transformation rules and then automated these changes, resulting in an increase of up to 39x, with an average increase of 14x in target codes compared to a previous state-of-the-art tool that relies solely on static analysis. We submitted patches generated by PyCraft to a range of projects, notably esteemed ones like microsoft/DeepSpeed and IBM/inFairness. Their developers accepted and merged 83% the 86 CPAT instances submitted through 44 pull requests. This confirms the usefulness of these changes.},
  address    = {New York, NY, USA},
  articleno  = {29},
  author     = {Dilhara, Malinda and Bellur, Abhiram and Bryksin, Timofey and Dig, Danny},
  doi        = {10.1145/3643755},
  issue_date = {July 2024},
  journal    = {Proc. ACM Softw. Eng.},
  keywords   = {Automation, Code Changes, Code Clone, Generative AI, Large Language Models, Machine Learning, Program by Example, Python, Test Case Generation, Transformation by Example},
  month      = {July},
  number     = {FSE},
  numpages   = {23},
  publisher  = {Association for Computing Machinery},
  title      = {Unprecedented Code Change Automation: The Fusion of LLMs and Transformation by Example},
  url        = {https://doi.org/10.1145/3643755},
  volume     = {1},
  year       = {2024}
}

@inproceedings{10.1145/3643795.3648379,
  abstract   = {Large Language Models (LLMs) represent a leap in artificial intelligence, excelling in tasks using human language(s). Although the main focus of general-purpose LLMs is not code generation, they have shown promising results in the domain. However, the usefulness of LLMs in an academic software engineering project has not been fully explored yet. In this study, we explore the usefulness of LLMs for 214 students working in teams consisting of up to six members. Notably, in the academic course through which this study is conducted, students were encouraged to integrate LLMs into their development tool-chain, in contrast to most other academic courses that explicitly prohibit the use of LLMs.In this paper, we analyze the AI-generated code, prompts used for code generation, and the human intervention levels to integrate the code into the code base. We also conduct a perception study to gain insights into the perceived usefulness, influencing factors, and future outlook of LLM from a computer science student's perspective. Our findings suggest that LLMs can play a crucial role in the early stages of software development, especially in generating foundational code structures, and helping with syntax and error debugging. These insights provide us with a framework on how to effectively utilize LLMs as a tool to enhance the productivity of software engineering students, and highlight the necessity of shifting the educational focus toward preparing students for successful human-AI collaboration.},
  address    = {New York, NY, USA},
  author     = {Rasnayaka, Sanka and Wang, Guanlin and Shariffdeen, Ridwan and Iyer, Ganesh Neelakanta},
  booktitle  = {Proceedings of the 1st International Workshop on Large Language Models for Code},
  doi        = {10.1145/3643795.3648379},
  isbn       = {9798400705793},
  keywords   = {LLM for code generation, software engineering},
  location   = {Lisbon, Portugal},
  numpages   = {8},
  pages      = {111–118},
  publisher  = {Association for Computing Machinery},
  series     = {LLM4Code '24},
  title      = {An Empirical Study on Usage and Perceptions of LLMs in a Software Engineering Project},
  url        = {https://doi.org/10.1145/3643795.3648379},
  year       = {2024}
}

@inproceedings{10.1145/3649165.3690116,
  abstract   = {Large Language Models (LLMs) have had considerable difficulty when prompted with mathematical and formal questions, especially those within theory of computing (ToC) courses. In this paper, we detail two experiments regarding our own ToC course and the ChatGPT LLM. For the first, we evaluated ChatGPT's ability to pass our own ToC course's exams. For the second, we created a database of sample ToC questions and responses to accommodate other ToC offerings' choices for topics and structure. We scored each of ChatGPT's outputs on these questions. Overall, we determined that ChatGPT can pass our ToC course, and is adequate at understanding common formal definitions and answering "simple''-style questions, e.g., true/false and multiple choice. However, ChatGPT often makes nonsensical claims in open-ended responses, such as proofs.},
  address    = {New York, NY, USA},
  author     = {Golesteanu, Matei A. and Vowinkel, Garrett B. and Dougherty, Ryan E.},
  booktitle  = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1},
  doi        = {10.1145/3649165.3690116},
  isbn       = {9798400705984},
  keywords   = {automata theory, chatgpt, computer science education, formal languages, large language model, theoretical computer science},
  location   = {Virtual Event, NC, USA},
  numpages   = {6},
  pages      = {33–38},
  publisher  = {Association for Computing Machinery},
  series     = {SIGCSE Virtual 2024},
  title      = {Can ChatGPT pass a Theory of Computing Course?},
  url        = {https://doi.org/10.1145/3649165.3690116},
  year       = {2024}
}

@inproceedings{10.1145/3649405.3659473,
  abstract   = {The advent of Large Language Models (LLMs) has created multiple challenges for the Computer Science Education Community. This research project aims at integrating LLMs into Object-Oriented Programming courses, by generating and evaluating new teaching methodologies and tools suitable for this paradigm's specificities.},
  address    = {New York, NY, USA},
  author     = {Cipriano, Bruno Pereira},
  booktitle  = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 2},
  doi        = {10.1145/3649405.3659473},
  isbn       = {9798400706035},
  keywords   = {bard, gpt-3.5, gpt-4, large language models, object-oriented programming},
  location   = {Milan, Italy},
  numpages   = {2},
  pages      = {832–833},
  publisher  = {Association for Computing Machinery},
  series     = {ITiCSE 2024},
  title      = {Towards the Integration of Large Language Models in an Object-Oriented Programming Course},
  url        = {https://doi.org/10.1145/3649405.3659473},
  year       = {2024}
}

@inproceedings{10.1145/3649405.3659495,
  abstract   = {Educators face the continuous challenge of updating their teaching materials with the fast-paced changes in cybersecurity and integrating emerging topics into their existing content. This work presents the development progress of a cybersecurity curriculum fine-tuned Large Language Model (LLM) designed to assist educators in creating engaging curricular materials for introductory cybersecurity topics. Recent advancements in Natural Language Processing (NLP) and the increased number of openly available LLMs like OpenAI's GPT and Meta's Llama present an opportunity to fine-tune these models for specific domains like cybersecurity. The fine-tuned LLMs offer a potential solution by reducing the amount of work necessary to regularly update curricular content, including lab assignments, assessments, in-class activities, and other additional resources for both synchronous and asynchronous classrooms.},
  address    = {New York, NY, USA},
  author     = {Flores, Paige and Kaza, Siddharth and Taylor, Blair},
  booktitle  = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 2},
  doi        = {10.1145/3649405.3659495},
  isbn       = {9798400706035},
  keywords   = {artificial intelligence, curriculum, cybersecurity education, large language models, security injections},
  location   = {Milan, Italy},
  numpages   = {1},
  pages      = {804},
  publisher  = {Association for Computing Machinery},
  series     = {ITiCSE 2024},
  title      = {Fine-Tuning AI to Assist in Building Curriculum for the CIA Triad and Cyber Kill Chain},
  url        = {https://doi.org/10.1145/3649405.3659495},
  year       = {2024}
}

@inproceedings{10.1145/3649405.3659514,
  abstract   = {This research has three prongs, with each comparing open- and closed-book exam questions across six years (2017-2023) in a final year undergraduate applied machine learning course. First, the authors evaluated the performance of numerous LLMs, compared to student performance, and comparing open and closed book exams. Second, at a micro level, the examination questions and categories for which LLMs were most and least effective were compared. This level of analysis is rarely if ever, discussed in the literature. The research finally investigates LLM detection techniques, specifically their efficacy in identifying replies created wholly by an LLM. It considers both raw LLM outputs and LLM outputs that have been tampered with by students, with an emphasis on academic integrity. This study is a staff-student research collaboration, featuring contributions from eight academic professionals and six students.},
  address    = {New York, NY, USA},
  author     = {Quille, Keith and Becker, Brett A. and Faherty, Roisin and Gordon, Damien and Harte, Miriam and Hensman, Svetlana and Hofmann, Markus and Nolan, Keith and O'Leary, Ciaran},
  booktitle  = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 2},
  doi        = {10.1145/3649405.3659514},
  isbn       = {9798400706035},
  keywords   = {ai, assessment, detection, large language models, llms, ml},
  location   = {Milan, Italy},
  numpages   = {1},
  pages      = {822},
  publisher  = {Association for Computing Machinery},
  series     = {ITiCSE 2024},
  title      = {LLMs in Open and Closed Book Examinations in a Final Year Applied Machine Learning Course (Early Findings)},
  url        = {https://doi.org/10.1145/3649405.3659514},
  year       = {2024}
}

@inproceedings{10.1145/3650212.3680328,
  abstract   = {With the proven effectiveness of Large Language Models (LLMs) in code-related tasks, researchers have explored their potential for program repair. However, existing repair benchmarks might have influenced LLM training data, potentially causing data leakage. To evaluate LLMs’ realistic repair capabilities, (i) we introduce an extensive, non-crawled benchmark TutorCode, comprising 1,239 C++ defect codes and associated information such as tutor guidance, solution description, failing test cases, and the corrected code. Our work assesses LLM’s repair performance on TutorCode, measuring repair correctness (TOP-5 and AVG-5) and patch precision (RPSR). (ii) We then provide a comprehensive investigation into which types of extra information can help LLMs improve their repair performance. Among these types, tutor guidance was the most effective information. To fully harness LLMs’ conversational capabilities and the benefits of augmented information, (iii) we introduce a novel conversational semi-automatic repair framework CREF assisting human programming tutors. It demonstrates a remarkable AVG-5 improvement of 17.2%-24.6% compared to the baseline, achieving an impressive AVG-5 of 76.6% when utilizing GPT-4. These results highlight the potential for enhancing LLMs’ repair capabilities through tutor interactions and historical conversations. The successful application of CREF in a real-world educational setting demonstrates its effectiveness in reducing tutors’ workload and improving students’ learning experience, showing promise for code review and other software engineering tasks.},
  address    = {New York, NY, USA},
  author     = {Yang, Boyang and Tian, Haoye and Pian, Weiguo and Yu, Haoran and Wang, Haitao and Klein, Jacques and Bissyand\'{e}, Tegawend\'{e} F. and Jin, Shunfu},
  booktitle  = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
  doi        = {10.1145/3650212.3680328},
  isbn       = {9798400706127},
  keywords   = {Large Language Model, Open Source, Program Repair},
  location   = {Vienna, Austria},
  numpages   = {13},
  pages      = {882–894},
  publisher  = {Association for Computing Machinery},
  series     = {ISSTA 2024},
  title      = {CREF: An LLM-Based Conversational Software Repair Framework for Programming Tutors},
  url        = {https://doi.org/10.1145/3650212.3680328},
  year       = {2024}
}

@inproceedings{10.1145/3650212.3680342,
  abstract   = {The remarkable capability of large language models (LLMs) in 































































generating high-quality code has drawn increasing attention 































































in the software testing community.































































However, existing code LLMs often demonstrate unsatisfactory capabilities in generating accurate, complete tests































































since they were trained on code snippets collected without 































































differentiating between code for testing and for other purposes.































































In this paper, we present a large-scale dataset, UniTSyn, which can enhance LLMs for Unit Test Synthesis. 































































Associating tests with the tested functions is crucial for LLMs to infer the expected behavior and the logic paths to be verified.































































By leveraging Language Server Protocol, UniTSyn achieves the challenging goal of collecting focal-test pairs without per-project execution setups or per-language heuristics, which tend to be fragile and difficult to scale.































































Containing 2.7 million focal-test pairs across five mainstream programming languages, it can enhance the test generation ability of LLMs.































































Our experiments demonstrate that, 































































by building an autoregressive LLM based on UniTSyn,































































we can achieve significant benefits in learning and understanding unit test representations, 































































resulting in improved generation accuracy and code coverage 































































across all the evaluated programming languages.},
  address    = {New York, NY, USA},
  author     = {He, Yifeng and Huang, Jiabo and Rong, Yuyang and Guo, Yiwen and Wang, Ethan and Chen, Hao},
  booktitle  = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
  doi        = {10.1145/3650212.3680342},
  isbn       = {9798400706127},
  keywords   = {Large language models, dataset, software testing, test case generation},
  location   = {Vienna, Austria},
  numpages   = {12},
  pages      = {1061–1072},
  publisher  = {Association for Computing Machinery},
  series     = {ISSTA 2024},
  title      = {UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing},
  url        = {https://doi.org/10.1145/3650212.3680342},
  year       = {2024}
}

@inproceedings{10.1145/3650212.3680354,
  abstract   = {Recently, deep learning-based test case generation approaches have been proposed to automate the generation of unit test cases. In this study, we leverage Transformer-based code models to generate















unit tests with the help of Domain Adaptation (DA) at a project level. Specifically, we use CodeT5, a relatively small language model trained on source code data, and fine-tune it on the test generation















task. Then, we apply domain adaptation to each target project data to learn project-specific knowledge (project-level DA). We use the Methods2test dataset to fine-tune CodeT5 for the test generation















task and the Defects4j dataset for project-level domain adaptation and evaluation. We compare our approach with (a) CodeT5 fine-tuned on the test generation without DA, (b) the A3Test tool, and (c)















GPT-4 on five projects from the Defects4j dataset. The results show that tests generated using DA can increase the line coverage by 18.62%, 19.88%, and 18.02% and mutation score by 16.45%, 16.01%,















and 12.99% compared to the above (a), (b), and (c) baselines, respectively. The overall results show consistent improvements in metrics such as parse rate, compile rate, BLEU, and CodeBLEU. In addition,















we show that our approach can be seen as a complementary solution alongside existing search-based test generation tools such as EvoSuite, to increase the overall coverage and mutation scores with an average of 34.42% and 6.8%, for line coverage and mutation score, respectively.},
  address    = {New York, NY, USA},
  author     = {Shin, Jiho and Hashtroudi, Sepehr and Hemmati, Hadi and Wang, Song},
  booktitle  = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
  doi        = {10.1145/3650212.3680354},
  isbn       = {9798400706127},
  keywords   = {Code Model, Domain Adaption, GPT, LLM, Test generation, Transformers},
  location   = {Vienna, Austria},
  numpages   = {12},
  pages      = {1211–1222},
  publisher  = {Association for Computing Machinery},
  series     = {ISSTA 2024},
  title      = {Domain Adaptation for Code Model-Based Unit Test Case Generation},
  url        = {https://doi.org/10.1145/3650212.3680354},
  year       = {2024}
}

@inproceedings{10.1145/3652620.3687776,
  abstract   = {This study introduces an innovative AI-powered scaffolding approach aimed at enhancing software modeling learning through UML diagrams. The focus of this research is on defining the principles and functions comprising the scaffolding. Leveraging recent advancements in generative AI, our approach provides a structured educational framework to improve comprehension and proficiency in modeling concepts. We present the initial implementation of the scaffolding, specifically highlighting the feedback function. By integrating theoretical insights with practical applications, this study seeks to advance Model-Driven Software Engineering education and underscores the potential of AI in enhancing instructional methodologies.},
  address    = {New York, NY, USA},
  author     = {Ardimento, Pasquale and Bernardi, Mario Luca and Cimitile, Marta and Scalera, Michele},
  booktitle  = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
  doi        = {10.1145/3652620.3687776},
  isbn       = {9798400706226},
  keywords   = {generative AI, education, software modelling, model-driven software engineering, UML, scaffolding},
  location   = {Linz, Austria},
  numpages   = {4},
  pages      = {103–106},
  publisher  = {Association for Computing Machinery},
  series     = {MODELS Companion '24},
  title      = {Enhancing Software Modeling Learning with AI-Powered Scaffolding},
  url        = {https://doi.org/10.1145/3652620.3687776},
  year       = {2024}
}

@inproceedings{10.1145/3652620.3687784,
  abstract   = {This paper introduces an advanced functionality designed to facilitate the learning of UML class diagram construction. Built upon an integrated Retrieval Augmented Generation Large Language Model, the functionality provides enriched feedback by leveraging accumulated knowledge. The functionality is implemented in an existing tool named UML Miner, a Visual Paradigm plugin that captures and analyzes student-generated UML diagrams by applying process mining techniques. By offering personalized feedback and continuous support during modeling, the tool aims to enhance learning outcomes and students' engagement.},
  address    = {New York, NY, USA},
  author     = {Ardimento, Pasquale and Bernardi, Mario Luca and Cimitile, Marta and Scalera, Michele},
  booktitle  = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
  doi        = {10.1145/3652620.3687784},
  isbn       = {9798400706226},
  keywords   = {learning, UML, software modeling, retrieval augmented generation, large language model, tool},
  location   = {Linz, Austria},
  numpages   = {5},
  pages      = {26–30},
  publisher  = {Association for Computing Machinery},
  series     = {MODELS Companion '24},
  title      = {A RAG-based Feedback Tool to Augment UML Class Diagram Learning},
  url        = {https://doi.org/10.1145/3652620.3687784},
  year       = {2024}
}

@inproceedings{10.1145/3656650.3656663,
  abstract   = {Recent research work on digital-well being considers it a matter of personal growth and education. Research in social digital well-being, in particular, invites young generations to consider the role of digital technologies for social well-being. It explored how to engage young generations in building socio-technical prototypes and reflecting on the impact of technology on people. This article fits into this broad line of research. It reports on a case study with social design students with no computing background. It invited them to consider the use of computing technologies in their social-design projects, and to reflect critically on their work. The design was structured with an ad hoc toolkit with various building materials, including cards for reflecting, and physical-computing devices for rapidly prototyping design ideas. The purpose of the toolkit is, on the one hand, to structure and constrain the generative design process and, on the other hand, to allow a certain degree of expressiveness and freedom to participants. The article reports the results of the case study and concludes with a discussion of how to engage non-computing experts in such a process, balancing freedom and guidance.},
  address    = {New York, NY, USA},
  articleno  = {50},
  author     = {Gennari, Rosella and Krik, Soufiane},
  booktitle  = {Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
  doi        = {10.1145/3656650.3656663},
  isbn       = {9798400717642},
  keywords   = {Responsible design, building interaction, end user, interaction design tools, prototyping, social interaction, well being},
  location   = {Arenzano, Genoa, Italy},
  numpages   = {9},
  publisher  = {Association for Computing Machinery},
  series     = {AVI '24},
  title      = {Responsible Design of Socio-Technical Solutions with Social Design Students: a Case Study},
  url        = {https://doi.org/10.1145/3656650.3656663},
  year       = {2024}
}

@inproceedings{10.1145/3656650.3656676,
  abstract   = {This paper introduces CWGPT, a ChatGPT-4-based tool designed for Cognitive Walkthrough (CW) inspired evaluations of web interfaces. The primary goal is to assist users, particularly students and inexperienced designers, in evaluating web interfaces. Our tool, operating as a conversational agent, provides detailed evaluations of a user-specified task by intelligently guessing the subtasks and actions required to accomplish them, answering the standard CW questions, and providing helpful feedback and practical suggestions to improve the usability of the analyzed interface. For our study, we selected a group of web applications designed by students from a Web and Software Architecture course. We compare the outcome of the CWs we executed on ten web apps against the corresponding CWGPT analyses. We then describe the study we conducted involving five author-students to assess the tool’s efficacy in helping them recognize and solve usability issues. In addition to introducing a novel adaptation of ChatGPT, the outcomes of the described experience underscore the promising potential of AI in usability evaluations.},
  address    = {New York, NY, USA},
  articleno  = {41},
  author     = {Bisante, Alba and Datla, Venkata Srikanth Varma and Panizzi, Emanuele and Trasciatti, Gabriella and Zeppieri, Stefano},
  booktitle  = {Proceedings of the 2024 International Conference on Advanced Visual Interfaces},
  doi        = {10.1145/3656650.3656676},
  isbn       = {9798400717642},
  keywords   = {AI, ChatGPT, Cognitive Walkthrough, GPT, HCI},
  location   = {Arenzano, Genoa, Italy},
  numpages   = {5},
  publisher  = {Association for Computing Machinery},
  series     = {AVI '24},
  title      = {Enhancing Interface Design with AI: An Exploratory Study on a ChatGPT-4-Based Tool for Cognitive Walkthrough Inspired Evaluations},
  url        = {https://doi.org/10.1145/3656650.3656676},
  year       = {2024}
}

@inproceedings{10.1145/3657604.3662042,
  abstract   = {Self-reflection on learning experiences constitutes a fundamental cognitive process, essential for consolidating knowledge and enhancing learning efficacy. However, traditional methods to facilitate reflection often face challenges in personalization, immediacy of feedback, engagement, and scalability. Integration of Large Language Models (LLMs) into the reflection process could mitigate these limitations. In this paper, we conducted two randomized field experiments in undergraduate computer science courses to investigate the potential of LLMs to help students engage in post-lesson reflection. In the first experiment (N=145), students completed a take-home assignment with the support of an LLM assistant; half of these students were then provided access to an LLM designed to facilitate self-reflection. The results indicated that the students assigned to LLM-guided reflection reported somewhat increased self-confidence compared to peers in a no-reflection control and a non-significant trend towards higher scores on a later assessment. Thematic analysis of students' interactions with the LLM showed that the LLM often affirmed the student's understanding, expanded on the student's reflection, and prompted additional reflection; these behaviors suggest ways LLM-interaction might facilitate reflection. In the second experiment (N=112), we evaluated the impact of LLM-guided self-reflection against other scalable reflection methods, such as questionnaire-based activities and review of key lecture slides, after assignment. Our findings suggest that the students in the questionnaire and LLM-based reflection groups performed equally well and better than those who were only exposed to lecture slides, according to their scores on a proctored exam two weeks later on the same subject matter. These results underscore the utility of LLM-guided reflection and questionnaire-based activities in improving learning outcomes. Our work highlights that focusing solely on the accuracy of LLMs can overlook their potential to enhance metacognitive skills through practices such as self-reflection. We discuss the implications of our research for the learning-at-scale community, highlighting the potential of LLMs to enhance learning experiences through personalized, engaging, and scalable reflection practices.},
  address    = {New York, NY, USA},
  author     = {Kumar, Harsh and Xiao, Ruiwei and Lawson, Benjamin and Musabirov, Ilya and Shi, Jiakai and Wang, Xinyuan and Luo, Huayin and Williams, Joseph Jay and Rafferty, Anna N. and Stamper, John and Liut, Michael},
  booktitle  = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
  doi        = {10.1145/3657604.3662042},
  isbn       = {9798400706332},
  keywords   = {field experiments, human-ai collaboration, large language models, learning engineering, self-reflection},
  location   = {Atlanta, GA, USA},
  numpages   = {12},
  pages      = {86–97},
  publisher  = {Association for Computing Machinery},
  series     = {L@S '24},
  title      = {Supporting Self-Reflection at Scale with Large Language Models: Insights from Randomized Field Experiments in Classrooms},
  url        = {https://doi.org/10.1145/3657604.3662042},
  year       = {2024}
}

@inproceedings{10.1145/3657604.3664660,
  abstract   = {Large language models (LLMs) can provide formative feedback in programming to help students improve the code they have written. We investigate the use of LLMs (GPT-4) to provide formative code feedback in a sophomore-level computer science (CS) course on data structures and algorithms. In three quizzes on recursion, half of the students randomly received GPT-4's feedback, while the other half received feedback from the course instructor. Students resubmitted their code based on the provided feedback. We found that students in the LLM-feedback condition scored higher in resubmissions than those receiving feedback from the instructor. Students perceived the two types of feedback as equally supportive of guiding resubmissions. We discuss the implications of using LLMs to provide formative feedback at scale in CS instruction.},
  address    = {New York, NY, USA},
  author     = {Nguyen, Ha and Stott, Nate and Allan, Vicki},
  booktitle  = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
  doi        = {10.1145/3657604.3664660},
  isbn       = {9798400706332},
  keywords   = {computer science education, feedback, large language models},
  location   = {Atlanta, GA, USA},
  numpages   = {5},
  pages      = {335–339},
  publisher  = {Association for Computing Machinery},
  series     = {L@S '24},
  title      = {Comparing Feedback from Large Language Models and Instructors: Teaching Computer Science at Scale},
  url        = {https://doi.org/10.1145/3657604.3664660},
  year       = {2024}
}

@inproceedings{10.1145/3657604.3664663,
  abstract   = {Supporting novice computer science students in learning the software development life cycle (SDLC) at scale is vital for ensuring the quality of future software systems. However, this presents unique challenges, including the need for effective interactive collaboration and access to diverse skill sets of members in the software development team. To address these problems, we present ''DevCoach'', an online system designed to support students learning the SDLC at scale by interacting with generative agents powered by large language models simulating members with different roles in a software development team. Our preliminary user study results reveal that DevCoach improves the experiences and outcomes for students, with regard to learning concepts in SDLC's ''Plan and Design'' and ''Develop'' phases. We aim to use our findings to enhance DevCoach to support the entire SDLC workflow by incorporating additional simulated roles and enabling students to choose their project topics. Future studies will be conducted in an online Software Engineering class at our institution, aiming to explore and inspire the development of intelligent systems that provide comprehensive SDLC learning experiences to students at scale.},
  address    = {New York, NY, USA},
  author     = {Wang, Tianjia and Ramanujan, Ramaraja and Lu, Yi and Mao, Chenyu and Chen, Yan and Brown, Chris},
  booktitle  = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
  doi        = {10.1145/3657604.3664663},
  isbn       = {9798400706332},
  keywords   = {computer science education, generative ai, software development life cycle, software engineering},
  location   = {Atlanta, GA, USA},
  numpages   = {5},
  pages      = {351–355},
  publisher  = {Association for Computing Machinery},
  series     = {L@S '24},
  title      = {DevCoach: Supporting Students in Learning the Software Development Life Cycle at Scale with Generative Agents},
  url        = {https://doi.org/10.1145/3657604.3664663},
  year       = {2024}
}

@inproceedings{10.1145/3658549.3658566,
  abstract   = {Generative Artificial Intelligence (GAI) has become a hot topic nowadays, as its powerful content generation models enable users to instantly create everything from digital media products to coding examples through simple text queries, providing more possibilities in the field of education. This study aims to investigate the impact of Generative AI intervention in teaching App Inventor programming courses, analyzing the differences between UI materials designed by traditional teachers based on their professional knowledge and experience, and UI materials created by Generative AI in classroom teaching. The study also evaluates the impact of Generative AI on students' learning outcomes and motivation through satisfaction and Technology Acceptance Model (TAM) questionnaires. The results indicate that UI materials produced through Generative AI effectively enhance students' satisfaction with the course and their acceptance of new technologies. Compared to traditional teaching methods, Generative AI significantly saves teachers' time and effort in designing materials while simultaneously improving teaching efficiency and quality.},
  address    = {New York, NY, USA},
  author     = {Ho, Chia-Ling and Liu, Xin-Ying and Qiu, Yu-Wei and Yang, Shih-Yang},
  booktitle  = {Proceedings of the 2024 International Conference on Information Technology, Data Science, and Optimization},
  doi        = {10.1145/3658549.3658566},
  isbn       = {9798400709180},
  keywords   = {Generative artificial intelligence, Intelligent assistant, Learning effectiveness, Programming course, User interface design},
  location   = {Taipei, Taiwan},
  numpages   = {5},
  pages      = {68–72},
  publisher  = {Association for Computing Machinery},
  series     = {I-DO '24},
  title      = {Research on Innovative Applications and Impacts of Using Generative AI for User Interface Design in Programming Courses},
  url        = {https://doi.org/10.1145/3658549.3658566},
  year       = {2024}
}

@inproceedings{10.1145/3661167.3661273,
  abstract   = {Software testing is a challenging topic in software engineering education and requires creative approaches to engage learners. For example, the Code Defenders game has students compete over a Java class under test by writing effective tests and mutants. While such gamified approaches deal with problems of motivation and engagement, students may nevertheless require help to put testing concepts into practice. The recent widespread diffusion of Generative AI and Large Language Models raises the question of whether and how these disruptive technologies could address this problem, for example, by providing explanations of unclear topics and guidance for writing tests. However, such technologies might also be misused or produce inaccurate answers, which would negatively impact learning. To shed more light on this situation, we conducted the first empirical study investigating how students learn and practice new software testing concepts in the context of the Code Defenders testing game, supported by a smart assistant based on a widely known, commercial Large Language Model. Our study shows that students had unrealistic expectations about the smart assistant, “blindly” trusting any output it generated, and often trying to use it to obtain solutions for testing exercises directly. Consequently, students who resorted to the smart assistant more often were less effective and efficient than those who did not. For instance, they wrote 8.6% fewer tests, and their tests were not useful in 78.0% of the cases. We conclude that giving unrestricted and unguided access to Large Language Models might generally impair learning. Thus, we believe our study helps to raise awareness about the implications of using Generative AI and Large Language Models in Computer Science Education and provides guidance towards developing better and smarter learning tools.},
  address    = {New York, NY, USA},
  author     = {Mezzaro, Simone and Gambi, Alessio and Fraser, Gordon},
  booktitle  = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
  doi        = {10.1145/3661167.3661273},
  isbn       = {9798400717017},
  keywords   = {ChatGPT, Computer Science Education, Generative AI, Smart Learning Assistant},
  location   = {Salerno, Italy},
  numpages   = {10},
  pages      = {555–564},
  publisher  = {Association for Computing Machinery},
  series     = {EASE '24},
  title      = {An Empirical Study on How Large Language Models Impact Software Testing Learning},
  url        = {https://doi.org/10.1145/3661167.3661273},
  year       = {2024}
}

@inproceedings{10.1145/3663649.3664368,
  abstract   = {SQL is the standard for database query languages and is taught in most introductory database courses. Query languages are illustrated and tested through toy examples: small, accessible, instances of databases. These are not always engaging, but coming up with new examples and questions is time-consuming. Existing research in Computer Science Education has shown that Large Language Models (LLMs) can generate coding exercises. However, this has not been demonstrated for SQL yet but could save teachers much time. In this paper, we study whether it is feasible to have ChatGPT-3.5 generate database schemas and associated SQL questions for teachers through a two-part study. Through a survey of educators, we found that creating a story and database schema for the SQL part is more time-consuming than the questions themselves. In our prompt engineering study, we identified prompts that were successful at creating database schemas, mock data, and exercises. However, although ChatGPT could help reduce the time required to create exams, some participants indicated that they are skeptical about using LLMs.},
  address    = {New York, NY, USA},
  author     = {Aerts, Willem and Fletcher, George and Miedema, Daphne},
  booktitle  = {Proceedings of the 3rd International Workshop on Data Systems Education: Bridging Education Practice with Education Research},
  doi        = {10.1145/3663649.3664368},
  isbn       = {9798400706783},
  keywords   = {Assessment, ChatGPT, Education, LLM, SQL},
  location   = {Santiago, AA, Chile},
  numpages   = {7},
  pages      = {13–19},
  publisher  = {Association for Computing Machinery},
  series     = {DataEd '24},
  title      = {A Feasibility Study on Automated SQL Exercise Generation with ChatGPT-3.5},
  url        = {https://doi.org/10.1145/3663649.3664368},
  year       = {2024}
}

@inproceedings{10.1145/3663649.3664371,
  abstract   = {Large Language Models (LLMs) have sparked a drastic improvement in the ways computers can understand, process, and generate language. As LLM-based offerings become mainstream, we explore the incorporation of such LLMs into introductory or undergraduate database systems education. Students and instructors are both faced with the calculator dilemma: while the use of LLM-based tools may “solve” tasks such as assignments and exams, do they impede or accelerate the learning itself? We review deficiencies of using existing off-the-shelf tools for learning, and further articulate the differentiated needs of database systems students as opposed to trained data practitioners. Building on our exploration, we outline a vision that integrates LLMs into database education in a principled manner, keeping pedagogical best practices in mind. If implemented correctly, we posit that LLMs can drastically amplify the impact of existing instruction, minimizing costs and barriers towards learning database systems fundamentals.},
  address    = {New York, NY, USA},
  author     = {Prakash, Kishore and Rao, Shashwat and Hamza, Rayan and Lukich, Jack and Chaudhari, Vatsal and Nandi, Arnab},
  booktitle  = {Proceedings of the 3rd International Workshop on Data Systems Education: Bridging Education Practice with Education Research},
  doi        = {10.1145/3663649.3664371},
  isbn       = {9798400706783},
  keywords   = {ChatGPT, database systems education, foundation models, intro to db, large language models, llm, undergrad databases},
  location   = {Santiago, AA, Chile},
  numpages   = {7},
  pages      = {33–39},
  publisher  = {Association for Computing Machinery},
  series     = {DataEd '24},
  title      = {Integrating LLMs into Database Systems Education},
  url        = {https://doi.org/10.1145/3663649.3664371},
  year       = {2024}
}

@inproceedings{10.1145/3674805.3690741,
  abstract   = {Large Language Models (LLM) have rapidly affirmed in the latest years as a means to support or substitute human actors in a variety of tasks. LLM agents can generate valid software models, because of their inherent ability in evaluating textual requirements provided to them in the form of prompts. The goal of this work is to evaluate the capability of LLM agents to correctly generate UML class diagrams in activities of Requirements Modeling in the field of Software Engineering. Our aim is to evaluate LLMs in an educational setting, i.e., understanding how valuable are the results of LLMs when compared to results made by human actors, and how valuable can LLM be to generate sample solutions to provide to students. For that purpose, we collected 20 exercises from a diverse set of web sources and compared the models generated by a human and an LLM solver in terms of syntactic, semantic, pragmatic correctness, and distance from a provided reference solution. Our results show that the solutions generated by an LLM solver typically present a significantly higher number of errors in terms of semantic quality and textual difference against the provided reference solution, while no significant difference is found in syntactic and pragmatic quality. We can therefore conclude that, with a limited amount of errors mostly related to the textual content of the solution, UML diagrams generated by LLM agents have the same level of understandability as those generated by humans, and exhibit the same frequency in violating rules of UML Class Diagrams.},
  address    = {New York, NY, USA},
  author     = {De Bari, Daniele and Garaccione, Giacomo and Coppola, Riccardo and Torchiano, Marco and Ardito, Luca},
  booktitle  = {Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
  doi        = {10.1145/3674805.3690741},
  isbn       = {9798400710476},
  keywords   = {Artificial Intelligence, Class Diagrams, Large Language Models, Software Modeling},
  location   = {Barcelona, Spain},
  numpages   = {7},
  pages      = {393–399},
  publisher  = {Association for Computing Machinery},
  series     = {ESEM '24},
  title      = {Evaluating Large Language Models in Exercises of UML Class Diagram Modeling},
  url        = {https://doi.org/10.1145/3674805.3690741},
  year       = {2024}
}

@inproceedings{10.1145/3677045.3685439,
  abstract   = {In this paper, we present how we used generative AI (GenAI) as a pedagogical tool for students taking a course in tangible interaction design. In this course, the students design different physical-digital objects (PDOs) to learn designing, sketching and prototyping with code and hardware. However, due to the short course duration these PDOs are not evaluated or explored with any kind of field or user study. Therefore we gave the students the exercise of doing user interviews with GenAI to explore their design ideas further. With this paper, we contribute a description and the outcomes of this approach, and highlight the pedagogical implications for student learning.},
  address    = {New York, NY, USA},
  articleno  = {23},
  author     = {R\"{o}nnberg, Niklas and B\"{o}r\"{u}tecene, Ahmet},
  booktitle  = {Adjunct Proceedings of the 2024 Nordic Conference on Human-Computer Interaction},
  doi        = {10.1145/3677045.3685439},
  isbn       = {9798400709654},
  keywords   = {Design, Education, Field study, Generative AI, User interview},
  location   = {Uppsala, Sweden},
  numpages   = {5},
  publisher  = {Association for Computing Machinery},
  series     = {NordiCHI '24 Adjunct},
  title      = {Use of Generative AI for Fictional Field Studies in Design Courses},
  url        = {https://doi.org/10.1145/3677045.3685439},
  year       = {2024}
}

@inproceedings{10.1145/3680533.3697064,
  abstract   = {CG (Computer Graphics) is a popular field of CS (Computer Science), but many students find this topic difficult due to it requiring a large number of skills, such as mathematics, programming, geometric reasoning, and creativity. Over the past few years, researchers have investigated ways to harness the power of GenAI (Generative Artificial Intelligence) to improve teaching. In CS, much of the research has focused on introductory computing. A recent study evaluating the performance of an LLM (Large Language Model), GPT-4 (text-only), on CG questions, indicated poor performance and reliance on detailed descriptions of image content, which often required considerable insight from the user to return reasonable results. So far, no studies have investigated the abilities of LMMs (Large Multimodal Models), or multimodal LLMs, to solve CG questions and how these abilities can be used to improve teaching.In this study, we construct two datasets of CG questions requiring varying degrees of visual perception skills and geometric reasoning skills, and evaluate the current state-of-the-art LMM, GPT-4o, on the two datasets. We find that although GPT-4o exhibits great potential in solving questions with visual information independently, major limitations still exist to the accuracy and quality of the generated results. We propose several novel approaches for CG educators to incorporate GenAI into CG teaching despite these limitations. We hope that our guidelines further encourage learning and engagement in CG classrooms.},
  address    = {New York, NY, USA},
  articleno  = {5},
  author     = {Feng, Tony Haoran and Denny, Paul and W\"{u}nsche, Burkhard C. and Luxton-Reilly, Andrew and Whalley, Jacqueline},
  booktitle  = {SIGGRAPH Asia 2024 Educator's Forum},
  doi        = {10.1145/3680533.3697064},
  isbn       = {9798400711367},
  keywords   = {Large Language Models, LLMs, Large Multimodal Models, LMMs, Visual Language Models, VLMs, Generative Artificial Intelligence, GenAI, GPT-4, GPT-4o, Visual Perception, Geometric Reasoning, Computer Graphics, Computing Education, Evaluation, Assessment},
  location   = {},
  numpages   = {8},
  publisher  = {Association for Computing Machinery},
  series     = {SA '24},
  title      = {An Eye for an AI: Evaluating GPT-4o's Visual Perception Skills and Geometric Reasoning Skills Using Computer Graphics Questions},
  url        = {https://doi.org/10.1145/3680533.3697064},
  year       = {2024}
}

@inproceedings{10.1145/3686852.3686887,
  abstract   = {The adoption of artificial intelligence (AI) technologies by businesses and corporations is rising. AI technologies continue to be adopted in cybersecurity for both defensive and offensive strategies. However, threat actors also persist in utilizing these technologies to enhance the speed, accuracy, and sophistication of their attacks. Hence, it is essential to train the next generation of cybersecurity learners not only on how to use AI technology but also on how to leverage these technologies to enhance the efficiency of their work. This extended abstract describes our exploratory work on the use of generative AI-based pedagogical approaches in cybersecurity education. This extended abstract will describe some preliminary findings on large language model-powered pedagogical approaches to cybersecurity education and training. These approaches will help cybersecurity educators enhance their teaching methods to equip learners with the essential skills needed to succeed in the dynamic field of cybersecurity.},
  address    = {New York, NY, USA},
  author     = {Chhetri, Chola},
  booktitle  = {Proceedings of the 25th Annual Conference on Information Technology Education},
  doi        = {10.1145/3686852.3686887},
  isbn       = {9798400711060},
  keywords   = {AI, Artificial intelligence, GenAI, LLM, cybersecurity, education., generative AI, large language models},
  location   = {El Paso, TX, USA},
  numpages   = {4},
  pages      = {163–166},
  publisher  = {Association for Computing Machinery},
  series     = {SIGITE '24},
  title      = {Exploring Large Language Model-Powered Pedagogical Approaches to Cybersecurity Education},
  url        = {https://doi.org/10.1145/3686852.3686887},
  year       = {2024}
}

@article{10.1145/3687038,
  abstract   = {Personalized chatbot-based teaching assistants can be crucial in addressing increasing classroom sizes, especially where direct teacher presence is limited. Large language models (LLMs) offer a promising avenue, with increasing research exploring their educational utility. However, the challenge lies not only in establishing the efficacy of LLMs but also in discerning the nuances of interaction between learners and these models, which impact learners' engagement and results. We conducted a formative study in an undergraduate computer science classroom (N=145) and a controlled experiment on Prolific (N=356) to explore the impact of four pedagogically informed guidance strategies on the learners' performance, confidence and trust in LLMs. Direct LLM answers marginally improved performance, while refining student solutions fostered trust. Structured guidance reduced random queries as well as instances of students copy-pasting assignment questions to the LLM. Our work highlights the role that teachers can play in shaping LLM-supported learning environments.},
  address    = {New York, NY, USA},
  articleno  = {499},
  author     = {Kumar, Harsh and Musabirov, Ilya and Reza, Mohi and Shi, Jiakai and Wang, Xinyuan and Williams, Joseph Jay and Kuzminykh, Anastasia and Liut, Michael},
  doi        = {10.1145/3687038},
  issue_date = {November 2024},
  journal    = {Proc. ACM Hum.-Comput. Interact.},
  keywords   = {artificial intelligence in education, collaborative learning with ai, human-ai collaboration, large language models, transparency, tutoring systems},
  month      = {November},
  number     = {CSCW2},
  numpages   = {30},
  publisher  = {Association for Computing Machinery},
  title      = {Guiding Students in Using LLMs in Supported Learning Environments: Effects on Interaction Dynamics, Learner Performance, Confidence, and Trust},
  url        = {https://doi.org/10.1145/3687038},
  volume     = {8},
  year       = {2024}
}

@inproceedings{10.1145/3691620.3695062,
  abstract   = {Providing personalized and timely feedback for student's programming assignments is useful for programming education. Automated program repair (APR) techniques have been used to fix the bugs in programming assignments, where the Large Language Models (LLMs) based approaches have shown promising results. Given the growing complexity of identifying and fixing bugs in advanced programming assignments, current fine-tuning strategies for APR are inadequate in guiding the LLM to identify bugs and make accurate edits during the generative repair process. Furthermore, the autoregressive decoding approach employed by the LLM could potentially impede the efficiency of the repair, thereby hindering the ability to provide timely feedback. To tackle these challenges, we propose FastFixer, an efficient and effective approach for programming assignment repair. To assist the LLM in accurately identifying and repairing bugs, we first propose a novel repair-oriented fine-tuning strategy, aiming to enhance the LLM's attention towards learning how to generate the necessary patch and its associated context. Furthermore, to speed up the patch generation, we propose an inference acceleration approach that is specifically tailored for the program repair task. The evaluation results demonstrate that FastFixer obtains an overall improvement of 20.46% in assignment fixing when compared to the state-of-the-art baseline. Considering the repair efficiency, FastFixer achieves a remarkable inference speedup of 16.67\texttimes{} compared to the autoregressive decoding algorithm.},
  address    = {New York, NY, USA},
  author     = {Liu, Fang and Liu, Zhenwei and Zhao, Qianhui and Jiang, Jing and Zhang, Li and Sun, Zian and Li, Ge and Li, Zhongqi and Ma, Yuchi},
  booktitle  = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
  doi        = {10.1145/3691620.3695062},
  isbn       = {9798400712487},
  keywords   = {automated program repair, large language models, programming education, inference acceleration},
  location   = {Sacramento, CA, USA},
  numpages   = {12},
  pages      = {669–680},
  publisher  = {Association for Computing Machinery},
  series     = {ASE '24},
  title      = {FastFixer: An Efficient and Effective Approach for Repairing Programming Assignments},
  url        = {https://doi.org/10.1145/3691620.3695062},
  year       = {2024}
}

@inproceedings{10.1145/3699538.3699541,
  abstract   = {Large language models (LLMs) provide round-the-clock personalized programming assistance, unlike course instructors or traditional online information sources such as Stack Overflow. While LLMs can aid in code generation, concerns about over-reliance and the impact on learning persist. This study discusses students’ experiences with LLMs in a software project course where students were allowed to use LLMs freely except for unit test generation. We conducted surveys during course instances in autumn 2023 and spring 2024. The surveys assessed the extent of LLM usage, methods of application, and perceived impact on learning. Results indicate diverse usage patterns, with many students finding LLMs beneficial for efficiency and problem-solving, though over-reliance and poor-quality outputs were noted concerns. The usage patterns can be linked to course performance and time spent on the project.},
  address    = {New York, NY, USA},
  articleno  = {23},
  author     = {Korpimies, Kai and Laaksonen, Antti and Luukkainen, Matti},
  booktitle  = {Proceedings of the 24th Koli Calling International Conference on Computing Education Research},
  doi        = {10.1145/3699538.3699541},
  isbn       = {9798400710384},
  keywords   = {Large language models, Computer Science Education, User Study, Code generation, Software project},
  location   = {},
  numpages   = {7},
  publisher  = {Association for Computing Machinery},
  series     = {Koli Calling '24},
  title      = {Unrestricted Use of LLMs in a Software Project Course: Student Perceptions on Learning and Impact on Course Performance},
  url        = {https://doi.org/10.1145/3699538.3699541},
  year       = {2024}
}

@inproceedings{10.1145/3701625.3701657,
  abstract   = {Context: Exploratory testing is essential in the software validation process as a way to find unexpected and critical failures in a short time, complementing documented functional test cases. However, creating scenarios to explore the software (such as test charters) can be time-consuming, and depending on the team’s experience, it may lack adequate coverage of functionalities and scenarios that target specific user profiles of the application. Objective: This article investigates how AI, through LLMs (Large Language Models), can assist in creating exploratory test charters that reflect the characteristics and needs of different user personas. Method: To achieve this, an experimental study was conducted where personas were used as input in ChatGPT 3.5 to generate exploratory test charters. The effectiveness of the approach was evaluated by Software Engineering students, who analyzed the performance and usefulness of the generated charters through a questionnaire based on the TAM model, supplemented by qualitative and quantitative analyses. Results: Data analysis indicated positive acceptance of ChatGPT 3.5 by the participants, highlighting its ease of use and perceived usefulness. Conclusion: This study contributes to the field of Software Engineering by demonstrating a practical application of artificial intelligence in the automated generation of test charters. ChatGPT 3.5 has proven to be a promising tool to support the creation of personalized exploratory test charters, contributing to software quality improvement. The integration of artificial intelligence techniques with user-centered design methods can significantly optimize the software testing process.},
  address    = {New York, NY, USA},
  author     = {de Almeida, \'{A}gatha and Collins, Eliane and Oran, Ana Carolina},
  booktitle  = {Proceedings of the XXIII Brazilian Symposium on Software Quality},
  doi        = {10.1145/3701625.3701657},
  isbn       = {9798400717772},
  keywords   = {Exploratory Testing, ChatGPT, Personas, Software Quality, Artificial Intelligence},
  location   = {},
  numpages   = {10},
  pages      = {179–188},
  publisher  = {Association for Computing Machinery},
  series     = {SBQS '24},
  title      = {AI in Service of Software Quality: How ChatGPT and Personas Are Transforming Exploratory Testing},
  url        = {https://doi.org/10.1145/3701625.3701657},
  year       = {2024}
}

@inproceedings{10.1145/3701625.3701681,
  abstract   = {Refactoring presents a complex computational challenge, and its learning is intricate, requiring a solid foundation in computational thinking, programming and object-oriented concepts. Moreover, making students realize the importance and benefits of refactoring is also challenging. To address this complexity, we introduce a refactoring teaching method based on Generative Artificial Intelligence (GAI), incorporating single-loop and double-loop learning principles, focusing on fostering deeper and critical learning. We used ChatGPT, a GAI-based tool, and conducted an eight-week mixed-methods study involving 23 computer science undergraduate students. The study involved applying four distinct projects extracted from GitHub, where participants were tasked with identifying code smells and performing the necessary refactoring to improve code quality. The primary focus was on identifying both the positive and negative aspects of the method, as well as delineating the computational thinking characteristics developed during the process. The results indicate that the use of ChatGPT facilitated the learning of refactoring, contributing to the development of numerous computational thinking skills, especially problem formulation, decomposition, and abstraction. Thus, this paper contributes a GAI-based teaching method along with evidence on how it helps students develop refactoring skills.},
  address    = {New York, NY, USA},
  author     = {Menolli, Andr\'{e} and Strik, Bruno and Rodrigues, Luiz},
  booktitle  = {Proceedings of the XXIII Brazilian Symposium on Software Quality},
  doi        = {10.1145/3701625.3701681},
  isbn       = {9798400717772},
  keywords   = {Generative Artificial Intelligence, ChatGPT, Refactory, Higher Education, Teaching, Computational Thinking},
  location   = {},
  numpages   = {12},
  pages      = {563–574},
  publisher  = {Association for Computing Machinery},
  series     = {SBQS '24},
  title      = {Teaching Refactoring to Improve Code Quality with ChatGPT: An Experience Report in Undergraduate Lessons},
  url        = {https://doi.org/10.1145/3701625.3701681},
  year       = {2024}
}

@inproceedings{10.1145/3701625.3701684,
  abstract   = {CONTEXT: Teaching project management is complex, and students often do not feel engaged or motivated. Professors can use many initiatives to improve the teaching and learning process. Tools like ChatGPT, when integrated into education, have generated considerable interest due to their potential to enrich students’ learning experiences. GOAL: This paper analyzes the impacts of using ChatGPT as a complementary tool in teaching Project Management in the Software Engineering course, highlighting its benefits and challenges. METHOD: We performed an exploratory study to identify the effects of using ChatGPT in teaching project management, evaluating learning, productivity, teamwork, student perceptions, and future expectations. RESULTS: The results indicate that ChatGPT contributed to improving content comprehension, developing critical skills, accelerating production, improving collaboration and communication, and increasing student engagement. However, challenges related to misuse and dependence on the tool were also identified. CONCLUSION: The integration of ChatGPT in teaching project management has shown promise, promoting a richer and more collaborative learning experience. The insights obtained provide directions for future implementations and research on the use of AI in project management education.},
  address    = {New York, NY, USA},
  author     = {Oran, Ana Carolina and Montenegro, Let\'{\i}cia Braga and Schuster, Hellmut Alencar and Duarte, Jos\'{e} Carlos and Silva, Williamson and Lima, Rayfran Rocha},
  booktitle  = {Proceedings of the XXIII Brazilian Symposium on Software Quality},
  doi        = {10.1145/3701625.3701684},
  isbn       = {9798400717772},
  keywords   = {Project management education, Software project management, ChatGPT, AI-assisted learning, Software engineering},
  location   = {},
  numpages   = {9},
  pages      = {596–604},
  publisher  = {Association for Computing Machinery},
  series     = {SBQS '24},
  title      = {Integrating ChatGPT in Project Management Education: Benefits and Challenges in the Academic Environment},
  url        = {https://doi.org/10.1145/3701625.3701684},
  year       = {2024}
}

@inproceedings{10.1145/3701625.3701687,
  abstract   = {Large Language Models (LLMs) are becoming common in educational settings. This trend presents a challenge for teachers, who must focus on teaching the proper usage of LLMs. In the context of Software Engineering (SE), ChatGPT can support various software development tasks. This work reports an experience with students using ChatGPT 3.5 to support the Requirements Engineering (RE) phase. We conducted a two-phase study with 42 students. First, the students elicited requirements for systems using RE techniques. Then, the students used ChatGPT 3.5 to generate requirements for the same systems. Finally, they compared both sets of requirements based on equivalence, innovation, and relevance. On average, 65.26% of the requirements generated by ChatGPT were considered equivalents to the requirements the students had elicited. However, students reported that ChatGPT generates broad and non-specific requirements. Students also reported that ChatGPT 3.5 can foster the requirements elicitation, but it is necessary to establish well-defined prompts for generating requirements.},
  address    = {New York, NY, USA},
  author     = {Sampaio, Savio Sousa and Lima, M\'{a}rcia Sampaio and de Souza, Eriky Rodrigues and Meireles, Maria Alcimar and Pessoa, Marcela Savia and Conte, Tayana Uchoa},
  booktitle  = {Proceedings of the XXIII Brazilian Symposium on Software Quality},
  doi        = {10.1145/3701625.3701687},
  isbn       = {9798400717772},
  keywords   = {Requirement Elicitation, ChatGPT 3.5, Software engineering education},
  location   = {},
  numpages   = {11},
  pages      = {624–634},
  publisher  = {Association for Computing Machinery},
  series     = {SBQS '24},
  title      = {Exploring the Use of Large Language Models in Requirements Engineering Education: An Experience Report with ChatGPT 3.5},
  url        = {https://doi.org/10.1145/3701625.3701687},
  year       = {2024}
}

@inproceedings{10.1145/3701716.3715199,
  abstract   = {Operating Systems (OS) courses are among the most challenging in computer science education due to the complexity of internal structures and the diversity of running environments. Traditional teaching methods often fail to address the diverse backgrounds, learning speeds, and practical needs of students. To tackle these challenges, we present SortingHat, a personalized digital teaching assistant tailored specifically for OS education. SortingHat integrates advanced AI technologies, including a retrieval-augmented generation (RAG) framework and multi-agent reinforcement learning (MARL), to deliver adaptive, scalable, and effective educational support. SortingHat features a 3D digital human interface powered by large language models (LLMs) to provide personalized, empathetic, and context-aware guidance. It generates tailored exercises based on each student's learning history and academic performance, reinforcing weak areas and challenging advanced concepts. Additionally, the system incorporates a robust evaluation pipeline that ensures fair, consistent, and unbiased grading of student submissions while delivering personalized, actionable feedback for improvement. By combining personalized guidance, adaptive content creation, and automated assessment, SortingHat transforms OS education into an engaging, immersive, and scalable experience.},
  address    = {New York, NY, USA},
  author     = {Zhang, Yifan and Zhao, Xinkui and Wang, Zuxin and Zhou, Zhengyi and Cheng, Guanjie and Deng, Shuiguang and Yin, Jianwei},
  booktitle  = {Companion Proceedings of the ACM on Web Conference 2025},
  doi        = {10.1145/3701716.3715199},
  isbn       = {9798400713316},
  keywords   = {digital human, education, large language models, multi agent reinforcement learning, retrieval augmented generation},
  location   = {Sydney NSW, Australia},
  numpages   = {4},
  pages      = {2951–2954},
  publisher  = {Association for Computing Machinery},
  series     = {WWW '25},
  title      = {SortingHat: Redefining Operating Systems Education with a Tailored Digital Teaching Assistant},
  url        = {https://doi.org/10.1145/3701716.3715199},
  year       = {2025}
}

@inproceedings{10.1145/3702163.3702182,
  abstract   = {Prompt and sufficient feedback is essential for students' academic learning since it enables them to review their learning techniques and improve their areas of weakness. Nevertheless, delivering personalised feedback to every student continues to be difficult&nbsp;for teachers due to its demanding and time-intensive nature. While automated feedback systems are available, their primary focus is providing feedback on a single subject, and most of them utilise statistical analysis or traditional machine learning techniques to provide feedback. Moreover, no feedback model utilises the same criteria to generate text-based feedback for more than one subject. Generative artificial intelligence (GEN AI) has recently made incredible progress, and large language models (LLMs) can retain the context from the vast amount of text. Hence, this research presents a framework that employs an innovative technique to offer text-based feedback to students in different fields of study. This framework employs two LLMs, one for generating the feedback and another for categorising it into separate subjects using suitable headings for structural organising. Consequently, the output produced by this technology corresponds to the original tone of the teacher.},
  address    = {New York, NY, USA},
  author     = {Yaqub, Irfan and Chen, Zhiyuan and Liao, Iman Yi and Maul, Tomas and Seow, Hsin-Vonn and Chandesa, Tissa},
  booktitle  = {Proceedings of the 2024 16th International Conference on Education Technology and Computers},
  doi        = {10.1145/3702163.3702182},
  isbn       = {9798400717819},
  keywords   = {Deep Learning Artificial Intelligence, Generative Artificial Intelligence, Large Language Model},
  location   = {},
  numpages   = {8},
  pages      = {130–137},
  publisher  = {Association for Computing Machinery},
  series     = {ICETC '24},
  title      = {A Novel Framework using Large Language Models to Automate Coursework Feedback for Computer Science modules},
  url        = {https://doi.org/10.1145/3702163.3702182},
  year       = {2025}
}

@inproceedings{10.1145/3702212.3702214,
  abstract   = {This paper explores how Generative AI (GenAI) can be introduced within summative assessment components in software engineering education. We present an example of an assessment which allows learners to use GenAI in a freeform, constructionist manner, as part of a large, software development project. This work is inspired by previously executed AI-focused assessments and surveys, which explicitly indicate that learners on an Applied Software Engineering Degree Apprenticeship Programme want to formally learn how to use GenAI tools when programming and their employers want to see these skills from graduates. The learning outcome of the assignment was for learners to explore a typical developmental pipeline as a solo developer, moving from design to development to finished product. Learners were marked exclusively on their end product and understanding of application components, not the written code itself, resulting in an assessment where the end product and project were prioritised over foundational code (which was adequately assessed in other components). The results show that all learners used GenAI to some extent during their project, and in all cases, they found it beneficial for large programming tasks. Learners were generally able to produce a larger, more comprehensive and more ambitious project, compared to previous years. It is proposed that removing the barrier to GenAI - and demystifying it - can encourage a constructionist approach to its use, and normalise it as a potential tool for programming.},
  address    = {New York, NY, USA},
  author     = {Clift, Lee and Petrovska, Olga},
  booktitle  = {Proceedings of the 9th Conference on Computing Education Practice},
  doi        = {10.1145/3702212.3702214},
  isbn       = {9798400711725},
  keywords   = {GenAI, software engineering, education, apprenticeship},
  location   = {},
  numpages   = {4},
  pages      = {5–8},
  publisher  = {Association for Computing Machinery},
  series     = {CEP '25},
  title      = {Learning without Limits: Analysing the Usage of Generative AI in a Summative Assessment},
  url        = {https://doi.org/10.1145/3702212.3702214},
  year       = {2025}
}

@inproceedings{10.1145/3706599.3720291,
  abstract   = {Integrating large language models (LLMs) like ChatGPT into computer science education offers transformative potential for complex courses such as data structures and algorithms (DSA). This study examines ChatGPT as a supplementary tool for teaching assistants (TAs), guided by structured prompts and human oversight, to enhance instruction and student outcomes. A controlled experiment compared traditional TA-led instruction with a hybrid approach where TAs used ChatGPT-4o and ChatGPT o1 to generate exercises, clarify concepts, and provide feedback. Structured prompts emphasized problem decomposition, real-world context, and code examples, enabling tailored support while mitigating over-reliance on AI. Results demonstrated the hybrid approach’s efficacy, with students in the ChatGPT-assisted group scoring 16.50 points higher on average and excelling in advanced topics. However, ChatGPT’s limitations necessitated TA verification. This framework highlights the dual role of LLMs: augmenting TA efficiency while ensuring accuracy through human oversight, offering a scalable solution for human-AI collaboration in education.},
  address    = {New York, NY, USA},
  articleno  = {567},
  author     = {Jamie, Pooriya and HajiHashemi, Reyhaneh and Alipour, Sharareh},
  booktitle  = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
  doi        = {10.1145/3706599.3720291},
  isbn       = {9798400713958},
  keywords   = {LLMs, ChatGPT, Teaching Assistant, Data Structures and Algorithms Course, Education},
  location   = {},
  numpages   = {7},
  publisher  = {Association for Computing Machinery},
  series     = {CHI EA '25},
  title      = {Utilizing ChatGPT in a Data Structures and Algorithms Course: A Teaching Assistant's Perspective},
  url        = {https://doi.org/10.1145/3706599.3720291},
  year       = {2025}
}

@inproceedings{10.1145/3713081.3731731,
  abstract   = {Compiler testing is critical and indispensable to improve the correctness of compilers. Spurred by recent advancements in Large Language Models (LLMs), LLM-based compiler testing techniques such as Fuzz4All, have demonstrated their potential in uncovering real bugs in diverse compilers and reducing the required engineering efforts in designing program generators. Given the continuous evolution of LLMs and the emergence of new LLM-based approaches, establishing robust baselines is crucial for rigorous evaluation and driving future advancements in this promising research direction.To this end, we introduce Kitten, a mutation-based, language-agnostic program generator. Kitten leverages a corpus of seed programs, analogous to the training set for LLMs, and utilizes the target language's syntax, akin to the knowledge learned by LLMs. Furthermore, Kitten's mutation operators can generate diverse test programs, demonstrating a behavior analogous to the ability of LLM inference to generate new code.Our evaluations demonstrate that, using existing compiler test suites as seed programs, Kitten outperforms Fuzz4All in terms of code coverage and bug detection capabilities. Within 24 hours, Kitten achieved 48.3%, 9.9%, and 33.8% higher coverage than Fuzz4All on GCC, LLVM, and Rustc, respectively, while identifying an average of 19.3, 20.3, and 15.7 bugs in these compilers across three runs. Over the course of nine months dedicated to Kitten's development and testing, we identified a total of 328 across the compilers GCC, LLVM, Rustc, Solc, JerryScript, scalac, and slang, of which 310 have been confirmed or fixed. We strongly believe that Kitten serves as an effective baseline, enabling the identification of limitations within existing LLM-based approaches and consequently driving advancements in this promising research direction.},
  address    = {New York, NY, USA},
  author     = {Xie, Yuanmin and Xu, Zhenyang and Tian, Yongqiang and Zhou, Min and Zhou, Xintong and Sun, Chengnian},
  booktitle  = {Proceedings of the 34th ACM SIGSOFT International Symposium on Software Testing and Analysis},
  doi        = {10.1145/3713081.3731731},
  isbn       = {9798400714740},
  keywords   = {compiler testing, language-agnostic code generation, benchmarking},
  location   = {Clarion Hotel Trondheim, Trondheim, Norway},
  numpages   = {5},
  pages      = {21–25},
  publisher  = {Association for Computing Machinery},
  series     = {ISSTA Companion '25},
  title      = {Kitten: A Simple Yet Effective Baseline for Evaluating LLM-Based Compiler Testing Techniques},
  url        = {https://doi.org/10.1145/3713081.3731731},
  year       = {2025}
}

@inproceedings{10.1145/3716640.3716654,
  abstract   = {Recent advancements in generative AI systems have raised concerns about academic integrity among educators. Beyond excelling at solving programming problems and text-based multiple-choice questions, recent research has also found that large multimodal models (LMMs) can solve Parsons problems based only on an image. However, such problems are still inherently text-based and rely on the capabilities of the models to convert the images of code blocks to their corresponding text. In this paper, we further investigate the capabilities of LMMs to solve graph and tree data structure problems based only on images. To achieve this, we computationally construct and evaluate a novel benchmark dataset comprising 9,072 samples of diverse graph and tree data structure tasks to assess the performance of the GPT-4o, GPT-4 with Vision (GPT-4V), Gemini 1.5 Pro, Gemini 1.5 Flash, Gemini 1.0 Pro Vision, and Claude 3 model families. GPT–4o and Gemini 1.5 Flash performed best on trees and graphs respectively. GPT-4o achieved 87.6% accuracy on tree samples, while Gemini 1.5 Pro, achieved 76.9% accuracy on graph samples. Our findings highlight the influence of structural and visual variations on model performance. This research not only introduces an LMM benchmark to facilitate replication and further exploration but also underscores the potential of LMMs in solving complex computing problems, with important implications for pedagogy and assessment practices.},
  address    = {New York, NY, USA},
  author     = {Gutierrez, Sebastian and Hou, Irene and Lee, Jihye and Angelikas, Kenneth and Man, Owen and Mettille, Sophia and Prather, James and Denny, Paul and MacNeil, Stephen},
  booktitle  = {Proceedings of the 27th Australasian Computing Education Conference},
  doi        = {10.1145/3716640.3716654},
  isbn       = {9798400714252},
  keywords   = {Generative AI, Academic Integrity, Computing Education, Large Multimodal Models, LMMs, Large Language Models, LLMs},
  location   = {},
  numpages   = {10},
  pages      = {124–133},
  publisher  = {Association for Computing Machinery},
  series     = {ACE '25},
  title      = {Seeing the Forest and the Trees: Solving Visual Graph and Tree Based Data Structure Problems using Large Multimodal Models},
  url        = {https://doi.org/10.1145/3716640.3716654},
  year       = {2025}
}

@inproceedings{10.1145/3716640.3716657,
  abstract   = {This study examines the use of large language models (LLMs) by undergraduate and graduate students for programming assignments in advanced computing classes. Unlike existing research, which primarily focuses on introductory classes and lacks in-depth analysis of actual student-LLM interactions, our work fills this gap. We conducted a comprehensive analysis involving 411 students from a Distributed Systems class at an Indian university, where they completed three programming assignments and shared their experiences through Google Form surveys and interviews.Our findings reveal that students leveraged LLMs for a variety of tasks, including code generation, debugging, conceptual inquiries, and test case creation. They employed a spectrum of prompting strategies, ranging from basic contextual prompts to advanced techniques like chain-of-thought prompting and iterative refinement. While students generally viewed LLMs as beneficial for enhancing productivity and learning, we noted a concerning trend of over-reliance, with many students submitting entire assignment descriptions to obtain complete solutions. Given the increasing use of LLMs in the software industry, our study highlights the need to update undergraduate curricula to include training on effective prompting strategies and to raise awareness about the benefits and potential drawbacks of LLM usage in academic settings.},
  address    = {New York, NY, USA},
  author     = {Arora, Utkarsh and Garg, Anupam and Gupta, Aryan and Jain, Samyak and Mehta, Ronit and Oberoi, Rupin and Prachi and Raina, Aryaman and Saini, Manav and Sharma, Sachin and Singh, Jaskaran and Tyagi, Sarthak and Kumar, Dhruv},
  booktitle  = {Proceedings of the 27th Australasian Computing Education Conference},
  doi        = {10.1145/3716640.3716657},
  isbn       = {9798400714252},
  keywords   = {Large Language Models, Computing Education, User Study},
  location   = {},
  numpages   = {10},
  pages      = {154–163},
  publisher  = {Association for Computing Machinery},
  series     = {ACE '25},
  title      = {Analyzing LLM Usage in an Advanced Computing Class in India},
  url        = {https://doi.org/10.1145/3716640.3716657},
  year       = {2025}
}

@article{10.1145/3717512.3717515,
  abstract   = {Large Language Models (LLMs) have demonstrated impressive abilities in tackling tasks across numerous domains. The capabilities of LLMs could potentially be applied to various computer networking tasks, including network synthesis, management, debugging, security, and education. However, LLMs can be unreliable: they are prone to reasoning errors and may hallucinate incorrect information. Their effectiveness and limitations in computer networking tasks remain unclear. In this paper, we attempt to understand the capabilities and limitations of LLMs in network applications. We evaluate misunderstandings regarding networking related concepts across 3 LLMs over 500 questions. We assess the reliability, explain-ability, and stability of LLM responses to networking questions. Furthermore, we investigate errors made, analyzing their cause, detectability, effects, and potential mitigation strategies.},
  address    = {New York, NY, USA},
  author     = {Anwar, Mubashir and Caesar, Matthew},
  doi        = {10.1145/3717512.3717515},
  issn       = {0146-4833},
  issue_date = {October 2024},
  journal    = {SIGCOMM Comput. Commun. Rev.},
  keywords   = {characterization study, computer networking, large language models},
  month      = {February},
  number     = {4},
  numpages   = {11},
  pages      = {14–24},
  publisher  = {Association for Computing Machinery},
  title      = {Understanding Misunderstandings: Evaluating LLMs on Networking Questions},
  url        = {https://doi.org/10.1145/3717512.3717515},
  volume     = {54},
  year       = {2025}
}

@inproceedings{10.1145/3722237.3722260,
  abstract   = {The advent of artificial intelligence (AI) has profoundly transformed the educational landscape. Many educators are exploring how AI tools can enhance learning instructional programs. However, there is less focus on how its application within design education—particularly when teaching user-centered design. This study developed an educational model utilizing AI for user-centered design curriculum. Based on design thinking theory, this model integrates ChatGPT and Midjourney into the divergent and convergent design phases to facilitate the workflow. The empirical research showed that educational model can foster students’ creativity and problem-solving skills. The findings highlight the efficacy of AI integration in curricula design and instructional practices.},
  address    = {New York, NY, USA},
  author     = {Wu, Yanan and Zeng, Xiaoping and Lin, Qibei},
  booktitle  = {Proceedings of the 2024 3rd International Conference on Artificial Intelligence and Education},
  doi        = {10.1145/3722237.3722260},
  isbn       = {9798400712692},
  keywords   = {Generative AI, design education, design thinking, instructional design, user-centered design},
  location   = {},
  numpages   = {7},
  pages      = {129–135},
  publisher  = {Association for Computing Machinery},
  series     = {ICAIE '24},
  title      = {Generative AI Integrated Educational Model for User-Centered Design},
  url        = {https://doi.org/10.1145/3722237.3722260},
  year       = {2025}
}

@inproceedings{10.1145/3723010.3723012,
  abstract   = {The way software is developed is changing rapidly due to the general availability of generative AI tools. As a result, the software engineering education that is part of every computer science program needs to change. Especially in software engineering courses, such AI tools need to be used and practiced in a meaningful and useful way. The programming project is one such course at our university, and the curriculum will be expanded accordingly in the future. In this paper we describe our approach and a user study among the participants of the last programming project, in which we collected experiences with the use of current AI tools, in particular highlighting their usefulness and limitations. Our study focuses on identifying which aspects of the course students used AI tools for, evaluating successful applications, and uncovering remaining challenges.},
  address    = {New York, NY, USA},
  author     = {Borghoff, Uwe M. and Minas, Mark and Schopp, Jannis},
  booktitle  = {Proceedings of the 6th European Conference on Software Engineering Education},
  doi        = {10.1145/3723010.3723012},
  isbn       = {9798400712821},
  keywords   = {software development project course, software engineering education, AI support, AI-based tutoring, experiments},
  location   = {},
  numpages   = {10},
  pages      = {161–170},
  publisher  = {Association for Computing Machinery},
  series     = {ECSEE '25},
  title      = {Generative AI in Student Software Development Projects: A User Study on Experiences and Self-Assessment},
  url        = {https://doi.org/10.1145/3723010.3723012},
  year       = {2025}
}

@inproceedings{10229416,
  abstract   = {In recent years, artificial intelligence (AI) has been increasingly used in education and supports teachers in creating educational material and students in their learning progress. AI-driven learning support has recently been further strengthened by the release of ChatGPT, in which users can retrieve explanations for various concepts in a few minutes through chat. However, to what extent the use of AI models, such as ChatGPT, is suitable for the creation of didactically and content-wise good exercises for programming courses is not yet known. Therefore, in this paper, we investigate the use of AI-generated exercises for beginner and intermediate programming courses in higher education using ChatGPT. We created 12 exercise sheets with ChatGPT for a beginner to intermediate programming course focusing on the objects-first approach. We report our process, prompts, and experience using ChatGPT for this task and outline good practices we identified. The generated exercises are assessed and revised, primarily using ChatGPT, until they met the requirements of the programming course. We assessed the quality of these exercises by using them in our external teaching assignment course at the University of Education Ludwigsburg and let the students evaluate them. Results indicate the quality of the generated exercises and the time-saving for creating them using ChatGPT. However, our experience showed that while it is fast to generate a good version of an exercise, almost every exercise requires minor manual changes to improve its quality.},
  author     = {Speth, Sandro and Meißner, Niklas and Becker, Steffen},
  booktitle  = {2023 IEEE 35th International Conference on Software Engineering Education and Training (CSEE&T)},
  doi        = {10.1109/CSEET58097.2023.00030},
  issn       = {2377-570X},
  keywords   = {Java;Software architecture;Education;Focusing;Manuals;Learning (artificial intelligence);Chatbots;AI-Generated Exercises;SE Education;Automatic Question Generation;Programming Course;ChatGPT},
  month      = {Aug},
  number     = {},
  pages      = {142-146},
  title      = {Investigating the Use of AI-Generated Exercises for Beginner and Intermediate Programming Courses: A ChatGPT Case Study},
  volume     = {},
  year       = {2023}
}

@inproceedings{10260964,
  abstract   = {With the rapid advancement of tools based on Artificial Intelligence, it is interesting to assess their usefulness in requirements engineering. In early experiments, we have seen that ChatGPT can detect inconsistency defects in natural language (NL) requirements, that traditional NLP tools cannot identify or can identify with difficulties even after domain-focused training. This study is devoted to specifically measuring the performance of ChatGPT in finding inconsistency in requirements. Positive results in this respect could lead to the use of ChatGPT to complement existing requirements analysis tools to automatically detect this important quality criterion. For this purpose, we consider GPT-3.5, the Generative Pretrained Transformer language model developed by OpenAI. We evaluate its ability to detect inconsistency by comparing its predictions with those obtained from expert judgments by students with a proven knowledge of RE issues on a few example requirements documents.},
  author     = {Fantechi, Alessandro and Gnesi, Stefania and Passaro, Lucia and Semini, Laura},
  booktitle  = {2023 IEEE 31st International Requirements Engineering Conference (RE)},
  doi        = {10.1109/RE57278.2023.00045},
  issn       = {2332-6441},
  keywords   = {Training;Codes;Natural languages;Refining;Manuals;Chatbots;Transformers;ChatGPT;Natural Language Requirements;Inconsistency Detection},
  month      = {Sep.},
  number     = {},
  pages      = {335-340},
  title      = {Inconsistency Detection in Natural Language Requirements using ChatGPT: a Preliminary Evaluation},
  volume     = {},
  year       = {2023}
}

@inproceedings{10270477,
  abstract   = {This paper presents a novel method for teaching software engineering using the AI tool, ChatGPT, to create an engaging and immersive learning platform. The technique emphasizes understanding requirements engineering principles via interactive exercises and hands-on examples. The approach involves ChatGPT assisting in collecting user stories, creating a use case and class diagrams, and formulating sequence diagrams. This method employs an agile strategy focusing on select user stories and encourages student interaction with ChatGPT for a deeper understanding of the subject. The goal is to demonstrate the potential of AI tools to transform software engineering education by providing practical applications and real-life scenarios. The research outlines a comprehensive plan for integrating ChatGPT into a software engineering syllabus, focusing on requirements engineering. The findings could significantly influence the teaching and understanding of software engineering principles, benefiting educators and students.},
  author     = {Abdelfattah, Aly Maher and Ali, Nabila Ahmed and Elaziz, Mohamed Abd and Ammar, Hany H},
  booktitle  = {2023 International Conference on Artificial Intelligence Science and Applications in Industry and Society (CAISAIS)},
  doi        = {10.1109/CAISAIS59399.2023.10270477},
  issn       = {},
  keywords   = {Learning systems;Industries;Education;Focusing;Virtual reality;Transforms;Learning (artificial intelligence);Software engineering education;Interactive learning environment;ChatGPT;Requirements engineering;Agile process;AI in education},
  month      = {Sep.},
  number     = {},
  pages      = {1-6},
  title      = {Roadmap for Software Engineering Education using ChatGPT},
  volume     = {},
  year       = {2023}
}

@inproceedings{10298494,
  abstract   = {Bug report management has been shown to be an important and time consuming software maintenance task. Often, the first step in managing bug reports is related to triaging a bug to the appropriate developer who is best suited to understand, localize, and fix the target bug. Additionally, assigning a given bug to a particular part of a software project can help to expedite the fixing process. However, despite the importance of these activities, they are quite challenging, where days can be spent on the manual triaging process. Past studies have attempted to leverage the limited textual data of bug reports to train text classification models that automate this process - to varying degrees of success. However, the textual representations and machine learning models used in prior work are limited by their expressiveness, often failing to capture nuanced textual patterns that might otherwise aid in the triaging process. Recently, large, transformer-based, pre-tained neural text representation techniques (i.e., large language models or LLMs) such as BERT and CodeBERT have achieved greater performance with simplified training procedures in several natural language processing tasks, including text classification. However, the potential for using these techniques to improve upon prior approaches for automated bug triaging is not well studied or understood. Therefore, in this paper we offer one of the first investigations that fine-tunes transformer-based language models for the task of bug triaging on four open source datasets, spanning a collective 53 years of development history with over 400 developers and over 150 software project components. Our study includes both a quantitative and qualitative analysis of effectiveness. Our findings illustrate that DeBERTa is the most effective technique across the triaging tasks of developer and component assignment, and the measured performance delta is statistically significant compared to other techniques. However, through our qualitative analysis, we also observe that each technique possesses unique abilities best suited to certain types of bug reports.},
  author     = {Dipongkor, Atish Kumar and Moran, Kevin},
  booktitle  = {2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  doi        = {10.1109/ASE56229.2023.00217},
  issn       = {2643-1572},
  keywords   = {Training;Software maintenance;Computer bugs;Text categorization;Manuals;Machine learning;Transformers;Bug Triaging;Transformer;LLMs;Text-Embedding;DL4SE},
  month      = {Sep.},
  number     = {},
  pages      = {1012-1023},
  title      = {A Comparative Study of Transformer-Based Neural Text Representation Techniques on Bug Triaging},
  volume     = {},
  year       = {2023}
}

@inproceedings{10304857,
  abstract   = {Along with the development of large language models (LLMs), e.g., ChatGPT, many existing approaches and tools for software security are changing. It is, therefore, essential to understand how security-aware these models are and how these models impact software security practices and education. In exercises of a software security course at our university, we ask students to identify and fix vulnerabilities we insert in a web application using state-of-the-art tools. After ChatGPT, especially the GPT-4 version of the model, we want to know how the students can possibly use ChatGPT to complete the exercise tasks. We input the vulnerable code to ChatGPT and measure its accuracy in vulnerability identification and fixing. In addition, we investigated whether ChatGPT can provide a proper source of information to support its outputs. Results show that ChatGPT can identify 20 of the 28 vulnerabilities we inserted in the web application in a white-box setting, reported three false positives, and found four extra vulnerabilities beyond the ones we inserted. ChatGPT makes nine satisfactory penetration testing and fixing recommendations for the ten vulnerabilities we want students to fix and can often point to related sources of information.},
  author     = {Li, Jingyue and Meland, Per Håkon and Notland, Jakob Svennevik and Storhaug, André and Tysse, Jostein Hjortland},
  booktitle  = {2023 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
  doi        = {10.1109/ESEM56168.2023.10304857},
  issn       = {},
  keywords   = {Codes;Education;Chatbots;Software;Security;Software measurement;Task analysis;Software security;artificial intelligence;large language models;ChatGPT;IT education},
  month      = {Oct},
  number     = {},
  pages      = {1-6},
  title      = {Evaluating the Impact of ChatGPT on Exercises of a Software Security Course},
  volume     = {},
  year       = {2023}
}

@inproceedings{10305701,
  abstract   = {The use of AI assistants, along with the challenges they present, has sparked significant debate within the community of computer science education. While these tools demonstrate the potential to support students' learning and instructors' teaching, they also raise concerns about enabling unethical uses by students. Previous research has suggested various strategies aimed at addressing these issues. However, they concentrate on introductory programming courses and focus on one specific type of problem. The present research evaluated the performance of ChatGPT, a state-of-the-art AI assistant, at solving 187 problems spanning three distinct types that were collected from six undergraduate computer science. The selected courses covered different topics and targeted different program levels. We then explored methods to modify these problems to adapt them to ChatGPT's capabilities to reduce potential misuse by students. Finally, we conducted semi-structured interviews with 11 computer science instructors. The aim was to gather their opinions on our problem modification methods, understand their perspectives on the impact of AI assistants on computer science education, and learn their strategies for adapting their courses to leverage these AI capabilities for educational improvement. The results revealed issues ranging from academic fairness to long-term impact on students' mental models. From our results, we derived design implications and recommended tools to help instructors design and create future course material that could more effectively adapt to AI assistants' capabilities.},
  author     = {Wang, Tianjia and Díaz, Daniel Vargas and Brown, Chris and Chen, Yan},
  booktitle  = {2023 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)},
  doi        = {10.1109/VL-HCC57772.2023.00018},
  issn       = {1943-6106},
  keywords   = {Visualization;Shape;Computational modeling;Education;Chatbots;Distance measurement;Computer science education;Computer science education;Large language model;ChatGPT;Interview},
  month      = {Oct},
  number     = {},
  pages      = {92-102},
  title      = {Exploring the Role of AI Assistants in Computer Science Education: Methods, Implications, and Instructor Perspectives},
  volume     = {},
  year       = {2023}
}

@inproceedings{10314211,
  abstract   = {Since its public launch at the end of 2022, ChatGPT has garnered global attention, showcasing the diverse capabilities of AI in tackling human tasks. Its rapid growth and widespread adoption have permeated every corner of our daily routine. This paper provides a quick peek at the impact of ChatGPT from the perspective of software engineering education. Specifically, to make our case study creative and interesting, we compare the impact answered by ChatGPT with the real feedback from existing literature. In this way, we explore the potential of ChatGPT as a teaching and learning tool in software engineering.},
  author     = {Li, Yihao and Xu, Jialong and Zhu, Yinghua and Liu, Huashuo and Liu, Pan},
  booktitle  = {2023 10th International Conference on Dependable Systems and Their Applications (DSA)},
  doi        = {10.1109/DSA59317.2023.00087},
  issn       = {2767-6684},
  keywords   = {Education;Chatbots;Task analysis;Artificial intelligence;Software engineering;ChatGPT;software engineering;education;teaching and learning},
  month      = {Aug},
  number     = {},
  pages      = {595-596},
  title      = {The Impact of ChatGPT on Software Engineering Education: A Quick Peek},
  volume     = {},
  year       = {2023}
}

@inproceedings{10342898,
  abstract   = {Generating high-quality multiple-choice questions (MCQs) is a time-consuming activity that has led practitioners and researchers to develop community question banks and reuse the same questions from semester to semester. This results in generic MCQs which are not relevant to every course. Template-based methods for generating MCQs require less effort but are similarly limited. At the same time, advances in natural language processing have resulted in large language models (LLMs) that are capable of doing tasks previously reserved for people, such as generating code, code explanations, and programming assignments. In this paper, we investigate whether these generative capabilities of LLMs can be used to craft high-quality M CQs more efficiently, thereby enabling instructors to focus on personalizing MCQs to each course and the associated learning goals. We used two LLMs, GPT-3 and GPT-4, to generate isomorphic MCQs based on MCQs from the Canterbury Question Bank and an Introductory to Low-level C Programming Course. We evaluated the resulting MCQs to assess their ability to generate correct answers based on the question stem, a task that was previously not possible. Finally, we investigate whether there is a correlation between model performance and the discrimination score of the associated MCQ to understand whether low discrimination questions required the model to do more inference and therefore perform poorly. GPT-4 correctly generated the answer for 78.5% of MCQs based only on the question stem. This suggests that instructors could use these models to quickly draft quizzes, such as during a live class, to identify misconceptions in real-time. We also replicate previous findings that GPT-3 performs poorly on answering, or in our case generating, correct answers to MCQs. We also present cases we observed where LLMs struggled to produce correct answers. Finally, we discuss implications for computing education.},
  author     = {Tran, Andrew and Angelikas, Kenneth and Rama, Egi and Okechukwu, Chiku and Smith, David H. and MacNeil, Stephen},
  booktitle  = {2023 IEEE Frontiers in Education Conference (FIE)},
  doi        = {10.1109/FIE58773.2023.10342898},
  issn       = {2377-634X},
  keywords   = {Codes;Correlation;Computational modeling;Education;Real-time systems;Natural language processing;Recycling;large language models;generative AI;multiple-choice questions;computing education},
  month      = {Oct},
  number     = {},
  pages      = {1-8},
  title      = {Generating Multiple Choice Questions for Computing Courses Using Large Language Models},
  volume     = {},
  year       = {2023}
}

@inproceedings{10342970,
  abstract   = {Generative AI assistants are AI-powered applications that can provide personalized responses to user queries or prompts. A variety of AI assistants have recently been released, and among the most popular is OpenAI's ChatGPT. In this work-in-progress in innovative practice, we explore evidence-based learning strategies and the integration of Generative AI for computer science and engineering education. We expect this research will lead to innovative pedagogical approaches to enhance undergraduate computer science and engineering education. In particular, we describe how ChatGPT was used in two computing-based courses: a Junior-level course in database systems and a Senior-level class in mobile application development. We identify four evidence-based learning strategies: well-defined learning goals, authentic learning experiences, structured learning progression, and strategic assessment. We align these strategies with the two aforementioned courses and evaluate the usefulness of ChatGPT specifically in achieving the learning goals. Combining Generative AI with evidence-based learning has the potential to transform modern education into a more personalized learning experience.},
  author     = {Lauren, Paula and Watta, Paul},
  booktitle  = {2023 IEEE Frontiers in Education Conference (FIE)},
  doi        = {10.1109/FIE58773.2023.10342970},
  issn       = {2377-634X},
  keywords   = {Computer science;Transforms;Chatbots;Mobile communication;Database systems;Artificial intelligence;Engineering education;Generative AI;Artificial Intelligence in Education (AIEd);pedagogy},
  month      = {Oct},
  number     = {},
  pages      = {1-5},
  title      = {Work-in-Progress: Integrating Generative AI with Evidence-based Learning Strategies in Computer Science and Engineering Education},
  volume     = {},
  year       = {2023}
}

@inproceedings{10342985,
  abstract   = {This Full Research paper presents a comparison of two codebook generation methods using natural language processing (NLP): a human and NLP collaboration method and a fully automated NLP method (referred to as Human-NLP and Auto-NLP, respectively). Codebook generation serves as a preliminary step in most qualitative projects, and using NLP as a tool can help support the analysis and efficiency of the researcher. By utilizing NLP in the early stages of codebook generation, there are opportunities for detailed and productive gains when working with large corpora of textual data. Using NLP at this stage also allows the researcher to make sense of any outputs generated through automated means rather than simply accepting the output as it is. The outcome of both methods tested in this work will be used to evaluate and apply the codes across a large dataset. The Human-NLP method involves generating the initial themes using a large-language model (LLM), and the researcher revises the codebook further. The Auto-NLP method involves generating three rounds of codes, summarizing the codes in each until a saturation level has been reached through the overarching themes. The dataset used for this study comes from an analysis of students' perception and recognition of ethical concepts after participating in a semester-long course focused on ethics, society, and technology. The course introduced students to traditional ethics topics, such as those around engineering disasters, but also explored developing topics, such as facial recognition, dataset bias, and the impact of technology on the global food supply. We collected data between fall 2020 and 2022 from six (6) iterations of a semester-long course. A total of 210 student responses to the question - what did this course teach you about ethics - were analyzed. The results from both Human-NLP and Auto-NLP methods were promising in the level of detail summarized and the similarity of themes across the data. Eight (8) themes were finalized through the Human-NLP method, and twelve (12) were generated through the Auto-NLP method. We present a discussion exploring these themes and the limitations of using these methods.},
  author     = {Hingle, Ashish and Katz, Andrew and Johri, Aditya},
  booktitle  = {2023 IEEE Frontiers in Education Conference (FIE)},
  doi        = {10.1109/FIE58773.2023.10342985},
  issn       = {2377-634X},
  keywords   = {Ethics;Codes;Face recognition;Collaboration;Natural language processing;Artificial intelligence;engineering ethics;natural language processing;codebook generation;generative AI},
  month      = {Oct},
  number     = {},
  pages      = {1-8},
  title      = {Exploring NLP-Based Methods for Generating Engineering Ethics Assessment Qualitative Codebooks},
  volume     = {},
  year       = {2023}
}

@inproceedings{10343052,
  abstract   = {With the emergence of Artificial Intelligent chatbot tools such as ChatGPT and code writing AI tools such as GitHub Copilot, educators need to question what and how we should teach our courses and curricula in the future. In reality, automated tools may result in certain academic fields being deeply reduced in the number of employable people. In this work, we make a case study of cybersecurity undergrad education by using the lens of “Understanding by Design” (UbD). First, we provide a broad understanding of learning objectives (LOs) in cybersecurity from a computer science perspective. Next, we dig a little deeper into a curriculum with an undergraduate emphasis on cybersecurity and examine the major courses and their LOs for our cybersecurity program at Miami University. With these details, we perform a thought experiment on how attainable the LOs are with the above-described tools, asking the key question “what needs to be enduring concepts?” learned in this process. If an LO becomes something that the existence of automation tools might be able to do, we then ask “what level is attainable for the LO that is not a simple query to the tools?”. With this exercise, we hope to establish an example of how to prompt ChatGPT to accelerate students in their achievements of LOs given the existence of these new AI tools, and our goal is to push all of us to leverage and teach these tools as powerful allies in our quest to improve human existence and knowledge.},
  author     = {Jamieson, Peter and Bhunia, Suman and Rao, Dhananjai M.},
  booktitle  = {2023 IEEE Frontiers in Education Conference (FIE)},
  doi        = {10.1109/FIE58773.2023.10343052},
  issn       = {2377-634X},
  keywords   = {Computer science;Vocabulary;Taxonomy;Education;Writing;Chatbots;Computer security},
  month      = {Oct},
  number     = {},
  pages      = {1-5},
  title      = {With ChatGPT, Do We have to Rewrite Our Learning Objectives - CASE Study in Cybersecurity},
  volume     = {},
  year       = {2023}
}

@inproceedings{10350777,
  abstract   = {The availability and effectiveness of generative AI tools challenge the currently established methods for learning, teaching and assessment. In this paper, we discuss their potential impact for model-driven software engineering education, both from the point of view of educators and students. The discussion highlights several opportunities and risks, which support the need of a critical perspective in the application of these tools.},
  author     = {Morales, Sergio and Planas, Elena and Clarisó, Robert and Gogolla, Martin},
  booktitle  = {2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)},
  doi        = {10.1109/MODELS-C59198.2023.00034},
  issn       = {},
  keywords   = {Learning systems;Adaptation models;Plagiarism;Education;Data models;Software;Reproducibility of results;Generative AI;education;software engineering;model-driven software engineering;modeling},
  month      = {Oct},
  number     = {},
  pages      = {110-113},
  title      = {Generative AI in Model-Driven Software Engineering Education: Friend or Foe?},
  volume     = {},
  year       = {2023}
}

@inproceedings{10366692,
  abstract   = {ChatGPT, an increasingly popular Large Language Model (LLM), has found widespread acceptance, especially among the younger generation, who rely on it for various tasks, such as comprehending complex course materials and tackling homework assignments. This surge in interest has drawn the attention of researchers, leading to numerous studies that delve into the advantages and disadvantages of the upcoming LLM dominant era. In our research, we explore the influence of ChatGPT and similar models on the field of software engineering, specifically from the perspective of software engineering students. Our main objective is to gain valuable insights into their usage habits and opinions through a comprehensive survey. The survey encompassed diverse questions, addressing the specific areas where ChatGPT was utilized for assistance and gathering students’ reflections on each aspect. We found that ChatGPT has garnered widespread acceptance among software engineering students, with 93% of them utilizing it for their projects. These students expressed satisfaction with the level of assistance provided, and most intend to continue using it as a valuable tool in their work. During our investigation, we also assessed the students’ awareness of the underlying technologies behind ChatGPT. Approximately half of the students demonstrated awareness of these technologies, while 38.7% had made extra efforts to explore prompt engineering to enhance ChatGPT’s productivity. However, an important finding was that 90.6% of the students reported experiencing hallucinations during their interactions with ChatGPT. These hallucinations were shared as examples, raising significant concerns that warrant further exploration and mitigation. Moreover, we delved into potential improvements and gathered valuable recommendations, which could help ChatGPT to become even more effective and dependable in its applications.},
  author     = {Hanifi, Khadija and Cetin, Orcun and Yilmaz, Cemal},
  booktitle  = {2023 IEEE 23rd International Conference on Software Quality, Reliability, and Security (QRS)},
  doi        = {10.1109/QRS60937.2023.00028},
  issn       = {2693-9177},
  keywords   = {Surveys;Software quality;Chatbots;Reliability engineering;Reflection;Software reliability;Security;ChatGPT;software engineering;academic education;generative AI;Large Language Models},
  month      = {Oct},
  number     = {},
  pages      = {196-205},
  title      = {On ChatGPT: Perspectives from Software Engineering Students},
  volume     = {},
  year       = {2023}
}

@article{10478897,
  abstract   = {The appearance of ChatGPT at the end of 2022 was a milestone in the field of Generative Artificial Intelligence. However, it also caused a shock in the academic world. For the first time, a simple interface allowed anyone to access a large language model and use it to generate text. These capabilities have a relevant impact on teaching-learning methodologies and assessment methods. This work aims to obtain an objective measure of ChatGPT’s possible performance in solving exams related to computer engineering. For this purpose, it has been tested with actual exams of 15 subjects of the Software Engineering branch of a Spanish university. All the questions of these exams have been extracted and adapted to a text format to obtain an answer. Furthermore, the exams have been rewritten to be corrected by the teaching staff. In light of the results, ChatGPT can achieve relevant performance in these exams; it can pass many questions and problems of different natures in multiple subjects. A detailed study of the results by typology of questions and problems is provided as a fundamental contribution, allowing recommendations to be considered in the design of assessment methods. In addition, an analysis of the impact of the non-deterministic aspect of ChatGPT on the answers to test questions is presented, and the need to use a strategy to reduce this effect for performance analysis is concluded.},
  author     = {Rodriguez-Echeverría, Roberto and Gutiérrez, Juan D. and Conejero, José M. and Prieto, Álvaro E.},
  doi        = {10.1109/RITA.2024.3381842},
  issn       = {1932-8540},
  journal    = {IEEE Revista Iberoamericana de Tecnologias del Aprendizaje},
  keywords   = {Chatbots;Education;Artificial intelligence;Guidelines;Oral communication;Computational modeling;Generative AI;Computer science education;Testing;Performance evaluation;Learning systems;Artificial intelligence;ChatGPT;education;experiment},
  month      = {},
  number     = {},
  pages      = {71-80},
  title      = {Analysis of ChatGPT Performance in Computer Engineering Exams},
  volume     = {19},
  year       = {2024}
}

@article{10494340,
  abstract   = {The use of generative AI chatbots in formative assessment can effectively gauge learning progress, increase the academic performance of students compared to a traditional methodology, and raise student awareness about the tradeoffs of employing generative AI in their work.},
  author     = {Cámara, Javier and Troya, Javier and Montes-Torres, Julio and Jaime, Francisco J.},
  doi        = {10.1109/MS.2024.3385309},
  issn       = {1937-4194},
  journal    = {IEEE Software},
  keywords   = {Unified modeling language;Chatbots;Software development management;Generative AI;Object oriented modeling;Educational courses;Curriculum development;Computer science education;Unified modeling language;Modeling},
  month      = {Nov},
  number     = {6},
  pages      = {73-81},
  title      = {Generative AI in the Software Modeling Classroom: An Experience Report With ChatGPT and Unified Modeling Language},
  volume     = {41},
  year       = {2024}
}

@article{10494570,
  abstract   = {Before interacting with real users, developers must be proficient in human–computer interaction (HCI) so as not to exhaust user patience and availability. For that, substantial training and practice are required, but it is costly to create a variety of high-quality HCI training materials. In this context, chat generative pretrained transformer (ChatGPT) and other chatbots based on large language models (LLMs) offer an opportunity to generate training materials of acceptable quality without foregoing specific human characteristics present in real-world scenarios. Personas is a user-centered design method that encompasses fictitious but believable user archetypes to help designers understand and empathize with their target audience during product design. We conducted an exploratory study on the Personas technique, addressing the validity and believability of interviews designed by HCI trainers and answered by ChatGPT-simulated users, which can be used as training material for persona creation. Specifically, we employed ChatGPT to respond to interviews designed by user experience (UX) experts. Two groups, HCI professors and professionals, then evaluated the validity of the generated materials considering quality, usefulness, UX, and ethics. The results show that both groups rated the interviews as believable and helpful for Personas training. However, some concerns about response repetition and low response variability suggested the need for further research on improved prompt design in order to generate more diverse and well-developed responses. The findings of this study provide insight into how HCI trainers can use ChatGPT to help their students master persona creation skills before working with real users in real-world scenarios for the first time.},
  author     = {Barambones, Jose and Moral, Cristian and de Antonio, Angélica and Imbert, Ricardo and Martínez-Normand, Loïc and Villalba-Mora, Elena},
  doi        = {10.1109/TLT.2024.3386095},
  issn       = {1939-1382},
  journal    = {IEEE Transactions on Learning Technologies},
  keywords   = {Interviews;Training;Chatbots;Surveys;Ethics;Task analysis;Recruitment;Chatbots;computer science education;human–computer interaction (HCI);large language model (LLM);training;user-centered design},
  month      = {},
  number     = {},
  pages      = {1460-1475},
  title      = {ChatGPT for Learning HCI Techniques: A Case Study on Interviews for Personas},
  volume     = {17},
  year       = {2024}
}

@article{10508087,
  abstract   = {ChatGPT has received considerable attention in education, particularly in programming education because of its capabilities in automated code generation and program repairing and scoring. However, few empirical studies have investigated the use of ChatGPT to customize a learning system for scaffolding students’ computational thinking. Therefore, this article proposes an intelligent programming scaffolding system using ChatGPT following the theoretical framework of computational thinking and scaffolding. A mixed-method study was conducted to investigate the affordance of the scaffolding system using ChatGPT, and the findings show that most students had positive attitudes about the proposed system, and it was effective in improving their computational thinking generally but not their problem-solving skills. Therefore, more scaffolding strategies are discussed with the aim of improving student computational thinking, especially regarding problem-solving skills. The findings of this study are expected to guide future designs of generative artificial intelligence tools embedded in intelligent learning systems to foster students’ computational thinking and programming learning.},
  author     = {Liao, Jian and Zhong, Linrong and Zhe, Longting and Xu, Handan and Liu, Ming and Xie, Tao},
  doi        = {10.1109/TLT.2024.3392896},
  issn       = {1939-1382},
  journal    = {IEEE Transactions on Learning Technologies},
  keywords   = {Chatbots;Education;Programming profession;Codes;Task analysis;Problem-solving;Encoding;Artificial-intelligence-generated content (AIGC);ChatGPT;computational thinking (CT);scaffolding},
  month      = {},
  number     = {},
  pages      = {1628-1642},
  title      = {Scaffolding Computational Thinking With ChatGPT},
  volume     = {17},
  year       = {2024}
}

@article{10518103,
  abstract   = {Software assistants have significantly impacted software development for both practitioners and students, particularly in capstone projects. The effectiveness of these tools varies based on their knowledge sources; assistants with localized domain-specific knowledge may have limitations, while tools, such as ChatGPT, using broad datasets, might offer recommendations that do not always match the specific objectives of a capstone course. Addressing a gap in current educational technology, this article introduces an AI Knowledge Assistant specifically designed to overcome the limitations of the existing tools by enhancing the quality and relevance of large language models (LLMs). It achieves this through the innovative integration of contextual knowledge from a local “lessons learned” database tailored to the capstone course. We conducted a study with 150 students using the assistant during their capstone course. Integrated into the Kanban project tracking system, the assistant offered recommendations using different strategies: direct searches in the lessons learned database, direct queries to a generative pretrained transformers (GPT) model, query enrichment with lessons learned before submission to GPT and large language model meta AI (LLaMa) models, and query enhancement with Stack Overflow data before GPT processing. Survey results underscored a strong preference among students for direct LLM queries and those enriched with local repository insights, highlighting the assistant's practical value. Furthermore, our linguistic analysis conclusively demonstrated that texts generated by the LLM closely mirrored the linguistic standards and topical relevance of university course requirements. This alignment not only fosters a deeper understanding of course content but also significantly enhances the material's applicability to real-world scenarios.},
  author     = {Neyem, Andrés and González, Luis A. and Mendoza, Marcelo and Alcocer, Juan Pablo Sandoval and Centellas, Leonardo and Paredes, Carlos},
  doi        = {10.1109/TLT.2024.3396735},
  issn       = {1939-1382},
  journal    = {IEEE Transactions on Learning Technologies},
  keywords   = {Software;Artificial intelligence;Task analysis;Software engineering;Codes;Chatbots;Knowledge engineering;Capstone courses;ChatGPT;context-aware learning;generative artificial intelligence (AI);large language models (LLMs);software engineering education},
  month      = {},
  number     = {},
  pages      = {1599-1614},
  title      = {Toward an AI Knowledge Assistant for Context-Aware Learning Experiences in Software Capstone Project Development},
  volume     = {17},
  year       = {2024}
}

@article{10546497,
  abstract   = {Generative Artificial Intelligence (AI), including large language and image models, have created new opportunities for pervasive computing education. How do we integrate emerging AI models and tools into our courses in a way that fosters critical engagement? How do we teach students to use AI models and tools responsibly, thoughtfully, and ethically, while being aware of their capabilities and limitations? In this article, we share insights from integrating generative AI tools and machine learning (ML) models into a project-based undergraduate tangible and embodied interaction (TEI) course by employing co-creation processes. TEI is an evolving area within human–computer interaction, which focuses on integrating computation into our daily physical environments and objects, thus fostering an embodied, multisensory, and often collaborative interaction experience. We use the term co-creation to describe a process, where humans and AI work together to create new artifacts or solve a problem. We integrated structured co-creation activities into various phases of the project including ideation, conceptual design, and prototyping. We describe practical ways and learning goals for integrating emerging generative AI tools and ML models into the project design process, provide insight on how novice interaction designers iterate and collaborate with generative AI and ML models, and reflect on the merits and limitations of using generative AI tools and ML models for project-based interaction design courses for pervasive computing.},
  author     = {Shaer, Orit and Cooper, Angelora},
  doi        = {10.1109/MPRV.2023.3346548},
  issn       = {1558-2590},
  journal    = {IEEE Pervasive Computing},
  keywords   = {Pervasive computing;Ethics;Generative AI;Computational modeling;Education;Collaboration;Machine learning;Large language models;Artificial intelligence;Machine learning;Educational courses},
  month      = {Jan},
  number     = {1},
  pages      = {63-69},
  title      = {Integrating Generative Artificial Intelligence to a Project-Based Tangible Interaction Course},
  volume     = {23},
  year       = {2024}
}

@inproceedings{10549015,
  abstract   = {Conversational Generative AI (convo-genAI) is revolutionizing Software Engineering (SE) as engineers and academics embrace this technology in their work. However, there is a gap in understanding the current potential and pitfalls of this technology, specifically in supporting students in SE tasks. In this work, we evaluate through a between-subjects study (N = 22) the effectiveness of ChatGPT, a convo-genAI platform, in assisting students in SE tasks. Our study did not find statistical differences in participants' productivity or self-efficacy when using ChatGPT as compared to traditional resources, but we found significantly increased frustration levels. Our study also revealed 5 distinct faults arising from violations of Human-AI interaction guidelines, which led to 7 different (negative) consequences on participants.},
  author     = {Choudhuri, Rudrajit and Liu, Dylan and Steinmacher, Igor and Gerosa, Marco and Sarma, Anita},
  booktitle  = {2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE)},
  doi        = {10.1145/3597503.3639201},
  issn       = {1558-1225},
  keywords   = {Productivity;Uncertainty;Generative AI;Navigation;Chatbots;Task analysis;Standards;Empirical Study;Software Engineering;Generative AI;ChatGPT},
  month      = {April},
  number     = {},
  pages      = {2270-2282},
  title      = {How Far Are We? The Triumphs and Trials of Generative AI in Learning Software Engineering},
  volume     = {},
  year       = {2024}
}

@article{10553643,
  abstract   = {The rise of artificial intelligence, particularly the emergence of large language models (LLMs) like ChatGPT, continuously reveals numerous advantages across various domains. However, the area of project management has not yet been sufficiently explored. This study fills the research gap by conducting an empirical evaluation of three well-known LLMs: OpenAI's ChatGPT-3.5 and ChatGPT-4, as well as Google's Bard. The evaluation involves subjecting these LLMs to tests designed to prepare professionals for project management certification by the Project Management Institute. The findings cast a positive light on all three LLMs, with each model achieving scores exceeding 82%. Key insights acquired include: LLMs demonstrate the ability to effectively answer project management certification exam questions; LLMs and project managers should be viewed as a dynamic and complementary partnership; and project management certification should evolve to include an assessment of how project managers collaborate with LLMs to enhance project management.},
  author     = {Karnouskos, Stamatis},
  doi        = {10.1109/OJIES.2024.3412222},
  issn       = {2644-1284},
  journal    = {IEEE Open Journal of the Industrial Electronics Society},
  keywords   = {Project management;Certification;Chatbots;Best practices;Generative AI;Artificial intelligence;Large language models;Bard;ChatGPT;Generative artificial intelligence (AI);large language models (LLMs);project management},
  month      = {},
  number     = {},
  pages      = {758-768},
  title      = {The Relevance of Large Language Models for Project Management},
  volume     = {5},
  year       = {2024}
}

@inproceedings{10554680,
  abstract   = {Large Language Models (LLMs) have emerged as promising tools to assist students while solving programming assignments. However, object-oriented programming (OOP), with its inherent complexity involving the identification of entities, relationships, and responsibilities, is not yet mastered by these tools. Contrary to introductory programming exercises, there exists a research gap with regard to the behavior of LLMs in OOP contexts. In this study, we experimented with three prominent LLMs - GPT-3.5, GPT-4, and Bard - to solve real-world OOP exercises used in educational settings, subsequently validating their solutions using an Automatic Assessment Tool (AAT). The findings revealed that while the models frequently achieved mostly working solutions to the exercises, they often overlooked the best practices of OOP. GPT-4 stood out as the most proficient, followed by GPT-3.5, with Bard trailing last. We advocate for a renewed emphasis on code quality when employing these models and explore the potential of pairing LLMs with AATs in pedagogical settings. In conclusion, while GPT-4 show-cases promise, the deployment of these models in OOP education still mandates supervision.},
  author     = {Cipriano, Bruno Pereira and Alves, Pedro},
  booktitle  = {2024 IEEE/ACM 46th International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)},
  doi        = {10.1145/3639474.3640052},
  issn       = {2832-7578},
  keywords   = {Training;Codes;Object oriented modeling;Complexity theory;Object recognition;Object oriented programming;Programming profession;programming assignments;teaching;object-oriented programming;object-oriented design;OOP best practices;large language models;gpt-3;gpt-4;bard},
  month      = {April},
  number     = {},
  pages      = {162-169},
  title      = {LLMs Still Can't Avoid Instanceof: An Investigation Into GPT-3.5, GPT-4 and Bard's Capacity to Handle Object-Oriented Programming Assignments},
  volume     = {},
  year       = {2024}
}

@inproceedings{10554752,
  abstract   = {With the rapid advancement of artificial intelligence (AI) in various domains, the education sector is set for transformation. The potential of AI-driven tools in enhancing the learning experience, especially in programming, is immense. However, the scientific evaluation of Large Language Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an AI-Tutor remains largely unexplored. Therefore, there is a need to understand how students interact with such AI-Tutors and to analyze their experiences. In this paper, we conducted an exploratory case study by integrating the GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis. Through a combination of empirical data collection and an exploratory survey, we identified different user types based on their interaction patterns with the AI-Tutor. Additionally, the findings highlight advantages, such as timely feedback and scalability. However, challenges like generic responses and students' concerns about a learning progress inhibition when using the AI-Tutor were also evident. This research adds to the discourse on AI's role in education.},
  author     = {Frankford, Eduard and Sauerwein, Clemens and Bassner, Patrick and Krusche, Stephan and Breu, Ruth},
  booktitle  = {2024 IEEE/ACM 46th International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)},
  doi        = {10.1145/3639474.3640061},
  issn       = {2832-7578},
  keywords   = {Training;Surveys;Analytical models;Scalability;Education;User interfaces;Data collection;Programming Education;Automated Programming Assessment Systems;Artificial Intelligence;ChatGPT;OpenAI;ChatBots},
  month      = {April},
  number     = {},
  pages      = {309-319},
  title      = {AI-Tutoring in Software Engineering Education: Experiences with Large Language Models in Programming Assessments},
  volume     = {},
  year       = {2024}
}

@article{10558738,
  abstract   = {How did an “old dog” signal processing professor approach learning and teaching the “new tricks” of generative artificial intelligence (AI)? This article overviews my recent experience in preparing and delivering a new course called “Computational Creativity,” reflecting on the methods I adopted compared to a traditional equations-on-a-whiteboard course. The technical material is qualitatively different from traditional signal processing, and the types of students who took the class and their approach to learning were different too. I learned a lot from the experience but also came away with bigger questions about the role of educators in the age of generative AI.},
  author     = {Radke, Richard J.},
  doi        = {10.1109/MSP.2024.3388166},
  issn       = {1558-0792},
  journal    = {IEEE Signal Processing Magazine},
  keywords   = {Generative AI;Education;Learning (artificial intelligence);Signal processing;Creativity;Educational courses;Creativity;Computational modeling},
  month      = {March},
  number     = {2},
  pages      = {6-10},
  title      = {A Signal Processor Teaches Generative Artificial Intelligence [SP Education]},
  volume     = {41},
  year       = {2024}
}

@inproceedings{10578680,
  abstract   = {AI -systems that are based on large language models, such as ChatGPT, have quickly increased their prowess over the last year, and at the same time became readily available. As of now, many disciplines gain experience in using tools such as ChatGPT in a professional setting - and software engineering is no exception. Just as with any new kind of tooling, it is to be expected that in the era of ChatGPT, some traditional skills of the discipline will become rather obsolete, while at the same time new skill sets emerge that will be required from future professionals. Therefore, as educators we must reconsider the skill set we aim at fostering in our software engineering students, and adapt our intended learning outcomes accordingly. Furthermore, we need to adapt both assessment strategies and the teaching and learning methods we employ, in order to provide our students with a study experience that adheres to the principle of constructive alignment.},
  author     = {Zönnchen, Benedikt and Thurner, Veronika and Böttcher, Axel},
  booktitle  = {2024 IEEE Global Engineering Education Conference (EDUCON)},
  doi        = {10.1109/EDUCON60312.2024.10578680},
  issn       = {2165-9567},
  keywords   = {Learning systems;Taxonomy;Chatbots;Engineering education;Programming profession;Software engineering;software engineering education;learning objectives;teaching methods;assessment;large language models},
  month      = {May},
  number     = {},
  pages      = {1-10},
  title      = {On the Impact of ChatGPT on Teaching and Studying Software Engineering},
  volume     = {},
  year       = {2024}
}

@inproceedings{10578869,
  abstract   = {The last year, we have witnessed the popularization of generative artificial intelligence. Its output includes text, code, image, audio, speech, voice, music, and video. Therefore, it impacts education courses where students are required to elaborate on any of these artifacts. In particular, the generation of code affects informatics courses, where assignments usually ask students to develop and deliver programming code. The impact of generative artificial intelligence on informatics courses has been mainly studied for introductory programming courses. These studies have shown that generative artificial intelligence is able to produce highly sophisticated programs, but also that its results and rationale can be inaccurate. Moreover, the impact of generative artificial intelligence has not been studied for other informatics subjects. In this paper, we present our preliminary experience and proposals on three advanced software courses, namely video games, advanced algorithms and language processors. For the video games course, we present the opportunities of use of generative artificial intelligence and the results of a survey conducted with students on their use to obtain different media products. For the algorithms course, we present the result of a session driven by the instructor on different design techniques, showing the merits and demerits of the answers generated. For the language processors course, a proposal of use of generative artificial intelligence is presented, broken down into the parts of a typical language processor. The paper concludes with some suggestions for instructors.},
  author     = {Palacios-Alonso, Daniel and Urquiza-Fuentes, Jaime and Velázquez-Iturbide, J. Ángel and Guillén-García, Julio},
  booktitle  = {2024 IEEE Global Engineering Education Conference (EDUCON)},
  doi        = {10.1109/EDUCON60312.2024.10578869},
  issn       = {2165-9567},
  keywords   = {Surveys;Video games;Program processors;Codes;Generative AI;Software algorithms;Software;informatics education;generative artificial intelligence;video games;advanced algorithms;language processors},
  month      = {May},
  number     = {},
  pages      = {1-10},
  title      = {Experiences and Proposals of Use of Generative AI in Advanced Software Courses},
  volume     = {},
  year       = {2024}
}

@inproceedings{10628461,
  abstract   = {The creation of a Software Requirements Specification (SRS) document is important for any software development project. Given the recent prowess of Large Language Models (LLMs) in answering natural language queries and generating sophisticated textual outputs, our study explores their capability to produce accurate, coherent, and structured drafts of these documents to accelerate the software development lifecycle. We assess the performance of GPT-4 and CodeLlama in drafting an SRS for a university club management system and compare it against human benchmarks using eight distinct criteria. Our results suggest that LLMs can match the output quality of an entry-level software engineer to generate an SRS, delivering complete and consistent drafts. We also evaluate the capabilities of LLMs to identify and rectify problems in a given requirements document. Our experiments indicate that GPT-4 is capable of identifying issues and giving constructive feedback for rectifying them, while CodeLlama's results for validation were not as encouraging. We repeated the generation exercise for four distinct use cases to study the time saved by employing LLMs for SRS generation. The experiment demonstrates that LLMs may facilitate a significant reduction in development time for entry-level software engineers. Hence, we conclude that the LLMs can be gainfully used by software engineers to increase productivity by saving time and effort in generating, validating and rectifying software requirements.},
  author     = {Krishna, Madhava and Gaur, Bhagesh and Verma, Arsh and Jalote, Pankaj},
  booktitle  = {2024 IEEE 32nd International Requirements Engineering Conference (RE)},
  doi        = {10.1109/RE59067.2024.00056},
  issn       = {2332-6441},
  keywords   = {Productivity;Accuracy;Large language models;Impedance matching;Natural languages;Benchmark testing;Software;Requirements engineering;software requirements specifications;empirical research;large language models},
  month      = {June},
  number     = {},
  pages      = {475-483},
  title      = {Using LLMs in Software Requirements Specifications: An Empirical Evaluation},
  volume     = {},
  year       = {2024}
}

@inproceedings{10628487,
  abstract   = {The pervasive use of textual formats in the documentation of software requirements presents a great opportunity for applying large language models (LLMs) to software engineering tasks. High-quality software requirements not only enhance the manual software development process but also position organizations to fully harness the potential of the emerging LLMs technology. This paper introduces a tailored LLM for automating the generation of code snippets from well-structured requirements documents. This LLM is augmented with knowledge, heuristics, and instructions that are pertinent to the software development process, requirements analysis, object-oriented design, and test-driven development, effectively emulating the expertise of a seasoned software engineer. We introduce a “Progressive Prompting” method that allows software engineers to engage with this LLM in a stepwise manner. Through this approach, the LLM incrementally tackles software development tasks by interpreting the provided requirements to extract functional requirements, using these to create object-oriented models, and subsequently generating unit tests and code based on the object-oriented designs. We demonstrate the LLM's proficiency in comprehending intricate user requirements and producing robust design and code solutions through a case study focused on the development of a web project. This study underscores the potential of integrating LLMs into the software development workflow to significantly enhance both efficiency and quality. The tailored LLM is available at https://chat.openai.com/g/g-bahoiKzkB-software-engineer-gpt.},
  author     = {Wei, Bingyang},
  booktitle  = {2024 IEEE 32nd International Requirements Engineering Conference (RE)},
  doi        = {10.1109/RE59067.2024.00049},
  issn       = {2332-6441},
  keywords   = {Knowledge engineering;Codes;Software design;Object oriented modeling;Refining;Software;Requirements engineering;Requirements Engineering;Large Language Models (LLMs);ChatGPT;Code Generation;Use Cases;Software Specification;Automated Software Engineering},
  month      = {June},
  number     = {},
  pages      = {416-422},
  title      = {Requirements are All You Need: From Requirements to Code with LLMs},
  volume     = {},
  year       = {2024}
}

@inproceedings{10651492,
  abstract   = {Teaching the Unified Modelling Language (UML) is a critical task in the frame of Software Engineering courses. Teachers need to understand the students’ behavior along with their modeling activities to provide suggestions and feedback to avoid more frequent mistakes and improve their capabilities. This paper presents a novel approach for teaching the UML in Software Engineering courses, focusing on understanding and improving student behavior and capabilities during modeling activities. It introduces a cloud-based tool that captures and analyzes UML diagrams created by students during their interactions with a UML modeling tool. The key aspect of the proposal is the integration of a Retrieval Augmented Generation Large Language Model (RAG-based LLM), which generates insightful feedback for students by leveraging knowledge acquired during the modeling process.The effectiveness of this method is demonstrated through an experiment involving a substantial dataset comprising 5,120 labeled UML models. The validation process confirms the performance of the UML RAG-based LLM in providing relevant feedback related to entities and relationships in the students’ models. Additionally, a qualitative analysis highlights the user satisfaction, underscoring its potential as a valuable tool in enhancing the learning experience in software modeling education.},
  author     = {Ardimento, Pasquale and Bernardi, Mario Luca and Cimitile, Marta},
  booktitle  = {2024 International Joint Conference on Neural Networks (IJCNN)},
  doi        = {10.1109/IJCNN60899.2024.10651492},
  issn       = {2161-4407},
  keywords   = {Analytical models;Accuracy;Statistical analysis;Unified modeling language;Education;Software;Robustness;Deep Learning;Generative AI;LLMs;Computing Education;UML;Software Modelling},
  month      = {June},
  number     = {},
  pages      = {1-8},
  title      = {Teaching UML using a RAG-based LLM},
  volume     = {},
  year       = {2024}
}

@inproceedings{10662985,
  abstract   = {In response to the growing enrolment in software engineering programs, there is a pressing need for scalable methods to provide effective feedback for students. We designed a GPT-driven interactive assistant, FeedbackPulse, to aid educators in delivering high-quality feedback and alleviating their workload. FeedbackPulse provides real-time, personalised feedback suggestions to educators for improving their feedback to students. Preliminary evaluations of FeedbackPulse in a software engineering course have demonstrated its promising capabilities.},
  author     = {Liao, Yiwen and Jiang, Yuchao and Chen, Zhangpeng and Suleiman, Basem},
  booktitle  = {2024 36th International Conference on Software Engineering Education and Training (CSEE&T)},
  doi        = {10.1109/CSEET62301.2024.10662985},
  issn       = {2377-570X},
  keywords   = {Pressing;Real-time systems;Software engineering},
  month      = {July},
  number     = {},
  pages      = {1-2},
  title      = {FeedbackPulse: GPT-Enabled Feedback Assistant for Software Engineering Educators},
  volume     = {},
  year       = {2024}
}

@inproceedings{10662994,
  abstract   = {A user story is used in agile methodology to describe functionality that is valuable to the user and may include criteria to determine if the developer has completed the story. This study investigates undergraduate computer science students using ChatGPT to create user stories from user feedback. The study compares aspects of the user stories created by students using ChatGPT with those not using ChatGPT. Are user stories written by students with AI assistance of higher or lower quality? How does the time spent writing the user story change with the use of ChatGPT? We evaluate student user stories using a modified INVEST story rating system. Evaluated user story properties include structure, independence, value, testability, and grammar. The results show that ChatGPT-assisted students produce higher-quality user stories than unassisted students. However, using ChatGPT to write user stories does not guarantee high quality. ChatGPT can fail to recognize dependencies between user feedback and create structurally incorrect user stories. We see a need for students to be trained in effectively using this tool by carefully examining AI-assisted output and making revisions.},
  author     = {Brockenbrough, Allan and Salinas, Dominic},
  booktitle  = {2024 36th International Conference on Software Engineering Education and Training (CSEE&T)},
  doi        = {10.1109/CSEET62301.2024.10662994},
  issn       = {2377-570X},
  keywords   = {Computer science;Generative AI;Chatbots;Grammar;Software engineering;user stories;ChatGPT;GPT-Generative AI;INVEST;software engineering education;LLM},
  month      = {July},
  number     = {},
  pages      = {1-5},
  title      = {Using Generative AI to Create User Stories in the Software Engineering Classroom},
  volume     = {},
  year       = {2024}
}

@inproceedings{10663027,
  abstract   = {The integration of Artificial Intelligence (AI) tech-nologies into educational settings has paved the way for inno-vative teaching and learning approaches. In Software Engineering (SE) education, using Unified Modeling Language (UML) diagrams is a fundamental teaching element for understanding complex software systems. This research addresses ChatGPT's ability to utilize UML class and sequence diagrams to create SE modeling exercises. We use ChatGPT to generate exercises based on the information from uploaded UML diagrams by analyzing textual UML representations such as Mermaid and graphical diagrams. The research explores ChatGPT's ability to synthesize UML-specific information from class and sequence diagrams, enabling the generation of various exercises tailored to strengthen conceptual understanding and practical application. Furthermore, we investigate generating graphical UML class and sequence diagrams based on natural language as input. By bridging the gap between AI -driven natural language understanding and the comprehension of UML diagrams, this study highlights the potential of ChatGPT to improve SE education. Our concise findings address educators, practitioners, and other researchers engaged in the field of SE education with a special focus on UML.},
  author     = {Speth, Sandro and Meißner, Niklas and Becker, Steffen},
  booktitle  = {2024 36th International Conference on Software Engineering Education and Training (CSEE&T)},
  doi        = {10.1109/CSEET62301.2024.10663027},
  issn       = {2377-570X},
  keywords   = {Reviews;Unified modeling language;Education;Manuals;Learning (artificial intelligence);Chatbots;Software systems;Data mining;Engineering education;Software engineering;AI-Generated Exercises;UML Modeling;Model Comprehension;ChatGPT;Software Engineering Education},
  month      = {July},
  number     = {},
  pages      = {1-5},
  title      = {ChatGPT's Aptitude in Utilizing UML Diagrams for Software Engineering Exercise Generation},
  volume     = {},
  year       = {2024}
}

@inproceedings{10663042,
  abstract   = {This pilot study focuses on allowing students to use generative Artificial intelligence (AI) tools for their learning and assignments. Therefore, the study looks to improve the assignments to assess their learning. Students in their course have both formative and summative assessments. Both these assessments consist of writing, quizzes, and presentations. These students are both from the undergraduate and graduate levels. The study seeks to make the assessments sustainable for all teaching levels. The change in writing assessment would help to assess students better. The assignments are tested on ChatGPT and Bard to check if a student gets a passing grade using an AI tool.},
  author     = {Datta, Soma},
  booktitle  = {2024 36th International Conference on Software Engineering Education and Training (CSEE&T)},
  doi        = {10.1109/CSEET62301.2024.10663042},
  issn       = {2377-570X},
  keywords   = {Generative AI;Education;Learning (artificial intelligence);Writing;Chatbots;Software engineering;Generative Artificial Intelligence;Software Engineering;Assessment;AI tools},
  month      = {July},
  number     = {},
  pages      = {1-2},
  title      = {Using Generative Artificial Intelligence Tools in Software Engineering Courses},
  volume     = {},
  year       = {2024}
}

@inproceedings{10663045,
  abstract   = {Artificial Intelligence (AI) approaches have been incorporated into modern learning environments and software engineering (SE) courses and curricula for several years. However, with the significant rise in popularity of large language models (LLMs) in general, and OpenAI's LLM-powered chatbot ChatGPT in particular in the last year, educators are faced with rapidly changing classroom environments and disrupted teaching principles. Examples range from programming assignment solutions that are fully generated via ChatGPT, to various forms of cheating during exams. However, despite these negative aspects and emerging challenges, AI tools in general, and LLM applications in particular, can also provide significant opportunities in a wide variety of SE courses, supporting both students and educators in meaningful ways. In this early research paper, we present preliminary results of a systematic analysis of current trends in the area of AI, and how they can be integrated into university-level SE curricula, guidelines, and approaches to support both instructors and learners. We collected both teaching and research papers and analyzed their potential usage in SE education, using the ACM Computer Science Curriculum Guidelines CS2023. As an initial outcome, we discuss a series of opportunities for AI applications and further research areas.},
  author     = {Vierhauser, Michael and Groher, Iris and Antensteiner, Tobias and Sauerwein, Clemens},
  booktitle  = {2024 36th International Conference on Software Engineering Education and Training (CSEE&T)},
  doi        = {10.1109/CSEET62301.2024.10663045},
  issn       = {2377-570X},
  keywords   = {Systematics;Large language models;Learning (artificial intelligence);Chatbots;Market research;Artificial intelligence;Engineering education;AI;Roadmap;Software Engineering Education},
  month      = {July},
  number     = {},
  pages      = {1-5},
  title      = {Towards Integrating Emerging AI Applications in SE Education},
  volume     = {},
  year       = {2024}
}

@inproceedings{10663054,
  abstract   = {The swift development of Artificial Intelligence (AI), namely the introduction of Large Language Models (LLMs), is drastically altering various industries and necessitating a major change in the way software engineering is taught. To equip upcoming software engineers with the knowledge and abilities to function in this AI-powered environment, curriculum and pedagogical techniques must be critically reevaluated. To better understand the integration of AI and LLMs into software engineering education, this study gives a thorough and critical analysis of the literature, looking at existing models, pedagogical frameworks, and enduring issues. We explore various approaches utilized by educational establishments, including as specialized AI and LLM courses, incorporating modules into pre-existing curricula, and utilizing open-source LLM materials. Our analysis, which is based on case studies and research data, thoroughly assesses how well these strategies enable software engineers to comprehend, make use of, and ethically create AI and LLMs. Key obstacles to the successful integration of AI and LLM are also identified by our analysis, including the inexperienced status of LLM educators, resource limitations, potential biases in AI and LLM algorithms, and insufficient instructor knowledge. Building on these discoveries, we provide solid answers to these problems and suggest interesting avenues for further study to improve the integration of AI and LLM. In the end, this study advocates for a multimodal strategy to get future software engineers ready for the impending AI and LLM future and secure their place in this quickly changing field.},
  author     = {Sah, Chandan Kumar and Xiaoli, Lian and Islam, Muhammad Mirajul and Islam, Md Kamrul},
  booktitle  = {2024 36th International Conference on Software Engineering Education and Training (CSEE&T)},
  doi        = {10.1109/CSEET62301.2024.10663054},
  issn       = {2377-570X},
  keywords   = {Ethics;Reviews;Navigation;Large language models;Education;Software algorithms;Solids;large language models (LLMs);software engineering education;artificial intelligence (AI);Pedagogical frame-works;curriculum integration;successful strategies;problems and solutions;Instructor Skill;Resource Limitations;LLM Bias;AI Bias;Open-Source LLM Resources},
  month      = {July},
  number     = {},
  pages      = {1-5},
  title      = {Navigating the AI Frontier: A Critical Literature Review on Integrating Artificial Intelligence into Software Engineering Education},
  volume     = {},
  year       = {2024}
}

@inproceedings{10663055,
  abstract   = {Generative AI, particularly Large Language Models (LLMs), presents innovative opportunities to enhance software engineering education. Open source LLMs such as LLaMA and Mistral leverage the potential of generative AI offering distinct advantages over proprietary options including transparency, customizability, collaboration, and cost savings. This paper de-velops a catalog of LLM prompt examples tailored for software engineering training, mapped to knowledge areas from the Soft-ware Engineering Body of Knowledge (SWEBoK) framework. Example prompts demonstrate LLMs' capabilities in eliciting requirements, diagram generation, API simulation, effort esti-mation through role-playing, and other areas. The methodology involves evaluating prompt responses from ChatGPT, Mistral, and LLaMA on representative tasks. Quantitative and qualitative analysis assesses quality, usefulness, and correctness. Findings show ChatGPT and Mistral outperforming LLaMA overall, but no model perfectly executes complex interactions. We examine implications and challenges of integrating open source LLMs into classrooms, emphasizing the need for oversight, verification, and prompt design aligned with pedagogical objectives.},
  author     = {Pereira, Juanan and López, Juan-Miguel and Garmendia, Xabier and Azanza, Maider},
  booktitle  = {2024 36th International Conference on Software Engineering Education and Training (CSEE&T)},
  doi        = {10.1109/CSEET62301.2024.10663055},
  issn       = {2377-570X},
  keywords   = {Training;Knowledge engineering;Costs;Generative AI;Large language models;Collaboration;Chatbots;Software Engineering Education;Open Source AI Models;Large Language Models;Prompt Engineering},
  month      = {July},
  number     = {},
  pages      = {1-10},
  title      = {Leveraging Open Source LLMs for Software Engineering Education and Training},
  volume     = {},
  year       = {2024}
}

@inproceedings{10664407,
  abstract   = {Since the emergence of GPT-3, Large Language Models (LLMs) have caught the eyes of researchers, practitioners, and educators in the field of software engineering. However, there has been relatively little investigation regarding the performance of LLMs in assisting with requirements analysis and UML modeling. This paper explores how LLMs can assist novice analysts in creating three types of typical UML models: use case models, class diagrams, and sequence diagrams. For this purpose, we designed the modeling tasks of these three UML models for 45 undergraduate students who participated in a requirements modeling course, with the help of LLMs. By analyzing their project reports, we found that LLMs can assist undergraduate students as novice analysts in UML modeling tasks, but LLMs also have shortcomings and limitations that should be considered when using them.},
  author     = {Wang, Beian and Wang, Chong and Liang, Peng and Li, Bing and Zeng, Cheng},
  booktitle  = {2024 IEEE International Conference on Software Services Engineering (SSE)},
  doi        = {10.1109/SSE62657.2024.00046},
  issn       = {},
  keywords   = {Analytical models;Atmospheric modeling;Large language models;Unified modeling language;Software;Software engineering;Large Language Model;Generative AI;Require-ments Analysis;UML Modeling;ChatGPT},
  month      = {July},
  number     = {},
  pages      = {249-257},
  title      = {How LLMs Aid in UML Modeling: An Exploratory Study with Novice Analysts},
  volume     = {},
  year       = {2024}
}

@inproceedings{10664905,
  abstract   = {The 2023 CS curriculum by ACM, IEEE, and AAAI identifies security as an independent knowledge area that develops the “security mindset” so that students are ready for the “continual changes” in computing. Likewise, the curriculum emphasises the coverage of “uses”, and “shortcomings/pitfalls” of practical AI-tools like ChatGPT. This paper presents our endeavors to approach those goals with the design of an Information Security course. Our course design bears the following distinct features: Certificate-readiness, where we align the knowledge areas with major security/ethical hacking certificates; Coverage of ChatGPT, where the uses of ChatGPT for assisting security tasks and security issues caused by ChatGPT usage are both addressed for the first time in the teaching; “Learn defending from attackers' perspective”, where labs of both offensive and defensive natures are developed to equally sharpen ethical hacking and hardening skills, and to facilitate the discussion on legal/ethical implications; Current and Representative, where ajust-enough set of representative and/or current security topics are selected in order and covered in respective modules in the most current form. In addition, we generalize our design principles and strategies, with the hope to shed lights on similar efforts in other institutions.},
  author     = {Wang, Yang and McCoey, Margaret and Hu, Qian and Jalalitabar, Maryam},
  booktitle  = {2024 IEEE Integrated STEM Education Conference (ISEC)},
  doi        = {10.1109/ISEC61299.2024.10664905},
  issn       = {2473-7623},
  keywords   = {Ethics;Generative AI;Education;Information security;Chatbots;Security;Computer crime;Security;Generative AI;ChatGPT},
  month      = {March},
  number     = {},
  pages      = {01-06},
  title      = {Teaching Security in the Era of Generative AI: A Course Design of Security + ChatGPT},
  volume     = {},
  year       = {2024}
}

@inproceedings{10685663,
  abstract   = {The transformative influence of generative artificial intelligence (AI), notably large language models (LLMs), has significantly reshaped the software engineering (SE) landscape, impacting various aspects of software development within industry and academia. The imperative to integrate generative AI into educational programs arises from the necessity to furnish graduates with contemporary methodologies that enhance software quality and streamline development processes. Nevertheless, a research gap exists concerning the systematic integration of established SE education guidelines with specific course contexts to strengthen SE education through incorporating generative AI. In response to this gap, our study presents a vision for integrating generative AI into SE education, with a particular emphasis on practical integration strategies aimed at endowing students with essential competencies tailored for contemporary software development. Aligning our vision with the knowledge domains within SE education, we delineate its application across specific areas such as code generation, auto test case completion, and others. The overall objective of these proposed initiatives is to furnish students in SE with an updated and immersive learning experience, thereby addressing the evolving demands of the field.},
  author     = {Li, Yishu and Keung, Jacky and Ma, Xiaoxue},
  booktitle  = {2024 International Symposium on Educational Technology (ISET)},
  doi        = {10.1109/ISET61814.2024.00019},
  issn       = {2766-2144},
  keywords   = {Industries;Systematics;Generative AI;Large language models;Software quality;Educational technology;Software engineering;software engineering;education;generative AI;large language models;code generation;auto test case completion},
  month      = {July},
  number     = {},
  pages      = {49-53},
  title      = {Integrating Generative AI in Software Engineering Education: Practical Strategies},
  volume     = {},
  year       = {2024}
}

@article{10689494,
  abstract   = {The integration of Large Language Models (LLMs), especially ChatGPT, into education is poised to revolutionize students' learning experiences by introducing innovative conversational learning methodologies. To empower students to fully leverage the capabilities of ChatGPT in educational scenarios, understanding students' interaction patterns with ChatGPT is crucial for instructors. However, this endeavor is challenging due to the absence of datasets focused on student-ChatGPT conversations and the complexities in identifying and analyzing the evolutional interaction patterns within conversations. To address these challenges, we collected conversational data from 48 students interacting with ChatGPT in a master's level data visualization course over one semester. We then developed a coding scheme, grounded in the literature on cognitive levels and thematic analysis, to categorize students' interaction patterns with ChatGPT. Furthermore, we present a visual analytics system, StuGPTViz, that tracks and compares temporal patterns in student prompts and the quality of ChatGPT's responses at multiple scales, revealing significant pedagogical insights for instructors. We validated the system's effectiveness through expert interviews with six data visualization instructors and three case studies. The results confirmed StuGPTViz's capacity to enhance educators' insights into the pedagogical value of ChatGPT. We also discussed the potential research opportunities of applying visual analytics in education and developing AI-driven personalized learning solutions.},
  author     = {Chen, Zixin and Wang, Jiachen and Xia, Meng and Shigyo, Kento and Liu, Dingdong and Zhang, Rong and Qu, Huamin},
  doi        = {10.1109/TVCG.2024.3456363},
  issn       = {1941-0506},
  journal    = {IEEE Transactions on Visualization and Computer Graphics},
  keywords   = {Data visualization;Chatbots;Oral communication;Education;Visual analytics;Artificial intelligence;Data collection;Visual analytics for education;ChatGPT for education;student-ChatGPT interaction},
  month      = {Jan},
  number     = {1},
  pages      = {908-918},
  title      = {StuGPTViz: A Visual Analytics Approach to Understand Student-ChatGPT Interactions},
  volume     = {31},
  year       = {2025}
}

@article{10706931,
  abstract   = {Contribution: This research explores the benefits and challenges of developing, deploying, and evaluating a large language model (LLM) chatbot, MoodleBot, in computer science classroom settings. It highlights the potential of integrating LLMs into LMSs like Moodle to support self-regulated learning (SRL) and help-seeking behavior. Background: Computer science educators face immense challenges incorporating novel tools into LMSs to create a supportive and engaging learning environment. MoodleBot addresses this challenge by offering an interactive platform for both students and teachers. Research Questions: Despite issues like bias, hallucinations, and teachers’ and educators’ resistance to embracing new (AI) technologies, this research investigates two questions: (RQ1) To what extent do students accept MoodleBot as a valuable tool for learning support? (RQ2) How accurately does MoodleBot churn out responses, and how congruent are these with the established course content? Methodology: This study reviews pedagogical literature on AI-driven chatbots and adopts the retrieval-augmented generation (RAG) approach for MoodleBot’s design and data processing. The technology acceptance model (TAM) evaluates user acceptance through constructs like perceived usefulness (PU) and Ease of Use. Forty-six students participated, with 30 completing the TAM questionnaire. Findings: LLM-based chatbots like MoodleBot can significantly improve the teaching and learning process. This study revealed a high accuracy rate (88%) in providing course-related assistance. Positive responses from students attest to the efficacy and applicability of AI-driven educational tools. These findings indicate that educational chatbots are suitable for integration into courses to improve personalized learning and reduce teacher administrative burden, although improvements in automated fact-checking are needed.},
  author     = {Neumann, Alexander Tobias and Yin, Yue and Sowe, Sulayman and Decker, Stefan and Jarke, Matthias},
  doi        = {10.1109/TE.2024.3467912},
  issn       = {1557-9638},
  journal    = {IEEE Transactions on Education},
  keywords   = {Chatbots;Education;Computer science;Databases;Accuracy;Mentoring;Information technology;Information systems;Adaptation models;Vectors;Chatbots;higher education;large language model (LLM);moodle;moodlebot},
  month      = {Feb},
  number     = {1},
  pages      = {103-116},
  title      = {An LLM-Driven Chatbot in Higher Education for Databases and Information Systems},
  volume     = {68},
  year       = {2025}
}

@inproceedings{10714575,
  abstract   = {Unlike traditional educational chatbots that rely on pre-programmed responses, large-language model-driven chatbots, such as ChatGPT, demonstrate remarkable versatility to serve as a dynamic resource for addressing student needs from understanding advanced concepts to solving complex problems. This work explores the impact of such technology on student learning in an interdisciplinary, project-oriented data visualization course. Throughout the semester, students engaged with ChatGPT across four distinct projects, designing and implementing data visualizations using a variety of tools such as Tableau, D3, and Vega-lite. We collected conversation logs and reflection surveys after each assignment and conducted interviews with selected students to gain deeper insights into their experiences with ChatGPT. Our analysis examined the advantages and barriers of using ChatGPT, students’ querying behavior, the types of assistance sought, and its impact on assignment outcomes and engagement. We discuss design considerations for an educational solution tailored for data visualization education, extending beyond ChatGPT’s basic interface.},
  author     = {Kim, Nam Wook and Ko, Hyung-Kwon and Myers, Grace and Bach, Benjamin},
  booktitle  = {2024 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)},
  doi        = {10.1109/VL/HCC60511.2024.00022},
  issn       = {1943-6106},
  keywords   = {Surveys;Visualization;Knowledge based systems;Education;Data visualization;Oral communication;Chatbots;Dynamic scheduling;Reflection;Interviews;ChatGPT;large language model;data visualization;education;project-based learning},
  month      = {Sep.},
  number     = {},
  pages      = {109-120},
  title      = {ChatGPT in Data Visualization Education: A Student Perspective},
  volume     = {},
  year       = {2024}
}

@inproceedings{10740416,
  abstract   = {The integration of Generative AI into software engineering education marks a transformative shift in teaching methodologies. This paper explores its potential, highlighting the benefits of enhancing student engagement, creativity, and efficiency while preparing them for industry challenges. Through a comprehensive analysis of 13 popular generative AI tools, we examine their roles in various software engineering tasks such as requirements analysis, design, coding, debugging, and testing. This paper contributes to the broader discourse on the future of software engineering education by offering evidence-based recommendations for leveraging generative AI to create adaptive and forward-thinking instructional strategies.},
  author     = {Yabaku, Mounika and Pombo, Nuno and Ouhbi, Sofia},
  booktitle  = {2024 IEEE 18th International Conference on Application of Information and Communication Technologies (AICT)},
  doi        = {10.1109/AICT61888.2024.10740416},
  issn       = {2472-8586},
  keywords   = {Generative AI;Education;Debugging;Software;Requirements engineering;Software measurement;Usability;Software engineering;Testing;Software development management;Generative AI;Large Language Models (LLMs);Software Engineering Education;Pedagogical Innovation;AIDriven Educational Tools},
  month      = {Sep.},
  number     = {},
  pages      = {1-7},
  title      = {Exploring the Potential Use of Generative AI in Software Engineering Education},
  volume     = {},
  year       = {2024}
}

@inproceedings{10766746,
  abstract   = {Learning programming in higher education faces significant challenges, including high dropout rates and difficulties in understanding abstract concepts. Previous studies have explored various teaching methods, but the effectiveness of Generative Artificial Intelligence (GenAI) in this context has not yet been widely investigated. This study compares the efficacy of GenAI with video-based active learning methods for teaching programming to university students. Through an experimental design with 40 computer engineering students, academic performance, perception of usefulness and ease of use, and satisfaction and motivation were evaluated. The results showed no statistically significant differences between the groups in academic performance, perception of usefulness and ease of use, or satisfaction and motivation. Both methods proved equally effective in improving learning and maintaining student motivation. These findings suggest that GenAI can be a viable alternative to traditional methods, offering opportunities to diversify pedagogical strategies in programming education. Educators are encouraged to consider integrating GenAI to complement existing methods, ensuring implementation that maintains high levels of perceived control, immersion, and curiosity among students.},
  author     = {Mellado, Rafael and Cubillos, Claudio and Ahumada, Giovanni},
  booktitle  = {2024 IEEE International Conference on Automation/XXVI Congress of the Chilean Association of Automatic Control (ICA-ACCA)},
  doi        = {10.1109/ICA-ACCA62622.2024.10766746},
  issn       = {},
  keywords   = {Training;Technology acceptance model;Generative AI;Education;Learning (artificial intelligence);Educational technology;Engineering students;Programming profession;Faces;Videos;generative artificial intelligence;computer programming;active learning},
  month      = {Oct},
  number     = {},
  pages      = {1-7},
  title      = {Effectiveness of Generative Artificial Intelligence in learning programming to higher education students},
  volume     = {},
  year       = {2024}
}

@inproceedings{10767612,
  abstract   = {The planning poker estimation technique encour-ages all team members to participate equally, which is essential in the training of future software engineers. By proposing a coordination scheme based on the experience and knowledge of the team members, it enforces the common ownership of effort estimation. Thus, it is crucial that all members contribute to the process [10]. However, given the personal factors that could affect team interaction dynamics, the contributions of team members could not be equally distributed, hindering the goal of the technique. Ensuring the equal participation of team members sets a challenge not only in the professional context but also for training future software developers and team managers [18] that must facilitate team collaboration. Hence, it is vital to detect team members' contributions in order to value collaboration in a development team. In this article, we present the analyses of the interventions of 13 groups of students from the Computer Engineering course at the University of Valparaiso during a user story estimation activity using planning poker. The experimental setup involved computer science undergraduate students, performing a learning activity regarding the Planning Poker estimation technique. The students' interventions were classified according to a human expert following a collaboration framework. Subsequently, they were classified using ChatGPT 4 using the Zero Shot technique in order to compare the automatically generated labels with those provided by human experts. The type of classification used was binary to determine whether or not the intervention analysed was a contribution. The analysis focused on evaluating the accuracy and consistency of ChatGPT in the contribution classification task, considering the model's ability to correctly identify the different types of interventions. The results of this comparison demonstrate the effectiveness of ChatGPT and its potential to assist in real-time evaluation and analysis tasks. This study enhances the understanding of how artificial intelligence tools can complement the work of human experts, improving efficiency and accuracy in educational and agile project management activities.},
  author     = {Miranda, Diego and Palma, Dayana and Fernández, Adrian and Noel, René and Cechinel, Cristian and Munoz, Roberto},
  booktitle  = {2024 43rd International Conference of the Chilean Computer Science Society (SCCC)},
  doi        = {10.1109/SCCC63879.2024.10767612},
  issn       = {2691-0632},
  keywords   = {Training;Computer science;Accuracy;Collaboration;Estimation;Agile project management;Chatbots;Software;Planning;Artificial intelligence;planning poker;contributions;zero shot;collab-oration},
  month      = {Oct},
  number     = {},
  pages      = {1-4},
  title      = {Enhancing Agile Project Management Education with AI: ChatGPT-4's Role in Evaluating Student Contributions},
  volume     = {},
  year       = {2024}
}

@inproceedings{10773596,
  abstract   = {The emergence of AI-powered chatbots like ChatGPT has generated excitement in many fields, including education. Some see this technology as a tool with a transformative impact similar to that of the printing press or the Internet. In this paper, we evaluate how effectively undergraduate computer engineering students can use this technology and the challenges they encounter in their interactions with ChatGPT. To this end, we examined whether students could ask effective questions to ChatGPT while learning software design patterns with its assistance. Based on our findings, we provide curriculum recommendations to improve the integration of ChatGPT into undergraduate computer engineering education.},
  author     = {Gerede, Çağdaş Evren},
  booktitle  = {2024 9th International Conference on Computer Science and Engineering (UBMK)},
  doi        = {10.1109/UBMK63289.2024.10773596},
  issn       = {2521-1641},
  keywords   = {Computer science;Software design;Education;Chatbots;Internet;Computer science education;Engineering students;Printing machinery;ChatGPT;AI in higher education;computer engineering education;software design patterns;self-learning;effective questioning},
  month      = {Oct},
  number     = {},
  pages      = {1092-1097},
  title      = {Are We Asking the Right Questions to ChatGPT for Learning Software Design Patterns?},
  volume     = {},
  year       = {2024}
}

@inproceedings{10795351,
  abstract   = {Modern code review (MCR) is recognized as an effective software quality assurance practice that is broadly adopted by open-source and commercial software projects. MCR is most effective when developers follow best practices, as it improves code quality, enhances knowledge transfer, increases team awareness and shares code ownership. However, prior work highlights that poor code review practices are common and often manifest in the form of low review participation and engagement, shallow review, and toxic communications. To address these issues, we introduce GitRev, a novel approach that applies gamification mechanisms to boost developer motivation and engagement. GitRev is built on top of a Large Language Model (LLM), used as a points-based reward system that leverages the code change context, and code review activities. We implement GitRev as a GitHub app with a web browser extension that consists of a client-side web browser extension that gamifies the GitHub user interface, and a server-side composed of a Node.js server for authentication and data management. To evaluate GitRev, we conduct a controlled experiment with 86 graduate and undergraduate students. Results indicate the promising potential of our approach for improving the code review process and developers' engagement. GitRev is publicly available at https://anonymous.40pen.science/r/GitRev-OB74},
  author     = {Khelifi, Jasem and Chouchen, Moataz and Ouni, Ali and Wang, Dong and Kula, Raula Gaikovina and Hamza, Salma and Mkaouer, Mohamed Wiem},
  booktitle  = {2024 IEEE International Conference on Source Code Analysis and Manipulation (SCAM)},
  doi        = {10.1109/SCAM63643.2024.00031},
  issn       = {2470-6892},
  keywords   = {Codes;Reviews;Source coding;Large language models;Software quality;User interfaces;Browsers;Servers;Knowledge transfer;Software development management;Modern Code Review;GitHub;Gamification},
  month      = {Oct},
  number     = {},
  pages      = {235-241},
  title      = {GitRev: An LLM-Based Gamification Framework for Modern Code Review Activities},
  volume     = {},
  year       = {2024}
}

@inproceedings{10837600,
  abstract   = {Powerful chatbots, based on intensively-trained large language models, have recently become available for consumer use. The ability of such chatbots to provide credible textual responses to sophisticated engineering problems has been demonstrated in various subfields. This paper seeks to gauge the extent to which such a chatbot can be prompted to complete a set of homework and project exercises for university-level courses in analog, digital, mixed -signal, and signal processing classes. The purpose of this paper is to delineate and clearly articulate the present capabilities of artificial intelligence tools to complete coursework taks across the field of circuit theory. Building on these research findings, this paper suggests practical ways to mitigate artifical intelligence chatbot tools' disription to academic integrity and genuine learning in universities.},
  author     = {Mormul, Yevhenii and Przybyszewski, Jan and Siriburanon, Teerachot and Healy, John and Cuffe, Paul},
  booktitle  = {2024 21st International Conference on Information Technology Based Higher Education and Training (ITHET)},
  doi        = {10.1109/ITHET61869.2024.10837600},
  issn       = {2473-2060},
  keywords   = {Training;Large language models;Buildings;Signal processing;Chatbots;Circuit synthesis;Information technology;Standards;Circuit theory},
  month      = {Nov},
  number     = {},
  pages      = {1-7},
  title      = {Gauging the Capability of Artificial Intelligence Chatbot Tools to Answer Textbook Coursework Exercises in Circuit Design Education},
  volume     = {},
  year       = {2024}
}

@inproceedings{10837605,
  abstract   = {This article focuses on integrating generative AI tools, particularly ChatGPT and Bard, into Scrum framework education to enhance student learning and collaboration. Drawing from feedback obtained from students, the study compares the effectiveness of these tools, highlighting ChatGPT's detailed responses and Bard's conciseness. Despite students' overall satisfaction with GAI integration, they highlighted the irreplaceable role of human educators. Areas for improvement include addressing technical issues and enhancing GAI adaptability. The students' recommendations include utilizing GAI for various tasks and providing clearer prompts and training. Moving forward, the focus should be on improving GAI tools to better comprehend context and adaptability, and exploring collaborative learning approaches. To evaluate the impact of GAI integration on student outcomes, long-term studies must be conducted. In summary, while generative AI has potential to enhance Scrum education, its incorporation should be balanced with human guidance to create dynamic and effective learning environments.},
  author     = {Boubakri, Meryem and Nafil, Khalid},
  booktitle  = {2024 21st International Conference on Information Technology Based Higher Education and Training (ITHET)},
  doi        = {10.1109/ITHET61869.2024.10837605},
  issn       = {2473-2060},
  keywords   = {Training;Surveys;Technological innovation;Generative AI;Federated learning;Reviews;Instruments;Collaboration;Project management;Chatbots;Education;Scrum;ChatGPT;Bard;Generative AI;Prompting},
  month      = {Nov},
  number     = {},
  pages      = {1-8},
  title      = {Enhancing Student Learning in Scrum Projects with Generative AI Assistance},
  volume     = {},
  year       = {2024}
}

@inproceedings{10837671,
  abstract   = {This study investigates the integration of generative AI into the Scrum planning poker process within an educational environment. Through the Character.ai platform, Students interacted with AI agents, including a Scrum Master and three Developers, using their mobile devices to conduct Planning Poker sessions. Quantitative criteria were developed to assess Agent interactions, Student participation, Collaboration, Communication, and Decision-making Patterns. Initial findings reveal encouraging levels of Student engagement and effective utilization of AI-supported techniques during the sessions. These results highlight the potential of generative AI to enhance educational experiences, particularly within the context of Agile project management methodologies.},
  author     = {Nafil, Khalid and Lefdaoui, Youssef},
  booktitle  = {2024 21st International Conference on Information Technology Based Higher Education and Training (ITHET)},
  doi        = {10.1109/ITHET61869.2024.10837671},
  issn       = {2473-2060},
  keywords   = {Training;Generative AI;Decision making;Collaboration;Agile project management;Mobile handsets;Planning;Information technology;Planning Poker;Scrum;Character.ai;Generative AI;Education},
  month      = {Nov},
  number     = {},
  pages      = {1-6},
  title      = {Innovative Approach to Agile Education: Generative AI-Supported Planning Poker Simulation},
  volume     = {},
  year       = {2024}
}

@article{10849533,
  abstract   = {Generative artificial intelligence (GenAI) is emerging as a transformative technology in higher education, particularly in programming instruction. However, its impact on learning, motivation, and the educational environment must still be fully understood. This study aims to determine the capacity of GenAI to generate effective computer programming learning in STEM university students, comparing it with active learning methods based on video. An experiment was conducted with 40 computer engineering students divided into two groups: one using GenAI (Google Gemini 1.5) and another employing educational videos. Pre- and post-tests of knowledge and the Intrinsic Motivation Inventory (IMI) were applied to evaluate learning, intrinsic motivation, and the learning environment. No significant differences in learning were found between the groups. However, GenAI significantly increased perceived autonomy and reduced perceived effort and pressure, while video-based learning significantly improved perceived competence. These findings suggest that both methods seem to motivate in diverse ways and that they could complement each other in an integrated teaching approach, offering new perspectives for designing programming learning environments in higher education.},
  author     = {Cubillos, Claudio and Mellado, Rafael and Cabrera-Paniagua, Daniel and Urra, Enrique},
  doi        = {10.1109/ACCESS.2025.3532883},
  issn       = {2169-3536},
  journal    = {IEEE Access},
  keywords   = {Programming profession;Education;Learning (artificial intelligence);Generative AI;Codes;Artificial intelligence;Focusing;Active learning;Ethics;Chatbots;Generative artificial intelligence;programming education;intrinsic motivation;active learning;engineering programming},
  month      = {},
  number     = {},
  pages      = {40438-40455},
  title      = {Generative Artificial Intelligence in Computer Programming: Does It Enhance Learning, Motivation, and the Learning Environment?},
  volume     = {13},
  year       = {2025}
}

@inproceedings{10850830,
  abstract   = {Machine learning has become recognized as an important field that is rapidly changing the entire world. Machine learning education primarily targets older students, with little consideration paid to their motivation or participation. This research project considers the integration of ControlNet, a neural network framework for picture production, into Python machine learning applications to enhance student learning via immediately apparent visual feedback. By leveraging ControlNet together with the generative model Stable Diffusion, students can observe the immediate effects of changes in source code on generated images, thereby reducing the gap between theoretical understanding and practical application. This approach promotes student engagement and offers a dynamic platform for studying fundamental machine learning principles, including classification, regression, and model training. The research presented here explains the implementation process, technical obstacles, and advantages of integrating ControlNet into learning environments, giving insights into its potential as an innovative pedagogical instrument. Student feedback and testing reveal that visualbased learning can enhance comprehension and retention of machine learning concepts, rendering this method of teaching an interesting direction for a further look in informatics education.},
  author     = {Kormaník, Tomáš and Gabonai, Michal Gabonai and Porubän, Jaroslav},
  booktitle  = {2024 International Conference on Emerging eLearning Technologies and Applications (ICETA)},
  doi        = {10.1109/ICETA63795.2024.10850830},
  issn       = {},
  keywords   = {Training;Visualization;Instruments;Source coding;Scalability;Education;Machine learning;Rendering (computer graphics);Problem-solving;Testing;Education;ControlNet;Stable Diffusion;Machine Learning},
  month      = {Oct},
  number     = {},
  pages      = {350-355},
  title      = {Using Machine Learning Concepts with ControlNet for Educational Advancements},
  volume     = {},
  year       = {2024}
}

@inproceedings{10850835,
  abstract   = {Large language models (LLMs) offer new educational and research opportunities by enabling personalized learning, providing tutoring and assistance and enhancing accessibility. This paper examines the current application solutions of LLMs in education and their usability, benefits, and challenges. Through our use case and experimental data on Turtlebot3 education robots, we discuss the potential of different transformer-based LLMs and their limitations to improve educational and research outcomes for students working with robotic systems in laboratory conditions. We also evaluated selected models based on quantitative and qualitative metrics to choose one which was best suited for our AI-based education and research assistant use case.},
  author     = {Krupáš, Maroš and Antonets, Liudmyla and Vaščák, Ján and Zolotová, Iveta},
  booktitle  = {2024 International Conference on Emerging eLearning Technologies and Applications (ICETA)},
  doi        = {10.1109/ICETA63795.2024.10850835},
  issn       = {},
  keywords   = {Measurement;Technological innovation;Systematics;Education;Transformers;Data models;Usability;Robots;Tuning;Testing;AI-based education and research assistant;Large Language Models (LLM);generative pre-trained transformers;robotics education;Turtlebot3},
  month      = {Oct},
  number     = {},
  pages      = {1-6},
  title      = {AI-Based Assistant: LLMs for Effective Robotics Education and Research},
  volume     = {},
  year       = {2024}
}

@inproceedings{10892822,
  abstract   = {This innovative practice full paper describes how to integrate generative Artificial Intelligence (AI) with Data Structures and Algorithm Analysis (CS2) homework at Oklahoma State University. Data Structures and Algorithm Analysis (CS2) course covers extremely important knowledge and skills of becoming a computer scientist. However, students might fail to meet the learning outcomes of CS2 course, somewhat due to the abstract nature of concepts but also because of a misunderstanding of concepts, the selection of inappropriate data structure and algorithm, a lack of effective debugging skills, and writing inefficient code. Currently we are in an Artificial Intelligence (AI) revolution, and generative AI (also widely known as AI chatbots) are already popular across college and university campuses. Generative AI that are designed to learn and mimic human conversation is capable of generating, translating, or paraphrasing text and answering questions in a way that is often indistinguishable from human-generated content. We investigate the above-mentioned potential challenges faced by students while learning CS2 course at Oklahoma State University (OSU) and redesign the course homework in Fall 2023 semester. The objectives of the redesigned course homework are to provide students with opportunities to use generative AI to support their learning in the CS2 course as well as measure the effectiveness of utilizing generative AI to improve student learning outcomes in the CS2 course. At the end of Fall 2023 semester, we conducted a student perception survey in the CS2 course and collected valuable feedback from 47 out of 61 students (77% response rate). In summary, 85.1%, 76.6%, 74.5%, 63.8%, and 70.2% respondents indicated that generative AI help to understand testing and debugging better, improve coding skills and code quality, design and implement efficient data structures and algorithms, select appropriate algorithms and data structures with the assistance of generative AI, and under-stand the importance of designing and implementing efficient data structures and algorithms, respectively. In this paper, we summarize the experience of redesigning CS2 course homework at OSU, share lessons learned, and provide candid suggestions for utilizing course homework in CS2 courses at other institutions.},
  author     = {Pu, Cong},
  booktitle  = {2024 IEEE Frontiers in Education Conference (FIE)},
  doi        = {10.1109/FIE61694.2024.10892822},
  issn       = {2377-634X},
  keywords   = {Surveys;Codes;Translation;Generative AI;Debugging;Writing;Data structures;Chatbots;Encoding;Testing;Computer Science;Data Structures;Algorithm Analysis;CS2;Artificial intelligence (AI);Generative AI;AI Chatbots},
  month      = {Oct},
  number     = {},
  pages      = {1-9},
  title      = {Integrating Generative AI with Data Structures and Algorithm Analysis Course Homework},
  volume     = {},
  year       = {2024}
}

@inproceedings{10892858,
  abstract   = {During the Software Development Lifecycle (SDLC), the first stage entails the Requirement Engineering phase. In this phase, engineers gather, analyze, and specify the requirements for a software system. Requirements playa crucial role in the SDLC as they establish the foundation for the entire system by defining the expected behaviors of the software system to be built. The resulting specifications are captured in a Software Requirement Specification (SRS) document. As part of the validation process, requirement specifications are traced. Requirement tracing involves linking the requirement to the artifacts where the customer requested the high-level requirement. Teaching proper requirements tracing can be challenging in a traditional classroom setting. It is essential to educate future software engineers on the proper process of developing an SRS document and of tracing requirements back to the originating artifact, which is also challenging due to the complexity and large scope of applying the complete requirements engineering process. Understanding how changes in customer needs can impact requirements is an imperative learning opportunity. In this work, we aim to incorporate the use of AI in the teaching of requirements tracing using Large Language Models. In this experiment, both GPT -3.5 and GPT -4 are provided the transcript of an interview between the customer and the engineering team, as well as the subsequent requirements elicited from that meeting and other customer provided artifacts. The GPTs are then instructed to determine which requirements can be traced back to the interview transcript. At the same time, the students (the requirements engineering team) conduct their own effort to trace requirements back to the original interview. The experiment was taken one step further to assess students' and the GPTs abilities to address requirements modifications. After another interview with the customer, where some needs were changed, some requirements were modified, and students, and GPTs were asked to trace the modified requirements to the new interview. The results proved that students are better than both GPT versions at tracing modified requirements, yet GPTs again identified requirements that students didn't trace back. The findings, illustrate that AI can help in the teaching of requirement tracing; these results suggest that while no AI model is currently capable of replacing real requirement engineers as they don't outperform students, it can be used as a tool to test the completeness of the requirement tracing process. We posit that GPT can be a tool for students to self-assess the degree to which their own requirements tracing is exhaustive.},
  author     = {Couder, Juan Ortiz and Pate, William C. and Machado, Daniel A. and Ochoa, Omar},
  booktitle  = {2024 IEEE Frontiers in Education Conference (FIE)},
  doi        = {10.1109/FIE61694.2024.10892858},
  issn       = {2377-634X},
  keywords   = {Training;Visualization;Atmospheric modeling;Prototypes;Software systems;Requirements engineering;Interviews;Artificial intelligence;Software engineering;Software development management;AI;Requirement Tracing;Education;Software Requirement Specification;Large Language Models},
  month      = {Oct},
  number     = {},
  pages      = {1-8},
  title      = {Incorporating AI in the Teaching of Requirements Tracing Within Software Engineering},
  volume     = {},
  year       = {2024}
}

@inproceedings{10892909,
  abstract   = {This research-to-practice paper presents a novel pedagogical tool for hardware cybersecurity education and workforce development. The growing importance of hardware security has made it essential for individuals and organizations to understand hardware security principles and best practices. However, the current educational curriculum falls short of fulfilling these emerging demands due to the rapidly changing hardware security landscape and limited opportunities for hands-on training. To address these challenges, we propose and have developed the Interactive Hardware and Cybersecurity (I-HaC) Educational Framework, a pedagogical educational framework that supplements existing courses by leveraging generative AI for individualized instruction related to hardware and cybersecurity, data mining, and applied Machine Learning (ML), as well as data visualization to enhance cybersecurity education and workforce development. The framework is designed to be utilized by graduate and undergraduate Electrical and Computer Engineering (ECE) and Computer Science (CS) students for a comprehensive introduction to cybersecurity exploits and countermeasures in an interactive manner with hands-on components. Using I-HaC, we have developed tailored lab components for a diverse range of students and intend to release I-HaC as open-source for the benefit of the ECE and CS education community.},
  author     = {Ghimire, Sujan and Chowdhury, Muhtasim Alam and Tsang, Ryan and Yarnell, Richard and Heckert, Emma and Carpenter, Jaeden and Lin, Yu-Zheng and Mamun, Muntasir and DeMara, Ronald F. and Rafatirad, Setareh and Satam, Pratik and Salehi, Soheil},
  booktitle  = {2024 IEEE Frontiers in Education Conference (FIE)},
  doi        = {10.1109/FIE61694.2024.10892909},
  issn       = {2377-634X},
  keywords   = {Training;Generative AI;Databases;Hardware security;Data visualization;Organizations;Machine learning;Ontologies;Computer security;Protection;National Vulnerability Database (NVD);Com-mon Vulnerability and Exposure (CVE);Common Weakness Enumeration (CWE);Hardware Security;Cybersecurity Education;Future Workforce Development},
  month      = {Oct},
  number     = {},
  pages      = {1-7},
  title      = {Interactive Framework for Cybersecurity Education and Future Workforce Development},
  volume     = {},
  year       = {2024}
}

@inproceedings{10892918,
  abstract   = {In recent years, remote laboratories have become integral to modern education, offering flexibility and accessibility compared to traditional, in-person labs. Integrating AI-powered assistance into remote labs has the potential to give them an edge by providing personalized learning experiences. This paper explores an innovative approach to promoting independent learning and critical thinking by embedding AI-driven support, using OpenAI's GPT-4 model, into a remote Field Programmable Gate Array (FPGA) laboratory. Through a web-based code editor, students write SystemVerilog programs and receive tailored assistance from the AI, while their designs are deployed on a Terasic DEl-SoC FPGA development board with real-time feedback via a live camera feed. The study, which involved students from an advanced digital design course interacting with the AI assistant, revealed strong engagement and positive feedback. Preliminary results indicate that AI-powered guidance can meaningfully boost student involvement, providing a scalable and effective framework for fostering active learning in engineering education.},
  author     = {Hussein, Rania and Zhang, Zhiyun and Amarante, Pedro and Hancock, Nate and Orduna, Pablo and Rodriguez-Gil, Luis},
  booktitle  = {2024 IEEE Frontiers in Education Conference (FIE)},
  doi        = {10.1109/FIE61694.2024.10892918},
  issn       = {2377-634X},
  keywords   = {Remote laboratories;Navigation;Logic gates;Real-time systems;Encoding;Trajectory;Feeds;Artificial intelligence;Engineering education;Field programmable gate arrays;AI assistance;remote laboratories;engineering education;GPT models;personalized learning},
  month      = {Oct},
  number     = {},
  pages      = {1-7},
  title      = {Integrating Personalized AI-Assisted Instruction Into Remote Laboratories: Enhancing Engineering Education with OpenAI's GPT Models},
  volume     = {},
  year       = {2024}
}

@inproceedings{10893046,
  abstract   = {This innovative practice full paper describes a pilot study exploring the integration of ChatGPT, a Conversational Large Language Model (LLM), into the student learning process in software engineering education, which emphasizes principles and methodologies in software development. Focused on a software engineering class, the study examines ChatGPT as a tool for problem clarification, modeling assistance, system design feedback, and implementation support in a project on modeling, designing, and implementing a solution using finite state processes and concurrent programming in Java. A survey designed for the case study collects insights into students' experiences with ChatGPT at different stages of the project. Student feedback on using ChatGPT and their performance on the project are analyzed to understand the impact of conversational LLMs on learning outcomes and to address whether there is room for improvement in enhancing the use of conversational LLMs in software engineering education.},
  author     = {Liu, Yi},
  booktitle  = {2024 IEEE Frontiers in Education Conference (FIE)},
  doi        = {10.1109/FIE61694.2024.10893046},
  issn       = {2377-634X},
  keywords   = {Surveys;Java;Software design;Large language models;Education;Programming;Chatbots;System analysis and design;Software engineering;Software development management;Conversational LLMs;ChatGPT;Software en-gineering education},
  month      = {Oct},
  number     = {},
  pages      = {1-8},
  title      = {Integrating Conversational Large Language Models into Student Learning: A Case Study of ChatGPT in Software Engineering Education},
  volume     = {},
  year       = {2024}
}

@inproceedings{10893118,
  abstract   = {This research-to-practice paper introduces a mini-course module designed to teach computer science students how to interact more efficiently with Generative AI(GAI). The rapid rise of GAI is transforming education by providing students with easy access to knowledge and answers to their questions, acting as a personal tutor. Particularly in the field of computer science, where GAI can easily generate code based on specific requirements, many instructors struggle to prevent students from using tools like ChatGPT for completing assigned programming assignments and homeworks. However, we argue that 1) the use of GAI is inevitable, necessitating a redesign of courses so that students cannot merely rely on GAI without actual learning; and 2) students' learning can be enhanced if they learn to use GAI more effectively. In this paper, we demonstrate how we integrate Project-Based Learning to design the course module in a concise yet effective manner, which not only facilitates students' learning of GAI but also enriches their learning in relation to the host course where this mini-course module is embedded. In particular, the goal of this module is to teach CS students: 1) the basic principles and workflow of GAI; 2) Prompt Engineering: how to craft questions to interact more effectively with GAI; and 3) Extending GAI: how to create interactive tools by training customized GAI models. Designed to be completed within two weeks, the mini-course module can easily be incorporated into host courses. This mini-course module was integrated into a graduate-level Artificial Intelligence course with 42 students in Winter 2024. To assess the module's impact on student learning and engagement, we conducted pre- and post-course surveys as well as student interviews. The results from the surveys and interviews highlighted key areas for improving the design of educational modules to better teach essential GAI skills. These insights focused on enhancing student engagement and learning efficiency within a concise time frame.},
  author     = {Kusam, Venkata Alekhya and Shrestha, Summit and Kattan, Khalid and Maxim, Bruce and Song, Zheng},
  booktitle  = {2024 IEEE Frontiers in Education Conference (FIE)},
  doi        = {10.1109/FIE61694.2024.10893118},
  issn       = {2377-634X},
  keywords   = {Surveys;Training;Codes;Navigation;Chatbots;Prompt engineering;Interviews;Programming profession;Generative AI;Course Module Design;Project Based Learning(PBL)},
  month      = {Oct},
  number     = {},
  pages      = {1-9},
  title      = {A PBL-Based Mini Course Module for Teaching Computer Science Students to Utilize Generative AI for Enhanced Learning},
  volume     = {},
  year       = {2024}
}

@inproceedings{10893139,
  abstract   = {This work-in-progress innovative practice paper describes a novel integration of Generative AI with Course-based Undergraduate Research Experiences (CUREs). CUREs integrate research activities into the curriculum, allowing all students in a course to participate in inquiry-based research projects. Generative Artificial Intelligence (AI) applications are advanced AI designed to generate human-like responses by processing natural language inputs. These applications leverage machine learning models to produce outputs that can assist users in a variety of tasks from writing to coding. The integration of Generative AI with CURE had been adopted in a text-based machine learning course during the Fall 2023 semester. A comparative analysis had been conducted on student survey responses from Fall 2022 and Fall 2023 to evaluate the effectiveness of Generative AI in a CURE integrated course. Descriptive statistics and statistical tests were conducted to assess differences in student perceptions between the two semesters. Although the differences were not statistically significant, the results indicate a promising trend towards improved student perceptions of both the overall course effectiveness and the benefits of Generative AI in enhancing various aspects of the research process, especially the literature review.},
  author     = {Lauren, Paula},
  booktitle  = {2024 IEEE Frontiers in Education Conference (FIE)},
  doi        = {10.1109/FIE61694.2024.10893139},
  issn       = {2377-634X},
  keywords   = {Surveys;Computer science;Generative AI;Natural languages;Machine learning;Writing;Market research;Encoding;Systematic literature review;Course-based Undergraduate Research Experiences (CUREs);Generative AI;Artificial Intelligence in Education (AIEd)},
  month      = {Oct},
  number     = {},
  pages      = {1-5},
  title      = {Work-in-Progress: Course-based Undergraduate Research Experience (CURE) with Generative AI in a Computer Science Course},
  volume     = {},
  year       = {2024}
}

@inproceedings{10893211,
  abstract   = {In this Innovative Practice full paper, we introduce Gurukul, an innovative coding platform designed to support teaching Data Structures and Algorithm (DSA) course by integrating advanced Large Language Models (LLMs). LLMs have emerged as powerful tools in Computer Science Education (CSEd), offering unparalleled opportunities for enhancing student comprehension and engagement. However, their use in educational settings presents challenges, including tendencies toward hallucination, contextual inaccuracies, and the risk of undermining critical thinking by providing explicit solutions. To address these challenges, and to explore how specialized LLMs can bolster learner engagement, we present Gurukul, a platform featuring dual innovations: Retrieval-Augmented Generation (RAG) and Guardrails. Gurukul offers a hands-on practice feature where students can solve DSA problems within a code editor, supported by a dynamically Guardrailed LLM that prevents the delivery of explicit solutions. Additionally, the platform's study feature utilizes RAG, drawing from OpenDSA as a trusted source, to ensure accurate and contextually relevant information is provided. To assess the platform's effectiveness, we conducted a User Study with students, and a User Expert Review with faculty from a U.S. public state university specializing in DSA courses. Our analysis of student usage patterns and perceptions, along with insights from instructors, reveal that Gurukul positively impacted student engagement and learning in DSA, demonstrating the potential of specialized LLMs to enhance educational outcomes in this field.},
  author     = {Rachha, Ashwin and Seyam, Mohammed},
  booktitle  = {2024 IEEE Frontiers in Education Conference (FIE)},
  doi        = {10.1109/FIE61694.2024.10893211},
  issn       = {2377-634X},
  keywords   = {Technological innovation;Codes;Reviews;Large language models;Retrieval augmented generation;Education;Data structures;Encoding;Data models;Computer science education;Large Language Models;Retrieval Augmented Generation;Guardrails;Computer Science Education;ChatGPT;AI in Education},
  month      = {Oct},
  number     = {},
  pages      = {1-9},
  title      = {LLM-Enhanced Learning Environments for CS: Exploring Data Structures and Algorithms with Gurukul},
  volume     = {},
  year       = {2024}
}

@inproceedings{10893214,
  abstract   = {This innovative practice WIP paper describes a research study that explores the integration of ChatGPT into the software testing curriculum and evaluates its effectiveness compared to human-generated testing artifacts. In a Capstone Project course, students were tasked with generating preparatory testing artifacts using ChatGPT prompts, which they had previously created manually. Their understanding and the effectiveness of the Artificial Intelligence generated artifacts were assessed through targeted questions. The results, drawn from this in-class assignment at a North American community college indicate that while ChatGPT can automate many testing preparation tasks, it cannot fully replace human expertise. However, students already familiar with Information Technology at the postgraduate level, found the integration of ChatGPT into their workflow to be straightforward. The study suggests that AI can be gradually introduced into software testing education to keep pace with technological advancements.},
  author     = {Haldar, Susmita and Pierce, Mary and Capretz, Luiz Fernando},
  booktitle  = {2024 IEEE Frontiers in Education Conference (FIE)},
  doi        = {10.1109/FIE61694.2024.10893214},
  issn       = {2377-634X},
  keywords   = {Software testing;Chatbots;Artificial intelligence;North America;Information technology;Software Testing Education;ChatGPT;Black-box Testing;Product Testing;Higher Education},
  month      = {Oct},
  number     = {},
  pages      = {1-5},
  title      = {WIP: Assessing the Effectiveness of ChatGPT in Preparatory Testing Activities},
  volume     = {},
  year       = {2024}
}

@inproceedings{10893284,
  abstract   = {This research to practice WIP paper describes capstone projects in software engineering which effectively combine theoretical education with practical skills, fostering career development in alignment with Career Construction Theory (CCT). This paper introduces a course designed to enhance students' career readiness by incorporating Agile methodologies for soft skills development, proficiency in modern technologies like Large Language Models (LLMs), and targeted career preparation such as resume building. The course's effectiveness, evaluated through CCT adaptability for 42 students, shows a positive impact on career preparedness in three of the four dimensions. This is the first attempt to measure the impact of a capstone course on career development using CCT adaptability. While the initial results are promising, further research is crucial to fully enhance all dimensions of CCT adaptability and to explore the scalability of this study across other engineering fields, potentially transforming engineering education and career preparation on a broader scale.},
  author     = {Song, Isabel Hyo Jung and Wang, Jingyi and Sunico, Rafael and Dahlstrom, Andrew},
  booktitle  = {2024 IEEE Frontiers in Education Conference (FIE)},
  doi        = {10.1109/FIE61694.2024.10893284},
  issn       = {2377-634X},
  keywords   = {Training;Career development;Engineering profession;Scalability;Large language models;Buildings;Project management;Engineering education;Software engineering;career development;software engineering education;capstone course;Career Construction Theory;Agile methodology;Large Language Model (LLM)},
  month      = {Oct},
  number     = {},
  pages      = {1-5},
  title      = {WIP: Enhancing Career Preparedness Through a Software Engineering Capstone Course Design},
  volume     = {},
  year       = {2024}
}

@inproceedings{10893294,
  abstract   = {This innovative practice full paper delves into the transformative role of Learning Assistants (LAs) in Computer Science education, focusing on enhancing student engagement and improving learning outcomes. The LA model, which aligns with Vygotsky's Social Constructivist Learning Theory, fosters an environment of student-centered learning and social interaction. In a pilot study conducted in Spring 2024 at a public university, the LA program is implemented in two computer science courses. A quasi-experimental design has been used to evaluate the impact of LA-facilitated team activities on student learning outcomes. The study compares a control group receiving traditional instruction with an experimental group participating in LA-facilitated team activities. The experimental group engaged in weekly team-based activities, guided by LAs and faculty, to reinforce class concepts and promote collaboration among team members. Student engagement and learning have been evaluated using feedback from students and LAs collected through Discussion Boards (DBs). Preliminary findings suggest that LA-facilitated in-class activities promote active learning and enhance problem-solving skills. LAs provide valuable support and guidance to students, particularly those struggling to understand complex concepts. The study tested a working model of AIELA, an innovative AI-powered chatbot that assists human LAs in supporting students through knowledge-reinforcing questions and multimodal data analysis, powered by OpenAI API's gpt-4-turbo model. This research is a step towards embracing the challenges of modern CS education, inspiring further innovation in this critical field. The findings will benefit educators seeking innovative strategies to enrich student engagement and learning in engineering and computing disciplines.},
  author     = {Ramasamy, Vijayalakshmi and Kulpinski, Eli and Beaupre, Thomas and Antreassian, Aaron and Jeong, Yunhwan and Clarke, Peter J. and Aiello, Anthony and Ray, Charles},
  booktitle  = {2024 IEEE Frontiers in Education Conference (FIE)},
  doi        = {10.1109/FIE61694.2024.10893294},
  issn       = {2377-634X},
  keywords   = {Technological innovation;Analytical models;Computational modeling;Education;Active learning;Collaboration;Prototypes;Data models;Problem-solving;Springs;Learning Assistant model;active learning strategies;Artificial Intelligence-enabled chatbot;student engagement},
  month      = {Oct},
  number     = {},
  pages      = {1-9},
  title      = {Enhancing CS Education with LAs Using AI-Empowered AIELA Program},
  volume     = {},
  year       = {2024}
}

@inproceedings{10893333,
  abstract   = {Since the introduction of generative artificial intelligence (GenAI), education in computer science has prompted efforts to incorporate it into the educational curriculum. This innovative practice full paper presents a study into using GenAI to enhance student learning of software engineering. It outlines the initiatives to introduce GenAI into a graduate-level software engineering course in Software Verification and Validation (SV&V). The paper presents the educational goals, methodologies and findings of these endeavors in this course. The primary education goal of this course is that students have a solid understanding of principles and practices of software quality assurance and seek to introduce students to diverse techniques employed for SV&V. The study presented in this paper centers on the practical application of GenAI within the domain of testing strategies. The paper introduces the findings of an exercise where GenAI was used to apply testing strategies for unit testing. The exercise consisted of the use of GenAI in the development of unit tests for an algorithm. Rigorous assessments were conducted to gauge the effectiveness of the unit tests developed for validating the accurate implementation of the algorithm. This exploration shed light on the tangible impact of GenAI on the precision and efficiency of unit testing procedures. The findings underscore the significance of encouraging students to actively explore emerging trends and methodologies in the realm of software verification and validation. By incorporating GenAI into the educational framework, students not only gain insights into the capabilities and limitations of this technology but also foster a mindset of continuous learning in software quality assurance. The paper demonstrates that it is not sufficient to use the test cases developed by GenAI for software validation since test cases recommended by GenAI do not cover corner cases which causes gaps in coverage in unit testing. The majority of the students were able to understand the limitation of GenAI in SV&V but appreciated its support in suggesting test cases for the most common cases. This exercise allowed students to enhance their creative problem-solving through human-guided AI partnership which is pivotal in cultivating a new generation of professionals capable of contributing to the ongoing evolution of software quality.},
  author     = {Tuzmen, Ayca},
  booktitle  = {2024 IEEE Frontiers in Education Conference (FIE)},
  doi        = {10.1109/FIE61694.2024.10893333},
  issn       = {2377-634X},
  keywords   = {Technological innovation;Codes;Generative AI;Education;Software algorithms;Software quality;Solids;Problem-solving;Testing;Software engineering;generative artificial intelligence;software verification and validation;unit testing},
  month      = {Oct},
  number     = {},
  pages      = {1-6},
  title      = {Use of Generative Artificial Intelligence in the Education of Software Verification and Validation},
  volume     = {},
  year       = {2024}
}

@inproceedings{10893343,
  abstract   = {This innovative practice full paper explores the use of AI technologies in user story generation. With the emergence of agile software development, generating comprehensive user stories that capture all necessary functionalities and perspectives has become crucial for software development. Every computing program in the United States requires a semester-or year-long senior capstone project, which requires student teams to gather and document technical requirements. Effective user story generation is crucial for successfully implementing software projects. However, user stories written in natural language can be prone to inherent defects such as incompleteness and incorrectness, which may creep in during the downstream development activities like software designs, construction, and testing. One of the challenges faced by software engineering educators is to teach students how to elicit and document requirements, which serve as a blueprint for software development. Advanced AI technologies have increased the popularity of large language models (LLMs) trained on large multimodal datasets. Therefore, utilizing LLM-based techniques can assist educators in helping students discover aspects of user stories that may have been overlooked or missed during the manual analysis of requirements from various stakeholders. The main goal of this research study is to investigate the potential application of OpenAI techniques in software development courses at two academic institutions to enhance software design and development processes, aiming to improve innovation and efficiency in team project-based educational settings. The data used for the study constitute student teams generating user stories by traditional methods (control) vs. student teams using OpenAI agents (treatment) such as gpt-4-turbo for generating user stories. The overarching research questions include: RQ-l) What aspects of user stories generated using OpenAI prompt engineering differ significantly from those generated using the traditional method? RQ-2) Can the prompt engineering data provide insights into the efficacy of the questions/prompts that affect the quality and comprehensiveness of user stories created by software development teams? Industry experts evaluated the user stories created and analyzed how prompt engineering affects the overall effectiveness and innovation of user story creation, which provided guidelines for incorporating AI-driven approaches into software development practices. Overall, this research seeks to contribute to the growing body of knowledge on the application of AI in software engineering education, specifically in user story generation. Investigating the use of AI technologies in user story generation could further enhance the usability of prompt engineering in agile software development environments. We plan to expand the study to investigate the long-term effects of prompt engineering on all phases of software development.},
  author     = {Ramasamy, Vijayalakshmi and Ramamoorthy, Suganya and Walia, Gursimran Singh and Kulpinski, Eli and Antreassian, Aaron},
  booktitle  = {2024 IEEE Frontiers in Education Conference (FIE)},
  doi        = {10.1109/FIE61694.2024.10893343},
  issn       = {2377-634X},
  keywords   = {Technical requirements;Technological innovation;Agile software development;Collaboration;Software;Prompt engineering;Stakeholders;Usability;Software engineering;Testing;Collaboration network;complex network analysis;structured collaboration network},
  month      = {Oct},
  number     = {},
  pages      = {1-8},
  title      = {Enhancing User Story Generation in Agile Software Development Through Open AI and Prompt Engineering},
  volume     = {},
  year       = {2024}
}

@inproceedings{10893407,
  abstract   = {This work in progress research paper describes a pilot study using a Large Language Model (LLM) Generative Pre-Trained Transformer-based (GPT) system that generates industry-style code reviews for student feedback on software development projects in Computer Science 2nd, 3rd, and 4th+ semester classes (CS2, CS3, CS4+) at an ABET accredited baccalaureate institution. Code reviews are a valuable, but work-intensive, component of the software engineering process and provide important training to undergraduate students in the form of mentor-peer knowledge transfer. Participants in this study engaged in iterative experiential learning using the Automatic Review Tool (ART), an artificial intelligence tool to support software engineering as an Automatic Static Analysis Tool in the Continuous Integration pipeline alongside software testing harnesses and code style checkers. This pilot study was based on earlier results from a full computer science second semester (CS2) class $(\mathrm{n} = 74)$ to develop an ART-generated code review intervention pilot study with a small group of students in CS2 / 3 and CS4. The project underway uses an experiential learning and iterative feedback process to answer research questions including “Does ART provide accurate and actionable code reviews for students” and “Which levels of students are best prepared to receive and use ART-based code reviews?” During this pilot study, the project used a mixed methods research approach with a series of surveys, code review interventions, and numerical analysis of the code reviews' accuracy. Results showed a reasonable degree of code review accuracy by ART and the students learned code review skills from interaction with the ART-based reviews they received. Ongoing work includes increasing the scale of data collection, using this work to refine and focus the ART-based reviews onto the categories of feedback that students find the most valuable, and building out a more modular tool for wider release in the academic community.},
  author     = {Crandall, Aaron S. and Fischer, Bryan J. and Crandall, Johannah L.},
  booktitle  = {2024 IEEE Frontiers in Education Conference (FIE)},
  doi        = {10.1109/FIE61694.2024.10893407},
  issn       = {2377-634X},
  keywords   = {Computer science;Training;Surveys;Codes;Accuracy;Reviews;Subspace constraints;Transformers;Iterative methods;Software engineering;Adaptive computer learning;Computer science;Qualitative;Mixed methods research;Code Reviews;Software Engineering Education},
  month      = {Oct},
  number     = {},
  pages      = {1-5},
  title      = {WIP: ARTful Insights from a Pilot Study on GPT-Based Automatic Code Reviews in Undergraduate Computer Science Programs},
  volume     = {},
  year       = {2024}
}

@inproceedings{10893452,
  abstract   = {This innovative practice full paper describes an indepth analysis of the pedagogical implications of incorporating generative artificial intelligence (genAI) tools, specifically Chat-GPT, into a data science course for postgraduate masters computing students. This research is grounded in the implementation of ChatGPT in a data analysis course, aiming to evaluate its effectiveness in fostering students' analytical and decision-making capabilities. The study employs a qualitative methodology to assess the educational outcomes of integrating ChatGPT, focusing on its impact on student engagement, learning efficiency, and the development of critical thinking skills in the context of data science. Through a combination of interviews, and analysis of students' project outcomes, we gather insights into the challenges and opportunities presented using genAI in the data science course. A notable innovation of our approach is the introduction of a dual-report assessment method, which not only evaluates the students' project results but also their proficiency in prompt engineering - a crucial skill for effective interaction with genAI tools. Our findings suggest that while students demonstrate enhanced data analysis skills, they also face difficulties in accurately framing queries to yield useful results from genAI, highlighting an essential area for further curriculum development. Further-more, the work delves into the pedagogical strategies that can optimize the benefits of genAI tools in education. It emphasizes the importance of a structured framework that guides students in the ethical use of genAI, encourages critical reflection on AI-generated content, and fosters a deeper understanding of the underlying algorithms and their implications for data science. The implications of this research extend beyond the classroom, offering valuable insights for instructors, curriculum developers, and policymakers on integrating AI technologies into educational practices. By providing a comprehensive overview of the benefits and challenges associated with the use of ChatGPT in data science education, this paper contributes to the ongoing dialogue on preparing students for a future where genAI might a significant role. In conclusion, this work highlights the potential of genAI to revolutionize data science education by enhancing analytical skills and decision-making capabilities. Continued exploration of effective strategies for integrating AI tools into learning environments, such as data science, is required to ensure that students are equipped with the knowledge and skills necessary to navigate the complexities of genAI for future employment.},
  author     = {Browning, Jonathan W. and Bustard, John and Anderson, Neil and Galway, Leo},
  booktitle  = {2024 IEEE Frontiers in Education Conference (FIE)},
  doi        = {10.1109/FIE61694.2024.10893452},
  issn       = {2377-634X},
  keywords   = {Knowledge engineering;Technological innovation;Navigation;Education;Decision making;Data science;Chatbots;Reflection;Prompt engineering;Interviews;computer engineering;data science;education;generative artificial intelligence;student experience},
  month      = {Oct},
  number     = {},
  pages      = {1-7},
  title      = {A Data Science Course Utilizing GenAI},
  volume     = {},
  year       = {2024}
}

@inproceedings{10893530,
  abstract   = {This research paper systematically identifies the perceptions of learning machine learning (ML) topics. To keep up with the ever-increasing need for professionals with ML expertise, for-profit and non-profit organizations conduct a wide range of ML-related courses at undergraduate and graduate levels. Despite the availability of ML-related education materials, there is lack of understanding how students perceive ML-related topics and the dissemination of ML-related topics. A systematic categorization of students' perceptions of these courses can aid educators in understanding the challenges that students face, and use that understanding for better dissemination of ML-related topics in courses. The goal of this paper is to help educators teach machine learning (ML) topics by providing an experience report of students' perceptions related to learning ML. We accomplish our research goal by conducting an empirical study where we deploy a survey with 83 students across five academic institutions. These students are recruited from a mixture of undergraduate and graduate courses. We apply a qualitative analysis technique called open coding to identify challenges that students encounter while studying ML-related topics. Using the same qualitative analysis technique we identify quality aspects do students prioritize ML-related topics. From our survey, we identify 11 challenges that students face when learning about ML topics, amongst which data quality is the most frequent, followed by hardware-related challenges. We observe the majority of the students prefer hands-on projects over theoretical lectures. Furthermore, we find the surveyed students to consider ethics, security, privacy, correctness, and performance as essential considerations while developing ML-based systems. Based on our findings, we recommend educators who teach ML-related courses to (i) incorporate hands-on projects to teach ML-related topics, (ii) dedicate course materials related to data quality, (iii) use lightweight virtualization tools to showcase computationally intensive topics, such as deep neural networks, and (iv) empirical evaluation of how large language models can be used in ML-related education.},
  author     = {Farhana, Effat and Wu, Fan and Shahriar, Hossain and Karmaker Santu, Shubhra Kanti and Rahman, Akond},
  booktitle  = {2024 IEEE Frontiers in Education Conference (FIE)},
  doi        = {10.1109/FIE61694.2024.10893530},
  issn       = {2377-634X},
  keywords   = {Surveys;Privacy;Ethics;Systematics;Data integrity;Education;Machine learning;Security;Virtualization;Faces;artifical intelligence;empirical study;machine learning;perception},
  month      = {Oct},
  number     = {},
  pages      = {1-9},
  title      = {Challenges and Preferences of Learning Machine Learning: A Student Perspective},
  volume     = {},
  year       = {2024}
}

@inproceedings{10923772,
  abstract   = {This work examines the usage and experience of students using generative AI (genAl) in a engineering entrepreneurship experiential-based learning course. It utilizes a project-based learning approach, where students are in teams of five. The course takes place over a full academic year, i.e., the fall and spring semesters. One of the aims of the course is to be as realistic as possible within an academic setting, with the students trying to create a new technology-based business, in the hopes that they will continue their ventures after the course concludes. Therefore, the teams were allowed to and encouraged to use any genAI they wish throughout the course to assist them. Outside of academia, it would be expected that a start-up founder would use genAl to speed up many aspects of starting their business. Therefore, we want to examine how the students use genAI without there being any constraints. As part of the summative assessment for the course each student peer assesses the members of their team. Included within that they also “peer assess” genAI as though it were another team member, which is a critical reflection of their genAI usage. This work addresses how students use genAI, in experiential-based learning courses when they are allowed to use it any way possible to assist them. The study uses a quantitative and qualitative approach with student data from their “peer assessment” (i.e., critical reflection) of their genAI usage from the academic year 2023/24. Within the peer assessment of the genAI team member the students answer Likert-scale statements to be rated on a five-point semantic differential scale. The results indicate a varied adoption and value of genAI across different project phases. While some students appreciated genAI for speeding up specific tasks, its contribution to creative processes like ideation was less impactful.},
  author     = {Browning, Jonathan W. and Bustard, John and Anderson, Neil and Galway, Leo},
  booktitle  = {2024 IEEE 13th International Conference on Engineering Education (ICEED)},
  doi        = {10.1109/ICEED62316.2024.10923772},
  issn       = {},
  keywords   = {Training;Electrical engineering;Generative AI;Soft sensors;Semantics;Project management;Entrepreneurship;Reflection;Springs;Engineering education;computer engineering;critical reflection;electrical engineering;experiential learning;student experience},
  month      = {Nov},
  number     = {},
  pages      = {1-6},
  title      = {Evaluating the Impact of Unrestricted GenAI Usage on Experiential-Based Learning},
  volume     = {},
  year       = {2024}
}

@inproceedings{10971486,
  abstract   = {This paper introduces Text2Net, an innovative text-based network simulation engine that leverages natural language processing (NLP) and large language models (LLMs) to transform plain-text descriptions of network topologies into dynamic, interactive simulations. Text2Net simplifies the process of configuring network simulations, eliminating the need for users to master vendor-specific syntaxes or navigate complex graphical interfaces. Through qualitative and quantitative evaluations, we demonstrate Text2Net's ability to significantly reduce the time and effort required to deploy network scenarios compared to traditional simulators like EVE-NG. By automating repetitive tasks and enabling intuitive interaction, Text2Net enhances accessibility for students, educators, and professionals. The system facilitates hands-on learning experiences for students that bridge the gap between theoretical knowledge and practical application. The results showcase its scalability across various network complexities, marking a significant step toward revolutionizing network education and professional use cases, such as proof-of-concept testing.},
  author     = {Marefat, Alireza and Nishar, Abbaas Alif Mohamed and Ashok, Ashwin},
  booktitle  = {SoutheastCon 2025},
  doi        = {10.1109/SoutheastCon56624.2025.10971486},
  issn       = {1558-058X},
  keywords   = {Protocols;Navigation;Scalability;Transforms;Routing;Rapid prototyping;Natural language processing;Virtual private networks;Pattern matching;Testing;Network Simulation and Emulation;Educational Technology;AI in Education;Interactive Learning Environments;Network Configuration Automation;AI-driven Network},
  month      = {March},
  number     = {},
  pages      = {625-630},
  title      = {Text2Net: Transforming Plain-text to a Dynamic Interactive Network Simulation Environment},
  volume     = {},
  year       = {2025}
}

@inproceedings{10981323,
  abstract   = {The integration of artificial intelligence into educational tools is transforming learning environments. In computer science, students frequently encounter challenges with complex concepts and practical applications. While LLMs offer valuable support, their responses can sometimes lack precision or rely on outdated information. Addressing these limitations is essential to enhance the effectiveness of AI-based tools in educational support. The main objective of this paper is to propose a tool-based RAG as automated assistant in an OOP Course. The tool was tested using common student queries and its responses were compared with those generated by ChatGPT. Initial observations suggest that RAG consistently generated contextually relevant responses due to its ability to access pertinent information from a structured knowledge base, resulting in more precise and applicable answers for students. The introduction of a RAG-based tool in classrooms has the potential to enhance student learning by providing instant, tailored responses to specific queries.},
  author     = {Soto, Wilson},
  booktitle  = {2025 IEEE Engineering Education World Conference (EDUNINE)},
  doi        = {10.1109/EDUNINE62377.2025.10981323},
  issn       = {},
  keywords   = {Computer science;Large language models;Knowledge based systems;Learning (artificial intelligence);Chatbots;Object oriented programming;Engineering education;Artificial Intelligence;Automated Assistant;Large Language Models (LLMs);OOP (Object-Oriented Programming);RAG (Retrieval-Augmented Generative)},
  month      = {March},
  number     = {},
  pages      = {1-6},
  title      = {Tool-Based Retrieval-Augmented Generative as an Automated Assistant in Object-Oriented Programming Course},
  volume     = {},
  year       = {2025}
}

@inproceedings{10989326,
  abstract   = {Providing high-quality instructional feedback is essential for enhancing learning and motivation. However, the large number of learners and time constraints of instructors can hinder this process. Automating instructional feedback through online learning environments presents a potential solution, but current systems may not adequately address complex learning assignments. Artificial intelligence technologies like ChatGPT can offer an opportunity to deliver feedback efficiently, especially for complex learning tasks. Despite its potential, relatively few studies have examined the distinctions between ChatGPT-generated and expert-based feedback and learners' perceptions of these feedback sources. This study stands among the limited research efforts that seek to compare instructional feedback produced by ChatGPT and that provided by experts, shedding light on learners' subjective assessment of such feedback in the context of programming projects. The findings showed that experts consistently rated programming projects higher than ChatGPT. However, ChatGPT demonstrated strengths in providing more comprehensive textual feedback on the learners' work. Notably, the learners rated ChatGPT feedback more favorably than feedback from domain experts. The study's findings are thoroughly discussed, and future research avenues are outlined.},
  author     = {Alshammari, Mohammad T.},
  booktitle  = {2025 4th International Conference on Computing and Information Technology (ICCIT)},
  doi        = {10.1109/ICCIT63348.2025.10989326},
  issn       = {},
  keywords   = {Codes;Accuracy;Education;Focusing;Learning (artificial intelligence);Writing;Chatbots;Time factors;Information technology;Programming profession;ChatGPT;programming;education;feedback},
  month      = {April},
  number     = {},
  pages      = {624-628},
  title      = {An Investigation into ChatGPT-Based Instructional Feedback on Programming Projects},
  volume     = {},
  year       = {2025}
}

@article{11005596,
  abstract   = {Generative AI has immense potential to create diverse computer graphics for various applications, but it also raises significant ethical issues. This paper examines the ethical landscape of using generative AI in computer graphics, highlighting key concerns such as the authenticity of generated content, intellectual property rights, and cultural appropriation. Additional ethical challenges include algorithmic bias in graphics generation, representation, privacy, inclusivity, and the impact on human-computer interaction and artistic integrity. The displacement of creative professionals, erosion of trust in visual media, and psychological effects of AI-generated content further complicate the ethical debate. Addressing these issues requires a comprehensive approach that integrates technological innovation with regulatory oversight, ethical education, and collaboration among stakeholders. By carefully considering these ethical dimensions, we can fully leverage generative AI's potential in computer graphics while mitigating its risks.},
  author     = {Routray, Sudhir K.},
  doi        = {10.1109/MCG.2025.3570722},
  issn       = {1558-1756},
  journal    = {IEEE Computer Graphics and Applications},
  keywords   = {Generative AI;Ethics;Artificial intelligence;Technological innovation;Creativity;Intellectual property;Training;Data models;Cultural differences;Computational modeling},
  month      = {},
  number     = {},
  pages      = {1-11},
  title      = {Ethical Considerations and Implications of Generative AI in Computer Graphics},
  volume     = {},
  year       = {2025}
}

@inproceedings{11016382,
  abstract   = {The emergence of generative artificial intelligence (GenAI) has brought both challenges and opportunities for education. In this paper, we propose a GenAI-empowered, group-based authentic assessment for a Network Engineering course. This group assignment promotes challenge-based learning (CBL) and leverages GenAI to enhance students' creativity, critical thinking, collaboration, and technical problem-solving skills, while also improving students' GenAI literacy through fostering their ability to effectively engage with GenAI tools. The authenticity of this assignment is reflected in two folds: 1) students engage in a real-world engineering challenge, roleplaying as network engineers, and 2) they develop essential skills for co-creating solutions using GenAI tools, a key competency for future engineers. The group assignment is structured into five stages, each aligned with Bloom's Taxonomy to progressively develop cognitive skills from understanding foundational knowledge to synthesis, evaluation, and creation. To mitigate challenges such as overreliance on GenAI tools and varying levels of digital literacy, we provide guidance on the responsible and ethical use of GenAI, design reflective assessment tasks with constructive feedback, and establish clear marking criteria that emphasise both the learning process and the final outputs of the assignment. Initial evaluation and feedback from trials have highlighted the effectiveness of using GenAI tools in addressing complex engineering challenges and the value of collaborating in a real-world engineering context. This innovative approach demonstrates the potential of GenAIempowered authentic assessments to enhance learning experiences in technical fields like Network Engineering.},
  author     = {Chen, Yue and Chai, Kok Keong and Loo, Jonathan and Moosaei, Reza and Obstfeld, Joel},
  booktitle  = {2025 IEEE Global Engineering Education Conference (EDUCON)},
  doi        = {10.1109/EDUCON62633.2025.11016382},
  issn       = {2165-9567},
  keywords   = {Ethics;Generative AI;Taxonomy;Documentation;Data collection;Teamwork;Problem-solving;Digital intelligence;Engineering education;Creativity;GenAI;Authentic Assessment;ChallengedBased Learning;Group-Based Learning;Network Engineering},
  month      = {April},
  number     = {},
  pages      = {1-7},
  title      = {GenAI-Empowered Group-Based Authentic Assessment for Network Engineering Courses},
  volume     = {},
  year       = {2025}
}

@inproceedings{11016449,
  abstract   = {This paper presents an AI-assisted approach that leverages Multimodal Large Language Models (MLLMs) to automate the generation of Multiple-Choice Questions (MCQs) for modules in engineering education. The system introduces a LOs extraction to MCQs generation pipeline, which extracts Learning Outcomes (LOs) from provided lecture notes and generates relevant MCQs with solutions and explanations based on the extracted LOs. By harnessing MLLMs' capabilities in vision and text comprehension, coupled with carefully crafted prompts from human educators, the tool efficiently produces context-relevant MCQs that can streamline teaching material development. The effectiveness of this AI-powered MCQ generation pipeline is investigated through experiments across a number of engineering modules with evaluations on the quality of the generated MCQs by human educators. The analysis of the evaluation results shows the AI tool's ability to generate MCQs that are well-aligned with LOs and exhibit strong contextual relevance, demonstrating the potential of AI-assisted approaches to enhance the efficiency of creating high-quality MCQs in engineering education. However, the variability in quality ratings across different aspects underscores the continued need for human expertise and oversight in the assessment design process. The findings provide useful insights into the capabilities and limitations of state-of-the-art multimodal language models in supporting assessment development in engineering education.},
  author     = {Shu, Chao and Yao, Na and Chen, Yue and Wijeratne, Vindya and Ma, Ling and Loo, Jonathan and Chai, Kok Keong and Alam, Atm and Abuelmaatti, Aisha},
  booktitle  = {2025 IEEE Global Engineering Education Conference (EDUCON)},
  doi        = {10.1109/EDUCON62633.2025.11016449},
  issn       = {2165-9567},
  keywords   = {Generative AI;Large language models;Pipelines;Learning (artificial intelligence);Question generation;Engineering education;Multiple-choice Question design;Learning Outcome authoring;Generative Artificial Intelligence (GenAI);Multimodal Large Language Models;Contextual generation;Engineering Education},
  month      = {April},
  number     = {},
  pages      = {1-9},
  title      = {Ai-Assisted Multiple-Choice Questions Generation with Multimodal Large Language Models in Engineering Higher Education},
  volume     = {},
  year       = {2025}
}

@inproceedings{11016463,
  abstract   = {This research examines the impact of ChatGPT on computer science education, focusing on its application in learning programming languages like SQL, C++, Python, and C#. Through a survey of 149 university students, the study identifies both the advantages and challenges of using ChatGPT as a virtual lab assistant. The results show that ChatGPT offers considerable assistance to students, especially in providing prompt feedback, facilitating debugging, and clarifying complex programming concepts. However, the research also points out significant challenges, including the potential for over-reliance on AI tools and worries about the accuracy of the responses generated by AI. Several students reported experiencing misleading or incomplete information from ChatGPT, indicating a need for enhancements in its accuracy and dependability. To address these challenges, the study proposes a transition from assignments focused on theory to personalized, project-based activities that foster independent problem-solving. In summary, this research sheds light on the advantages and obstacles of incorporating ChatGPT into computer science courses. Although ChatGPT provides beneficial support for learning, its function should be supplementary rather than central to programming education. The results highlight the necessity of thoughtful integration, promoting self-directed learning while utilizing AI's capabilities to foster engagement and offer immediate assistance.},
  author     = {Bhatt, Vishwa and Yu, Zhixin and Hou, Yunfei and Jin, Jennifer},
  booktitle  = {2025 IEEE Global Engineering Education Conference (EDUCON)},
  doi        = {10.1109/EDUCON62633.2025.11016463},
  issn       = {2165-9567},
  keywords   = {Training;Surveys;Accuracy;Systematics;Debugging;Chatbots;Prompt engineering;Problem-solving;Artificial intelligence;Programming profession;ChatGPT;Generative AI;Programming education;AI dependency;AI accuracy;Programming Tutor},
  month      = {April},
  number     = {},
  pages      = {1-10},
  title      = {ChatGPT as a Programming Tutor: Student Perceptions, Effectiveness, and Challenges},
  volume     = {},
  year       = {2025}
}

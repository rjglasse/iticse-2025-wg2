@inproceedings{10.1145/3657604.3664665,
author = {Koutcheme, Charles and Hellas, Arto},
title = {Propagating Large Language Models Programming Feedback},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3664665},
doi = {10.1145/3657604.3664665},
abstract = {Large language models (LLMs) such as GPT-4 have emerged as promising tools for providing programming feedback. However, effective deployment of LLMs in massive classes and Massive Open Online Courses (MOOCs) raises financial concerns, calling for methods to minimize the number of calls to the APIs and systems serving such powerful models. In this article, we revisit the problem of 'propagating feedback' within the contemporary landscape of LLMs. Specifically, we explore feedback propagation as a way to reduce the cost of leveraging LLMs for providing programming feedback at scale. Our study investigates the effectiveness of this approach in the context of students requiring next-step hints for Python programming problems, presenting initial results that support the viability of the approach. We discuss our findings' implications and suggest directions for future research in optimizing feedback mechanisms for large-scale educational environments.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {366–370},
numpages = {5},
keywords = {computer science education, large language models, programming feedback},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@inproceedings{10.1145/3641555.3705215,
author = {Niousha, Rose and O'Neill, Abigail and Chen, Ethan and Malhotra, Vedansh and Akram, Bita and Norouzi, Narges},
title = {LLM-KCI: Leveraging Large Language Models to Identify Programming Knowledge Components},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705215},
doi = {10.1145/3641555.3705215},
abstract = {Identifying Knowledge Components (KCs) in computer science education improves curriculum design and teaching strategies. We introduce a framework using Large Language Models to identify KCs from programming assignments automatically. Our framework helps educators align assignments with course objectives. GPT-4 identifies relevant KCs well, though there's a low match with expert-generated KCs at the course level. At the problem level, performance is lower, but key KCs are reasonably identified.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1557–1558},
numpages = {2},
keywords = {cs1, knowledge component, large language model},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3641555.3705195,
author = {Marwan, Samiha and Ibrahim, Mohamed and Morrison, Briana},
title = {How Good are Large Language Models at Generating Subgoal Labels?},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705195},
doi = {10.1145/3641555.3705195},
abstract = {The use of subgoal labels in introduction to programming classrooms has been shown to improve student performance, learning, retention, and reduce students' drop out rates. However, creating and adding subgoal labels to programming assignments is often hard to articulate and very time-intensive for instructors. In Computing Education Research, Large Language Models (LLMs) have been widely used to generate human-like outputs such as worked examples and source code. In this work, we explore whether ChatGPT could be used to generate high-quality and appropriate subgoal labels in two programming curricula. Our qualitative data analysis suggests that LLMs can assist instructors in creating subgoal labels in their classrooms, opening up directions to empower students' learning experience in programming classrooms.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1541–1542},
numpages = {2},
keywords = {large language models, subgoal labels, subgoals},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3641554.3701841,
author = {Aljedaani, Wajdi and Eler, Marcelo Medeiros and Parthasarathy, P D},
title = {Enhancing Accessibility in Software Engineering Projects with Large Language Models (LLMs)},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701841},
doi = {10.1145/3641554.3701841},
abstract = {Digital accessibility ensures that digital products and services are usable by a diverse range of users, regardless of their physical or cognitive abilities. While numerous standards and guidelines have been established to aid developers in creating accessible content, studies reveal a persistent lack of accessibility in many web and mobile applications. This gap is often attributed to barriers such as lack of awareness, insufficient knowledge, absence of specific requirements, time constraints, and lack of executive support. In this context, we aim to address the lack of awareness and knowledge challenges by proposing a hands-on approach that leverages the capabilities of Large Language Models (LLMs) like ChatGPT to enhance students' accessibility awareness, knowledge, and practical skills. We engaged software engineering students in tasks involving website development and accessibility evaluation using checker tools, and we utilized ChatGPT 3.5 to fix identified accessibility issues. Our findings suggest that practical assignments significantly enhance learning outcomes, as interactions with LLMs allow students to develop a deeper understanding of accessibility concepts. This approach not only reinforces theoretical knowledge but also highlights the real-world impact of their work. The results indicate that combining practical assignments with AI-driven support effectively improves students' proficiency in web accessibility.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {25–31},
numpages = {7},
keywords = {chatgpt 3.5, digital accessibility, large language models, llms, project based learning, software engineering, wcag},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3641554.3701906,
author = {Hassan, Mohammed and Chen, Yuxuan and Denny, Paul and Zilles, Craig},
title = {On Teaching Novices Computational Thinking by Utilizing Large Language Models Within Assessments},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701906},
doi = {10.1145/3641554.3701906},
abstract = {Novice programmers often struggle to develop computational thinking (CT) skills in introductory programming courses. This study investigates the use of Large Language Models (LLMs) to provide scalable, strategy-driven feedback to teach CT. Through think-aloud interviews with 17 students solving code comprehension and writing tasks, we found that LLMs effectively guided decomposition and program development tool usage. Challenges included students seeking direct answers or pasting feedback without considering suggested strategies. We discuss how instructors should integrate LLMs into assessments to support students' learning of CT.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {471–477},
numpages = {7},
keywords = {code comprehension, debuggers, execution, large language models},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3626253.3633436,
author = {Leinonen, Juho and MacNeil, Stephen and Denny, Paul and Hellas, Arto},
title = {Using Large Language Models for Teaching Computing},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3633436},
doi = {10.1145/3626253.3633436},
abstract = {In the past year, large language models (LLMs) have taken the world by storm, demonstrating their potential as a transformative force in many domains including computing education. Computing education researchers have found that LLMs can solve most assessments in introductory programming courses, including both traditional code writing tasks and other popular tasks such as Parsons problems. As more and more students start to make use of LLMs, the question instructors might ask themselves is "what can I do?". We propose that one promising way forward is to integrate LLMs into teaching practice, providing all students with an equal opportunity to learn how to interact productively with LLMs as well as encounter and understand their limitations. In this workshop, we first present state-of-the-art research results on how to utilize LLMs in computing education practice, after which participants will take part in hands-on activities using LLMs. We end the workshop by brainstorming ideas with participants around adapting their classrooms to most effectively integrate LLMs while avoiding some common pitfalls.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1901},
numpages = {1},
keywords = {generative ai, large language models, teaching practice},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3716640.3716652,
author = {Edwards, John and Hellas, Arto and Leinonen, Juho},
title = {On the Opportunities of Large Language Models for Programming Process Data},
year = {2025},
isbn = {9798400714252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716640.3716652},
doi = {10.1145/3716640.3716652},
abstract = {Computing educators and researchers have long used programming process data to understand how students construct programs and address challenges. Despite its potential, fully automated feedback systems remain underexplored. The emergence of Large Language Models (LLMs) offers new opportunities for analyzing programming data and providing formative feedback. This study explores using LLMs to summarize programming processes and deliver formative feedback. A case study analyzed keystroke-level data from an introductory programming course, processed into code snapshots. Three state-of-the-art LLMs – Claude 3 Opus, GPT-4 Turbo, and LLaMa2 70B Chat – were evaluated for their feedback capabilities. Results show LLMs effectively provide tailored feedback, emphasizing incremental development, algorithmic planning, and code readability. Our findings highlight the potential of combining keystroke data with LLMs to automate formative feedback, showing that the computing education research and practice community is again one step closer to automating formative programming process feedback.},
booktitle = {Proceedings of the 27th Australasian Computing Education Conference},
pages = {105–113},
numpages = {9},
keywords = {programming process data, large language models, generative AI, programming process feedback, programming process summarization, keystroke data},
location = {
},
series = {ACE '25}
}

@inproceedings{10.1145/3641555.3705235,
author = {Gonzalez, Elias and Chan, Joel and Weintrop, David},
title = {Quack! Configuring Large Language Models to Serve as Rubber Duck Coding Assistants},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705235},
doi = {10.1145/3641555.3705235},
abstract = {The emergence of Generative Artificial Intelligence (GenAI) tools broadly, and Large Language Models (LLMs) specifically, are equipping introductory programming instructors with a whole new class of pedagogical tools. While GenAI certainly poses threats to time-honored instructional techniques, it also provides opportunities for new forms of instructional support. In this work, we introduce our strategy for configuring an LLM to serve as a ''rubber duck debugging'' coding assistant to help novice programmers when they encounter difficulties in programming assignments. The key contribution of this work is not in the idea of using LLMs for debugging itself (which has already been demonstrated elsewhere, e.g., [3]) but to demonstrate the ease, flexibility, and pedagogical potential of the strategy. In particular, through carefully crafted prompts and easily accessible platforms, rubber duck LLMs can assist learners with specific questions while also situating those questions alongside larger computer science concepts and computational thinking practices. This work contributes an easily replicated and model-agnostic instructional strategy that productively and responsibly leverages the power of LLMs to assist novice programmers in developing foundational programming skills.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1463–1464},
numpages = {2},
keywords = {computer science education, generative ai, introductory programming, large language models},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3686852.3686887,
author = {Chhetri, Chola},
title = {Exploring Large Language Model-Powered Pedagogical Approaches to Cybersecurity Education},
year = {2024},
isbn = {9798400711060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686852.3686887},
doi = {10.1145/3686852.3686887},
abstract = {The adoption of artificial intelligence (AI) technologies by businesses and corporations is rising. AI technologies continue to be adopted in cybersecurity for both defensive and offensive strategies. However, threat actors also persist in utilizing these technologies to enhance the speed, accuracy, and sophistication of their attacks. Hence, it is essential to train the next generation of cybersecurity learners not only on how to use AI technology but also on how to leverage these technologies to enhance the efficiency of their work. This extended abstract describes our exploratory work on the use of generative AI-based pedagogical approaches in cybersecurity education. This extended abstract will describe some preliminary findings on large language model-powered pedagogical approaches to cybersecurity education and training. These approaches will help cybersecurity educators enhance their teaching methods to equip learners with the essential skills needed to succeed in the dynamic field of cybersecurity.},
booktitle = {Proceedings of the 25th Annual Conference on Information Technology Education},
pages = {163–166},
numpages = {4},
keywords = {AI, Artificial intelligence, GenAI, LLM, cybersecurity, education., generative AI, large language models},
location = {El Paso, TX, USA},
series = {SIGITE '24}
}

@book{10.1145/3708897,
author = {Giacaman, Nasser and Terragni, Valerio},
title = {Empowering Computing Students with Large Language Models by Developing an Escape Room Game},
year = {2025},
isbn = {9798400714450},
abstract = {In this project, computing students learn to integrate large language models (LLMs) into a software system. Students develop a Java application with a basic graphical user interface (GUI) using JavaFX, gain practical experience with prompt engineering, and learn about the impact of LLM parameters and conversational roles. Students are provided with a Javabased API that connects with OpenAI's GPT model. The project emphasizes teaching students to manage LLM API calls, enhance GUI responsiveness, and improve the user experience all in the context of an AI-powered application. This experience equips them with critical skills in software development and AI application. It prepares them for advanced software development by learning how to create effective LLM prompts to create intelligent and user-friendly applications. We share the experience of using this project and provide guidelines for assessing it in a second-year software engineering undergraduate course, where students' prior programming experience is limited to the prerequisite CS2 course on object-oriented programming. In the case study we present, the project involved developing a riddle-solving escape room, which we called EscAIpe Room.},
numpages = {6}
}

@inproceedings{10.1145/3576882.3617921,
author = {Prasad, Siddhartha and Greenman, Ben and Nelson, Tim and Krishnamurthi, Shriram},
title = {Generating Programs Trivially: Student Use of Large Language Models},
year = {2023},
isbn = {9798400700484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576882.3617921},
doi = {10.1145/3576882.3617921},
abstract = {Educators have been concerned about the capability of large language models to automatically generate programs in response to textual prompts. However, little is known about whether and how students actually use these tools.In the context of an upper-level formal methods course, we gave students access to large language models. They were told they could use the models freely. We built a Visual Studio Code extension to simplify access to these models. We also paid for an account so students could use the models for free without worrying about cost.In this experience report we analyze the outcomes. We see how students actually do and do not use the models. We codify the different uses they make. Most of all, we notice that students actually do not use them very much at all, and provide insight into the many reasons why not. We believe such experiments can help rebalance some of the public narrative about such tools.},
booktitle = {Proceedings of the ACM Conference on Global Computing Education Vol 1},
pages = {126–132},
numpages = {7},
keywords = {formal methods, large language models, properties, testing},
location = {Hyderabad, India},
series = {CompEd 2023}
}

@article{10.1145/3680410,
author = {H\"{u}ttel, Hans},
title = {On Program Synthesis and Large Language Models},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {68},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/3680410},
doi = {10.1145/3680410},
abstract = {Why it is unlikely new developments in machine intelligence will eventually make&nbsp;programming obsolete.},
journal = {Commun. ACM},
month = dec,
pages = {33–35},
numpages = {3}
}

@inproceedings{10.1145/3708394.3708437,
author = {Wang, Xiaohui and Yu, Ruijie and Zhang, Yu and Xu, Yanyan},
title = {English Composition Image Automatic Scoring Based on Multi-modal Large Language Models},
year = {2025},
isbn = {9798400710650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708394.3708437},
doi = {10.1145/3708394.3708437},
abstract = {Large language models (LLMs) bring significant opportunities in the field of education evaluation. Nowadays, most existing LLMs are introduced to generate reasonable scores for electronic and structurized documents. However, such systems require huge efforts for manual typewriting or advanced optical character recognition techniques. To this end, this work directly processes the scanning copy of English composition with multi-modal LLMs. Specifically, this research aims to utilize multi-modal LLMs for automated essay scoring (AES) and evaluate its reliability and accuracy. We take the English composition images of 1,511 tenth-grade examinees in a standardized test environment as the research objects for automated scoring, and explore the performance of the multi-modal LLM GPT-4o in evaluating composition image data. The study compares the consistency of the composition image scores of GPT-4o under zero-shot and two-shot prompts with the scores of the marking experts, and verifies them through multiple indicators such as the exact agreement coefficient, Pearson correlation coefficient, Coefficient of determination, Root Mean Square Error, and the probability that the GPT-4o score falls within a given confidence interval. Especially, comparing the scores given by experts, the R-square of the GPT-4o scoring reaches 0.66. The results show that GPT-4o has the ability to learn from two samples through prompt, can quickly adapt and provide high-quality and personalized evaluations for examinees, and can provide valuable support for human scoring.},
booktitle = {Proceeding of the 2024 International Conference on Artificial Intelligence and Future Education},
pages = {247–254},
numpages = {8},
keywords = {Automated Essay Scoring, GPT-4o, Multi-modal large language model, Prompt Learning},
location = {
},
series = {AIFE '24}
}

@inproceedings{10.1145/3686852.3687069,
author = {Zhang, He and Xie, Jingyi and Wu, Chuhao and Cai, Jie and Kim, Chanmin and Carroll, John M.},
title = {The Future of Learning: Large Language Models through the Lens of Students},
year = {2024},
isbn = {9798400711060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686852.3687069},
doi = {10.1145/3686852.3687069},
abstract = {As Large-Scale Language Models (LLMs) continue to evolve, they demonstrate significant enhancements in performance and an expansion of functionalities, impacting various domains, including education. In this study, we conducted interviews with 14 students to explore their everyday interactions with ChatGPT. Our preliminary findings reveal that students grapple with the dilemma of utilizing ChatGPT’s efficiency for learning and information seeking, while simultaneously experiencing a crisis of trust and ethical concerns regarding the outcomes and broader impacts of ChatGPT. The students perceive ChatGPT as being more “human-like” compared to traditional AI. This dilemma, characterized by mixed emotions, inconsistent behaviors, and an overall positive attitude towards ChatGPT, underscores its potential for beneficial applications in education and learning. However, we argue that despite its human-like qualities, the advanced capabilities of such intelligence might lead to adverse consequences. Therefore, it’s imperative to approach its application cautiously and strive to mitigate potential harms in future developments.},
booktitle = {Proceedings of the 25th Annual Conference on Information Technology Education},
pages = {12–18},
numpages = {7},
keywords = {ChatGPT, Large language models, education, incidental learning, qualitative},
location = {El Paso, TX, USA},
series = {SIGITE '24}
}

@article{10.1145/3688087,
author = {Liu, Bingbin and He-Yueya, Joy},
title = {Improving Instructional Materials with Large Language Models},
year = {2024},
issue_date = {Fall 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/3688087},
doi = {10.1145/3688087},
abstract = {In this interview, Joy He-Yueya discusses her work on using GPTs to evaluate the quality of instructional materials, how GPTs compare to human evaluators, and the implications of using these models.},
journal = {XRDS},
month = oct,
pages = {56–58},
numpages = {3}
}

@inproceedings{10.1145/3641554.3701917,
author = {Wang, Kevin Shukang and Lawrence, Ramon},
title = {Quantitative Evaluation of Using Large Language Models and Retrieval-Augmented Generation in Computer Science Education},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701917},
doi = {10.1145/3641554.3701917},
abstract = {Generative artificial intelligence (GenAI) is transforming Computer Science education, and every instructor is reflecting on how AI will impact their courses. Instructors must determine how students may use AI for course activities and what AI systems they will support and encourage students to use. This task is challenging with the proliferation of large language models (LLMs) and related AI systems. The contribution of this work is an experimental evaluation of the performance of multiple open-source and commercial LLMs utilizing retrieval-augmented generation in answering questions for computer science courses and a cost-benefit analysis for instructors when determining what systems to use. A key factor is the time an instructor has to maintain their supported AI systems and the most effective activities for improving their performance. The paper offers recommendations for deploying, using, and enhancing AI in educational settings.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1183–1189},
numpages = {7},
keywords = {artificial intelligence, human-in-the-loop, large language model, question answering, retrieval-augmented generation},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3649217.3653624,
author = {Grande, Virginia and Kiesler, Natalie and Francisco R., Mar\'{\i}a Andre\'{\i}na},
title = {Student Perspectives on Using a Large Language Model (LLM) for an Assignment on Professional Ethics},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653624},
doi = {10.1145/3649217.3653624},
abstract = {The advent of Large Language Models (LLMs) started a serious discussion among educators on how LLMs would affect, e.g., curricula, assessments, and students' competencies. Generative AI and LLMs also raised ethical questions and concerns for computing educators and professionals.This experience report presents an assignment within a course on professional competencies, including some related to ethics, that computing master's students need in their careers. For the assignment, student groups discussed the ethical process by Lennerfors et al. by analyzing a case: a fictional researcher considers whether to attend the real CHI 2024 conference in Hawaii. The tasks were (1) to participate in in-class discussions on the case, (2) to use an LLM of their choice as a discussion partner for said case, and (3) to document both discussions, reflecting on their use of the LLM.Students reported positive experiences with the LLM as a way to increase their knowledge and understanding, although some identified limitations. The LLM provided a wider set of options for action in the studied case, including unfeasible ones. The LLM would not select a course of action, so students had to choose themselves, which they saw as coherent.From the educators' perspective, there is a need for more instruction for students using LLMs: some students did not perceive the tools as such but rather as an authoritative knowledge base. Therefore, this work has implications for educators considering the use of LLMs as discussion partners or tools to practice critical thinking, especially in computing ethics education.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {478–484},
numpages = {7},
keywords = {chatgpt, ethics, experience report, large language models, llms, student perspective},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3627217.3627221,
author = {Wang, Zixuan and Denny, Paul and Leinonen, Juho and Luxton-Reilly, Andrew},
title = {Leveraging Large Language Models for Analysis of Student Course Feedback},
year = {2023},
isbn = {9798400708404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627217.3627221},
doi = {10.1145/3627217.3627221},
abstract = {This study investigates the use of large language models, specifically ChatGPT, to analyse the feedback from a Summative Evaluation Tool (SET) used to collect student feedback on the quality of teaching. We find that these models enhance comprehension of SET scores and the impact of context on student evaluations. This work aims to reveal hidden patterns in student evaluation data, demonstrating a positive first step towards automated, detailed analysis of student feedback.},
booktitle = {Proceedings of the 16th Annual ACM India Compute Conference},
pages = {76–79},
numpages = {4},
keywords = {Large Language Model, Natural Language Processing, Student Evaluation of Teaching, Student Feedback},
location = {Hyderabad, India},
series = {COMPUTE '23}
}

@inproceedings{10.1145/3627673.3679664,
author = {Guo, Yuxiang and Shen, Shuanghong and Liu, Qi and Huang, Zhenya and Zhu, Linbo and Su, Yu and Chen, Enhong},
title = {Mitigating Cold-Start Problems in Knowledge Tracing with Large Language Models: An Attribute-aware Approach},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679664},
doi = {10.1145/3627673.3679664},
abstract = {Knowledge Tracing (KT) is a crucial research task for dynamically monitoring students' knowledge states, particularly in online education systems. Recently, knowledge tracing has gained significant attention and in-depth research. Most existing methods rely on students' response data for question understanding and modeling, which helps better updating students' knowledge states. Meanwhile, question ID is utilized to indicate and represent questions. However, this presents a challenge when transitioning to new, cold-start questions that few students has answered before. Also, prior work has overlooked the semantic modeling of questions, which could better assist in modeling the transfer of students' knowledge states. In this paper, we explore leveraging the power of Large Language Models (LLMs) to help understand questions for knowledge tracing, which benefits mitigating cold-start and sparse problems and modeling the transfer of students' knowledge states in a sophisticated manner. Specifically, we first design an attribute estimation module to estimate the attribute of the questions (e.g., difficulty, ability requirements, expected response time) by prompting Large Language Models. Subsequently, we have developed a question embedding module that incorporates graph attention network to effectively utilizing these attributes. Extensive experiments on various datasets demonstrate that our model outperforms existing state-of-the-art models and effectively addresses the problems of cold-start and sparsity. In addition, due to the estimation of multiple attributes of the questions, our model exhibits superior interpretability.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {727–736},
numpages = {10},
keywords = {knowledge tracing, large language model, question attributes},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3545945.3569770,
author = {Leinonen, Juho and Hellas, Arto and Sarsa, Sami and Reeves, Brent and Denny, Paul and Prather, James and Becker, Brett A.},
title = {Using Large Language Models to Enhance Programming Error Messages},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569770},
doi = {10.1145/3545945.3569770},
abstract = {A key part of learning to program is learning to understand programming error messages. They can be hard to interpret and identifying the cause of errors can be time-consuming. One factor in this challenge is that the messages are typically intended for an audience that already knows how to program, or even for programming environments that then use the information to highlight areas in code. Researchers have been working on making these errors more novice friendly since the 1960s, however progress has been slow. The present work contributes to this stream of research by using large language models to enhance programming error messages with explanations of the errors and suggestions on how to fix them. Large language models can be used to create useful and novice-friendly enhancements to programming error messages that sometimes surpass the original programming error messages in interpretability and actionability. These results provide further evidence of the benefits of large language models for computing educators, highlighting their use in areas known to be challenging for students. We further discuss the benefits and downsides of large language models and highlight future streams of research for enhancing programming error messages.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {563–569},
numpages = {7},
keywords = {ai, codex, compiler error messages, large language models, programming error messages, syntax error messages},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3636243.3636259,
author = {Roest, Lianne and Keuning, Hieke and Jeuring, Johan},
title = {Next-Step Hint Generation for Introductory Programming Using Large Language Models},
year = {2024},
isbn = {9798400716195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636243.3636259},
doi = {10.1145/3636243.3636259},
abstract = {Large Language Models possess skills such as answering questions, writing essays or solving programming exercises. Since these models are easily accessible, researchers have investigated their capabilities and risks for programming education. This work explores how LLMs can contribute to programming education by supporting students with automated next-step hints. We investigate prompt practices that lead to effective next-step hints and use these insights to build our StAP-tutor. We evaluate this tutor by conducting an experiment with students, and performing expert assessments. Our findings show that most LLM-generated feedback messages describe one specific next step and are personalised to the student’s code and approach. However, the hints may contain misleading information and lack sufficient detail when students approach the end of the assignment. This work demonstrates the potential for LLM-generated feedback, but further research is required to explore its practical implementation.},
booktitle = {Proceedings of the 26th Australasian Computing Education Conference},
pages = {144–153},
numpages = {10},
keywords = {Generative AI, Large Language Models, Next-step hints, automated feedback, learning programming},
location = {Sydney, NSW, Australia},
series = {ACE '24}
}

@inproceedings{10.1145/3649165.3690099,
author = {Pang, Amy and Padiyath, Aadarsh and Viramontes Vargas, Diego and Ericson, Barbara Jane},
title = {Examining the Relationship between Socioeconomic Status and Beliefs about Large Language Models in an Undergraduate Programming Course},
year = {2024},
isbn = {9798400705984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649165.3690099},
doi = {10.1145/3649165.3690099},
abstract = {Research on students' use of large language models (LLMs) in academic settings has increased recently, focusing on usage patterns, tasks, and instructor policies. However, there is limited research on the relationships between students' socioeconomic backgrounds, perceptions, and usage of these resources. As socioeconomic factors may shape students' approach to learning, it is important to understand their impact on students' perceptions and attitudes towards emerging technologies like LLMs. Thus, we analyzed a quantitative and internally consistent student survey (N=144) and qualitative interview (N=2) responses of students taking an undergraduate-level programming course at a public university for correlations between socioeconomic background, attitudes towards LLMs, and LLM usage. Regression analysis found a significant positive association between socioeconomic status (SES) and belief that LLM use will lead to career success. Qualitative interviews suggested low-SES students perceived LLMs as helpful tools for debugging and learning concepts, but not as a significant factor in long-term career success. Rather, programming knowledge itself was still paramount for career success. Our findings contribute to our understanding of the complex influences social and cultural factors have on students' perceptions and attitudes towards LLMs.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1},
pages = {172–178},
numpages = {7},
keywords = {large language models, socioeconomic status, student attitudes},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@article{10.5555/3737313.3737332,
author = {Chamberlain, Devin and Levine, David B. and Pitcairn, Abigail and Snow, Nicholas and Sweeney, Benjamin},
title = {Large Language Models and Introductory Lab Exercises: Susceptibility, Resistance, and Potential},
year = {2025},
issue_date = {April 2025},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {8},
issn = {1937-4771},
abstract = {Three student personas were created, each representing a way in which current students interact with AI tools such as ChatGPT when completing introductory computer science assignments. Four undergraduate students assumed the role of each of the personas in turn and two semesters worth of current assignments were completed in each persona. The results and experiences were then analyzed to determine aspects of the assignments that made it more (or less) difficult to complete them using the AI tools, with an eye towards whether small changes in phrasing or requirements might result in significant changes in this metric.Three of the main takeaways were that LLMs are more difficult for students to use when assignments 1) consist of many small steps, 2) make use of external code libraries, or 3) involve spatial reasoning.Finally, the student/persona experiences helped to generate a list of opportunities for instructors to proactively include the use of AI tools in current assignments without sacrificing any of the current learning objectives.The initial phase involved labs from one institution and used only one AI tool, but follow-up work involving the use of other tools and labs from other institutions validated those core conclusions. A student survey (as well as other published literature) also validated the choice of personas.},
journal = {J. Comput. Sci. Coll.},
month = may,
pages = {49–63},
numpages = {15}
}

@inproceedings{10.1145/3626253.3635624,
author = {Fan, Aysa X. and Hendrawan, Rully A. and Shi, Yang and Ma, Qianou},
title = {Enhancing Code Tracing Question Generation with Refined Prompts in Large Language Models},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635624},
doi = {10.1145/3626253.3635624},
abstract = {This study refines Large Language Models (LLMs) prompts to enhance the generation of code tracing questions, where the new expert-guided prompts consider features identified from prior research. Expert evaluations compared new LLM-generated questions against previously preferred ones, revealing improved quality in aspects like complexity and concept coverage. While providing insights into effective question generation and affirming LLMs' potential in educational content creation, the study also contributes an expert-evaluated question dataset to the computing education community. However, generating high-quality reverse tracing questions remains a nuanced challenge, indicating a need for further LLM prompting refinement.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1640–1641},
numpages = {2},
keywords = {computer science education, large language model, programming education, tracing question},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3585059.3611409,
author = {Gumina, Sharon and Dalton, Travis and Gerdes, John},
title = {Teaching IT Software Fundamentals: Strategies and Techniques for Inclusion of Large Language Models: Strategies and Techniques for Inclusion of Large Language Models},
year = {2023},
isbn = {9798400701306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585059.3611409},
doi = {10.1145/3585059.3611409},
abstract = {This paper argues for the inclusion of tools that utilize Artificial Intelligence (AI) Large Language Models (LLMs) in information technology (IT) undergraduate courses that teach the fundamentals of software. LLM tools have become widely available and disrupt traditional methods for teaching software concepts. Learning objectives are compromised when students submit AI-generated code for a classroom assignment without comprehending or validating the code. Since LLM tools including OpenAI Codex, Copilot by GitHub, and ChatGPT are being used in industry for software development, students need to be familiar with their use without compromising student learning. Incorporating LLM tools into the curriculum prepares students for real-world software development. However, students still need to understand software fundamentals including how to write and debug code. There are many challenges associated with the inclusion of AI tools into the IT curriculum that need to be addressed and mitigated. This paper presents strategies and techniques to integrate student use of LLM tools, assist students’ interaction with the tools, and help prepare students for careers that increasingly use AI tools to design, develop, and maintain software.},
booktitle = {Proceedings of the 24th Annual Conference on Information Technology Education},
pages = {60–65},
numpages = {6},
location = {Marietta, GA, USA},
series = {SIGITE '23}
}

@inproceedings{10.1145/3587102.3588785,
author = {Leinonen, Juho and Denny, Paul and MacNeil, Stephen and Sarsa, Sami and Bernstein, Seth and Kim, Joanne and Tran, Andrew and Hellas, Arto},
title = {Comparing Code Explanations Created by Students and Large Language Models},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588785},
doi = {10.1145/3587102.3588785},
abstract = {Reasoning about code and explaining its purpose are fundamental skills for computer scientists. There has been extensive research in the field of computing education on the relationship between a student's ability to explain code and other skills such as writing and tracing code. In particular, the ability to describe at a high-level of abstraction how code will behave over all possible inputs correlates strongly with code writing skills. However, developing the expertise to comprehend and explain code accurately and succinctly is a challenge for many students. Existing pedagogical approaches that scaffold the ability to explain code, such as producing exemplar code explanations on demand, do not currently scale well to large classrooms. The recent emergence of powerful large language models (LLMs) may offer a solution. In this paper, we explore the potential of LLMs in generating explanations that can serve as examples to scaffold students' ability to understand and explain code. To evaluate LLM-created explanations, we compare them with explanations created by students in a large course (n ≈ 1000) with respect to accuracy, understandability and length. We find that LLM-created explanations, which can be produced automatically on demand, are rated as being significantly easier to understand and more accurate summaries of code than student-created explanations. We discuss the significance of this finding, and suggest how such models can be incorporated into introductory programming education.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {124–130},
numpages = {7},
keywords = {CS1, ChatGPT, GPT-3, GPT-4, code comprehension, code explanations, foundation models, large language models, natural language generation, resource generation},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3661167.3661273,
author = {Mezzaro, Simone and Gambi, Alessio and Fraser, Gordon},
title = {An Empirical Study on How Large Language Models Impact Software Testing Learning},
year = {2024},
isbn = {9798400717017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3661167.3661273},
doi = {10.1145/3661167.3661273},
abstract = {Software testing is a challenging topic in software engineering education and requires creative approaches to engage learners. For example, the Code Defenders game has students compete over a Java class under test by writing effective tests and mutants. While such gamified approaches deal with problems of motivation and engagement, students may nevertheless require help to put testing concepts into practice. The recent widespread diffusion of Generative AI and Large Language Models raises the question of whether and how these disruptive technologies could address this problem, for example, by providing explanations of unclear topics and guidance for writing tests. However, such technologies might also be misused or produce inaccurate answers, which would negatively impact learning. To shed more light on this situation, we conducted the first empirical study investigating how students learn and practice new software testing concepts in the context of the Code Defenders testing game, supported by a smart assistant based on a widely known, commercial Large Language Model. Our study shows that students had unrealistic expectations about the smart assistant, “blindly” trusting any output it generated, and often trying to use it to obtain solutions for testing exercises directly. Consequently, students who resorted to the smart assistant more often were less effective and efficient than those who did not. For instance, they wrote 8.6% fewer tests, and their tests were not useful in 78.0% of the cases. We conclude that giving unrestricted and unguided access to Large Language Models might generally impair learning. Thus, we believe our study helps to raise awareness about the implications of using Generative AI and Large Language Models in Computer Science Education and provides guidance towards developing better and smarter learning tools.},
booktitle = {Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering},
pages = {555–564},
numpages = {10},
keywords = {ChatGPT, Computer Science Education, Generative AI, Smart Learning Assistant},
location = {Salerno, Italy},
series = {EASE '24}
}

@article{10.5555/3737313.3737340,
author = {Crocetti, Giancarlo and Bak, Seonwoo and Noory, Naqib A. and Vautor-Laplaceliere, Daena D.},
title = {Evaluating the Pedagogical Impact of Large Language Models on Programming Skills in Higher Education},
year = {2025},
issue_date = {April 2025},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {8},
issn = {1937-4771},
abstract = {This empirical study investigated the impact of Generative AI (GenAI) tools, particularly large language models (LLMs), on college students' Python programming skills in a graduate-level data science course. Using a pretest-posttest methodology and accounting for variables like prior programming experience, the research examined how guided LLM usage affected students' self-assessed programming abilities. The findings revealed that while LLMs positively influenced students' capacity to develop complex applications, work with Python libraries, and write quality code, they had no significant impact on students' grasp of fundamental Python concepts or their general comfort with the language. These results suggest that LLMs serve as effective tools for advancing practical programming skills but cannot substitute for the foundational programming knowledge that must be developed through traditional learning.},
journal = {J. Comput. Sci. Coll.},
month = may,
pages = {163–177},
numpages = {15}
}

@inproceedings{10.1145/3632620.3671098,
author = {Padiyath, Aadarsh and Hou, Xinying and Pang, Amy and Viramontes Vargas, Diego and Gu, Xingjian and Nelson-Fromm, Tamara and Wu, Zihan and Guzdial, Mark and Ericson, Barbara},
title = {Insights from Social Shaping Theory: The Appropriation of Large Language Models in an Undergraduate Programming Course},
year = {2024},
isbn = {9798400704758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632620.3671098},
doi = {10.1145/3632620.3671098},
abstract = {The capability of large language models (LLMs) to generate, debug, and explain code has sparked the interest of researchers and educators in undergraduate programming, with many anticipating their transformative potential in programming education. However, decisions about why and how to use LLMs in programming education may involve more than just the assessment of an LLM’s technical capabilities. Using the social shaping of technology theory as a guiding framework, our study explores how students’ social perceptions influence their own LLM usage. We then examine the correlation of self-reported LLM usage with students’ self-efficacy and midterm performances in an undergraduate programming course. Triangulating data from an anonymous end-of-course student survey (n = 158), a mid-course self-efficacy survey (n=158), student interviews (n = 10), self-reported LLM usage on homework, and midterm performances, we discovered that students’ use of LLMs was associated with their expectations for their future careers and their perceptions of peer usage. Additionally, early self-reported LLM usage in our context correlated with lower self-efficacy and lower midterm scores, while students’ perceived over-reliance on LLMs, rather than their usage itself, correlated with decreased self-efficacy later in the course.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1},
pages = {114–130},
numpages = {17},
keywords = {Generative AI, Large Language Models, Self-Efficacy, Social Shaping Theory, Technology Appropriation Model},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@article{10.1145/3688089,
author = {Zhou, Kyrie Zhixuan and Kilhoffer, Zachary and Sanfilippo, Madelyn Rose and Underwood, Ted and Gumusel, Ece and Wei, Mengyi and Choudhry, Abhinav and Xiong, Jinjun},
title = {Ethics, Governance, and User Mental Models for Large Language Models in Computing Education},
year = {2024},
issue_date = {Fall 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/3688089},
doi = {10.1145/3688089},
abstract = {Large language models like ChatGPT are disrupting many industries, including computing education. How should policy evolve to improve learning outcomes?},
journal = {XRDS},
month = oct,
pages = {46–51},
numpages = {6}
}

@inproceedings{10.1145/3649165.3690100,
author = {MacNeil, Stephen and Rogalska, Magdalena and Leinonen, Juho and Denny, Paul and Hellas, Arto and Crosland, Xandria},
title = {Synthetic Students: A Comparative Study of Bug Distribution Between Large Language Models and Computing Students},
year = {2024},
isbn = {9798400705984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649165.3690100},
doi = {10.1145/3649165.3690100},
abstract = {Large language models (LLMs) present an exciting opportunity for generating synthetic classroom data. Such data could include code containing a typical distribution of errors, simulated student behavior to address the cold start problem when developing education tools, and synthetic user data when access to authentic data is restricted due to privacy reasons. In this research paper, we conduct a comparative study examining the distribution of bugs generated by LLMs in contrast to those produced by computing students. Leveraging data from two previous large-scale analyses of student-generated bugs, we investigate whether LLMs can be coaxed to exhibit bug patterns that are similar to authentic student bugs when prompted to inject errors into code. The results suggest that unguided, LLMs do not generate plausible error distributions, and many of the generated errors are unlikely to be generated by real students. However, with guidance including descriptions of common errors and typical frequencies, LLMs can be shepherded to generate realistic distributions of errors in synthetic code.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1},
pages = {137–143},
numpages = {7},
keywords = {buggy code, generative ai, gpt-4, llms, synthetic data},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@inproceedings{10.1145/3649217.3653600,
author = {Villegas Molina, Ismael and Montalvo, Audria and Zhong, Shera and Jordan, Mollie and Soosai Raj, Adalbert Gerald},
title = {Generation and Evaluation of a Culturally-Relevant CS1 Textbook for Latines using Large Language Models},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653600},
doi = {10.1145/3649217.3653600},
abstract = {In the United States, culturally relevant computing (CRC) is one of the most popular pedagogical implementations for Latin American (Latine) students. Culturally-relevant learning resources are a valuable tool for implementing CRC. However, the traditional method of creation and maintenance of textbooks takes a significant amount of time and effort. Given the duration required for textbook production, the development of culturally-relevant learning resources may become lengthened, as it requires close attention both on the material and the incorporation of cultural referents. In order to accelerate the process, we used the advancement of large language models (LLMs) to our advantage. Through prompt engineering, we created a series of prompts to produce a textbook for an introductory computer science course (CS1) that incorporates Latine culture. This textbook was evaluated on metrics regarding sensibility, correctness, readability, linguistic approachability, appropriateness of examples, and cultural relevance. Overall, the generated textbook was mainly sensible, correct, readable, and linguistically approachable. Code examples were not always appropriate due to the usage of libraries that are not typically used in a CS1 course. The cultural relevance was apparent, but it often included surface-level cultural referents. The main incorporation of culture was through geographical locations and people's names. This suggests that the use of LLMs to generate textbooks may serve as a valuable first step for writing culturally-relevant learning resources. Though this study focuses on Latines, our results and prompts may be applicable for generating culturally-relevant CS1 textbooks for other cultures.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {325–331},
numpages = {7},
keywords = {Latina, Latine, Latino, Latinx, computer science textbook, culturally relevant resources, large language models, resource generation},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@article{10.5555/3715602.3715612,
author = {Crandall, Johannah L. and Crandall, Aaron S.},
title = {Large Language Model-Supported Software Testing with the CS Matrix Taxonomy},
year = {2024},
issue_date = {October 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {1},
issn = {1937-4771},
abstract = {New breakthroughs in code synthesis from Generative Pre-Trained Transformers (GPT) and Large Language Model (LLM) algorithms are driving significant changes to software engineering education. Having algorithms able to generate components of a software project means that software developers will need stronger skills in requirements specification to guide code generation as well as stronger skills in code review, testing, and integration to incorporate AI-generated code into projects. Shifts in industry and classroom practices are already occurring with the availability of inline code generation tools like GitHub's Copilot, which makes discussion of pedagogical strategies in this area a timely topic. Of immediate concern in computer science education is the potential for LLM-generated code and code help to undermine the learning of CS students. In order to avoid such undermining in even intentional uses of LLM-enhanced learning supports, it is necessary to clarify the roles such supports need to play in the pedagogical process. The Computer Science Matrix Taxonomy provides a strong framework for organizing software testing learning outcomes as well as delineating the operational space in which LLM-based feedback tools should operate to support those learning outcomes. In this paper, the authors operationalize the CS Matrix Taxonomy for software testing learning outcomes and illustrate the integration of LLM-generated test strategy suggestions as an extension of the peer coding/testing model. The work includes examples of AI-generated code testing suggestions that students would use to help guide their own code synthesis for assignments or projects.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {49–58},
numpages = {10}
}

@article{10.1145/3703408,
author = {Murali, Pavithra Sripathanallur},
title = {Leveraging Large Language Models for Automated Program Repair in Programming Education},
year = {2025},
issue_date = {Winter 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1528-4972},
url = {https://doi.org/10.1145/3703408},
doi = {10.1145/3703408},
journal = {XRDS},
month = jan,
pages = {58–60},
numpages = {3}
}

@inproceedings{10.1145/3568812.3603482,
author = {Tran, Andrew and Li, Linxuan and Rama, Egi and Angelikas, Kenneth and Macneil, Stephen},
title = {Using Large Language Models to Automatically Identify Programming Concepts in Code Snippets},
year = {2023},
isbn = {9781450399753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568812.3603482},
doi = {10.1145/3568812.3603482},
abstract = {Curating course material that aligns with students’ learning goals is a challenging and time-consuming task that instructors undergo when preparing their curricula. For instance, it is a challenge to find multiple-choice questions or example codes that demonstrate recursion in an unlabeled question bank or repository. Recently, Large Language Models (LLMs) have demonstrated the capability to generate high-quality learning materials at scale. In this poster, we use LLMs to identify programming concepts found within code snippets, allowing instructors to quickly curate their course materials. We compare programming concepts generated by LLMs with concepts generated by experts to see the extent to which they agree. The agreement was calculated using Cohen’s Kappa.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2},
pages = {22–23},
numpages = {2},
keywords = {computer science education, explanations, large language models},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@article{10.5555/3715602.3715614,
author = {Rhee, Junghwan and Shrestha, Aakankshya and Qian, Gang and Zuo, Fei and Fu, Jicheng and Park, Myungah and Qu, Xianshan and Mylavarapu, Goutam and Sung, Hong},
title = {An Evaluation on the Impact of Large Language Models on Computer Science Curricula},
year = {2024},
issue_date = {October 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {1},
issn = {1937-4771},
abstract = {Since their introduction, large language model (LLM) services have been widely used in our society, including the computer science education area. While this technology provides various types of intelligent assistance to users, its capabilities and impact on computer science education regarding students' learning need further study. In this paper, we present our manual assessment of LLM services' ability to solve questions in various course assignments and projects in our computer science curriculum. Based on the result of the study, we provide our observations of the extent of LLM services' impact on different computer science disciplines. Suggestions are summarized and offered to computer science instructors on the possible strategies for dealing with LLMs in current and future computer science curriculum designs.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {70–80},
numpages = {11}
}

@inproceedings{10.1145/3657604.3662042,
author = {Kumar, Harsh and Xiao, Ruiwei and Lawson, Benjamin and Musabirov, Ilya and Shi, Jiakai and Wang, Xinyuan and Luo, Huayin and Williams, Joseph Jay and Rafferty, Anna N. and Stamper, John and Liut, Michael},
title = {Supporting Self-Reflection at Scale with Large Language Models: Insights from Randomized Field Experiments in Classrooms},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3662042},
doi = {10.1145/3657604.3662042},
abstract = {Self-reflection on learning experiences constitutes a fundamental cognitive process, essential for consolidating knowledge and enhancing learning efficacy. However, traditional methods to facilitate reflection often face challenges in personalization, immediacy of feedback, engagement, and scalability. Integration of Large Language Models (LLMs) into the reflection process could mitigate these limitations. In this paper, we conducted two randomized field experiments in undergraduate computer science courses to investigate the potential of LLMs to help students engage in post-lesson reflection. In the first experiment (N=145), students completed a take-home assignment with the support of an LLM assistant; half of these students were then provided access to an LLM designed to facilitate self-reflection. The results indicated that the students assigned to LLM-guided reflection reported somewhat increased self-confidence compared to peers in a no-reflection control and a non-significant trend towards higher scores on a later assessment. Thematic analysis of students' interactions with the LLM showed that the LLM often affirmed the student's understanding, expanded on the student's reflection, and prompted additional reflection; these behaviors suggest ways LLM-interaction might facilitate reflection. In the second experiment (N=112), we evaluated the impact of LLM-guided self-reflection against other scalable reflection methods, such as questionnaire-based activities and review of key lecture slides, after assignment. Our findings suggest that the students in the questionnaire and LLM-based reflection groups performed equally well and better than those who were only exposed to lecture slides, according to their scores on a proctored exam two weeks later on the same subject matter. These results underscore the utility of LLM-guided reflection and questionnaire-based activities in improving learning outcomes. Our work highlights that focusing solely on the accuracy of LLMs can overlook their potential to enhance metacognitive skills through practices such as self-reflection. We discuss the implications of our research for the learning-at-scale community, highlighting the potential of LLMs to enhance learning experiences through personalized, engaging, and scalable reflection practices.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {86–97},
numpages = {12},
keywords = {field experiments, human-ai collaboration, large language models, learning engineering, self-reflection},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@inproceedings{10.1145/3626253.3635542,
author = {Smith, David H. and Zilles, Craig},
title = {Evaluating Large Language Model Code Generation as an Autograding Mechanism for "Explain in Plain English" Questions},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635542},
doi = {10.1145/3626253.3635542},
abstract = {The ability of students to ''Explain in Plain English'' (EiPE) the purpose of code is a critical skill for students in introductory programming courses to develop. EiPE questions serve as both a mechanism for students to develop and demonstrate code comprehension skills. However, evaluating this skill has been challenging as manual grading is time consuming and not easily automated. The process of constructing a prompt for the purposes of code generation for a Large Language Model, such OpenAI's GPT-4, bears a striking resemblance to constructing EiPE responses. In this paper, we explore the potential of using test cases run on code generated by GPT-4 from students' EiPE responses as a grading mechanism for EiPE questions. We applied this proposed grading method to a corpus of EiPE responses collected from past exams, then measured agreement between the results of this grading method and human graders. Overall, we find moderate agreement between the human raters and the results of the unit tests run on the generated code. This appears to be attributable to GPT-4's code generation being more lenient than human graders on low-level descriptions of code.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1824–1825},
numpages = {2},
keywords = {autograding, eipe, gpt-4, large language models},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3631802.3631830,
author = {Liffiton, Mark and Sheese, Brad E and Savelka, Jaromir and Denny, Paul},
title = {CodeHelp: Using Large Language Models with Guardrails for Scalable Support in Programming Classes},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631830},
doi = {10.1145/3631802.3631830},
abstract = {Computing educators face significant challenges in providing timely support to students, especially in large class settings. Large language models (LLMs) have emerged recently and show great promise for providing on-demand help at a large scale, but there are concerns that students may over-rely on the outputs produced by these models. In this paper, we introduce CodeHelp, a novel LLM-powered tool designed with guardrails to provide on-demand assistance to programming students without directly revealing solutions. We detail the design of the tool, which incorporates a number of useful features for instructors, and elaborate on the pipeline of prompting strategies we use to ensure generated outputs are suitable for students. To evaluate CodeHelp, we deployed it in a first-year computer and data science course with 52 students and collected student interactions over a 12-week period. We examine students’ usage patterns and perceptions of the tool, and we report reflections from the course instructor and a series of recommendations for classroom use. Our findings suggest that CodeHelp is well-received by students who especially value its availability and help with resolving errors, and that for instructors it is easy to deploy and complements, rather than replaces, the support that they provide to students.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {8},
numpages = {11},
keywords = {Guardrails, Intelligent programming tutors, Intelligent tutoring systems, Large language models, Natural language interfaces, Novice programmers, Programming assistance},
location = {Koli, Finland},
series = {Koli Calling '23}
}

@inproceedings{10.1145/3626253.3635592,
author = {Niousha, Rose and Hoq, Muntasir and Akram, Bita and Norouzi, Narges},
title = {Use of Large Language Models for Extracting Knowledge Components in CS1 Programming Exercises},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635592},
doi = {10.1145/3626253.3635592},
abstract = {This study utilizes large language models to extract foundational programming concepts in programming assignments in a CS1 course. We seek to answer the following research questions: RQ1. How effectively can large language models identify knowledge components in a CS1 course from programming assignments? RQ2. Can large language models be used to extract program-level knowledge components, and how can the information be used to identify students' misconceptions? Preliminary results demonstrated a high similarity between course-level knowledge components retrieved from a large language model and that of an expert-generated list.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1762–1763},
numpages = {2},
keywords = {cs1, curriculum design, knowledge component},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3636243.3636245,
author = {Macneil, Stephen and Denny, Paul and Tran, Andrew and Leinonen, Juho and Bernstein, Seth and Hellas, Arto and Sarsa, Sami and Kim, Joanne},
title = {Decoding Logic Errors: A Comparative Study on Bug Detection by Students and Large Language Models},
year = {2024},
isbn = {9798400716195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636243.3636245},
doi = {10.1145/3636243.3636245},
abstract = {Identifying and resolving logic errors can be one of the most frustrating challenges for novices programmers. Unlike syntax errors, for which a compiler or interpreter can issue a message, logic errors can be subtle. In certain conditions, buggy code may even exhibit correct behavior – in other cases, the issue might be about how a problem statement has been interpreted. Such errors can be hard to spot when reading the code, and they can also at times be missed by automated tests. There is great educational potential in automatically detecting logic errors, especially when paired with suitable feedback for novices. Large language models (LLMs) have recently demonstrated surprising performance for a range of computing tasks, including generating and explaining code. These capabilities are closely linked to code syntax, which aligns with the next token prediction behavior of LLMs. On the other hand, logic errors relate to the runtime performance of code and thus may not be as well suited to analysis by LLMs. To explore this, we investigate the performance of two popular LLMs, GPT-3 and GPT-4, for detecting and providing a novice-friendly explanation of logic errors. We compare LLM performance with a large cohort of introductory computing students (n = 964) solving the same error detection task. Through a mixed-methods analysis of student and model responses, we observe significant improvement in logic error identification between the previous and current generation of LLMs, and find that both LLM generations significantly outperform students. We outline how such models could be integrated into computing education tools, and discuss their potential for supporting students when learning programming.},
booktitle = {Proceedings of the 26th Australasian Computing Education Conference},
pages = {11–18},
numpages = {8},
keywords = {bug detection, computing education, generative AI, large language models, programming errors},
location = {Sydney, NSW, Australia},
series = {ACE '24}
}

@inproceedings{10.1145/3649409.3691089,
author = {Gupta, Anisha and Monahan, Robert and Vandenberg, Jessica and Smith, Andy and Elsayed, Rasha and Fox, Kimkinyona and Minogue, James and Oliver, Kevin and Hubbard Cheuoua, Aleata and Ringstaff, Cathy and Mott, Bradford},
title = {Leveraging Large Language Models for Automated Assessment of Elementary Students' Block-Based Narrative Programs},
year = {2024},
isbn = {9798400706042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649409.3691089},
doi = {10.1145/3649409.3691089},
abstract = {Recent years have seen increasing awareness of the need to engage young learners in computational thinking (CT). Integrating digital storytelling, where students create short narratives, and CT offers significant potential for promoting interdisciplinary learning for students; however, it is critical to provide both teachers and students with automated support. A promising approach for enabling support is to leverage advances in Large Language Models (LLMs), which have demonstrated considerable potential for assessing both programming and natural language artifacts. In this work, we investigate the capabilities of LLMs to automatically assess student-created block-based programs developed using a narrative-centered learning environment that engages upper elementary students (ages 9 to 11) in learning CT and physical science through the creation of interactive science narratives. Using the narrative programs created by 28 students, we explore the efficacy of LLMs to assess the programs across two dimensions.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 2},
pages = {318–319},
numpages = {2},
keywords = {k-12 education, natural language processing},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@inproceedings{10.1145/3545947.3569630,
author = {MacNeil, Stephen and Tran, Andrew and Leinonen, Juho and Denny, Paul and Kim, Joanne and Hellas, Arto and Bernstein, Seth and Sarsa, Sami},
title = {Automatically Generating CS Learning Materials with Large Language Models},
year = {2023},
isbn = {9781450394338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545947.3569630},
doi = {10.1145/3545947.3569630},
abstract = {Recent breakthroughs in Large Language Models (LLMs), such as GPT-3 and Codex, now enable software developers to generate code based on a natural language prompt. Within computer science education, researchers are exploring the potential for LLMs to generate code explanations and programming assignments using carefully crafted prompts. These advances may enable students to interact with code in new ways while helping instructors scale their learning materials. However, LLMs also introduce new implications for academic integrity, curriculum design, and software engineering careers. This workshop will demonstrate the capabilities of LLMs to help attendees evaluate whether and how LLMs might be integrated into their pedagogy and research. We will also engage attendees in brainstorming to consider how LLMs will impact our field.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1176},
numpages = {1},
keywords = {code generation, computer science education, copilot, explanations, large language models},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3649217.3653554,
author = {Liu, Suqing and Yu, Zezhu and Huang, Feiran and Bulbulia, Yousef and Bergen, Andreas and Liut, Michael},
title = {Can Small Language Models With Retrieval-Augmented Generation Replace Large Language Models When Learning Computer Science?},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653554},
doi = {10.1145/3649217.3653554},
abstract = {Leveraging Large Language Models (LLMs) for personalized learning and support is becoming a promising tool in computing education. AI Assistants can help students with programming, problem-solving, converse with them to clarify course content, explain error messages to help with debugging, and much more. However, using cloud-based LLMs poses risks around data security, privacy, but also control of the overarching system.To address these concerns, we created a locally-stored Small Language Model (SLM) that leverages different Retrieval-Augmented Generation (RAG) methods to support computing students' learning. We compare one SLM (neural-chat-7b-v3 - fine-tuned version of Mistral-7B-v0.1) against two popular LLMs (gpt-3.5-turbo and gpt-4-32k) to see the viability for computing educators to use in their course(s).We use conversations from a CS1 course (N = 1,260), providing students with an AI Assistant (using gpt-3.5-turbo) to help them learn content and support problem-solving while completing their Python programming assignment. In total, we had 269 students use the AI Assistant, with a total of 1,988 questions asked. Using this real conversational data, we re-ran student questions using our novel SLM (neural-chat-7b-v3 testing nine different RAG methods) and gpt-4-32k, then compared those results against the original gpt-3.5-turbo responses. Our findings indicate that using an SLM with RAG can perform similarly, if not better, than LLMs. This shows that it is possible for computing educators to use SLMs (with RAG) in their course(s) as a tool for scalable learning, supporting content understanding and problem-solving needs, while employing their own policies on data privacy and security.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {388–393},
numpages = {6},
keywords = {computing education, conversational agent, cs1, intelligence concentration, intelligent teaching assistant, intelligent tutoring system, large language models, locally deployable ai, personalized ai agent, retrieval augmented generation, small language models},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3636243.3636249,
author = {Sheese, Brad and Liffiton, Mark and Savelka, Jaromir and Denny, Paul},
title = {Patterns of Student Help-Seeking When Using a Large Language Model-Powered Programming Assistant},
year = {2024},
isbn = {9798400716195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636243.3636249},
doi = {10.1145/3636243.3636249},
abstract = {Providing personalized assistance at scale is a long-standing challenge for computing educators, but a new generation of tools powered by large language models (LLMs) offers immense promise. Such tools can, in theory, provide on-demand help in large class settings and be configured with appropriate guardrails to prevent misuse and mitigate common concerns around learner over-reliance. However, the deployment of LLM-powered tools in authentic classroom settings is still rare, and very little is currently known about how students will use them in practice and what type of help they will seek. To address this, we examine students’ use of an innovative LLM-powered tool that provides on-demand programming assistance without revealing solutions directly. We deployed the tool for 12 weeks in an introductory computer and data science course&nbsp;(n = 52), collecting more than 2,500 queries submitted by students throughout the term. We manually categorized all student queries based on the type of assistance sought, and we automatically analyzed several additional query characteristics. We found that most queries requested immediate help with programming assignments, whereas fewer requests asked for help on related concepts or for deepening conceptual understanding. Furthermore, students often provided minimal information to the tool, suggesting this is an area in which targeted instruction would be beneficial. We also found that students who achieved more success in the course tended to have used the tool more frequently overall. Lessons from this research can be leveraged by programming educators and institutions who plan to augment their teaching with emerging LLM-powered tools.},
booktitle = {Proceedings of the 26th Australasian Computing Education Conference},
pages = {49–57},
numpages = {9},
keywords = {Guardrails, Intelligent programming tutors, Intelligent tutoring systems, Large language models, Natural language interfaces, Novice programmers, Programming assistance},
location = {Sydney, NSW, Australia},
series = {ACE '24}
}

@inproceedings{10.1145/3626252.3630928,
author = {Poulsen, Seth and Sarsa, Sami and Prather, James and Leinonen, Juho and Becker, Brett A. and Hellas, Arto and Denny, Paul and Reeves, Brent N.},
title = {Solving Proof Block Problems Using Large Language Models},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630928},
doi = {10.1145/3626252.3630928},
abstract = {Large language models (LLMs) have recently taken many fields, including computer science, by storm. Most recent work on LLMs in computing education has shown that they are capable of solving most introductory programming (CS1) exercises, exam questions, Parsons problems, and several other types of exercises and questions. Some work has investigated the ability of LLMs to solve CS2 problems as well. However, it remains unclear how well LLMs fare against more advanced upper-division coursework, such as proofs in algorithms courses. After all, while known to be proficient in many programming tasks, LLMs have been shown to have more difficulties in forming mathematical proofs.In this paper, we investigate the ability of LLMs to solve mathematical proofs by using Proof Blocks, a tool previously shown to efficaciously teach proofs to students. Our results show that GPT-3.5 is almost completely unable to provide correct solutions (11.4%), while GPT-4 shows a significant increase in correctness (64.8%). However, even given this improvement, current models still struggle to correctly order lines in a proof. It remains an open question whether this is a temporary situation or if LLMs will continue to struggle to solve these types of exercises in the future.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1063–1069},
numpages = {7},
keywords = {ai, algorithms, artificial intelligence, chatgpt, code generation, generative ai, gpt-3, gpt-4, large language models, openai, proof blocks, proofs},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3568812.3603453,
author = {Tran, Minh},
title = {Prompt Engineering for Large Language Models to Support K-8 Computer Science Teachers in Creating Culturally Responsive Projects},
year = {2023},
isbn = {9781450399753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568812.3603453},
doi = {10.1145/3568812.3603453},
abstract = {The power of large language models has opened up opportunities for educational use. In computing education, recent studies have demonstrated the potential of these models to improve learning and teaching experiences in university-level programming courses. However, research into leveraging them to aid computer science instructors in curriculum development and course material design is relatively sparse, especially at the K-12 level. This work aims to fill this gap by exploring the capability of large language models in ideating and designing culturally responsive projects for elementary and middle school programming classes. Our ultimate goal is to support K-8 teachers in effectively extracting suggestions from large language models by only using natural language modifications. Furthermore, we aim to develop a comprehensive assessment framework for culturally responsive AI-generated project ideas. We also hope to provide valuable insight into teachers’ perspectives on large language models and their integration into teaching practices.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2},
pages = {110–112},
numpages = {3},
keywords = {culturally responsive pedagogy, large language models},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3626252.3630822,
author = {Taylor, Andrew and Vassar, Alexandra and Renzella, Jake and Pearce, Hammond},
title = {dcc --help: Transforming the Role of the Compiler by Generating Context-Aware Error Explanations with Large Language Models},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630822},
doi = {10.1145/3626252.3630822},
abstract = {In the challenging field of introductory programming, high enrolments and failure rates drive us to explore tools and systems to enhance student outcomes, especially automated tools that scale to large cohorts. This paper presents and evaluates the dcc --help tool, an integration of a Large Language Model (LLM) into the Debugging C Compiler (DCC) to generate unique, novice-focused explanations tailored to each error. dcc --help prompts an LLM with contextual information of compile- and run-time error occurrences, including the source code, error location and standard compiler error message. The LLM is instructed to generate novice-focused, actionable error explanations and guidance, designed to help students understand and resolve problems without providing solutions. dcc --help was deployed to our CS1 and CS2 courses, with 2,565 students using the tool over 64,000 times in ten weeks. We analysed a subset of these error/explanation pairs to evaluate their properties, including conceptual correctness, relevancy, and overall quality. We found that the LLM-generated explanations were conceptually accurate in 90% of compile-time and 75% of run-time cases, but often disregarded the instruction not to provide solutions in code. Our findings, observations and reflections following deployment indicate that dcc --help provides novel opportunities for scaffolding students' introduction to programming.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1314–1320},
numpages = {7},
keywords = {ai in cs1, ai in education, compiler error messages, cs1, debugging, error message enhancement, generative ai, large language models, programming error messages},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3626686.3626704,
author = {Wang, Dejiang and Zhai, Zhuoran and Cheong, Ngai and Peng, Li},
title = {Script-Generated Picture Book Technology Based on Large Language Models and AIGC},
year = {2023},
isbn = {9798400708527},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626686.3626704},
doi = {10.1145/3626686.3626704},
abstract = {This paper mainly discusses how to use the large language models such as GPT and Ernie model combined with the AIGC tools represented by stable diffusion, which uses a random story script to generate images with fixed style, character characteristics, and continuous plots. The article provides a detailed introduction to how to build an assembly line, using a large language model and a story script to generate the prompt words required for stable diffusion. Subsequently, by comparing the characteristics of traditional picture book production and the image results of using language models word prompts, summarize the limitations of text to images. This leads to a supervised multi round iterative LoRA model scheme that utilizes the CLIP to achieve character IP fixation. Simultaneously using the ControlNet model and inpainting to preprocess and reprocess the image can achieve controllable character poses and fixed backgrounds in the picture book. Finally, we will evaluate and summarize the new scheme and analyze its strengths in picture book creation accordingly.},
booktitle = {Proceedings of the 7th International Conference on Digital Technology in Education},
pages = {104–108},
numpages = {5},
location = {Hangzhou, China},
series = {ICDTE '23}
}

@inproceedings{10.1145/3649165.3690101,
author = {Hellas, Arto and Leinonen, Juho and Lepp\"{a}nen, Leo},
title = {Experiences from Integrating Large Language Model Chatbots into the Classroom},
year = {2024},
isbn = {9798400705984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649165.3690101},
doi = {10.1145/3649165.3690101},
abstract = {We provided students access to a state-of-the-art large language model (LLM) chatbot through the online materials of three university-level courses. One of the courses focused on software engineering with LLMs, while the two other courses were not directly related to LLMs. The chatbot used OpenAI GPT-4 without additional filters or system prompts.  Our results suggest that only a minority of students engage with the chatbot in the courses that do not relate to LLMs. At the same time, unsurprisingly, nearly all students in the LLM-focused course leveraged the chatbot. In all courses, the majority of the chatbot usage came from a few superusers, whereas the majority of the students did not heavily use the chatbot even though it effectively provided free access to OpenAI's GPT-4 model (which would have otherwise required a paid subscription at the time of the study). We observe that in addition to students using the chatbot for course-specific purposes, many use the chatbot for their own purposes.  Overall, our results suggest that the worst fears of educators -- all students overrelying on chatbots -- did not materialize. Finally, we discuss potential reasons for low usage, including the need for more tailored and scaffolded chatbot experiences targeted for specific types of use cases.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1},
pages = {46–52},
numpages = {7},
keywords = {chatbots, classroom experiences, experience report, generative ai, large language models, usage analysis},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@inproceedings{10.1145/3501709.3544280,
author = {MacNeil, Stephen and Tran, Andrew and Mogil, Dan and Bernstein, Seth and Ross, Erin and Huang, Ziheng},
title = {Generating Diverse Code Explanations using the GPT-3 Large Language Model},
year = {2022},
isbn = {9781450391955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501709.3544280},
doi = {10.1145/3501709.3544280},
abstract = {Good explanations are essential to efficiently learning introductory programming concepts [10]. To provide high-quality explanations at scale, numerous systems automate the process by tracing the execution of code [8, 12], defining terms [9], giving hints [16], and providing error-specific feedback [10, 16]. However, these approaches often require manual effort to configure and only explain a single aspect of a given code segment. Large language models (LLMs) are also changing how students interact with code [7]. For example, Github's Copilot can generate code for programmers [4], leading researchers to raise concerns about cheating [7]. Instead, our work focuses on LLMs' potential to support learning by explaining numerous aspects of a given code snippet. This poster features a systematic analysis of the diverse natural language explanations that GPT-3 can generate automatically for a given code snippet. We present a subset of three use cases from our evolving design space of AI Explanations of Code.},
booktitle = {Proceedings of the 2022 ACM Conference on International Computing Education Research - Volume 2},
pages = {37–39},
numpages = {3},
keywords = {natural language processing, large language models, computer science education, code explanations},
location = {Lugano and Virtual Event, Switzerland},
series = {ICER '22}
}

@article{10.1145/3688078,
author = {Liu, Bingbin},
title = {Large Language Models in Education Embracing Opportunities, Confronting Challenges, and Shaping the Next Chapter Together},
year = {2024},
issue_date = {Fall 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/3688078},
doi = {10.1145/3688078},
journal = {XRDS},
month = oct,
pages = {7–9},
numpages = {3}
}

@inproceedings{10.1145/3545947.3573358,
author = {MacNeil, Stephen and Kim, Joanne and Leinonen, Juho and Denny, Paul and Bernstein, Seth and Becker, Brett A. and Wermelinger, Michel and Hellas, Arto and Tran, Andrew and Sarsa, Sami and Prather, James and Kumar, Viraj},
title = {The Implications of Large Language Models for CS Teachers and Students},
year = {2023},
isbn = {9781450394338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545947.3573358},
doi = {10.1145/3545947.3573358},
abstract = {The introduction of Large Language Models (LLMs) has generated a significant amount of excitement both in industry and among researchers. Recently, tools that leverage LLMs have made their way into the classroom where they help students generate code and help instructors generate learning materials. There are likely many more uses of these tools -- both beneficial to learning and possibly detrimental to learning. To help ensure that these tools are used to enhance learning, educators need to not only be familiar with these tools, but with their use and potential misuse. The goal of this BoF is to raise awareness about LLMs and to build a learning community around their use in computing education. Aligned with this goal of building an inclusive learning community, our BoF is led by globally distributed discussion leaders, including undergraduate researchers, to facilitate multiple coordinated discussions that can lead to a broader conversation about the role of LLMs in CS education.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1255},
numpages = {1},
keywords = {artificial intelligence, code explanations, code generation, computer science education, copilot, gpt-3, large language models},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@article{10.5555/3722479.3722507,
author = {Crocetti, Giancarlo and Bak, Seonwoo and Vautor-Laplaceliere, Daena D. and Noory, Naqib A.},
title = {Evaluating the Pedagogical Impact of Large Language Models on Programming Skills in Data Science Programs in Higher Education},
year = {2024},
issue_date = {October 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {3},
issn = {1937-4771},
abstract = {The integration of GenAI (GenAI), such as large language models (LLMs), in education has raised the question of how it will alter the students' training and learning outcomes. To better understand the phenomenon, this empirical study explores whether college students find GenAI tools helpful in advancing their skills, particularly Python programming proficiency.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {63–64},
numpages = {2}
}

@inproceedings{10.1145/3626253.3635604,
author = {Weber, Jason Lee and Martinez Neda, Barbara and Carbajal Juarez, Kitana and Wong-Ma, Jennifer and Gago-Masague, Sergio and Ziv, Hadar},
title = {Measuring CS Student Attitudes Toward Large Language Models},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635604},
doi = {10.1145/3626253.3635604},
abstract = {With the mainstream adoption of Large Language Models (LLMs), members of both academia and the media have raised concerns around their impact on student learning and pedagogy. Many students and educators wonder about the pedagogical fit of this emerging technology. We aim to measure the adoption of and attitudes toward LLMs among the CS student population at an R1 University to determine how students are using these new tools. To this end, we conducted a large survey study targeting two populations participating in computing courses at the university: intro-sequence students (ISS) and experienced students (ES).In our preliminary results from Spring 2023, we've found several significant differences among the views of over 700 respondents across the two groups. Most students reported LLMs' unparalleled potential for quick information access, yet many harbor concerns about the reliability of the LLM responses, and the impact on academic integrity. Additionally, while ES have rapidly integrated LLMs into their learning, ISS remain cautious of the tools, highlighting a stark contrast in adoption rates between the groups.LLMs are clearly going to reshape pedagogical approaches and student engagement. Our study hopes to provide insight on the nuanced student attitudes toward LLMs. For example, the notable reservations expressed by ISS illustrate an imperative for careful, informed, and ethical integration to ensure these tools enhance rather than compromise the educational experience. In the future, we plan to continue tracking student attitudes in order to gain further understanding of the changing perceptions of LLMs and their impact.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1846–1847},
numpages = {2},
keywords = {academic integrity, ai tools, chatgpt, faculty perception, generative ai, large language models (llms), student perception},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3649217.3653533,
author = {Bernstein, Seth and Denny, Paul and Leinonen, Juho and Kan, Lauren and Hellas, Arto and Littlefield, Matt and Sarsa, Sami and Macneil, Stephen},
title = {"Like a Nesting Doll": Analyzing Recursion Analogies Generated by CS Students Using Large Language Models},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653533},
doi = {10.1145/3649217.3653533},
abstract = {Grasping complex computing concepts often poses a challenge for students who struggle to anchor these new ideas to familiar experiences and understandings. To help with this, a good analogy can bridge the gap between unfamiliar concepts and familiar ones, providing an engaging way to aid understanding. However, creating effective educational analogies is difficult even for experienced instructors. We investigate to what extent large language models (LLMs), specifically ChatGPT, can provide access to personally relevant analogies on demand. Focusing on recursion, a challenging threshold concept, we conducted an investigation analyzing the analogies generated by more than 350 first-year computing students. They were provided with a code snippet and tasked to generate their own recursion-based analogies using ChatGPT, optionally including personally relevant topics in their prompts. We observed a great deal of diversity in the analogies produced with student-prescribed topics, in contrast to the otherwise generic analogies, highlighting the value of student creativity when working with LLMs. Not only did students enjoy the activity and report an improved understanding of recursion, but they described more easily remembering analogies that were personally and culturally relevant.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {122–128},
numpages = {7},
keywords = {analogies, computing education, large language models},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3581754.3584111,
author = {Cao, Chen},
title = {Scaffolding CS1 Courses with a Large Language Model-Powered Intelligent Tutoring System},
year = {2023},
isbn = {9798400701078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581754.3584111},
doi = {10.1145/3581754.3584111},
abstract = {Programming skills are rapidly becoming essential for many educational paths and career opportunities. Yet, for many international students, the traditional approach to teaching introductory programming courses can be a significant challenge due to the complexities of the language, the lack of prior programming knowledge, and the language and cultural barriers. This study explores how large language models and gamification can scaffold coding learning and increase Chinese students’ sense of belonging in introductory programming courses. In this project, a gamification intelligent tutoring system was developed to adapt to Chinese international students’ learning needs and provides scaffolding to support their success in introductory computer programming courses. My research includes three studies: a formative study, a user study of an initial prototype, and a computer simulation study with a user study in progress. Both qualitative and quantitative data were collected through surveys, observations, focus group discussions and computer simulation. The preliminary findings suggest that GPT-3-enhanced gamification has great potential in scaffolding introductory programming learning by providing adaptive and personalised feedback, increasing students’ sense of belonging, and reducing their anxiety about learning programming.},
booktitle = {Companion Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {229–232},
numpages = {4},
location = {Sydney, NSW, Australia},
series = {IUI '23 Companion}
}

@inproceedings{10.1145/3568813.3600139,
author = {Hellas, Arto and Leinonen, Juho and Sarsa, Sami and Koutcheme, Charles and Kujanp\"{a}\"{a}, Lilja and Sorva, Juha},
title = {Exploring the Responses of Large Language Models to Beginner Programmers’ Help Requests},
year = {2023},
isbn = {9781450399760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568813.3600139},
doi = {10.1145/3568813.3600139},
abstract = {Background and Context: Over the past year, large language models (LLMs) have taken the world by storm. In computing education, like in other walks of life, many opportunities and threats have emerged as a consequence. Objectives: In this article, we explore such opportunities and threats in a specific area: responding to student programmers’ help requests. More specifically, we assess how good LLMs are at identifying issues in problematic code that students request help on. Method: We collected a sample of help requests and code from an online programming course. We then prompted two different LLMs (OpenAI Codex and GPT-3.5) to identify and explain the issues in the students’ code and assessed the LLM-generated answers both quantitatively and qualitatively. Findings: GPT-3.5 outperforms Codex in most respects. Both LLMs frequently find at least one actual issue in each student program (GPT-3.5 in 90% of the cases). Neither LLM excels at finding all the issues (GPT-3.5 finding them 57% of the time). False positives are common (40% chance for GPT-3.5). The advice that the LLMs provide on the issues is often sensible. The LLMs perform better on issues involving program logic rather than on output formatting. Model solutions are frequently provided even when the LLM is prompted not to. LLM responses to prompts in a non-English language are only slightly worse than responses to English prompts. Implications: Our results continue to highlight the utility of LLMs in programming education. At the same time, the results highlight the unreliability of LLMs: LLMs make some of the same mistakes that students do, perhaps especially when formatting output as required by automated assessment systems. Our study informs teachers interested in using LLMs as well as future efforts to customize LLMs for the needs of programming education.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1},
pages = {93–105},
numpages = {13},
keywords = {CS1, GPT, OpenAI Codex, automatic feedback, help seeking, introductory programming education, large language models, student questions},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3545945.3569785,
author = {MacNeil, Stephen and Tran, Andrew and Hellas, Arto and Kim, Joanne and Sarsa, Sami and Denny, Paul and Bernstein, Seth and Leinonen, Juho},
title = {Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569785},
doi = {10.1145/3545945.3569785},
abstract = {Advances in natural language processing have resulted in large language models (LLMs) that can generate code and code explanations. In this paper, we report on our experiences generating multiple code explanation types using LLMs and integrating them into an interactive e-book on web software development. Three different types of explanations -- a line-by-line explanation, a list of important concepts, and a high-level summary of the code -- were created. Students could view explanations by clicking a button next to code snippets, which showed the explanation and asked about its utility. Our results show that all explanation types were viewed by students and that the majority of students perceived the code explanations as helpful to them. However, student engagement varied by code snippet complexity, explanation type, and code snippet length. Drawing on our experiences, we discuss future directions for integrating explanations generated by LLMs into CS classrooms.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {931–937},
numpages = {7},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3568812.3603476,
author = {Phung, Tung and P\u{a}durean, Victor-Alexandru and Cambronero, Jos\'{e} and Gulwani, Sumit and Kohn, Tobias and Majumdar, Rupak and Singla, Adish and Soares, Gustavo},
title = {Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors},
year = {2023},
isbn = {9781450399753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568812.3603476},
doi = {10.1145/3568812.3603476},
abstract = {Generative AI and large language models hold great promise in enhancing computing education by powering next-generation educational technologies. State-of-the-art models like OpenAI’s ChatGPT&nbsp;[8] and GPT-4&nbsp;[9] could enhance programming education in various roles, e.g., by acting as a personalized digital tutor for a student, a digital assistant for an educator, and a digital peer for collaborative learning&nbsp;[1, 2, 7]. In our work, we seek to comprehensively evaluate and benchmark state-of-the-art large language models for various scenarios in programming education. Recent works have evaluated several large language models in the context of programming education&nbsp;[4, 6, 10, 11, 12]. However, these works are limited for several reasons: they have typically focused on evaluating a specific model for a specific education scenario (e.g., generating explanations), or have considered models that are already outdated (e.g., OpenAI’s Codex&nbsp;[3] is no longer publicly available since March 2023). Consequently, there is a lack of systematic study that benchmarks state-of-the-art models for a comprehensive set of programming education scenarios. In our work, we systematically evaluate two models, ChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human tutors for a variety of scenarios in programming education. These scenarios are designed to capture distinct roles these models could play, namely digital tutors, assistants, and peers, as discussed above. More concretely, we consider the following six scenarios: (1) program repair, i.e., fixing a student’s buggy program; (2) hint generation, i.e., providing a natural language hint to the student to help resolve current issues; (3) grading feedback, i.e., grading a student’s program w.r.t. a given rubric; (4) peer programming, i.e., completing a partially written program or generating a sketch for the solution program; (5) task creation, i.e., generating new tasks that exercise specific types of concepts or bugs; (6) contextualized explanation, i.e., explaining specific concepts or functions in the context of a given program. Our study uses a mix of quantitative and qualitative evaluation to compare the performance of these models with the performance of human tutors. We conduct our evaluation based on 5 introductory Python programming problems with a diverse set of input/output specifications. For each of these problems, we consider 5 buggy programs based on publicly accessible submissions from geeksforgeeks.org &nbsp;[5] (see Figure&nbsp;1); these buggy programs are picked to capture different types of bugs for each problem. We will provide a detailed analysis of the data and results in a longer version of this poster. Our preliminary results show that GPT-4 drastically outperforms ChatGPT (based on GPT-3.5) and comes close to human tutors’ performance for several scenarios.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2},
pages = {41–42},
numpages = {2},
keywords = {ChatGPT, generative AI, introductory programming education, large language models},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3627673.3679760,
author = {Fu, Lingyue and Guan, Hao and Du, Kounianhua and Lin, Jianghao and Xia, Wei and Zhang, Weinan and Tang, Ruiming and Wang, Yasheng and Yu, Yong},
title = {SINKT: A Structure-Aware Inductive Knowledge Tracing Model with Large Language Model},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679760},
doi = {10.1145/3627673.3679760},
abstract = {Knowledge Tracing (KT) aims to determine whether students will respond correctly to the next question, which is a crucial task in intelligent tutoring systems (ITS). In educational KT scenarios, transductive ID-based methods often face severe data sparsity and cold start problems, where interactions between individual students and questions are sparse, and new questions and concepts consistently arrive in the database. In addition, existing KT models only implicitly consider the correlation between concepts and questions, lacking direct modeling of the more complex relationships in the heterogeneous graph of concepts and questions. In this paper, we propose a &lt;u&gt;S&lt;/u&gt;tructure-aware &lt;u&gt;IN&lt;/u&gt;ductive &lt;u&gt;K&lt;/u&gt;nowledge &lt;u&gt;T&lt;/u&gt;racing model with large language model (dubbed SINKT), which, for the first time, introduces large language models (LLMs) and realizes inductive knowledge tracing. Firstly, SINKT utilizes LLMs to introduce structural relationships between concepts and constructs a hetero- geneous graph for concepts and questions. Secondly, by encoding concepts and questions with LLMs, SINKT incorporates semantic information to aid prediction. Finally, SINKT predicts the student's response to the target question by interacting with the student's knowledge state and the question representation. Experiments on four real-world datasets demonstrate that SINKT achieves state-of-the-art performance among 12 existing transductive KT models. Additionally, we explore the performance of SINKT on the inductive KT task and provide insights into various modules.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {632–642},
numpages = {11},
keywords = {inductive learning, knowledge tracing, online education},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3568813.3600142,
author = {Savelka, Jaromir and Agarwal, Arav and An, Marshall and Bogart, Chris and Sakr, Majd},
title = {Thrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Programming Courses},
year = {2023},
isbn = {9781450399760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568813.3600142},
doi = {10.1145/3568813.3600142},
abstract = {This paper studies recent developments in large language models’ (LLM) abilities to pass assessments in introductory and intermediate Python programming courses at the postsecondary level. The emergence of ChatGPT resulted in heated debates of its potential uses (e.g., exercise generation, code explanation) as well as misuses in programming classes (e.g., cheating). Recent studies show that while the technology performs surprisingly well on diverse sets of assessment instruments employed in typical programming classes the performance is usually not sufficient to pass the courses. The release of GPT-4 largely emphasized notable improvements in the capabilities related to handling assessments originally designed for human test-takers. This study is the necessary analysis in the context of this ongoing transition towards mature generative AI systems. Specifically, we report the performance of GPT-4, comparing it to the previous generations of GPT models, on three Python courses with assessments ranging from simple multiple-choice questions (no code involved) to complex programming projects with code bases distributed into multiple files (599 exercises overall). Additionally, we analyze the assessments that were not handled well by GPT-4 to understand the current limitations of the model, as well as its capabilities to leverage feedback provided by an auto-grader. We found that the GPT models evolved from completely failing the typical programming class’ assessments (the original GPT-3) to confidently passing the courses with no human involvement (GPT-4). While we identified certain limitations in GPT-4’s handling of MCQs and coding exercises, the rate of improvement across the recent generations of GPT models strongly suggests their potential to handle almost any type of assessment widely used in higher education programming courses. These findings could be leveraged by educators and institutions to adapt the design of programming assessments as well as to fuel the necessary discussions into how programming classes should be updated to reflect the recent technological developments. This study provides evidence that programming instructors need to prepare for a world in which there is an easy-to-use widely accessible technology that can be utilized by learners to collect passing scores, with no effort whatsoever, on what today counts as viable programming knowledge and skills assessments.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1},
pages = {78–92},
numpages = {15},
keywords = {AI code generation, AlphaCode, ChatGPT, Codex, GPT, GitHub Copilot, MCQ, Multiple-choice question answering, Python course, coding exercises, generative pre-trained transformers, introductory and intermediate programming, programming knowledge assessment},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3626252.3630897,
author = {Jordan, Mollie and Ly, Kevin and Soosai Raj, Adalbert Gerald},
title = {Need a Programming Exercise Generated in Your Native Language? ChatGPT's Got Your Back: Automatic Generation of Non-English Programming Exercises Using OpenAI GPT-3.5},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630897},
doi = {10.1145/3626252.3630897},
abstract = {Large language models (LLMs) like ChatGPT are changing computing education and may create additional barriers to those already faced by non-native English speakers (NNES) learning computing. We investigate an opportunity for a positive impact of LLMs on NNES through multilingual programming exercise generation. Following previous work with LLM exercise generation in English, we prompt OpenAI GPT-3.5 in 4 natural languages (English, Tamil, Spanish, and Vietnamese) to create introductory programming problems, sample solutions, and test cases. We evaluate these problems on their sensibility, readability, translation, sample solution accuracy, topicality, and cultural relevance. We find that problems generated in English, Spanish, and Vietnamese are largely sensible, easily understood, and accurate in their sample solutions. However, Tamil problems are mostly non-sensible and have a much lower passing test rate, indicating that the abilities of LLMs for problem generation are not generalizable across languages. Our analysis suggests that these problems could not be given verbatim to students, but with minimal effort, most errors can be fixed. We further discuss the benefits of these problems despite their flaws, and their opportunities to provide personalized and culturally relevant resources for students in their native languages.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {618–624},
numpages = {7},
keywords = {introductory programming, large language models, non-native english speakers, problem generation},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3664476.3670446,
author = {Ohm, Marc and Bungartz, Christian and Boes, Felix and Meier, Michael},
title = {Assessing the Impact of Large Language Models on Cybersecurity Education: A Study of ChatGPT's Influence on Student Performance},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664476.3670446},
doi = {10.1145/3664476.3670446},
abstract = {The popularity of chatbots to facilitate day-to-day business, including students and their study exercises, is on the rise. This paper investigates the extent and effects on the academic performance of students that leverage such tools. While many other approaches are hypothesized and discussed, we measure empirically. We recorded and compared the performance of cybersecurity students in weekly exercises and final exams over a period of three years. This allows us to have three groups with varying degrees of ChatGPT influence, namely no access, uncontrolled access, and controlled access. In an anonymous survey, we found that approximately 80% of our students utilize ChatGPT during the weekly assignments in 2023. However, none of them indicated this on their submission, despite it being a mandatory requirement. Through statistical analysis of achieved points in our sample groups, we identified that students perform similarly on the weekly assignments. However, their performance on the final examination deteriorates.},
booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security},
articleno = {104},
numpages = {7},
keywords = {ChatGPT, Education, Teaching},
location = {Vienna, Austria},
series = {ARES '24}
}

@inproceedings{10.1145/3568812.3603474,
author = {Singla, Adish},
title = {Evaluating ChatGPT and GPT-4 for Visual Programming},
year = {2023},
isbn = {9781450399753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568812.3603474},
doi = {10.1145/3568812.3603474},
abstract = {Generative AI has the potential to drastically improve the landscape of computing education by automatically generating personalized feedback and content. In particular, this potential lies in the advanced capabilities of state-of-the-art deep generative and large language models such as OpenAI’s Codex&nbsp;[7], ChatGPT&nbsp;[11], and GPT-4&nbsp;[12]. In our work, we seek to investigate the capabilities of these models in visual programming domains popularly used for K-8 programming education, including domains like Scratch&nbsp;[17], Hour of Code: Maze Challenge by Code.org&nbsp;[4, 5], and Karel&nbsp;[13]. Recent works have shown us sparks of advanced capabilities of such models for various education scenarios in introductory Python programming&nbsp;[2, 14, 18, 20]. In fact, a study in 2022 had ranked Codex in the top quartile w.r.t students in a large Python programming course&nbsp;[8]. However, all these works consider only text-based Python programming and leave open the question of how well these models would perform for visual programming. The main research question is: Do state-of-the-art neural generative models show advanced capabilities for visual programming on par with their capabilities on text-based Python programming?In our work, we evaluate these models for visual programming based on the following three settings designed to capture various generative and problem-solving capabilities: We conduct our evaluation based on 10 representative tasks from two visual programming domains: Hour of Code: Maze Challenge by Code.org&nbsp;[4, 5] and Intro to Programming with Karel course by CodeHS.com&nbsp;[3, 13]. As illustrative examples, Figures&nbsp;1,&nbsp;2,&nbsp;and&nbsp;3 show the output of GPT-4 in three settings for Maze18 task. We will provide the detailed analysis and prompts used in a longer version of this poster. Our preliminary results for ChatGPT (based on GPT-3.5) and GPT-4 show that these models perform poorly and produce incorrect output the majority of the time. These results highlight that state-of-the-art neural generative models like GPT-4 still struggle to combine spatial, logical, and programming skills crucial for visual programming. As the next step, it would be important to curate novel benchmarks that the research community can use to evaluate improvements in future versions of these models for visual programming.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2},
pages = {14–15},
numpages = {2},
keywords = {ChatGPT, block-based visual programming, generative AI, introductory programming education, large language models},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3649217.3653612,
author = {Koutcheme, Charles and Dainese, Nicola and Sarsa, Sami and Hellas, Arto and Leinonen, Juho and Denny, Paul},
title = {Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653612},
doi = {10.1145/3649217.3653612},
abstract = {Large language models (LLMs) have shown great potential for the automatic generation of feedback in a wide range of computing contexts. However, concerns have been voiced around the privacy and ethical implications of sending student work to proprietary models. This has sparked considerable interest in the use of open source LLMs in education, but the quality of the feedback that such open models can produce remains understudied. This is a concern as providing flawed or misleading generated feedback could be detrimental to student learning. Inspired by recent work that has utilised very powerful LLMs, such as GPT-4, to evaluate the outputs produced by less powerful models, we conduct an automated analysis of the quality of the feedback produced by several open source models using a dataset from an introductory programming course. First, we investigate the viability of employing GPT-4 as an automated evaluator by comparing its evaluations with those of a human expert. We observe that GPT-4 demonstrates a bias toward positively rating feedback while exhibiting moderate agreement with human raters, showcasing its potential as a feedback evaluator. Second, we explore the quality of feedback generated by several leading open-source LLMs by using GPT-4 to evaluate the feedback. We find that some models offer competitive performance with popular proprietary LLMs, such as ChatGPT, indicating opportunities for their responsible use in educational settings.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {52–58},
numpages = {7},
keywords = {automatic evaluation, automatic feedback, code llama, generative ai, gpt-4, large language models, llm-as-a-judge, llms, open source, programming feedback, zephyr},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3649217.3653594,
author = {Azaiz, Imen and Kiesler, Natalie and Strickroth, Sven},
title = {Feedback-Generation for Programming Exercises With GPT-4},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653594},
doi = {10.1145/3649217.3653594},
abstract = {Ever since Large Language Models (LLMs) and related applications have become broadly available, several studies investigated their potential for assisting educators and supporting students in higher education. LLMs such as Codex, GPT-3.5, and GPT 4 have shown promising results in the context of large programming courses, where students can benefit from feedback and hints if provided timely and at scale. This paper explores the quality of GPT-4 Turbo's generated output for prompts containing both the programming task specification and a student's submission as input. Two assignments from an introductory programming course were selected, and GPT-4 was asked to generate feedback for 55 randomly chosen, authentic student programming submissions. The output was qualitatively analyzed regarding correctness, personalization, fault localization, and other features identified in the material. Compared to prior work and analyses of GPT-3.5, GPT-4 Turbo shows notable improvements. For example, the output is more structured and consistent. GPT-4 Turbo can also accurately identify invalid casing in student programs' output. In some cases, the feedback also includes the output of the student program. At the same time, inconsistent feedback was noted such as stating that the submission is correct but an error needs to be fixed. The present work increases our understanding of LLMs' potential, limitations, and how to integrate them into e-assessment systems, pedagogical scenarios, and instructing students who are using applications based on GPT-4.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {31–37},
numpages = {7},
keywords = {GPT-4 turbo, LLMs, assessment, benchmarking, formative feedback, introductory programming, large language models, personalized feedback},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3623762.3633499,
author = {Prather, James and Denny, Paul and Leinonen, Juho and Becker, Brett A. and Albluwi, Ibrahim and Craig, Michelle and Keuning, Hieke and Kiesler, Natalie and Kohn, Tobias and Luxton-Reilly, Andrew and MacNeil, Stephen and Petersen, Andrew and Pettit, Raymond and Reeves, Brent N. and Savelka, Jaromir},
title = {The Robots Are Here: Navigating the Generative AI Revolution in Computing Education},
year = {2023},
isbn = {9798400704055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623762.3633499},
doi = {10.1145/3623762.3633499},
abstract = {Recent advancements in artificial intelligence (AI) and specifically generative AI (GenAI) are threatening to fundamentally reshape computing and society. Largely driven by large language models (LLMs), many tools are now able to interpret and generate both natural language instructions and source code. These capabilities have sparked urgent questions in the computing education community around how educators should adapt their pedagogy to address the challenges and to leverage the opportunities presented by this new technology. In this working group report, we undertake a comprehensive exploration of generative AI in the context of computing education and make five significant contributions. First, we provide a detailed review of the literature on LLMs in computing education and synthesise findings from 71 primary articles, nearly 80% of which have been published in the first 8 months of 2023. Second, we report the findings of a survey of computing students and instructors from across 20 countries, capturing prevailing attitudes towards GenAI/LLMs and their use in computing education contexts. Third, to understand how pedagogy is already changing, we offer insights collected from in-depth interviews with 22 computing educators from five continents. Fourth, we use the ACM Code of Ethics to frame a discussion of ethical issues raised by the use of large language models in computing education, and we provide concrete advice for policy makers, educators, and students. Finally, we benchmark the performance of several current GenAI models/tools on various computing education datasets, and highlight the extent to which the capabilities of current models are rapidly improving.There is little doubt that LLMs and other forms of GenAI will have a profound impact on computing education over the coming years. However, just as the technology will continue to improve, so will our collective knowledge about how to leverage these new models and tools in educational settings. We expect many important conversations around this topic will emerge as the community explores how to provide more effective, inclusive, and personalised learning experiences. Our aim is that this report will serve as a focal point for both researchers and practitioners who are exploring, adapting, using, and evaluating GenAI and LLM-based tools in computing classrooms.},
booktitle = {Proceedings of the 2023 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {108–159},
numpages = {52},
keywords = {ai, artificial intelligence, chatgpt, code generation, codex, computer programming, copilot, cs1, curriculum, generative ai, github, gpt, gpt-3, gpt-4, large language models, llm, llms, novice programming, openai, pedagogical practices, programming},
location = {Turku, Finland},
series = {ITiCSE-WGR '23}
}

@inproceedings{10.1145/3649165.3690116,
author = {Golesteanu, Matei A. and Vowinkel, Garrett B. and Dougherty, Ryan E.},
title = {Can ChatGPT pass a Theory of Computing Course?},
year = {2024},
isbn = {9798400705984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649165.3690116},
doi = {10.1145/3649165.3690116},
abstract = {Large Language Models (LLMs) have had considerable difficulty when prompted with mathematical and formal questions, especially those within theory of computing (ToC) courses. In this paper, we detail two experiments regarding our own ToC course and the ChatGPT LLM. For the first, we evaluated ChatGPT's ability to pass our own ToC course's exams. For the second, we created a database of sample ToC questions and responses to accommodate other ToC offerings' choices for topics and structure. We scored each of ChatGPT's outputs on these questions. Overall, we determined that ChatGPT can pass our ToC course, and is adequate at understanding common formal definitions and answering "simple''-style questions, e.g., true/false and multiple choice. However, ChatGPT often makes nonsensical claims in open-ended responses, such as proofs.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1},
pages = {33–38},
numpages = {6},
keywords = {automata theory, chatgpt, computer science education, formal languages, large language model, theoretical computer science},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@inproceedings{10.1145/3641555.3705180,
author = {Brilliantova, Angelina and Butler, Zack and Bez\'{a}kov\'{a}, Ivona},
title = {Exploring ChatGPT as a Qualitative Research Assistant},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705180},
doi = {10.1145/3641555.3705180},
abstract = {In many CS educational research studies, students are surveyed to understand their reactions to a particular pedagogical approach or tool. These surveys, as well as other types of evaluations, often invite students to provide open-ended feedback about their experiences. However, analyzing these comments can prove to be a challenge, especially to CS educators who may not have strong expertise in qualitative research methods. In addition, in a large study, evaluating all of the provided comments can consume a significant amount of researcher time. In this work, we undertook two separate conversations with ChatGPT in which we prompted it to perform qualitative analysis of a set of comments collected in an earlier study. This allowed us to begin to judge how effectively a modern large language model can serve as an assistant in qualitative analysis. We found that with the prompts we used, ChatGPT can reliably build a set of reasonable labels (codes) for a set of comments, but the application of its labels to specific comments may or may not be effective and human researchers still need to use care and their own understanding in interpreting its output.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1397–1398},
numpages = {2},
keywords = {chatgpt, grounded theory, large-language models, qualitative analysis},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3631802.3631845,
author = {Pirttinen, Nea and Leinonen, Juho},
title = {Could ChatGPT Be Used for Reviewing Learnersourced Exercises?},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631845},
doi = {10.1145/3631802.3631845},
abstract = {Large language models and tools based on large language models such as ChatGPT have received intense attention in the past year in computing education. In this work, we explore whether ChatGPT could be used to review learnersourced exercises. One of the major downsides of learnersourcing is the dubious quality of the created content, leading to many systems using peer review for curating the content. Our results suggest that ChatGPT is not yet ready for this task.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {42},
numpages = {2},
keywords = {ChatGPT, LLMs, crowdsourcing, generative AI, large language models, learnersourcing, reviews},
location = {Koli, Finland},
series = {Koli Calling '23}
}

@inproceedings{10.1145/3613905.3647967,
author = {Kimmel, Bailey and Geisert, Austin Lee and Yaro, Lily and Gipson, Brendan and Hotchkiss, Ronald Taylor and Osae-Asante, Sidney Kwame and Vaught, Hunter and Wininger, Grant and Yamaguchi, Chase},
title = {Enhancing Programming Error Messages in Real Time with Generative AI},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3647967},
doi = {10.1145/3613905.3647967},
abstract = {Generative AI is changing the way that many disciplines are taught, including computer science. Researchers have shown that generative AI tools are capable of solving programming problems, writing extensive blocks of code, and explaining complex code in simple terms. Particular promise has been shown in using generative AI to enhance programming error messages. Both students and instructors have complained for decades that these messages are often cryptic and difficult to understand. Yet recent work has shown that students make fewer repeated errors when enhanced via GPT-4. We extend this work by implementing feedback from ChatGPT for all programs submitted to our automated assessment tool, Athene, providing help for compiler, run-time, and logic errors. Our results indicate that adding generative AI to an automated assessment tool does not necessarily make it better and that design of the interface matters greatly to the usability of the feedback that GPT-4 provided.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {608},
numpages = {7},
keywords = {AI, Artificial Intelligence, Automatic Code Generation, CS1, ChatGPT, Codex, Copilot, GPT-4, GitHub, HCI, Introductory Programming, LLM, Large Language Models, Novice Programming, OpenAI},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{10.1145/3702386.3702388,
author = {Xu, Xiao},
title = {Comparative Analysis of GPT-4o and GPT-4.0 in Business Ethics Role-Play Simulations},
year = {2025},
isbn = {9798400710131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702386.3702388},
doi = {10.1145/3702386.3702388},
abstract = {The rapid advancement of artificial intelligence (AI) technologies has opened new frontiers in educational methodologies, particularly in enhancing interactive learning environments. This paper examines the integration of two AI-driven models, ChatGPT-4.0 and its advanced iteration, GPT-4o, into the teaching of complex subjects such as climate risk management within higher education. Utilizing role-play simulations, a method proven to effectively deepen understanding and engagement, we explore how these models enhance traditional educational approaches by providing dynamic, real-time interactions that mimic real-world decision-making processes. Our comparative analysis focuses on the performance of these models in terms of response time, emotional intelligence, and quality of engagement. The findings indicate that GPT-4o, with its quicker response times and enhanced emotional recognition capabilities, significantly improves learner engagement and the effectiveness of role-play simulations. This study highlights the potential of AI to not only complement but substantially enrich pedagogical practices, offering educators valuable insights into selecting appropriate AI tools for their instructional needs. Through this exploration, we advocate for a hybrid educational model that synergistically combines the strengths of both traditional and AI-enhanced learning, proposing a future where education is more adaptive, personalized, and aligned with the evolving demands of the digital age.},
booktitle = {Proceedings of the 2024 International Conference on Artificial Intelligence and Teacher Education},
pages = {57–63},
numpages = {7},
keywords = {ChatGPT, Large Language Model, Role-play, generative AI},
location = {
},
series = {ICAITE '24}
}

@inproceedings{10.1145/3649165.3690123,
author = {Ramesh, Aninditha and Agarwal, Arav and Doughty, Jacob Arthur and Ramaneti, Ketan and Savelka, Jaromir and Sakr, Majd},
title = {A Benchmark for Testing the Capabilities of LLMs in Assessing the Quality of Multiple-choice Questions in Introductory Programming Education},
year = {2024},
isbn = {9798400705984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649165.3690123},
doi = {10.1145/3649165.3690123},
abstract = {There has been a growing interest in utilizing large language models (LLMs) for numerous educational applications. Recent studies have focused on the use of LLMs for generating various educational artifacts for programming education, such as programming exercises, model solutions, or multiple-choice questions (MCQs). The ability to efficiently and reliably assess the quality of such artifacts, both automatically and human generated, has become of paramount importance. Hence, there is a pressing need to develop and make available robust benchmarks. In this paper, we investigate an example use case of assessing the quality of programming MCQs. To that end, we carefully curated a data set of 192 MCQs annotated with quality scores based on a rubric that evaluates crucial aspects such as, e.g., their clarity, the presence of a single correct answer, and the quality of distractors. The results show that the task presents a considerable challenge even to the state-of-the-art LLMs and, hence, further research is needed. To further such research efforts in this important area we release the dataset as well as the extensible evaluation pipeline to the public.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1},
pages = {193–199},
numpages = {7},
keywords = {assessments, automated evaluation, claude, computing education, gpt-4, large language models, llama, llms, mcqs, multiple choice questions},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@article{10.1145/3676281,
author = {Wang, Jieshu and Kiran, Elif and Aurora, S.R. and Simeone, Michael and Lobo, Jose},
title = {ChatGPT on ChatGPT: An Exploratory Analysis of its Performance in the Public Sector Workplace},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676281},
doi = {10.1145/3676281},
abstract = {This study explores the impact of Generative Artificial Intelligence (GenAI), in particular, ChatGPT, on the public sector workforce in the United States, focusing on task replacement, assistance potential, and the evolving landscape of skills. Utilizing GPT-4 to evaluate 1,022 core tasks across 51 public sector occupations, we provide an exploratory analysis of the roles susceptible to ChatGPT automation and those in which ChatGPT can augment human efforts. Our findings reveal that while 63% of tasks are resistant to ChatGPT replacement, primarily due to their requirement for physical presence, emotional intelligence, and complex decision-making, tasks that are routine, rule-based, and involving basic content generation show a high potential for automation. The study also identifies key skills that will remain vital, those likely to become obsolete, and new skills that will emerge as essential, highlighting the need for a strategic approach to workforce development in the face of AI advancements. In particular, our findings underscore the growing importance of skills in applying AI technologies and the ability to validate and interpret AI-generated content for humans to remain competitive. We offer insights into public-sector-specific impacts and propose a methodological framework for future research, emphasizing the importance of adapting educational curricula and policies to prepare for an AI-integrated future.},
note = {Just Accepted},
journal = {Digit. Gov.: Res. Pract.},
month = jul,
keywords = {Public sector, Workforce, Artificial intelligence, Large language models, ChatGPT, Future of work}
}

@article{10.1145/3617367,
author = {Prather, James and Reeves, Brent N. and Denny, Paul and Becker, Brett A. and Leinonen, Juho and Luxton-Reilly, Andrew and Powell, Garrett and Finnie-Ansley, James and Santos, Eddie Antonio},
title = {“It’s Weird That it Knows What I Want”: Usability and Interactions with Copilot for Novice Programmers},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1073-0516},
url = {https://doi.org/10.1145/3617367},
doi = {10.1145/3617367},
abstract = {Recent developments in deep learning have resulted in code-generation models that produce source code from natural language and code-based prompts with high accuracy. This is likely to have profound effects in the classroom, where novices learning to code can now use free tools to automatically suggest solutions to programming exercises and assignments. However, little is currently known about how novices interact with these tools in practice. We present the first study that observes students at the introductory level using one such code auto-generating tool, Github Copilot, on a typical introductory programming (CS1) assignment. Through observations and interviews we explore student perceptions of the benefits and pitfalls of this technology for learning, present new observed interaction patterns, and discuss cognitive and metacognitive difficulties faced by students. We consider design implications of these findings, specifically in terms of how tools like Copilot can better support and scaffold the novice programming experience.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = nov,
articleno = {4},
numpages = {31},
keywords = {OpenAI, novice programming, LLM, large language models, introductory programming, HCI, GPT-3, GitHub, CS1, Copilot, Codex, automatic code generation, Artificial Intelligence, AI}
}

@inproceedings{10.1145/3639474.3640058,
author = {Lehtinen, Teemu and Koutcheme, Charles and Hellas, Arto},
title = {Let's Ask AI About Their Programs: Exploring ChatGPT's Answers To Program Comprehension Questions},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640058},
doi = {10.1145/3639474.3640058},
abstract = {Recent research has explored the creation of questions from code submitted by students. These Questions about Learners' Code (QLCs) are created through program analysis, exploring execution paths, and then creating code comprehension questions from these paths and the broader code structure. Responding to the questions requires reading and tracing the code, which is known to support students' learning. At the same time, computing education researchers have witnessed the emergence of Large Language Models (LLMs) that have taken the community by storm. Researchers have demonstrated the applicability of these models especially in the introductory programming context, outlining their performance in solving introductory programming problems and their utility in creating new learning resources. In this work, we explore the capability of the state-of-the-art LLMs (GPT-3.5 and GPT-4) in answering QLCs that are generated from code that the LLMs have created. Our results show that although the state-of-the-art LLMs can create programs and trace program execution when prompted, they easily succumb to similar errors that have previously been recorded for novice programmers. These results demonstrate the fallibility of these models and perhaps dampen the expectations fueled by the recent LLM hype. At the same time, we also highlight future research possibilities such as using LLMs to mimic students as their behavior can indeed be similar for some specific tasks.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {221–232},
numpages = {12},
keywords = {QLCs, large language models, artificial intelligence, introductory programming, program comprehension},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1145/3585059.3611431,
author = {Zheng, Yong},
title = {ChatGPT for Teaching and Learning: An Experience from Data Science Education},
year = {2023},
isbn = {9798400701306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585059.3611431},
doi = {10.1145/3585059.3611431},
abstract = {ChatGPT, an implementation and application of large language models, has gained significant popularity since its initial release. Researchers have been exploring ways to harness the practical benefits of ChatGPT in real-world scenarios. Educational researchers have investigated its potential in various subjects, e.g., programming, mathematics, finance, clinical decision support, etc. However, there has been limited attention given to its application in data science education. This paper aims to bridge that gap by utilizing ChatGPT in a data science course, gathering perspectives from students, and presenting our experiences and feedback on using ChatGPT for teaching and learning in data science education. The findings not only distinguish data science education from other disciplines but also uncover new opportunities and challenges associated with incorporating ChatGPT into the data science curriculum.},
booktitle = {Proceedings of the 24th Annual Conference on Information Technology Education},
pages = {66–72},
numpages = {7},
keywords = {ChatGPT, data analytics, data science, large language model},
location = {Marietta, GA, USA},
series = {SIGITE '23}
}

@inproceedings{10.1145/3649165.3690113,
author = {Kasinidou, Maria and Kleanthous, Styliani and Otterbacher, Jahna},
title = {"We have to learn to work with such systems": Students' Perceptions of ChatGPT After a Short Educational Intervention on NLP},
year = {2024},
isbn = {9798400705984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649165.3690113},
doi = {10.1145/3649165.3690113},
abstract = {Natural Language Processing (NLP) is a critical area of AI that is increasingly integrated into everyday life. The public regularly engages with systems such as Siri, Alexa, and more recently, ChatGPT, yet few understand how these systems work. In this paper, we examine how students perceive NLP technologies after completing a unit on NLP within an AI course designed for non-CS majors. We further present our students' perspectives on the banning of ChatGPT in Italy, where the course was delivered. The NLP unit featured a lecture, an interactive session, and a practical assignment wherein students developed a smart assistant responsive to textual commands. Students, after creating their smart assistants, highlighted challenges such as inadequate training datasets and natural language ambiguity. Opinions on ChatGPT's ban varied, with privacy concerns prevailing. However, a consensus emerged in favor of educational efforts to raise awareness about technology limitations, advocating understanding over outright bans in anticipation of their inevitable integration into daily life.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1},
pages = {74–80},
numpages = {7},
keywords = {artificial intelligence, chatgpt, large language models, natural language processing},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@inproceedings{10.1145/3626252.3630784,
author = {Rogers, Michael P. and Hillberg, Hannah Miller and Groves, Christopher L.},
title = {Attitudes Towards the Use (and Misuse) of ChatGPT: A Preliminary Study},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630784},
doi = {10.1145/3626252.3630784},
abstract = {ChatGPT is the front end to a powerful large language model that has garnered widespread attention in many fields of study, including computer science (CS), where it promises to be transformational. As educators, we are just starting to grapple with the ramifications of this new technology, including implications for what we teach, how we teach, and how we grade. The decisions educators make moving forward depend heavily on the prevalence of students' use (and misuse) of ChatGPT in the classroom. Further, predictors of nefarious use could aid educators as well. We conducted an online survey to capture CS student awareness of, experience with, and attitudes toward ChatGPT. Through quantitative and qualitative analysis, we found that awareness of ChatGPT is generally high, and it is more frequently being used as a study tool than to complete students' work for them. Most students are aware of the potential for abuse in academic pursuits, but a notable minority of students admit to using it unscrupulously and to the potential for it to interfere with their learning. We conclude with a discussion of factors to consider as educators modify their approaches and develop guidelines for ChatGPT usage in their classrooms.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1147–1153},
numpages = {7},
keywords = {academic misconduct, artificial intelligence, chatgpt, large language models, student survey},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3649409.3691086,
author = {Velez, Xavier},
title = {Understanding Algorithmic Problem Solving using LLMs},
year = {2024},
isbn = {9798400706042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649409.3691086},
doi = {10.1145/3649409.3691086},
abstract = {With the rapid advancement of Large Language Models (LLMs) many instructors for Computer Science courses have begun to opt to allow students to use them as an additional educational resource but often warn that the output may be unreliable. Recent research on LLMs has demonstrated their ability to interpret commands in natural language and produce code in a variety of programming languages. However, it is not clear how well LLMs fair in tackling more complex problem set ups, like those typically seen in Algorithms courses in which students are provided natural language descriptions of an ambiguous problem and use what they learn to map the problem to an algorithmic solution. In this paper, we explore use of LLMs, such as OpenAI's GPT-4o, as tools for assisting students with complex Computer Science curricula, such as algorithmic problem solving. We specifically aim to see if using prompt refinement techniques, LLMs are capable of taking a problem statement in plain English and performing the following tasks: providing both a natural language description and code solution in the Python programming language, producing an analytical argument for the solutions correctness, and finally providing runtime analysis for the produced solution. Our experiments show that GPT-4o is well suited to solving problems like LeetCode 75 that have been seen during training, and prompt-refinement helps with those that have not been seen.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 2},
pages = {327–328},
numpages = {2},
keywords = {GPT-4o, algorithms, large language models},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@inproceedings{10.1145/3716640.3716657,
author = {Arora, Utkarsh and Garg, Anupam and Gupta, Aryan and Jain, Samyak and Mehta, Ronit and Oberoi, Rupin and Prachi and Raina, Aryaman and Saini, Manav and Sharma, Sachin and Singh, Jaskaran and Tyagi, Sarthak and Kumar, Dhruv},
title = {Analyzing LLM Usage in an Advanced Computing Class in India},
year = {2025},
isbn = {9798400714252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716640.3716657},
doi = {10.1145/3716640.3716657},
abstract = {This study examines the use of large language models (LLMs) by undergraduate and graduate students for programming assignments in advanced computing classes. Unlike existing research, which primarily focuses on introductory classes and lacks in-depth analysis of actual student-LLM interactions, our work fills this gap. We conducted a comprehensive analysis involving 411 students from a Distributed Systems class at an Indian university, where they completed three programming assignments and shared their experiences through Google Form surveys and interviews.Our findings reveal that students leveraged LLMs for a variety of tasks, including code generation, debugging, conceptual inquiries, and test case creation. They employed a spectrum of prompting strategies, ranging from basic contextual prompts to advanced techniques like chain-of-thought prompting and iterative refinement. While students generally viewed LLMs as beneficial for enhancing productivity and learning, we noted a concerning trend of over-reliance, with many students submitting entire assignment descriptions to obtain complete solutions. Given the increasing use of LLMs in the software industry, our study highlights the need to update undergraduate curricula to include training on effective prompting strategies and to raise awareness about the benefits and potential drawbacks of LLM usage in academic settings.},
booktitle = {Proceedings of the 27th Australasian Computing Education Conference},
pages = {154–163},
numpages = {10},
keywords = {Large Language Models, Computing Education, User Study},
location = {
},
series = {ACE '25}
}

@inproceedings{10.1145/3641554.3701932,
author = {Farinetti, Laura and Cagliero, Luca},
title = {A Critical Approach to ChatGPT: An Experience in SQL Learning},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701932},
doi = {10.1145/3641554.3701932},
abstract = {ChatGPT potential value in education is broadly recognized and many studies report experiments of its use inside or outside the classroom by students and teachers. On the other hand, the use of ChatGPT rises lots of concerns about well-known problems such as hallucination, plagiarism, overreliance, or misinformation. It is of primary importance to teach students a correct and constructive use of ChatGPT and a critical approach to its returned outputs. The paper presents a classroom experience where students were asked to interact with ChatGPT in the context of a database course. The declared challenge for the students was, given a set of predefined relational database schemata, to invent questions for ChatGPT and try to force wrong SQL solutions. Students had to record the question, the ChatGPT solution, their solution, and the comments about the eventual ChatGPT syntactical and/or semantical errors. This gamification approach was meant to enhance students' motivation, but the main teachers' goal was to make them reflect critically (i) on ChatGPT output, experiencing that it does make mistakes, (ii) on the interpretation of ChatGPT errors, and (iii) on the possible strategies for forcing ChatGPT errors. The experiment involved 166 B.S. students in Engineering and the collected data have been analyzed under different points of view to get an insight into the approach and the critical attitude of the students. The paper reports the results of this analysis and discusses the impact of the activity on learning by analyzing the correlation between students' participation and exam performance.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {318–324},
numpages = {7},
keywords = {critical thinking, database education, human-computer interaction, large language model, sql},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3641555.3705183,
author = {Brockenbrough, Allan and Feild, Henry and Salinas, Dominic},
title = {Exploring LLMs Impact on Student-Created User Stories and Acceptance Testing in Software Development},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705183},
doi = {10.1145/3641555.3705183},
abstract = {In Agile software development methodology, a user story describes a new feature or functionality from an end user's perspective. The user story details may also incorporate acceptance testing criteria, which can be developed through negotiation with users. When creating stories from user feedback, the software engineer may maximize their usefulness by considering story attributes, including scope, independence, negotiability, and testability. This study investigates how LLMs (large language models), with guided instructions, affect undergraduate software engineering students' ability to transform user feedback into user stories. Students, working individually, were asked to analyze user feedback comments, appropriately group related items, and create user stories following the principles of INVEST, a framework for assessing user stories. We found that LLMs help students develop valuable stories with well-defined acceptance criteria. However, students tend to perform better without LLMs when creating user stories with an appropriate scope.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1401–1402},
numpages = {2},
keywords = {LLM, generative AI, large language model, user story},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3689535.3689546,
author = {Andrei, Oana and Sojtory, Zoltan},
title = {LLM-aided Pair Programming for Algorithm Tracing},
year = {2024},
isbn = {9798400711770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689535.3689546},
doi = {10.1145/3689535.3689546},
abstract = {The recent widespread popularity of generative AI models has inspired the development of large-language model (LLM) based tools for educational purposes. We explore the impact of LLM-based tools on pair programming for algorithm tracing with the aim of addressing challenges inherent to pair programming. We designed and developed a GPT-4 based tool, TraceCompanion, that acts as students’ pair programming partner for algorithm tracing. We describe insights gained from running a pilot study to investigate students’ interactions with the tool and their initial perceptions.},
booktitle = {Proceedings of the 2024 Conference on United Kingdom &amp; Ireland Computing Education Research},
articleno = {20},
numpages = {1},
keywords = {algorithm tracing, large language models, pair programming},
location = {Manchester, United Kingdom},
series = {UKICER '24}
}

@inproceedings{10.1145/3636243.3636247,
author = {Hou, Irene and Man, Owen and Mettille, Sophia and Gutierrez, Sebastian and Angelikas, Kenneth and MacNeil, Stephen},
title = {More Robots are Coming: Large Multimodal Models (ChatGPT) can Solve Visually Diverse Images of Parsons Problems},
year = {2024},
isbn = {9798400716195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636243.3636247},
doi = {10.1145/3636243.3636247},
abstract = {Large language models are reshaping computing education. Based on recent research, these models explain code better than students, answer multiple choice questions at or above the class average, and generate code that can pass automated tests in introductory courses. In response to these capabilities, instructors have quickly adjusted their courses and assessment methods to align with shifting learning goals and the increased risk of academic integrity issues. While some scholars have advocated for the integration of visual problems as a safeguard against the capabilities of language models, new multimodal models now have vision and language capabilities that may allow them to analyze and solve visual problems. In this paper, we compare the large multimodal model (LMMs) GPT-4V with Bard, an LLM that uses Google Lens for text recognition. We find that LMMs, which have learned both pixel features (from images) and text features (from prompts) in the same embedding space, performed substantially better than Bard which uses a piecemeal approach. With a specific focus on Parsons problems presented across diverse visual representations, our results show that GPT-4V solved 96.7% these visual problems, struggling minimally with a single Parsons problem. Conversely, Bard performed poorly by only solving 69.2% of problems, struggling with common issues like hallucinations and refusals. These findings suggest that merely transitioning to visual programming problems might not be a panacea to issues of academic integrity in the generative AI era.},
booktitle = {Proceedings of the 26th Australasian Computing Education Conference},
pages = {29–38},
numpages = {10},
keywords = {Bard, ChatGPT, GPT-4V, Generative AI, LLMs, Parsons Problems, computing education, visual programming problems},
location = {Sydney, NSW, Australia},
series = {ACE '24}
}

@inproceedings{10.1145/3626252.3630874,
author = {Shen, Yiyin and Ai, Xinyi and Soosai Raj, Adalbert Gerald and Leo John, Rogers Jeffrey and Syamkumar, Meenakshi},
title = {Implications of ChatGPT for Data Science Education},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630874},
doi = {10.1145/3626252.3630874},
abstract = {ChatGPT is a conversational AI platform that can produce code to solve problems when provided with a natural language prompt. Prior work on similar AI models has shown that they perform well on typical intro-level Computer Science problems. However, little is known about the performance of such tools on Data Science (DS) problems. In this work, we assess the performance of ChatGPT on assignments from three DS courses with varying difficulty levels. First, we apply the raw assignment prompts provided to the students and find that ChatGPT performs well on assignments with dataset(s) descriptions and progressive question prompts, which divide the programming requirements into sub-problems. Then, we perform prompt engineering on the assignments for which ChatGPT had low performance. We find that the following prompt engineering techniques significantly increased ChatGPT's performance: breaking down abstract questions into steps, breaking down steps into multiple prompts, providing descriptions of the dataset(s), including algorithmic details, adding specific instructions to entice specific actions, and removing extraneous information. Finally, we discuss how our findings suggest potential changes to curriculum design of DS courses.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1230–1236},
numpages = {7},
keywords = {data science education, large language models, prompt engineering},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3649409.3691074,
author = {Zarb, Mark and Brown, John N.A. and Goodfellow, Martin and Liaskos, Konstantinos and Young, Tiffany},
title = {Ethical Implications of Gen-AI and LLMs in Computing Education},
year = {2024},
isbn = {9798400706042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649409.3691074},
doi = {10.1145/3649409.3691074},
abstract = {The panel convenes five educators to discuss the ethical implications of utilising Generative AI (Gen-AI) and Large Language Models (LLMs) in computing education. Their expertise spans various domains, including organising national workshops on the implications of generative AI tools, conducting surveys on their use within curricula, implementing institutional policies related to technology use, and engaging with students directly in the classroom. They reflect on the evolution of Gen-AI and LLMs from challenging-to-use technologies to indispensable tools for users of all levels. Furthermore, they examine the ethical dilemmas arising from the widespread adoption of these technologies in educational contexts, particularly regarding issues of originality, integrity, and responsible use. In addition, they explore practical strategies for integrating ethics education into computing curriculum design and classroom practices. This includes discussions on the role of educators in guiding students towards ethical technology usage, addressing uncertainties surrounding Gen-AI tools, and fostering a culture of responsible innovation within educational institutions. Through their collective insights and experiences, the panel aims to provide recommendations for navigating the ethical complexities inherent in the integration of Gen-AI technologies into computing education curricula.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 2},
pages = {293–294},
numpages = {2},
keywords = {ChatGPT, curriculum design, ethics, generative AI, large language models, responsibility},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@inproceedings{10.1145/3632620.3671097,
author = {Ali, Murtaza and Rao, Prerna and Mai, Yifan and Xie, Benjamin},
title = {Using Benchmarking Infrastructure to Evaluate LLM Performance on CS Concept Inventories: Challenges, Opportunities, and Critiques},
year = {2024},
isbn = {9798400704758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632620.3671097},
doi = {10.1145/3632620.3671097},
abstract = {BACKGROUND AND CONTEXT. The pace of advancement of large language models (LLMs) motivates the use of existing infrastructure to automate the evaluation of LLM performance on computing education tasks. Concept inventories are well suited for evaluation because of their careful design and prior validity evidence. OBJECTIVES. Our research explores the feasibility of using an automated benchmarking framework to evaluate computer science (CS) concept inventories. We explore three primary objectives: evaluation of LLM performance on the SCS1 and BDSI concept inventories; informal expert panel review of items which had variations between LLM and expected student performance; and description of challenges with using benchmarking infrastructure as a methodological innovation. METHOD. We used the Holistic Evaluation of Language Models (HELM) framework to evaluate the SCS1 and BDSI against 10 LLMS with zero-shot and few-shot in-context learning: GPT (3.5, 4.0), Claude (1.3, 2.0, 2.1), Llama (7B, 13B, 70B), Mistral v0.1 7B, and Mixtral 8x7B. We used psychometric data from prior studies to measure knowledge levels for each LLM run. We then conducted an informal expert review to qualitatively explore how question design, CS content knowledge, and LLM design may explain differences between LLM and expected student performances. FINDINGS. Our quantitative analysis found that most LLM response patterns reflected a below average introductory computing student with the SCS1 and did not fit the psychometric 2PL model for the BDSI. Our qualitative analysis identified that LLMs performed well on code infill questions, but poorly on nested conditionals, runtime analysis, and longer questions. We also identified several methodological challenges related to item security, translation, the structure when using HELM. IMPLICATIONS. We consider the feasibility of using automated benchmarking as a methodology to support more reproducible, replicable, and rigorous investigations to understand the intersection of LLM capabilities, computing concepts, and assessment design. We also consider connections between psychometric approaches and LLM evaluations to inform the design of computing assessments that are more resilient to LLM advancements.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1},
pages = {452–468},
numpages = {17},
keywords = {benchmarking, computing education, concept inventories, large language models, psychometrics},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@inproceedings{10.1145/3689535.3689554,
author = {Santos, Eddie Antonio and Becker, Brett A.},
title = {Not the Silver Bullet: LLM-enhanced Programming Error Messages are Ineffective in Practice},
year = {2024},
isbn = {9798400711770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689535.3689554},
doi = {10.1145/3689535.3689554},
abstract = {The sudden emergence of large language models (LLMs) such as ChatGPT has had a disruptive impact throughout the computing education community. LLMs have been shown to excel at producing correct code to CS1 and CS2 problems, and can even act as friendly assistants to students learning how to code. Recent work shows that LLMs demonstrate unequivocally superior results in being able to explain and resolve compiler error messages—for decades, one of the most frustrating parts of learning how to code. However, LLM-generated error message explanations have only been assessed by expert programmers in artificial conditions. This work sought to understand how novice programmers resolve programming error messages (PEMs) in a more realistic scenario. We ran a within-subjects study with n = 106 participants in which students were tasked to fix six buggy C programs. For each program, participants were randomly assigned to fix the problem using either a stock compiler error message, an expert-handwritten error message, or an error message explanation generated by GPT-4. Despite promising evidence on synthetic benchmarks, we found that GPT-4 generated error messages outperformed conventional compiler error messages in only 1 of the 6 tasks, measured by students’ time-to-fix each problem. Handwritten explanations still outperform LLM and conventional error messages, both on objective and subjective measures.},
booktitle = {Proceedings of the 2024 Conference on United Kingdom &amp; Ireland Computing Education Research},
articleno = {5},
numpages = {7},
keywords = {AI, C, CS1, GPT-4, GenAI, Generative AI, LLMs, PEM, compiler error messages, computing education, debugging, feedback, large language models, novice programmers, programming error messages},
location = {Manchester, United Kingdom},
series = {UKICER '24}
}

@inproceedings{10.1145/3716640.3716656,
author = {Vadaparty, Annapurna and Geng, Francis and Smith, David H and Benario, Jamie Gorson and Zingaro, Daniel and Porter, Leo},
title = {Achievement Goals in CS1-LLM},
year = {2025},
isbn = {9798400714252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716640.3716656},
doi = {10.1145/3716640.3716656},
abstract = {Introduction: The emergence and widespread adoption of generative AI (GenAI) chatbots such as ChatGPT, and programming assistants such as GitHub Copilot, have radically redefined the landscape of programming education. This calls for replication of studies and reexamination of findings from pre-GenAI CS contexts to understand the impact on students. Objectives: Achievement Goals are well studied in computing education and can be predictive of student interest and exam performance. The objective in this study is to compare findings from prior achievement goal studies in CS1 courses with new CS1 courses that emphasize the use of human-GenAI collaborative coding. Methods: In a CS1 course that integrates GenAI, we use linear regression to explore the relationship between achievement goals and prior experience on student interest, exam performance, and perceptions of GenAI. Results: As with prior findings in traditional CS1 classes, Mastery goals are correlated with interest in computing. Contradicting prior CS1 findings, normative goals are correlated with exam scores. Normative and mastery goals correlate with students’ perceptions of learning with GenAI. Mastery goals weakly correlate with reading and testing code output from GenAI.},
booktitle = {Proceedings of the 27th Australasian Computing Education Conference},
pages = {144–153},
numpages = {10},
keywords = {CS1, CS1-LLM, Copilot, Achievement Goals, Large Language Models},
location = {
},
series = {ACE '25}
}

@inproceedings{10.1145/3641554.3701946,
author = {Li, Nero and Broner, Shahar and Kim, Yubin and Mizuo, Katrina and Sauder, Elijah and To, Claire and Wang, Albert and Gila, Ofek and Shindler, Michael},
title = {Investigating the Capabilities of Generative AI in Solving Data Structures, Algorithms, and Computability Problems},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701946},
doi = {10.1145/3641554.3701946},
abstract = {There is both great hope and concern about the future of Computer Science practice and education concerning the recent advent of large language models (LLMs).We present the first study to extensively evaluate the ability of such a model to solve problems in Computer Science Theory. Specifically, we tested 165 exam-level problems across 16 specific topics related to computer science theory, ranging from preliminary data structures to algorithm design paradigms to theory of computation (automata and complexity). Our results use the recent popular models (GPT-4 and GPT-4o). This is a rapidly evolving field, with model performance continuously improving. We present our results primarily as an indication of what they can already achieve-equivalently how they can already be useful-today, fully expecting them to improve even further in the near future. Our results show that what was very recently a state-of-the-art model (GPT-4) can solve 77% of free-response problems in data structures and algorithms with little to no guidance. The latest model, GPT-4o, can solve around 46% of the Theory of Computation problems we posed, with predictable categories for which problems it could not solve. When broken down by topic, the model can solve 80% of problems in 4 out of the 15 topics and at least half in 8 other topics. Other problems, namely more visual problems, either require more substantial coaching or seem to still be beyond the capabilities of the language model--for now. By understanding the strengths and limitations of these models for solving theory problems, we can open the door to future work, ranging from human educational assessment on the topic to automated tutors for learners of the subject.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {659–665},
numpages = {7},
keywords = {algorithm design techniques, chatgpt, computational thinking, computer-assisted instruction, data structures, generative ai, gpt-4, gpt-4o, large language models},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3636243.3636252,
author = {Jury, Breanna and Lorusso, Angela and Leinonen, Juho and Denny, Paul and Luxton-Reilly, Andrew},
title = {Evaluating LLM-generated Worked Examples in an Introductory Programming Course},
year = {2024},
isbn = {9798400716195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636243.3636252},
doi = {10.1145/3636243.3636252},
abstract = {Worked examples, which illustrate the process for solving a problem step-by-step, are a well-established pedagogical technique that has been widely studied in computing classrooms. However, creating high-quality worked examples is very time-intensive for educators, and thus learners tend not to have access to a broad range of such examples. The recent emergence of powerful large language models (LLMs), which appear capable of generating high-quality human-like content, may offer a solution. Separate strands of recent work have shown that LLMs can accurately generate code suitable for a novice audience, and that they can generate high-quality explanations of code. Therefore, LLMs may be well suited to creating a broad range of worked examples, overcoming the bottleneck of manual effort that is currently required. In this work, we present a novel tool, ‘WorkedGen’, which uses an LLM to generate interactive worked examples. We evaluate this tool with both an expert assessment of the content, and a user study involving students in a first-year Python programming course (n = ~400). We find that prompt chaining and one-shot learning are useful strategies for optimising the output of an LLM when producing worked examples. Our expert analysis suggests that LLMs generate clear explanations, and our classroom deployment revealed that students find the LLM-generated worked examples useful for their learning. We propose several avenues for future work, including investigating WorkedGen’s value in a range of programming languages, and with more complex questions suitable for more advanced courses.},
booktitle = {Proceedings of the 26th Australasian Computing Education Conference},
pages = {77–86},
numpages = {10},
keywords = {CS1, GPT-3.5, LLM, chat-GPT, computing education, large language models, worked examples},
location = {Sydney, NSW, Australia},
series = {ACE '24}
}

@inproceedings{10.1145/3689535.3689553,
author = {Stone, Irene},
title = {Exploring Human-Centered Approaches in Generative AI and Introductory Programming Research: A Scoping Review},
year = {2024},
isbn = {9798400711770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689535.3689553},
doi = {10.1145/3689535.3689553},
abstract = {Recent advancements in generative artificial intelligence are poised to reshape introductory programming education, challenging conventional teaching methodologies. This paper presents a scoping review that explores the current understanding of integrating generative artificial intelligence tools in the learning of introductory programming. Through an analysis of 28 selected studies, this review provides a snapshot of the landscape in mid-2024, presenting benefits, concerns, and recommendations surrounding the use of generative artificial intelligence within programming education. It finds insufficient guidance on how to implement recommended pedagogical strategies, limited consideration of student perceptions and experiences, and a predominance of short study time frames. Additionally, there is a significant research gap in second-level education, particularly in the United Kingdom and Ireland. The paper discusses how these gaps signal a need for more human-centered approaches in the current research. The paper concludes with recommendations for future research, aiming to inspire further inquiry and advance the understanding of generative artificial intelligence’s role in programming education from a human-centered perspective.},
booktitle = {Proceedings of the 2024 Conference on United Kingdom &amp; Ireland Computing Education Research},
articleno = {4},
numpages = {7},
keywords = {AI, CS1, ChatGPT, LLMs, artificial intelligence, code generation, generative AI, human-centered, learner perspectives, novice programming, pedagogical practices, programming, python, student-centered},
location = {Manchester, United Kingdom},
series = {UKICER '24}
}

@inproceedings{10.1145/3613904.3642785,
author = {Park, Hyanghee and Ahn, Daehwan},
title = {The Promise and Peril of ChatGPT in Higher Education: Opportunities, Challenges, and Design Implications},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642785},
doi = {10.1145/3613904.3642785},
abstract = {A growing number of students in higher education are using ChatGPT for various educational purposes, ranging from seeking information to writing essays. Although many universities have officially banned the use of ChatGPT because of its potential harm and unintended consequences, it is still important to uncover how students leverage ChatGPT for learning, what challenges emerge, and how we can make better use of ChatGPT in higher education. Thus, we conducted focus group workshops and a series of participatory design sessions with thirty students who have actively interacted with ChatGPT for one semester in university and with other five stakeholders (e.g., professors, AI experts). Based on these, this paper identifies real opportunities and challenges of utilizing and designing ChatGPT for higher education.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {271},
numpages = {21},
keywords = {AI in Education, ChatGPT, Higher education, Large Language Models},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3716640.3716651,
author = {Qiao, Shuying and Denny, Paul and Giacaman, Nasser},
title = {Oversight in Action: Experiences with Instructor-Moderated LLM Responses in an Online Discussion Forum},
year = {2025},
isbn = {9798400714252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716640.3716651},
doi = {10.1145/3716640.3716651},
abstract = {The integration of large language models (LLMs) into computing education offers many potential benefits to student learning, and several novel pedagogical approaches have been reported in the literature. However LLMs also present challenges, one of the most commonly cited being that of student over-reliance. This challenge is compounded by the fact that LLMs are always available to provide instant help and solutions to students, which can undermine their ability to independently solve problems and diagnose and resolve errors. Providing instructor oversight of LLM-generated content can mitigate this problem, however it is often not practical in real-time learning contexts. Online class discussion forums, which are widely used in computing education, present an opportunity for exploring instructor oversight because they operate asynchronously. Unlike real-time interactions, the discussion forum format aligns with the expectation that responses may take time, making oversight not only feasible but also pedagogically appropriate. In this practitioner paper, we present the design, deployment, and evaluation of a ‘bot’ module that is controlled by the instructor, and integrated into an online discussion forum. The bot assists the instructor by generating draft responses to student questions, which are reviewed, modified, and approved before release. Key features include the ability to leverage course materials, access archived discussions, and publish responses anonymously to encourage open participation. We report our experiences using this tool in a 12-week second-year software engineering course on object-oriented programming. Instructor feedback confirmed the tool successfully alleviated workload but highlighted a need for improvement in handling complex, context-dependent queries. We report the features that were viewed as most beneficial, and suggest avenues for future exploration.},
booktitle = {Proceedings of the 27th Australasian Computing Education Conference},
pages = {95–104},
numpages = {10},
keywords = {Large language models, LLMs, discussion forums, instructor-in-the-loop, software engineering education, chatbots, computing education},
location = {
},
series = {ACE '25}
}

@inproceedings{10.1145/3631802.3631806,
author = {Kazemitabaar, Majeed and Hou, Xinying and Henley, Austin and Ericson, Barbara Jane and Weintrop, David and Grossman, Tovi},
title = {How Novices Use LLM-based Code Generators to Solve CS1 Coding Tasks in a Self-Paced Learning Environment},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631806},
doi = {10.1145/3631802.3631806},
abstract = {As Large Language Models (LLMs) gain in popularity, it is important to understand how novice programmers use them and the effect they have on learning to code. We present the results of a thematic analysis on a data set from 33 learners, aged 10-17, as they independently learned Python by working on 45 code-authoring tasks with access to an AI Code Generator based on OpenAI Codex. We explore several important questions related to how learners used LLM-based AI code generators, and provide an analysis of the properties of the written prompts and the resulting AI generated code. Specifically, we explore (A) the context in which learners use Codex, (B) what learners are asking from Codex in terms of syntax and logic, (C) properties of prompts written by learners in terms of relation to task description, language, clarity, and prompt crafting patterns, (D) properties of the AI-generated code in terms of correctness, complexity, and accuracy, and (E) how learners utilize AI-generated code in terms of placement, verification, and manual modifications. Furthermore, our analysis reveals four distinct coding approaches when writing code with an AI code generator: AI Single Prompt, where learners prompted Codex once to generate the entire solution to a task; AI Step-by-Step, where learners divided the problem into parts and used Codex to generate each part; Hybrid, where learners wrote some of the code themselves and used Codex to generate others; and Manual coding, where learners wrote the code themselves. Our findings reveal consistently positive trends between learners’ utilization of the Hybrid coding approach and their post-test evaluation scores, while showing consistent negative trends between the AI Single Prompt and the post-test evaluation scores. Furthermore, we offer insights into novice learners’ use of AI code generators in a self-paced learning environment, highlighting signs of over-reliance, self-regulation, and opportunities for enhancing AI-assisted learning tools.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {3},
numpages = {12},
keywords = {ChatGPT, Copilot, Introductory Programming, Large Language Models, OpenAI Codex, Self-paced Learning, Self-regulation},
location = {Koli, Finland},
series = {Koli Calling '23}
}

@inproceedings{10.1145/3641555.3705061,
author = {Liu, Rongxin and Xu, Benjamin and Perez, Christopher and Zhao, Julianna and Zhukovets, Yuliia and Malan, David J.},
title = {Assessment in CS50 with AI: Leveraging Generative Artificial Intelligence for Personalized Student Evaluation},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705061},
doi = {10.1145/3641555.3705061},
abstract = {The scalability challenges of code review and pair-programming assessments in large computer science courses, such as CS50 at Harvard University, have opened up opportunities for the application of Generative AI. Leveraging large language models (LLMs), CS50.ai offers a suite of AI-based tools that assist both students and instructors in mastering course material while overcoming the limitations posed by human resource constraints. This demo highlights how generative AI can be employed to conduct code reviews and pair-programming simulations, providing real-time feedback, code explanations, and collaborative programming insights. By integrating these AI tools into students' learning journeys, we aim to mimic the 1:1 interaction between instructor and student, improving both formative and summative assessments. We will showcase how these tools are implemented to scale personalized feedback, ensure academic integrity, and maintain pedagogical efficacy. Our presentation will also reflect on lessons learned from deploying these AI-driven tools in recent course offerings.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1735},
numpages = {1},
keywords = {AI, LLMs, artificial intelligence, generative AI, large language models},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3641555.3705237,
author = {Mittal, Meenakshi and Bailey, Azalea and Phelps, Victoria and Miroyan, Mihran and Mitra, Chancharik and Jain, Rishi and Niousha, Rose and Ranade, Gireeja and Norouzi, Narges},
title = {Raising the Bar: Automating Consistent and Equitable Student Support with LLMs},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705237},
doi = {10.1145/3641555.3705237},
abstract = {Large Language Models (LLMs) can be used to automate many aspects of the educational field. In this paper, we look into the benefits of automating responses to student questions in course discussion forums using our Retrieval-Augmented Generation (RAG)-based LLM pipeline (Edison). Our research questions are:RQ1 How do the responses generated by Edison compare to those of TAs in terms of level of detail and use of examples?RQ2 How does the tone of responses generated by Edison compare to that of TA responses?RQ3 Are responses generated by Edison more self-consistent than TA responses?Our results suggest that Edison generates responses with more detail, examples, positive tone, and self-consistency than TAs. We envision Edison being used as a baseline for TAs to build responses on, reduce response times, and promote equitable feedback.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1549–1550},
numpages = {2},
keywords = {cs1, discussion forum, large language models, student feedback},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3587103.3594206,
author = {Prather, James and Denny, Paul and Leinonen, Juho and Becker, Brett A. and Albluwi, Ibrahim and Caspersen, Michael E. and Craig, Michelle and Keuning, Hieke and Kiesler, Natalie and Kohn, Tobias and Luxton-Reilly, Andrew and MacNeil, Stephen and Petersen, Andrew and Pettit, Raymond and Reeves, Brent N. and Savelka, Jaromir},
title = {Transformed by Transformers: Navigating the AI Coding Revolution for Computing Education: An ITiCSE Working Group Conducted by Humans},
year = {2023},
isbn = {9798400701399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587103.3594206},
doi = {10.1145/3587103.3594206},
abstract = {The recent advent of highly accurate and scalable large language models (LLMs) has taken the world by storm. From art to essays to computer code, LLMs are producing novel content that until recently was thought only humans could produce. Recent work in computing education has sought to understand the capabilities of LLMs for solving tasks such as writing code, explaining code, creating novel coding assignments, interpreting programming error messages, and more. However, these technologies continue to evolve at an astonishing rate leaving educators little time to adapt. This working group seeks to document the state-of-the-art for code generation LLMs, detail current opportunities and challenges related to their use, and present actionable approaches to integrating them into computing curricula.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2},
pages = {561–562},
numpages = {2},
keywords = {AI, CS1, GPT, GitHub, LLM, artificial intelligence, code generation, codex, computer programming, copilot, large language models, novice programming, openAI, pedagogical practices},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3641554.3701940,
author = {Riazi, Sara and Rooshenas, Pedram},
title = {LLM-Driven Feedback for Enhancing Conceptual Design Learning in Database Systems Courses},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701940},
doi = {10.1145/3641554.3701940},
abstract = {The integration of LLM-generated feedback into educational settings has shown promise in enhancing student learning outcomes. This paper presents a novel LLM-driven system that provides targeted feedback for conceptual designs in a Database Systems course. The system converts student-created entity-relationship diagrams (ERDs) into JSON format, allows the student to prune the diagram by isolating a relationship, extracts relevant requirements for the selected relationship, and utilizes a large language model (LLM) to generate detailed feedback. Additionally, the system creates a tailored set of questions and answers to further aid student understanding. Our pilot implementation in a Database System course demonstrates effective feedback generation that helped the students improve their design skills.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1001–1007},
numpages = {7},
keywords = {conceptual design, database systems, educational technology, large language models, llm-generated feedback},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3641554.3701934,
author = {Kerslake, Chris and Denny, Paul and Smith, David H. and Leinonen, Juho and MacNeil, Stephen and Luxton-Reilly, Andrew and Becker, Brett A.},
title = {Exploring Student Reactions to LLM-Generated Feedback on Explain in Plain English Problems},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701934},
doi = {10.1145/3641554.3701934},
abstract = {Code reading and comprehension skills are essential for novices learning programming, and explain-in-plain-English tasks (EiPE) are a well-established approach for assessing these skills. However, manual grading of EiPE tasks is time-consuming and this has limited their use in practice. To address this, we explore an approach where students explain code samples to a large language model (LLM) which generates code based on their explanations. This generated code is then evaluated using test suites, and shown to students along with the test results. We are interested in understanding how automated formative feedback from an LLM guides students' subsequent prompts towards solving EiPE tasks. We analyzed 177 unique attempts on four EiPE exercises from 21 students, looking at what kinds of mistakes they made and how they fixed them. We found that when students made mistakes, they identified and corrected them using either a combination of the LLM-generated code and test case results, or they switched from describing the purpose of the code to describing the sample code line-by-line until the LLM-generated code exactly matched the obfuscated sample code. Our findings suggest both optimism and caution with the use of LLMs for unmonitored formative feedback. We identified false positive and negative cases, helpful variable naming, and clues of direct code recitation by students. For most students, this approach represents an efficient way to demonstrate and assess their code comprehension skills. However, we also found evidence of misconceptions being reinforced, suggesting the need for further work to identify and guide students more effectively.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {575–581},
numpages = {7},
keywords = {eipe, explain in plain english, formative feedback, large language models, llm, misconceptions, qualitative analysis},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3627217.3627235,
author = {Venkatesh, Varshini and Venkatesh, Vaishnavi and Kumar, Viraj},
title = {Evaluating Copilot on CS1 Code Writing Problems with Suppressed Specifications},
year = {2023},
isbn = {9798400708404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627217.3627235},
doi = {10.1145/3627217.3627235},
abstract = {Code writing problems in introductory programming (CS1) courses typically ask students to write simple functions or programs based on detailed natural-language specifications. These details can be leveraged by large language models (LLMs), accessible to students via tools such as GitHub Copilot, to generate solutions that are often correct. CS1 instructors who are unwilling or unable to prohibit such usage must consider variants of traditional code writing problems that align with their learning objectives but are more difficult for LLMs to solve. Since LLMs are sensitive to the level of details in their prompts, it is natural to consider variants where details are progressively trimmed from the specifications of traditional code writing problems, and consequent ambiguities are clarified via examples. We consider an extreme variant, where all natural language is suppressed except for meaningful names of functions and their arguments. We evaluate the performance of Copilot on suppressed specification versions of 153 such problems drawn from the CodeCheck repository. If Copilot initially fails to generate a correct solution, we augment each suppressed specification with as few clarifying examples as possible to obtain a correct solution. Copilot solves 134 problems (87%) with just 0.7 examples on average, requiring no examples in 78 instances. Thus, modifying traditional code-writing problems by merely trimming specification details is unlikely to thwart sophisticated LLMs such as GitHub Copilot.},
booktitle = {Proceedings of the 16th Annual ACM India Compute Conference},
pages = {104–107},
numpages = {4},
keywords = {CS1, code writing, large language models},
location = {Hyderabad, India},
series = {COMPUTE '23}
}

@inproceedings{10.1145/3706468.3706479,
author = {R\"{u}dian, Sylvio and Podelo, Julia and Ku\v{z}\'{\i}lek, Jakub and Pinkwart, Niels},
title = {Feedback on Feedback: Student’s Perceptions for Feedback from Teachers and Few-Shot LLMs},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706479},
doi = {10.1145/3706468.3706479},
abstract = {Large language models (LLMs) can be a valuable resource for generating texts and performing various instruction-based tasks. In this paper, we explored the use of LLMs, particularly for generating feedback for students in higher education. More precisely, we conducted an experiment to examine students’ perceptions regarding LLM-generated feedback. This has the overall aim of assisting teachers in the feedback creation process. First, we examine the different student perceptions regarding the feedback that students got without being aware of whether it was created by their teacher or an LLM. Our results reveal that the feedback source has not impacted how it was perceived by the students, except in cases where repetitive content has been generated, which is a known limitation of LLMs. Second, students have been asked to identify whether the feedback comes from an LLM or the teacher. The results demonstrate, that students were unable to identify the feedback source. A small subset of indicators has been identified, that clearly revealed from whom the feedback comes from. Third, student perceptions are analyzed while knowing that feedback has been auto-generated. This examination indicates that generated feedback is likely to be met with resistance. It contradicts the findings of the first examination. This emphasizes the need of a teacher-in-the-loop approach when employing auto-generated feedback in higher education.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {82–92},
numpages = {11},
keywords = {Large Language Models, Prompt Engineering, Feedback Indicators, Language Learning},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3641554.3701785,
author = {Ramirez Osorio, Valeria and Zavaleta Bernuy, Angela and Simion, Bogdan and Liut, Michael},
title = {Understanding the Impact of Using Generative AI Tools in a Database Course},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701785},
doi = {10.1145/3641554.3701785},
abstract = {Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) have led to changes in educational practices by creating opportunities for personalized learning and immediate support. Computer science student perceptions and behaviors towards GenAI tools have been studied, but the effects of such tools on student learning have yet to be determined conclusively. We investigate the impact of GenAI tools on computing students' performance in a database course and aim to understand why students use GenAI tools in assignments. Our mixed-methods study (N=226) asked students to self-report whether they used a GenAI tool to complete a part of an assignment and why. Our results reveal that students utilizing GenAI tools performed better on the assignment part in which LLMs were permitted but did worse in other parts of the assignment and in the course overall. Also, those who did not use GenAI tools viewed more discussion board posts and participated more than those who used ChatGPT. This suggests that using GenAI tools may not lead to better skill development or mental models, at least not if the use of such tools is unsupervised, and that engagement with official course help supports may be affected. Further, our thematic analysis of reasons for using or not using GenAI tools, helps understand why students are drawn to these tools. Shedding light into such aspects empowers instructors to be proactive in how to encourage, supervise, and handle the use or integration of GenAI into courses, fostering good learning habits.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {959–965},
numpages = {7},
keywords = {computing education, databases, generative artificial intelligence, large language models, student behavior, student performance},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3699538.3699541,
author = {Korpimies, Kai and Laaksonen, Antti and Luukkainen, Matti},
title = {Unrestricted Use of LLMs in a Software Project Course: Student Perceptions on Learning and Impact on Course Performance},
year = {2024},
isbn = {9798400710384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3699538.3699541},
doi = {10.1145/3699538.3699541},
abstract = {Large language models (LLMs) provide round-the-clock personalized programming assistance, unlike course instructors or traditional online information sources such as Stack Overflow. While LLMs can aid in code generation, concerns about over-reliance and the impact on learning persist. This study discusses students’ experiences with LLMs in a software project course where students were allowed to use LLMs freely except for unit test generation. We conducted surveys during course instances in autumn 2023 and spring 2024. The surveys assessed the extent of LLM usage, methods of application, and perceived impact on learning. Results indicate diverse usage patterns, with many students finding LLMs beneficial for efficiency and problem-solving, though over-reliance and poor-quality outputs were noted concerns. The usage patterns can be linked to course performance and time spent on the project.},
booktitle = {Proceedings of the 24th Koli Calling International Conference on Computing Education Research},
articleno = {23},
numpages = {7},
keywords = {Large language models, Computer Science Education, User Study, Code generation, Software project},
location = {
},
series = {Koli Calling '24}
}

@inproceedings{10.1145/3641555.3705212,
author = {Baek, Jeonghun and Yamazaki, Tetsuro and Morihata, Akimasa and Mori, Junichiro and Yamakata, Yoko and Taura, Kenjiro and Chiba, Shigeru},
title = {Leveraging LLM for Detecting and Explaining LLM-generated Code in Python Programming Courses},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705212},
doi = {10.1145/3641555.3705212},
abstract = {As large language models (LLMs) have become more advanced, generating code to solve exercises in programming courses has become significantly easier. However, this convenience raises the concern of over-reliance on these tools, potentially hindering students from developing independent coding skills. To address this concern, we introduce an LLM-based detector that not only detects LLM-generated code but also explains the reasons for its judgments. These reasons provide insight into the characteristics of LLM-generated code, enhancing transparency in the detection process. We evaluate the detector in an introductory Python programming course, achieving over 99% accuracy. Additionally, instructors manually reviewed the reasons provided by the detector and verified that 64.7% of reasons for classifying code as LLM-generated were appropriate. These reasons can also serve as feedback, helping students improve their coding skills by understanding the characteristics of expert-level LLM-generated code.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1369–1370},
numpages = {2},
keywords = {detecting and explaining llm-generated code, large language model, llm-based detector, llm-generated code, python programming courses, reasons for judgment},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3626252.3630863,
author = {Del Carpio Gutierrez, Andre and Denny, Paul and Luxton-Reilly, Andrew},
title = {Evaluating Automatically Generated Contextualised Programming Exercises},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630863},
doi = {10.1145/3626252.3630863},
abstract = {Introductory programming courses often require students to solve many small programming exercises as part of their learning. Researchers have previously suggested that the context used in the problem description for these exercises is likely to impact student engagement and motivation. Furthermore, supplying programming exercises that use a broad range of contexts or even allowing students to select contexts to personalize their own exercises, may support the interests of a diverse student population. Unfortunately, it is time-consuming for instructors to create large numbers of programming exercises that provide a wide range of contextualized problems. However, recent work has shown that large language models may be able to automate the mass production of programming exercises, reducing the burden on instructors. In this research, we explore the potential of OpenAI's GPT-4 to create high-quality and novel programming exercises that implement various contexts. Finally, through prompt engineering, we compare different prompting strategies used to generate many programming exercises with various contextualized problem descriptions and then evaluate the quality of the exercises generated.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {289–295},
numpages = {7},
keywords = {chatgpt, cs1, gpt-4, large language models, novice programmers, openai, programming exercises, prompt engineering},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3716640.3716647,
author = {Leinonen, Juho and Denny, Paul and Kiljunen, Olli and MacNeil, Stephen and Sarsa, Sami and Hellas, Arto},
title = {LLM-itation is the Sincerest Form of Data: Generating Synthetic Buggy Code Submissions for Computing Education},
year = {2025},
isbn = {9798400714252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716640.3716647},
doi = {10.1145/3716640.3716647},
abstract = {There is a great need for data in computing education research. Data is needed to understand how students behave, to train models of student behavior to optimally support students, and to develop and validate new assessment tools and learning analytics techniques. However, relatively few computing education datasets are shared openly, often due to privacy regulations and issues in making sure the data is anonymous. Large language models (LLMs) offer a promising approach to create large-scale, privacy-preserving synthetic data, which can be used to explore various aspects of student learning, develop and test educational technologies, and support research in areas where collecting real student data may be challenging or impractical. This work explores generating synthetic buggy code submissions for introductory programming exercises using GPT-4o. We compare the distribution of test case failures between synthetic and real student data from two courses to analyze the accuracy of the synthetic data in mimicking real student data. Our findings suggest that LLMs can be used to generate synthetic incorrect submissions that are not significantly different from real student data with regard to test case failure distributions. Our research contributes to the development of reliable synthetic datasets for computing education research and teaching, potentially accelerating progress in the field while preserving student privacy.},
booktitle = {Proceedings of the 27th Australasian Computing Education Conference},
pages = {56–63},
numpages = {8},
keywords = {generative AI, genAI, large language models, LLMs, GPT-4o, prompt engineering, synthetic data, bugs, submissions, data generation},
location = {
},
series = {ACE '25}
}

@inproceedings{10.1145/3641555.3705277,
author = {Tsang, Jedidiah and Li, Carol and Park, Su Min and Yan, Lisa},
title = {Using LLMs to Detect the Presence of Learning Outcomes in Submitted Work Within Computing Ethics Courses},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705277},
doi = {10.1145/3641555.3705277},
abstract = {This study investigates how large language models (LLMs) can identify the presence of learning outcomes within student submitted work in a computing ethics course. To do so, we craft a codebook to spot key learning outcomes, such as the usage of critical reasoning and awareness of various social issues. We leverage the GPT-4o and GPT-3.5-turbo LLMs to apply codes onto 8,500 pieces of student submitted work. We then use Cohen's kappa to assess interrater reliability and compare human reviewers' coding to outputs from those models, finding that GPT-4o performed just as well as the agreement between human reviewers. We then use the model outputs to identify specific course readings that students engaged particularly deeply with to better inform our computing ethics instruction.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1641–1642},
numpages = {2},
keywords = {codebook, computing ethics, critical consciousness, large language models, positionality},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3632620.3671116,
author = {Prather, James and Reeves, Brent N and Leinonen, Juho and MacNeil, Stephen and Randrianasolo, Arisoa S and Becker, Brett A. and Kimmel, Bailey and Wright, Jared and Briggs, Ben},
title = {The Widening Gap: The Benefits and Harms of Generative AI for Novice Programmers},
year = {2024},
isbn = {9798400704758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632620.3671116},
doi = {10.1145/3632620.3671116},
abstract = {Novice programmers often struggle through programming problem solving due to a lack of metacognitive awareness and strategies. Previous research has shown that novices can encounter multiple metacognitive difficulties while programming, such as forming incorrect conceptual models of the problem or having a false sense of progress after testing their solution. Novices are typically unaware of how these difficulties are hindering their progress. Meanwhile, many novices are now programming with generative AI (GenAI), which can provide complete solutions to most introductory programming problems, code suggestions, hints for next steps when stuck, and explain cryptic error messages. Its impact on novice metacognition has only started to be explored. Here we replicate a previous study that examined novice programming problem solving behavior and extend it by incorporating GenAI tools. Through 21 lab sessions consisting of participant observation, interview, and eye tracking, we explore how novices are coding with GenAI tools. Although 20 of 21 students completed the assigned programming problem, our findings show an unfortunate divide in the use of GenAI tools between students who did and did not struggle. Some students who did not struggle were able to use GenAI to accelerate, creating code they already intended to make, and were able to ignore unhelpful or incorrect inline code suggestions. But for students who struggled, our findings indicate that previously known metacognitive difficulties persist, and that GenAI unfortunately can compound them and even introduce new metacognitive difficulties. Furthermore, struggling students often expressed cognitive dissonance about their problem solving ability, thought they performed better than they did, and finished with an illusion of competence. Based on our observations from both groups, we propose ways to scaffold the novice GenAI experience and make suggestions for future work.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1},
pages = {469–486},
numpages = {18},
keywords = {CS1, ChatGPT, Copilot, generative AI, large language models, metacognition},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@inproceedings{10.1145/3649405.3659504,
author = {Bernstein, Seth and Denny, Paul and Leinonen, Juho and Littlefield, Matt and Hellas, Arto and MacNeil, Stephen},
title = {Analyzing Students' Preferences for LLM-Generated Analogies},
year = {2024},
isbn = {9798400706035},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649405.3659504},
doi = {10.1145/3649405.3659504},
abstract = {Introducing students to new concepts in computer science can often be challenging, as these concepts may differ significantly from their existing knowledge and conceptual understanding. To address this, we employed analogies to help students connect new concepts to familiar ideas. Specifically, we generated analogies using large language models (LLMs), namely ChatGPT, and used them to help students make the necessary connections. In this poster, we present the results of our survey, in which students were provided with two analogies relating to different computing concepts, and were asked to describe the extent to which they were accurate, interesting, and useful. This data was used to determine how effective LLM-generated analogies can be for teaching computer science concepts, as well as how responsive students are to this approach.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 2},
pages = {812},
numpages = {1},
keywords = {analogies, computer science education, large language models},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3649165.3690111,
author = {Poitras, Eric and Crane, Brent and Siegel, Angela},
title = {Generative AI in Introductory Programming Instruction: Examining the Assistance Dilemma with LLM-Based Code Generators},
year = {2024},
isbn = {9798400705984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649165.3690111},
doi = {10.1145/3649165.3690111},
abstract = {Problem decomposition is an important skill in programming, allowing learners to break down complex tasks into manageable subgoals. However, translating these subgoals into executable code poses a significant challenge for novice programmers. In this study conducted in an introductory programming course, learners received instruction in stepwise refinement and integration of AI-generated code within their assignments. Throughout the course, learners were permitted to rely on AI code generators, following opportunities to receive feedback on their ability to read and write code without AI assistance.  Our findings show that learners frequently relied on AI-generated code when working on assignments outside the classroom, but that the frequency of reliance varied from one assignment to another. The reliance on AI-generated code was not correlated with the learners' year in their degree, nor whether they were enrolled in a CS degree or not. Instead, it was associated with their prior knowledge, as learners who were less proficient in reading and writing code were more likely to seek AI assistance.  AI tools were primarily used to translate subgoals into code, fix errors, and explain algorithmic concepts. Few learners encountered difficulties in understanding or integrating AI generated code into their solutions. Overall, learner performance in meeting assignment requirements was relatively high, regardless of their prior knowledge or reliance on AI code generators. We conclude that leveraging the capabilities of generative AI can effectively bridge the gap between problem-solving and implementation, enabling learners to engage in skills that might otherwise be beyond their reach.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1},
pages = {186–192},
numpages = {7},
keywords = {ai coding assistants, ai-assisted pair programming, chatgpt, generative ai, gpt-3.5, introductory programming, large language models},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@inproceedings{10.1145/3587102.3588805,
author = {Reeves, Brent and Sarsa, Sami and Prather, James and Denny, Paul and Becker, Brett A. and Hellas, Arto and Kimmel, Bailey and Powell, Garrett and Leinonen, Juho},
title = {Evaluating the Performance of Code Generation Models for Solving Parsons Problems With Small Prompt Variations},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588805},
doi = {10.1145/3587102.3588805},
abstract = {The recent emergence of code generation tools powered by large language models has attracted wide attention. Models such as OpenAI Codex can take natural language problem descriptions as input and generate highly accurate source code solutions, with potentially significant implications for computing education. Given the many complexities that students face when learning to write code, they may quickly become reliant on such tools without properly understanding the underlying concepts. One popular approach for scaffolding the code writing process is to use Parsons problems, which present solution lines of code in a scrambled order. These remove the complexities of low-level syntax, and allow students to focus on algorithmic and design-level problem solving. It is unclear how well code generation models can be applied to solve Parsons problems, given the mechanics of these models and prior evidence that they underperform when problems include specific restrictions. In this paper, we explore the performance of the Codex model for solving Parsons problems over various prompt variations. Using a corpus of Parsons problems we sourced from the computing education literature, we find that Codex successfully reorders the problem blocks about half of the time, a much lower rate of success when compared to prior work on more free-form programming tasks. Regarding prompts, we find that small variations in prompting have a noticeable effect on model performance, although the effect is not as pronounced as between different problems.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {299–305},
numpages = {7},
keywords = {CS1, GPT-3, GitHub, ML, academic integrity, ai, artificial intelligence, chatgpt, code generation, code writing, codex, computer programming, copilot, deep learning, generative ai, introductory programming, large language models, machine learning, natural language processing, neural networks, novice programming, openAI},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3641555.3705189,
author = {Wang, Jiayi and Xiao, Ruiwei and Tseng, Ying-Jui},
title = {Generating AI Literacy MCQs: A Multi-Agent LLM Approach},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705189},
doi = {10.1145/3641555.3705189},
abstract = {Artificial intelligence (AI) is transforming society, making it crucial to prepare the next generation through AI literacy in K-12 education. However, scalable and reliable AI literacy materials and assessment resources are lacking. To address this gap, our study presents a novel approach to generating multiple-choice questions (MCQs) for AI literacy assessments. Our method utilizes large language models (LLMs) to automatically generate scalable, high-quality assessment questions. These questions align with user-provided learning objectives, grade levels, and Bloom's Taxonomy levels. We introduce an iterative workflow incorporating LLM-powered critique agents to ensure the generated questions meet pedagogical standards. In the preliminary evaluation, experts expressed strong interest in using the LLM-generated MCQs, indicating that this system could enrich existing AI literacy materials and provide a valuable addition to the toolkit of K-12 educators.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1651–1652},
numpages = {2},
keywords = {AI literacy, LLM-based agent, assessment, large language model, multi-agent workflow, question generation},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3657604.3662032,
author = {Hou, Xinying and Wu, Zihan and Wang, Xu and Ericson, Barbara J.},
title = {CodeTailor: LLM-Powered Personalized Parsons Puzzles for Engaging Support While Learning Programming},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3662032},
doi = {10.1145/3657604.3662032},
abstract = {Learning to program can be challenging, and providing high-quality and timely support at scale is hard. Generative AI and its products, like ChatGPT, can create a solution for most intro-level programming problems. However, students might use these tools to just generate code for them, resulting in reduced engagement and limited learning. In this paper, we present CodeTailor, a system that leverages a large language model (LLM) to provide personalized help to students while still encouraging cognitive engagement. CodeTailor provides a personalized Parsons puzzle to support struggling students. In a Parsons puzzle, students place mixed-up code blocks in the correct order to solve a problem. A technical evaluation with previous incorrect student code snippets demonstrated that CodeTailor could deliver high-quality (correct, personalized, and concise) Parsons puzzles based on their incorrect code. We conducted a within-subjects study with 18 novice programmers. Participants perceived CodeTailor as more engaging than just receiving an LLM-generated solution (the baseline condition). In addition, participants applied more supported elements from the scaffolded practice to the posttest when using CodeTailor than baseline. Overall, most participants preferred using CodeTailor versus just receiving the LLM-generated code for learning. Qualitative observations and interviews also provided evidence for the benefits of CodeTailor, including thinking more about solution construction, fostering continuity in learning, promoting reflection, and boosting confidence. We suggest future design ideas to facilitate active learning opportunities with generative AI techniques.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {51–62},
numpages = {12},
keywords = {active learning, generative ai, gpt, introductory programming, large language models, parsons problems, personalization},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@inproceedings{10.1145/3641554.3701945,
author = {Liu, Rongxin and Zhao, Julianna and Xu, Benjamin and Perez, Christopher and Zhukovets, Yuliia and Malan, David J.},
title = {Improving AI in CS50: Leveraging Human Feedback for Better Learning},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701945},
doi = {10.1145/3641554.3701945},
abstract = {In 2023, we developed and deployed AI-based tools in CS50 at Harvard University to provide students with 24/7 interactive assistance, approximating a 1:1 teacher-to-student ratio. These tools offer code explanations, style suggestions, and responses to course-related inquiries, emulating human educators to foster critical thinking. However, maintaining alignment with instructional goals is challenging, especially with frequent updates to the underlying large language models (LLMs). We thus propose a continuous improvement process for LLM-based systems using a collaborative human-in-the-loop approach. We introduce a systematic evaluation framework for assessing and refining the performance of AI-based tutors, combining human-graded and model-graded evaluations. Using few-shot prompting and fine-tuning, we aim to ensure our AI tools adopt pedagogically sound teaching styles. Fine-tuning with a small, high-quality dataset has shown significant improvements in aligning with teaching goals, as confirmed through multi-turn conversation evaluations. Additionally, our framework includes a model-evaluation backend that teaching assistants periodically review, ensuring the AI system remains effective and aligned with instructional objectives. This paper offers insights into our methods and the impact of these AI tools on CS50 and contributes to the discourse on AI in education, showcasing scalable, personalized learning enhancements.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {715–721},
numpages = {7},
keywords = {ai, artificial intelligence, generative ai, large language models, llms},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3632620.3671103,
author = {Logacheva, Evanfiya and Hellas, Arto and Prather, James and Sarsa, Sami and Leinonen, Juho},
title = {Evaluating Contextually Personalized Programming Exercises Created with Generative AI},
year = {2024},
isbn = {9798400704758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632620.3671103},
doi = {10.1145/3632620.3671103},
abstract = {Programming skills are typically developed through completing various hands-on exercises. Such programming problems can be contextualized to students’ interests and cultural backgrounds. Prior research in educational psychology has demonstrated that context personalization of exercises stimulates learners’ situational interests and positively affects their engagement. However, creating a varied and comprehensive set of programming exercises for students to practice on is a time-consuming and laborious task for computer science educators. Previous studies have shown that large language models can generate conceptually and contextually relevant programming exercises. Thus, they offer a possibility to automatically produce personalized programming problems to fit students’ interests and needs. This article reports on a user study conducted in an elective introductory programming course that included contextually personalized programming exercises created with GPT-4. The quality of the exercises was evaluated by both the students and the authors. Additionally, this work investigated student attitudes towards the created exercises and their engagement with the system. The results demonstrate that the quality of exercises generated with GPT-4 was generally high. What is more, the course participants found them engaging and useful. This suggests that AI-generated programming problems can be a worthwhile addition to introductory programming courses, as they provide students with a practically unlimited pool of practice material tailored to their personal interests and educational needs.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1},
pages = {95–113},
numpages = {19},
keywords = {automatic exercise generation, context personalization, generative AI, large language models},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@inproceedings{10.1145/3641555.3705121,
author = {Harrington, Brian and Alnoor, Ahmad Zubair and Haqiqi, Pedram and Hoseininia, Zahra and Lin, Kai and Lodi, Maliha and Mirza, Asad and Wolfe, Leah and Zhang, Kevin},
title = {A Systematic Literature Mapping of Early Generative AI Research is CS Education},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705121},
doi = {10.1145/3641555.3705121},
abstract = {The widespread release of generative AI tools has led to a rapid rise in publications evaluating their impact on CS education. While there is no doubt that the area is new and rapidly evolving, it is important to begin to catalogue and map the literature at this early stage. In this work, we systematically search and map 82 papers evaluating the impact of generative AI tools on CS education. We then build a literature map of these papers using the axes of population, use of generative AI, and method of evaluation. This work will serve as both a snapshot of the first generation of generative AI papers in the field, and a road-map for further classification and literature review as the field develops.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1467–1468},
numpages = {2},
keywords = {gen ai, generative ai, large language models, literature map, llm},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3649217.3653584,
author = {Vadaparty, Annapurna and Zingaro, Daniel and Smith IV, David H. and Padala, Mounika and Alvarado, Christine and Gorson Benario, Jamie and Porter, Leo},
title = {CS1-LLM: Integrating LLMs into CS1 Instruction},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653584},
doi = {10.1145/3649217.3653584},
abstract = {The recent, widespread availability of Large Language Models (LLMs) like ChatGPT and GitHub Copilot may impact introductory programming courses (CS1) both in terms of what should be taught and how to teach it. Indeed, recent research has shown that LLMs are capable of solving the majority of the assignments and exams we previously used in CS1. In addition, professional software engineers are often using these tools, raising the question of whether we should be training our students in their use as well. This experience report describes a CS1 course at a large research-intensive university that fully embraces the use of LLMs from the beginning of the course. To incorporate the LLMs, the course was intentionally altered to reduce emphasis on syntax and writing code from scratch. Instead, the course now emphasizes skills needed to successfully produce software with an LLM. This includes explaining code, testing code, and decomposing large problems into small functions that are solvable by an LLM. In addition to frequent, formative assessments of these skills, students were given three large, open-ended projects in three separate domains (data science, image processing, and game design) that allowed them to showcase their creativity in topics of their choosing. In an end-of-term survey, students reported that they appreciated learning with the assistance of the LLM and that they interacted with the LLM in a variety of ways when writing code. We provide lessons learned for instructors who may wish to incorporate LLMs into their course.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {297–303},
numpages = {7},
keywords = {copilot, cs1, generative ai, introductory programming, llm},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3657604.3662040,
author = {Gabbay, Hagit and Cohen, Anat},
title = {Combining LLM-Generated and Test-Based Feedback in a MOOC for Programming},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3662040},
doi = {10.1145/3657604.3662040},
abstract = {In large-scale programming courses, providing learners with immediate and effective feedback is a significant challenge. This study explores the potential of Large Language Models (LLMs) to generate feedback on code assignments and to address the gaps in Automated Test-based Feedback (ATF) tools commonly employed in programming courses. We applied dedicated metrics in a Massive Open Online Course (MOOC) on programming to assess the correctness of feedback generated by two models, GPT-3.5-turbo and GPT-4, using a reliable ATF as a benchmark. The findings point to effective error detection, yet the feedback is often inaccurate, with GPT-4 outperforming GPT-3.5-turbo. We used insights gained from the prompt practices to develop Gipy, an application for submitting course assignments and obtaining LLM-generated feedback. Learners participating in a field experiment perceived the feedback provided by Gipy as moderately valuable, while at the same time recognizing its potential to complement ATF. Given the learners' critique and their awareness of the limitations of LLM-generated feedback, the studied implementation may be able to take advantage of the best of both ATF and LLMs as feedback resources. Further research is needed to assess the impact of LLM-generated feedback on learning outcomes and explore the capabilities of more advanced models.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {177–187},
numpages = {11},
keywords = {MOOC for programming, automated feedback, generative AI, large language models (LLMs), programming education},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@inproceedings{10.1145/3626252.3630909,
author = {Denny, Paul and Leinonen, Juho and Prather, James and Luxton-Reilly, Andrew and Amarouche, Thezyrie and Becker, Brett A. and Reeves, Brent N.},
title = {Prompt Problems: A New Programming Exercise for the Generative AI Era},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630909},
doi = {10.1145/3626252.3630909},
abstract = {Large language models (LLMs) are revolutionizing the field of computing education with their powerful code-generating capabilities. Traditional pedagogical practices have focused on code writing tasks, but there is now a shift in importance towards reading, comprehending and evaluating LLM-generated code. Alongside this shift, an important new skill is emerging -- the ability to solve programming tasks by constructing good prompts for code-generating models. In this work we introduce a new type of programming exercise to hone this nascent skill: 'Prompt Problems'. Prompt Problems are designed to help students learn how to write effective prompts for AI code generators. A student solves a Prompt Problem by crafting a natural language prompt which, when provided as input to an LLM, outputs code that successfully solves a specified programming task. We also present a new web-based tool called Promptly which hosts a repository of Prompt Problems and supports the automated evaluation of prompt-generated code. We deploy Promptly in one CS1 and one CS2 course and describe our experiences, which include student perceptions of this new type of activity and their interactions with the tool. We find that students are enthusiastic about Prompt Problems, and appreciate how the problems engage their computational thinking skills and expose them to new programming constructs. We discuss ideas for the future development of new variations of Prompt Problems, and the need to carefully study their integration into classroom practice.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {296–302},
numpages = {7},
keywords = {ai code generation, artificial intelligence, generative ai, large language models, llms, prompt engineering, prompt problems},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3701716.3717810,
author = {Syah, Riza Alaudin and Haryanto, Christoforus Yoga and Lomempow, Emily and Malik, Krishna and Putra, Irvan},
title = {EdgePrompt: Engineering Guardrail Techniques for Offline LLMs in K-12 Educational Settings},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717810},
doi = {10.1145/3701716.3717810},
abstract = {EdgePrompt is a prompt engineering framework that implements pragmatic guardrails for Large Language Models (LLMs) in the K-12 educational settings through structured prompting inspired by neural-symbolic principles. The system addresses educational disparities in Indonesia's Frontier, Outermost, Underdeveloped (3T) regions by enabling offline-capable content safety controls. It combines: (1) content generation with structured constraint templates, (2) assessment processing with layered validation, and (3) lightweight storage for content and result management. The framework implements a multi-stage verification workflow that maintains safety boundaries while preserving model capabilities in connectivity-constrained environments. Initial deployment targets Grade 5 language instruction, demonstrating effective guardrails through structured prompt engineering without formal symbolic reasoning components.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {1635–1638},
numpages = {4},
keywords = {ai safety, content filtering, edge computing, educational technology, guardrails, k-12 education, large language models, offline ai, prompt engineering},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3663649.3664368,
author = {Aerts, Willem and Fletcher, George and Miedema, Daphne},
title = {A Feasibility Study on Automated SQL Exercise Generation with ChatGPT-3.5},
year = {2024},
isbn = {9798400706783},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663649.3664368},
doi = {10.1145/3663649.3664368},
abstract = {SQL is the standard for database query languages and is taught in most introductory database courses. Query languages are illustrated and tested through toy examples: small, accessible, instances of databases. These are not always engaging, but coming up with new examples and questions is time-consuming. Existing research in Computer Science Education has shown that Large Language Models (LLMs) can generate coding exercises. However, this has not been demonstrated for SQL yet but could save teachers much time. In this paper, we study whether it is feasible to have ChatGPT-3.5 generate database schemas and associated SQL questions for teachers through a two-part study. Through a survey of educators, we found that creating a story and database schema for the SQL part is more time-consuming than the questions themselves. In our prompt engineering study, we identified prompts that were successful at creating database schemas, mock data, and exercises. However, although ChatGPT could help reduce the time required to create exams, some participants indicated that they are skeptical about using LLMs.},
booktitle = {Proceedings of the 3rd International Workshop on Data Systems Education: Bridging Education Practice with Education Research},
pages = {13–19},
numpages = {7},
keywords = {Assessment, ChatGPT, Education, LLM, SQL},
location = {Santiago, AA, Chile},
series = {DataEd '24}
}

@inproceedings{10.1145/3626252.3630927,
author = {Kirova, Vassilka D. and Ku, Cyril S. and Laracy, Joseph R. and Marlowe, Thomas J.},
title = {Software Engineering Education Must Adapt and Evolve for an LLM Environment},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630927},
doi = {10.1145/3626252.3630927},
abstract = {In the era of artificial intelligence (AI), generative AI, and Large Language Models (LLMs) in particular, have become increasingly significant in various sectors. LLMs such as GPT expand their applications, from content creation to advanced code completion. They offer unmatched opportunities but pose unique challenges to the software engineering domain. This paper discusses the necessity and urgency for software engineering education to adapt and evolve to prepare software engineers for the emerging LLM environment. While existing literature and social media have investigated AI's integration into various educational spheres, there is a conspicuous gap in examining the specifics of LLMs' implications for software engineering education. We explore the goals of software engineering education, and changes to software engineering, software engineering education, course pedagogy, and ethics. We argue that a holistic approach is needed, combining technical skills, ethical awareness, and adaptable learning strategies. This paper seeks to contribute to the ongoing conversation about the future of software engineering education, emphasizing the importance of adapting and evolving to remain in sync with rapid advancements in AI and LLMs. It is hoped that this exploration will provide valuable insights for educators, curriculum developers, and policymakers in software engineering.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {666–672},
numpages = {7},
keywords = {chatgpt, generative ai, large language models (llms), responsible ai, software engineering, software engineering education, software engineering ethics, software ethics},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3641555.3704765,
author = {Liu, Rongxin and Malan, David J. and Zhukovets, Yuliia and Lloyd, Doug},
title = {Teaching with AI (GPT)},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3704765},
doi = {10.1145/3641555.3704765},
abstract = {Teaching computer science at scale can be challenging. From our experience in CS50, Harvard University's introductory course, we've seen firsthand the impactful role that generative artificial intelligence can play in education. Recognizing its potential and stakes, we integrated OpenAI's GPT into our own teaching methodology. The goal was to emulate a 1:1 teacher-to-student ratio, incorporating "pedagogical guardrails" to maintain instructional integrity. The result was a personalized, AI-powered bot in the form of a friendly rubber duck aimed at delivering instructional responses and troubleshooting without giving outright solutions. In this tutorial, we share our journey and offer insights into responsibly harnessing AI in educational settings. Participants will gain hands-on experience working with GPT through OpenAI's latest APIs, understanding and crafting prompts, answering questions using embedding-based search, and finally, collaboratively building their own AI chatbot. Ultimately, we'll not only share lessons learned from our own approach but also equip educators hands-on with the knowledge and tools with which they, too, can implement these technologies in their unique teaching environments.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1773},
numpages = {1},
keywords = {AI, AI ethics, ChatGPT, GPT, generative AI, programming, prompt, prompt engineering},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3626252.3630938,
author = {Liu, Rongxin and Zenke, Carter and Liu, Charlie and Holmes, Andrew and Thornton, Patrick and Malan, David J.},
title = {Teaching CS50 with AI: Leveraging Generative Artificial Intelligence in Computer Science Education},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630938},
doi = {10.1145/3626252.3630938},
abstract = {In Summer 2023, we developed and integrated a suite of AI-based software tools into CS50 at Harvard University. These tools were initially available to approximately 70 summer students, then to thousands of students online, and finally to several hundred on campus during Fall 2023. Per the course's own policy, we encouraged students to use these course-specific tools and limited the use of commercial AI software such as ChatGPT, GitHub Copilot, and the new Bing. Our goal was to approximate a 1:1 teacher-to-student ratio through software, thereby equipping students with a pedagogically-minded subject-matter expert by their side at all times, designed to guide students toward solutions rather than offer them outright. The tools were received positively by students, who noted that they felt like they had "a personal tutor.'' Our findings suggest that integrating AI thoughtfully into educational settings enhances the learning experience by providing continuous, customized support and enabling human educators to address more complex pedagogical issues. In this paper, we detail how AI tools have augmented teaching and learning in CS50, specifically in explaining code snippets, improving code style, and accurately responding to curricular and administrative queries on the course's discussion forum. Additionally, we present our methodological approach, implementation details, and guidance for those considering using these tools or AI generally in education.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {750–756},
numpages = {7},
keywords = {ai, artificial intelligence, generative ai, large language models, llms},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@article{10.1145/3687038,
author = {Kumar, Harsh and Musabirov, Ilya and Reza, Mohi and Shi, Jiakai and Wang, Xinyuan and Williams, Joseph Jay and Kuzminykh, Anastasia and Liut, Michael},
title = {Guiding Students in Using LLMs in Supported Learning Environments: Effects on Interaction Dynamics, Learner Performance, Confidence, and Trust},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW2},
url = {https://doi.org/10.1145/3687038},
doi = {10.1145/3687038},
abstract = {Personalized chatbot-based teaching assistants can be crucial in addressing increasing classroom sizes, especially where direct teacher presence is limited. Large language models (LLMs) offer a promising avenue, with increasing research exploring their educational utility. However, the challenge lies not only in establishing the efficacy of LLMs but also in discerning the nuances of interaction between learners and these models, which impact learners' engagement and results. We conducted a formative study in an undergraduate computer science classroom (N=145) and a controlled experiment on Prolific (N=356) to explore the impact of four pedagogically informed guidance strategies on the learners' performance, confidence and trust in LLMs. Direct LLM answers marginally improved performance, while refining student solutions fostered trust. Structured guidance reduced random queries as well as instances of students copy-pasting assignment questions to the LLM. Our work highlights the role that teachers can play in shaping LLM-supported learning environments.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {499},
numpages = {30},
keywords = {artificial intelligence in education, collaborative learning with ai, human-ai collaboration, large language models, transparency, tutoring systems}
}

@inproceedings{10.1145/3636555.3636846,
author = {Phung, Tung and P\u{a}durean, Victor-Alexandru and Singh, Anjali and Brooks, Christopher and Cambronero, Jos\'{e} and Gulwani, Sumit and Singla, Adish and Soares, Gustavo},
title = {Automating Human Tutor-Style Programming Feedback: Leveraging GPT-4 Tutor Model for Hint Generation and GPT-3.5 Student Model for Hint Validation},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636846},
doi = {10.1145/3636555.3636846},
abstract = {Generative AI and large language models hold great promise in enhancing programming education by automatically generating individualized feedback for students. We investigate the role of generative AI models in providing human tutor-style programming hints to help students resolve errors in their buggy programs. Recent works have benchmarked state-of-the-art models for various feedback generation scenarios; however, their overall quality is still inferior to human tutors and not yet ready for real-world deployment. In this paper, we seek to push the limits of generative AI models toward providing high-quality programming hints and develop a novel technique, GPT4HINTS-GPT3.5VAL. As a first step, our technique leverages GPT-4 as a “tutor” model to generate hints – it boosts the generative quality by using symbolic information of failing test cases and fixes in prompts. As a next step, our technique leverages GPT-3.5, a weaker model, as a “student” model to further validate the hint quality – it performs an automatic quality validation by simulating the potential utility of providing this feedback. We show the efficacy of our technique via extensive evaluation using three real-world datasets of Python programs covering a variety of concepts ranging from basic algorithms to regular expressions and data analysis using pandas library.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {12–23},
numpages = {12},
keywords = {ChatGPT, Feedback Generation, GPT4, Generative AI, Programming Education},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3649217.3653527,
author = {Martini, Simone},
title = {Teaching Programming in the Age of Generative AI},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653527},
doi = {10.1145/3649217.3653527},
abstract = {Programming has been considered the "essence of informatics" since the beginning of computing as a discipline. But programming in the fifties was very different from what we know today, and one of the goals (or dreams) throughout the history of programming language technology, has been "automatic programming''---the ability to automatically generate computer code starting from a high(er)-level description of the specification of that code. What this meant changed over the years, from punching paper tape, to compiling high-level programming languages, to program synthesis.Today, however, the availability of machine learning artefacts that produce high-level code from natural language specifications has completely changed the traditional meaning. To the extent that some computer scientists have begun to question the received wisdom that the core of their discipline is deeply rooted in programming.If programming and programming languages are no longer the essence of computer science, this changes the epistemology of the discipline itself. Moreover, if we are at the end of programming, we should also change the curriculum, where programming, algorithms and programming languages play a major role. Several recent papers reviewed the performance of code generators based on large language models on typical CS1 problems (e.g., from the many possible citations and how machine learning impacts K-12 teaching.Starting from this data, I will argue for the role of programming in the curriculum, distinguishing between programming taught as part of a holistic curriculum (as in some non-technical high schools) or as a vocational tool. I will use Simondon's notion of (closed and open) technical object as an interpretive lens, together with Calvino's reflections on the availability of writing machines capable of replacing the poet and the author.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {1–2},
numpages = {2},
keywords = {epistemology, large language models, programming},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3649217.3653602,
author = {Mahon, Joyce and Mac Namee, Brian and Becker, Brett A.},
title = {Guidelines for the Evolving Role of Generative AI in Introductory Programming Based on Emerging Practice},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653602},
doi = {10.1145/3649217.3653602},
abstract = {In the rapidly evolving Generative AI (GenAI) landscape, source code and natural language are being mixed and used in new ways. This presents opportunities for rethinking teaching practice in Introductory Programming (CS1) courses that includes, but goes beyond, assessment. In this paper we examine the reasons why and how instructors who are early adopters of GenAI are using it in their teaching, and why others are not. We also explore the changes and adaptations that are currently being made to practice. This is achieved by synthesizing insights from several recent studies that have collected primary data from introductory programming instructors who are teaching with, considering teaching with, or actively not teaching with GenAI.Due to the fast pace of GenAI development and adoption, the fixed-pace and cyclical nature of education, and the relatively slow pace of research (including ethical approvals) and publication cycles, research with primary data from instructors is only being published relatively recently. In computing education, there is not yet enough published research with primary data from CS1 instructors to warrant a systematic literature review, although in the next year this will likely be possible. Based on an analysis of the nascent research that has been published, we propose emerging and flexible guidelines on how CS1 instructors could adapt their practice based on what others have done so far. These guidelines highlight important factors to consider when integrating GenAI in CS1 courses, which for many is only beginning.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {10–16},
numpages = {7},
keywords = {CS1, LLM, artificial intelligence, automated/assisted code generation, chatgpt, computing education, copilot, generative AI, introductory programming, k-12, large language model, machine learning, novice programmer, school},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3568813.3600138,
author = {Lau, Sam and Guo, Philip},
title = {From "Ban It Till We Understand It" to "Resistance is Futile": How University Programming Instructors Plan to Adapt as More Students Use AI Code Generation and Explanation Tools such as ChatGPT and GitHub Copilot},
year = {2023},
isbn = {9781450399760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568813.3600138},
doi = {10.1145/3568813.3600138},
abstract = {Over the past year (2022–2023), recently-released AI tools such as ChatGPT and GitHub Copilot have gained significant attention from computing educators. Both researchers and practitioners have discovered that these tools can generate correct solutions to a variety of introductory programming assignments and accurately explain the contents of code. Given their current capabilities and likely advances in the coming years, how do university instructors plan to adapt their courses to ensure that students still learn well? To gather a diverse sample of perspectives, we interviewed 20 introductory programming instructors (9 women + 11 men) across 9 countries (Australia, Botswana, Canada, Chile, China, Rwanda, Spain, Switzerland, United States) spanning all 6 populated continents. To our knowledge, this is the first empirical study to gather instructor perspectives about how they plan to adapt to these AI coding tools that more students will likely have access to in the future. We found that, in the short-term, many planned to take immediate measures to discourage AI-assisted cheating. Then opinions diverged about how to work with AI coding tools longer-term, with one side wanting to ban them and continue teaching programming fundamentals, and the other side wanting to integrate them into courses to prepare students for future jobs. Our study findings capture a rare snapshot in time in early 2023 as computing instructors are just starting to form opinions about this fast-growing phenomenon but have not yet converged to any consensus about best practices. Using these findings as inspiration, we synthesized a diverse set of open research questions regarding how to develop, deploy, and evaluate AI coding tools for computing education.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1},
pages = {106–121},
numpages = {16},
keywords = {AI coding tools, ChatGPT, Copilot, LLM, instructor perspectives},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3716640.3716654,
author = {Gutierrez, Sebastian and Hou, Irene and Lee, Jihye and Angelikas, Kenneth and Man, Owen and Mettille, Sophia and Prather, James and Denny, Paul and MacNeil, Stephen},
title = {Seeing the Forest and the Trees: Solving Visual Graph and Tree Based Data Structure Problems using Large Multimodal Models},
year = {2025},
isbn = {9798400714252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716640.3716654},
doi = {10.1145/3716640.3716654},
abstract = {Recent advancements in generative AI systems have raised concerns about academic integrity among educators. Beyond excelling at solving programming problems and text-based multiple-choice questions, recent research has also found that large multimodal models (LMMs) can solve Parsons problems based only on an image. However, such problems are still inherently text-based and rely on the capabilities of the models to convert the images of code blocks to their corresponding text. In this paper, we further investigate the capabilities of LMMs to solve graph and tree data structure problems based only on images. To achieve this, we computationally construct and evaluate a novel benchmark dataset comprising 9,072 samples of diverse graph and tree data structure tasks to assess the performance of the GPT-4o, GPT-4 with Vision (GPT-4V), Gemini 1.5 Pro, Gemini 1.5 Flash, Gemini 1.0 Pro Vision, and Claude 3 model families. GPT–4o and Gemini 1.5 Flash performed best on trees and graphs respectively. GPT-4o achieved 87.6% accuracy on tree samples, while Gemini 1.5 Pro, achieved 76.9% accuracy on graph samples. Our findings highlight the influence of structural and visual variations on model performance. This research not only introduces an LMM benchmark to facilitate replication and further exploration but also underscores the potential of LMMs in solving complex computing problems, with important implications for pedagogy and assessment practices.},
booktitle = {Proceedings of the 27th Australasian Computing Education Conference},
pages = {124–133},
numpages = {10},
keywords = {Generative AI, Academic Integrity, Computing Education, Large Multimodal Models, LMMs, Large Language Models, LLMs},
location = {
},
series = {ACE '25}
}

@inproceedings{10.1145/3545945.3569823,
author = {Denny, Paul and Kumar, Viraj and Giacaman, Nasser},
title = {Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569823},
doi = {10.1145/3545945.3569823},
abstract = {GitHub Copilot is an artificial intelligence tool for automatically generating source code from natural language problem descriptions. Since June 2022, Copilot has officially been available for free to all students as a plug-in to development environments like Visual Studio Code. Prior work exploring OpenAI Codex, the underlying model that powers Copilot, has shown it performs well on typical CS1 problems thus raising concerns about its potential impact on how introductory programming courses are taught. However, little is known about the types of problems for which Copilot does not perform well, or about the natural language interactions that a student might have with Copilot when resolving errors. We explore these questions by evaluating the performance of Copilot on a publicly available dataset of 166 programming problems. We find that it successfully solves around half of these problems on its very first attempt, and that it solves 60% of the remaining problems using only natural language changes to the problem description. We argue that this type of prompt engineering, which we believe will become a standard interaction between human and Copilot when it initially fails, is a potentially useful learning activity that promotes computational thinking skills, and is likely to change the nature of code writing skill development.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1136–1142},
numpages = {7},
keywords = {artificial intelligence, cs1, foundation models, github copilot, introductory programming, large language models, openai},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3626253.3635427,
author = {Liu, Rongxin and Zenke, Carter and Liu, Charlie and Holmes, Andrew and Thornton, Patrick and Malan, David J.},
title = {Teaching CS50 with AI: Leveraging Generative Artificial Intelligence in Computer Science Education},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635427},
doi = {10.1145/3626253.3635427},
abstract = {CS50.ai is an AI-based educational tool developed and integrated into CS50 at Harvard University using large language models (LLMs), supporting both in-person and online learners. CS50.ai encapsulates a variety of AI-based tools designed to enhance students' learning by approximating a 1:1 teacher-to-student ratio. We showcase: "Explain Highlighted Code," a Visual Studio (VS) Code extension that provides just-in-time explanations of code snippets; style50, a VS Code extension that offers formatting suggestions and explanations thereof; and our "CS50 Duck," an AI-based chatbot for course-related questions, implemented both as a VS Code extension and as a standalone web application. We also demonstrate the integration of our tools into Ed, the course's discussion forum. This demo will illustrate the functionality and effectiveness of these tools as well as the pedagogical "guardrails" that we put in place to ensure secure and fair usage of these tools, while sharing insights from our own experience therewith this past summer and fall.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1927},
numpages = {1},
keywords = {ai, artificial intelligence, generative ai, large language models, llms},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3632620.3671092,
author = {Yang, Stephanie and Zhao, Hanzhang and Xu, Yudian and Brennan, Karen and Schneider, Bertrand},
title = {Debugging with an AI Tutor: Investigating Novice Help-seeking Behaviors and Perceived Learning},
year = {2024},
isbn = {9798400704758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632620.3671092},
doi = {10.1145/3632620.3671092},
abstract = {Debugging is a crucial skill for programmers, yet it can be challenging for novices to learn. The introduction of large language models (LLMs) has opened up new possibilities for providing personalized debugging support to students. However, concerns have been raised about potential student over-reliance on LLM-based tools. This mixed-methods study investigates how a pedagogically-designed LLM-based chatbot supports students’ debugging efforts in an introductory programming course. We conducted interviews and debugging think-aloud tasks with 20 students at three points throughout the semester. We specifically focused on characterizing when students initiate help from the chatbot during debugging, how they engage with the chatbot’s responses, and how they describe their learning experiences with the chatbot. By analyzing data from the debugging tasks, we identified varying help-seeking behaviors and levels of engagement with the chatbot’s responses, depending on students’ familiarity with the suggested strategies. Interviews revealed that students appreciated the content and experiential knowledge provided by the chatbot, but did not view it as a primary source for learning debugging strategies. Additionally, students self-identified certain chatbot usage behaviors as negative, “non-ideal” engagement and others as positive, “learning-oriented” usage. Based on our findings, we discuss pedagogical implications and future directions for designing pedagogical chatbots to support debugging.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1},
pages = {84–94},
numpages = {11},
keywords = {AI tutoring, LLMs, debugging, help-seeking, large language models, programming education},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@inproceedings{10.1145/3706599.3720203,
author = {Tang, Xiaohang and Wong, Sam and Huynh, Marcus and He, Zicheng and Yang, Yalong and Chen, Yan},
title = {SPHERE: Supporting Personalized Feedback at Scale in Programming Classrooms with Structured Review of Generative AI Outputs},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3720203},
doi = {10.1145/3706599.3720203},
abstract = {This paper introduces SPHERE, a system that enables instructors to effectively create and review personalized feedback for in-class coding activities. Comprehensive personalized feedback is crucial for programming learning. However, providing such feedback in large programming classrooms poses significant challenges for instructors. While Large Language Models (LLMs) offer potential assistance, how to efficiently ensure the quality of LLM-generated feedback remains an open question. SPHERE guides instructors’ attention to critical students’ issues, empowers them with guided control over LLM-generated feedback, and provides visual scaffolding to facilitate verification of feedback quality. Our between-subject study with 20 participants demonstrates SPHERE’s effectiveness in creating more high-quality feedback while not increasing the time spent on the overall review process compared to a baseline system. This work contributes a synergistic approach to scaling personalized feedback in programming education, addressing the challenges of real-time response, issue prioritization, and large-scale personalization.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {467},
numpages = {17},
keywords = {Generative AI, Large Language Model, Programming Education at Scale, Feedback, Computing Education},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3626252.3630817,
author = {Fernandez, Amanda S. and Cornell, Kimberly A.},
title = {CS1 with a Side of AI: Teaching Software Verification for Secure Code in the Era of Generative AI},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630817},
doi = {10.1145/3626252.3630817},
abstract = {As AI-generated code promises to become an increasingly relied upon tool for software developers, there is a temptation to call for significant changes to early computer science curricula. A move from syntax-focused topics in CS1 toward abstraction and high-level application design seems motivated by the new large language models (LLMs) recently made available. In this position paper however, we advocate for an approach more informed by the AI itself - teaching early CS learners not only how to use the tools but also how to better understand them. Novice programmers leveraging AI-code-generation without proper understanding of syntax or logic can create "black box" code with significant security vulnerabilities. We outline methods for integrating basic AI knowledge and traditional software verification steps into CS1 along with LLMs, which will better prepare students for software development in professional settings.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {345–351},
numpages = {7},
keywords = {ai, artificial intelligence, code generation, copilot, cs1, gpt-4, introductory programming, large language model, llm, machine learning, novice programmers, programming, prompt engineering, secure code, software verification},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3649217.3653582,
author = {Smith, David H. and Zilles, Craig},
title = {Code Generation Based Grading: Evaluating an Auto-grading Mechanism for "Explain-in-Plain-English" Questions},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653582},
doi = {10.1145/3649217.3653582},
abstract = {Comprehending and conveying the purpose of code is often cited as being a key learning objective within introductory programming courses. To address this objective, "Explain in Plain English'' questions, where students are shown a segment of code and asked to provide an abstract description of the code's purpose, have been adopted. However, given EiPE questions require a natural language response, they often require manual grading which is time-consuming for course staff and delays feedback for students. With the advent of large language models (LLMs) capable of generating code, responses to EiPE questions can be used to generate code segments, the correctness of which can then be easily verified using test cases. We refer to this approach as "Code Generation Based Grading'' (CGBG) and in this paper we explore its agreement with human graders using EiPE responses from past exams in an introductory programming course taught in Python. Overall, we find that all CGBG approaches achieve moderate agreement with human graders with the primary area of disagreement being its leniency with respect to low-level and line-by-line descriptions of code.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {171–177},
numpages = {7},
keywords = {auto-grading, eipe, gpt-4, large language models},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3636243.3636257,
author = {Budhiraja, Ritvik and Joshi, Ishika and Challa, Jagat Sesh and Akolekar, Harshal D. and Kumar, Dhruv},
title = {“It's not like Jarvis, but it's pretty close!” - Examining ChatGPT's Usage among Undergraduate Students in Computer Science},
year = {2024},
isbn = {9798400716195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636243.3636257},
doi = {10.1145/3636243.3636257},
abstract = {Large language models (LLMs) such as ChatGPT and Google Bard have garnered significant attention in the academic community. Previous research has evaluated these LLMs for various applications such as generating programming exercises and solutions. However, these evaluations have predominantly been conducted by instructors and researchers, not considering the actual usage of LLMs by students. This study adopts a student-first approach to comprehensively understand how undergraduate computer science students utilize ChatGPT, a popular LLM, released by OpenAI. We employ a combination of student surveys and interviews to obtain valuable insights into the benefits, challenges, and suggested improvements related to ChatGPT. Our findings suggest that a majority of students (over 57%) have a convincingly positive outlook towards adopting ChatGPT as an aid in coursework-related tasks. However, our research also highlights various challenges that must be resolved for long-term acceptance of ChatGPT amongst students. The findings from this investigation have broader implications and may be applicable to other LLMs and their role in computing education.},
booktitle = {Proceedings of the 26th Australasian Computing Education Conference},
pages = {124–133},
numpages = {10},
keywords = {ChatGPT, Computer Science Education, User Study},
location = {Sydney, NSW, Australia},
series = {ACE '24}
}

@inproceedings{10.1145/3576123.3576134,
author = {Finnie-Ansley, James and Denny, Paul and Luxton-Reilly, Andrew and Santos, Eddie Antonio and Prather, James and Becker, Brett A.},
title = {My AI Wants to Know if This Will Be on the Exam: Testing OpenAI’s Codex on CS2 Programming Exercises},
year = {2023},
isbn = {9781450399418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576123.3576134},
doi = {10.1145/3576123.3576134},
abstract = {The introduction of OpenAI Codex sparked a surge of interest in the impact of generative AI models on computing education practices. Codex is also the underlying model for GitHub Copilot, a plugin which makes AI-generated code accessible to students through auto-completion in popular code editors. Research in this area, particularly on the educational implications, is nascent and has focused almost exclusively on introductory programming (or CS1) questions. Very recent work has shown that Codex performs considerably better on typical CS1 exam questions than most students. It is not clear, however, what Codex’s limits are with regard to more complex programming assignments and exams. In this paper, we present results detailing how Codex performs on more advanced CS2 (data structures and algorithms) exam questions taken from past exams. We compare these results to those of students who took the same exams under normal conditions, demonstrating that Codex outscores most students. We consider the implications of such tools for the future of undergraduate computing education.},
booktitle = {Proceedings of the 25th Australasian Computing Education Conference},
pages = {97–104},
numpages = {8},
keywords = {AI, AlphaCode, CS1, CS2, Codex, DeepMind, GPT-3, GitHub, OpenAI, academic integrity, algorithms, artificial intelligence, code generation, copilot, data structures, deep learning, introductory programming, machine learning, neural networks, novice programming},
location = {Melbourne, VIC, Australia},
series = {ACE '23}
}

@article{10.1145/3649850,
author = {Zhang, Jialu and Cambronero, Jos\'{e} Pablo and Gulwani, Sumit and Le, Vu and Piskac, Ruzica and Soares, Gustavo and Verbruggen, Gust},
title = {PyDex: Repairing Bugs in Introductory Python Assignments using LLMs},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3649850},
doi = {10.1145/3649850},
abstract = {Students often make mistakes in their introductory programming assignments as part of their learning process. Unfortunately, providing custom repairs for these mistakes can require a substantial amount of time and effort from class instructors. Automated program repair (APR) techniques can be used to synthesize such fixes. Prior work has explored the use of symbolic and neural techniques for APR in the education domain. Both types of approaches require either substantial engineering efforts or large amounts of data and training. We propose to use a large language model trained on code, such as Codex (a version of GPT), to build an APR system -- PyDex -- for introductory Python programming assignments. Our system can fix both syntactic and semantic mistakes by combining multi-modal prompts, iterative querying, test-case-based selection of few-shots, and program chunking. We evaluate PyDex on 286 real student programs and compare to three baselines, including one that combines a state-of-the-art Python syntax repair engine, BIFI, and a state-of-the-art Python semantic repair engine for student assignments, Refactory. We find that PyDex can fix more programs and produce smaller patches on average.},
journal = {Proc. ACM Program. Lang.},
month = apr,
articleno = {133},
numpages = {25},
keywords = {AI for programming education, automated program repair, large language models}
}

@inproceedings{10.1145/3641554.3701858,
author = {Tran, Minh and Gonzalez-Maldonado, David and Zhou, Elaine and Franklin, Diana},
title = {Can GPT Help? Supporting Teachers to Brainstorm Customized Instructional Scratch Projects},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701858},
doi = {10.1145/3641554.3701858},
abstract = {While many recent studies have explored how large language models can transform computer science instruction from the instructor perspective, they are primarily at the college level. Thus, little is known about using large language models towards curriculum development and teacher supports outside of the college setting. Given the emphasis placed on culturally responsive teaching at the K-8 level and well-documented evidence of insensitive and inaccurate language model outputs from a cultural perspective, it is imperative to perform systematic and principled research before considering their use in this setting.This paper explores the potential of teachers using large language models to brainstorm instructional Scratch projects. Specifically, we use GPT-3 to mimic structured projects from an existing computer science curriculum but situate the generated projects in different contexts/themes. We qualitatively analyze 300 project ideas generated by GPT and find 81% of the generated ideas satisfy our metrics for technical alignment and theme quality. We identify two major weaknesses: code complexity of generated projects and presence of potential insensitive elements that would require human filtering. We conclude that, while not ready as a student-facing solution, teachers could use GPT to effectively brainstorm customized instructional materials.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1134–1140},
numpages = {7},
keywords = {curriculum customization, k-8, large language models, scratch programming, teacher supports},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3657604.3662039,
author = {Smith, David H. and Denny, Paul and Fowler, Max},
title = {Prompting for Comprehension: Exploring the Intersection of Explain in Plain English Questions and Prompt Writing},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3662039},
doi = {10.1145/3657604.3662039},
abstract = {Learning to program requires the development of a variety of skills including the ability to read, comprehend, and communicate the purpose of code. In the age of large language models (LLMs), where code can be generated automatically, developing these skills is more important than ever for novice programmers. The ability to write precise natural language descriptions of desired behavior is essential for eliciting code from an LLM, and the code that is generated must be understood in order to evaluate its correctness and suitability. In introductory computer science courses, a common question type used to develop and assess code comprehension skill is the 'Explain in Plain English' (EiPE) question. In these questions, students are shown a segment of code and asked to provide a natural language description of that code's purpose. The adoption of EiPE questions at scale has been hindered by: 1) the difficulty of automatically grading short answer responses and 2) the ability to provide effective and transparent feedback to students. To address these shortcomings, we explore and evaluate a grading approach where a student's EiPE response is used to generate code via an LLM, and that code is evaluated against test cases to determine if the description of the code was accurate. This provides a scalable approach to creating code comprehension questions and enables feedback both through the code generated from a student's description and the results of test cases run on that code. We evaluate students' success in completing these tasks, their use of the feedback provided by the system, and their perceptions of the activity.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {39–50},
numpages = {12},
keywords = {CS1, EIPE, LLMs, code comprehension, explain in plain English, introductory programming, large language models, prompting},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@inproceedings{10.1145/3641555.3705171,
author = {Gonzaga, Justin T. and Jiang, Yuchao and Vassar, Alexandra},
title = {Empowering CS1 Educators: Enhancing Automated Feedback Instruction with Cognitive Load Theory},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705171},
doi = {10.1145/3641555.3705171},
abstract = {Delivering personalised and timely feedback is crucial for helping students address gaps in their understanding. However, the increasing demands of large class sizes make this task particularly challenging for CS1 educators, especially for casual teaching assistants who lack formal training and experience. Existing feedback training methods are often inconsistent and ineffective, leaving educators unprepared to handle diverse student needs.To address this, we designed an adaptive fading procedure based on Cognitive Load Theory (CLT) to support educators in delivering high-quality, personalised feedback. This pedagogical technique is integrated into FeedbackPulse-CLT, an automated tool that evaluates feedback in real-time and provides guidance for improvement. This paper outlines our approach to designing scalable, evidence-based feedback instruction using Generative AI and large language models (LLMs) to overcome feedback quality concerns in CS1 education.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1461–1462},
numpages = {2},
keywords = {cognitive load theory, cs1, feedback, generative ai, large language models},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3649217.3653621,
author = {Margulieux, Lauren E. and Prather, James and Reeves, Brent N. and Becker, Brett A. and Cetin Uzun, Gozde and Loksa, Dastyni and Leinonen, Juho and Denny, Paul},
title = {Self-Regulation, Self-Efficacy, and Fear of Failure Interactions with How Novices Use LLMs to Solve Programming Problems},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653621},
doi = {10.1145/3649217.3653621},
abstract = {We explored how undergraduate introductory programming students naturalistically used generative AI to solve programming problems. We focused on the relationship between their use of AI to their self-regulation strategies, self-efficacy, and fear of failure in programming. In this repeated-measures, mixed-methods research, we examined students' patterns of using generative AI with qualitative student reflections and their self-regulation, self-efficacy, and fear of failure with quantitative instruments at multiple times throughout the semester. We also explored the relationships among these variables to learner characteristics, perceived usefulness of AI, and performance. Overall, our results suggest that student factors affect their baseline use of AI. In particular, students with higher self-efficacy, lower fear of failure, or higher prior grades tended to use AI less or later in the problem-solving process and rated it as less useful than others. Interestingly, we found no relationship between students' self-regulation strategies and their use of AI. Students who used AI less or later in problem-solving also had higher grades in the course, but this is most likely due to prior characteristics as our data do not suggest that this is a causal relationship.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {276–282},
numpages = {7},
keywords = {CS1, LLMs, artificial intelligence, copilot, fear of failure, generative ai, introductory programming, large language models, metacognition, self-efficacy, self-regulated learning, self-regulation},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3626252.3630880,
author = {Sheard, Judy and Denny, Paul and Hellas, Arto and Leinonen, Juho and Malmi, Lauri and Simon},
title = {Instructor Perceptions of AI Code Generation Tools - A Multi-Institutional Interview Study},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630880},
doi = {10.1145/3626252.3630880},
abstract = {Much of the recent work investigating large language models and AI Code Generation tools in computing education has focused on assessing their capabilities for solving typical programming problems and for generating resources such as code explanations and exercises. If progress is to be made toward the inevitable lasting pedagogical change, there is a need for research that explores the instructor voice, seeking to understand how instructors with a range of experiences plan to adapt. In this paper, we report the results of an interview study involving 12 instructors from Australia, Finland and New Zealand, in which we investigate educators' current practices, concerns, and planned adaptations relating to these tools. Through this empirical study, our goal is to prompt dialogue between researchers and educators to inform new pedagogical strategies in response to the rapidly evolving landscape of AI code generation tools.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1223–1229},
numpages = {7},
keywords = {ai code generation, generative ai, instructor perceptions, interview study, large language models, llms, programming education},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3639474.3640076,
author = {Xue, Yuankai and Chen, Hanlin and Bai, Gina R. and Tairas, Robert and Huang, Yu},
title = {Does ChatGPT Help With Introductory Programming?An Experiment of Students Using ChatGPT in CS1},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640076},
doi = {10.1145/3639474.3640076},
abstract = {Generative AI, notably ChatGPT, has garnered attention in computer science education. This paper presents a controlled experiment that explores ChatGPT's role in CS1 in a classroom setting. Specifically, we aim to investigate the impact of ChatGPT on student learning outcomes and their behaviors when working on programming assignments. Participants were tasked with creating a UML diagram and subsequently implementing its design through programming, followed by a closed-book post-evaluation and a post-survey. All the participants were required to screen-record the whole process. In total, 56 participants were recruited, with 48 successful screen recordings. Participants in the Experimental Group can access ChatGPT 3.5 and other online resources, such as Google and Stack Overflow when creating the UML diagram and programming; however, participants in the Control Group can access all online resources except for ChatGPT (i.e., the only design variable is the access to ChatGPT). Finally, we measured and analyzed participants' learning outcomes through their UML diagram, programming, and post-evaluation scores. We also analyzed the time participants took to complete the tasks and their interactions with ChatGPT and other resources from the screen recordings. After finishing the tasks, student participants also provided their perceptions of using ChatGPT in CS1 through a post-survey.With rigorous quantitative and qualitative analysis, we found that (1) using ChatGPT does not present a significant impact on students' learning performance in the CS1 assignment-style tasks; (2) once using ChatGPT, students' tendency to explore other traditional educational resources is largely reduced (though available) and they tend to rely solely on ChatGPT, and this reliance on ChatGPT did not guarantee enhanced learning performance; (3) the majority of students hold neutral views on ChatGPT's role in CS1 programming but most of them raised concerns about its potential ethical issues and inconsistent performance across different tasks. We hope this study can help educators and students better understand the impact of ChatGPT in CS1 and inspire future work to provide proper guidelines for using ChatGPT in introductory programming classes.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {331–341},
numpages = {11},
keywords = {CS education, CS1, generative AI, ChatGPT, OOP},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1145/3545945.3569759,
author = {Becker, Brett A. and Denny, Paul and Finnie-Ansley, James and Luxton-Reilly, Andrew and Prather, James and Santos, Eddie Antonio},
title = {Programming Is Hard - Or at Least It Used to Be: Educational Opportunities and Challenges of AI Code Generation},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569759},
doi = {10.1145/3545945.3569759},
abstract = {The introductory programming sequence has been the focus of much research in computing education. The recent advent of several viable and freely-available AI-driven code generation tools present several immediate opportunities and challenges in this domain. In this position paper we argue that the community needs to act quickly in deciding what possible opportunities can and should be leveraged and how, while also working on overcoming otherwise mitigating the possible challenges. Assuming that the effectiveness and proliferation of these tools will continue to progress rapidly, without quick, deliberate, and concerted efforts, educators will lose advantage in helping shape what opportunities come to be, and what challenges will endure. With this paper we aim to seed this discussion within the computing education community.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {500–506},
numpages = {7},
keywords = {ai, alphacode, amazon, artificial intelligence, code generation, codewhisperer, codex, copilot, cs1, cs2, github, google, gpt-3, introductory programming, large language model, llm, machine learning, midjourney, novice programmers, openai, programming, tabnine},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3641554.3701867,
author = {Yeh, Thomas Y. and Tran, Karena and Gao, Ge and Yu, Tyler and Fong, Wai On and Chen, Tzu-Yi},
title = {Bridging Novice Programmers and LLMs with Interactivity},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701867},
doi = {10.1145/3641554.3701867},
abstract = {While Large Language Models (LLMs) enable experienced programmers to increase their productivity, LLMs' impact on learning and productivity for novices is currently unclear. Recent work showed novice programmers struggle with prompting LLMs for code generation and suggested that the use of LLMs in CS education could exacerbate existing equity issues. Educators are now faced with the difficult question of whether and when to incorporate the use of LLMs into the CS curriculum without adversely impacting student learning and equity. To address these concerns, we study the effects of using an interactive LLM on code generation with novice programmers. We find that using our interactive LLM improves the accuracy of code generation over the baseline LLM. Additionally, after using the interactive LLM, novices write improved prompts even when using the baseline LLM. Based on our findings, we plan to create iGPTs, a set of customized, interactive LLMs spanning CS education learning goals as templates to facilitate LLM integration for improving student learning and retention.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1295–1301},
numpages = {7},
keywords = {cs1, generative ai, llms, novice programmers},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.1145/3711026,
author = {Keppler, Samantha and Sinchaisri, Wichinpong Park and Snyder, Clare},
title = {Making ChatGPT Work for Me},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3711026},
doi = {10.1145/3711026},
abstract = {Increasingly, work happens through human collaboration with generative AI (e.g., ChatGPT). In this paper, we present a qualitative study of this collaboration for real-life work tasks. We focus our study on US K12 public school teachers (N = 24) who regularly design and complete text-generation tasks such as creating quizzes, slide decks, word problems, reading passages, lesson plans, classroom activities, and projects. In one-on-one video- and audio-recorded virtual sessions, we observe each teacher using ChatGPT-4 for work tasks of their choosing for 15 minutes, then debrief their experience. Analyzing 201 prompts inputted by the 24 teachers, we uncover four main modes with which the teachers request support from ChatGPT: (1) make for me (55% of prompts), (2) find for me (15%), (3) jump-start for me (10.5%), and (4) iterate with me (15.5%). The first three modes (make, find, and jump-start) are often requests of generative AI to do something, whereas the fourth mode (iterate) is a request of generative AI to think. In a follow-up survey of the same 24 teachers, most report using multiple modes for their work, but infrequently. Our study contributes new data and knowledge about how teachers are coming to understand whether and how to integrate generative AI into their teaching preparation routines.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {CSCW128},
numpages = {23},
keywords = {K12 education, generative AI, human-computer interaction}
}

@inproceedings{10.1145/3511861.3511863,
author = {Finnie-Ansley, James and Denny, Paul and Becker, Brett A. and Luxton-Reilly, Andrew and Prather, James},
title = {The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming},
year = {2022},
isbn = {9781450396431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511861.3511863},
doi = {10.1145/3511861.3511863},
abstract = {Recent advances in artificial intelligence have been driven by an exponential growth in digitised data. Natural language processing, in particular, has been transformed by machine learning models such as OpenAI’s GPT-3 which generates human-like text so realistic that its developers have warned of the dangers of its misuse. In recent months OpenAI released Codex, a new deep learning model trained on Python code from more than 50 million GitHub repositories. Provided with a natural language description of a programming problem as input, Codex generates solution code as output. It can also explain (in English) input code, translate code between programming languages, and more. In this work, we explore how Codex performs on typical introductory programming problems. We report its performance on real questions taken from introductory programming exams and compare it to results from students who took these same exams under normal conditions, demonstrating that Codex outscores most students. We then explore how Codex handles subtle variations in problem wording using several published variants of the well-known “Rainfall Problem” along with one unpublished variant we have used in our teaching. We find the model passes many test cases for all variants. We also explore how much variation there is in the Codex generated solutions, observing that an identical input prompt frequently leads to very different solutions in terms of algorithmic approach and code length. Finally, we discuss the implications that such technology will have for computing education as it continues to evolve, including both challenges and opportunities.},
booktitle = {Proceedings of the 24th Australasian Computing Education Conference},
pages = {10–19},
numpages = {10},
keywords = {novice programming, neural networks, machine learning, introductory programming, deep learning, copilot, code writing, code generation, artificial intelligence, academic integrity, OpenAI, GitHub, GPT-3, Codex, CS1, AI},
location = {Virtual Event, Australia},
series = {ACE '22}
}

@inproceedings{10.1145/3689187.3709614,
author = {Prather, James and Leinonen, Juho and Kiesler, Natalie and Gorson Benario, Jamie and Lau, Sam and MacNeil, Stephen and Norouzi, Narges and Opel, Simone and Pettit, Vee and Porter, Leo and Reeves, Brent N. and Savelka, Jaromir and Smith, David H. and Strickroth, Sven and Zingaro, Daniel},
title = {Beyond the Hype: A Comprehensive Review of Current Trends in Generative AI Research, Teaching Practices, and Tools},
year = {2025},
isbn = {9798400712081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689187.3709614},
doi = {10.1145/3689187.3709614},
abstract = {Generative AI (GenAI) is advancing rapidly, and the literature in computing education is expanding almost as quickly. Initial responses to GenAI tools were mixed between panic and utopian optimism. Many were fast to point out the opportunities and challenges of GenAI. Researchers reported that these new tools are capable of solving most introductory programming tasks and are causing disruptions throughout the curriculum. These tools can write and explain code, enhance error messages, create resources for instructors, and even provide feedback and help for students like a traditional teaching assistant. In 2024, new research started to emerge on the effects of GenAI usage in the computing classroom. These new data involve the use of GenAI to support classroom instruction at scale and to teach students how to code with GenAI. In support of the former, a new class of tools is emerging that can provide personalized feedback to students on their programming assignments or teach both programming and prompting skills at the same time. With the literature expanding so rapidly, this report aims to summarize and explain what is happening on the ground in computing classrooms. We provide a systematic literature review; a survey of educators and industry professionals; and interviews with educators using GenAI in their courses, educators studying GenAI, and researchers who create GenAI tools to support computing education. The triangulation of these methods and data sources expands the understanding of GenAI usage and perceptions at this critical moment for our community.},
booktitle = {2024 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {300–338},
numpages = {39},
keywords = {artificial intelligence, computing education, genai, generative ai, large language models, pedagogical practices, teaching computing},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3649409.3691076,
author = {Wiese, Eliane S. and Finnie-Ansley, James and Duran, Rodrigo and Cunningham, Kathryn and Demirtas, Mehmet Arif},
title = {Challenges and Solutions for Teaching Decomposition and Planning Skills in CS1},
year = {2024},
isbn = {9798400706042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649409.3691076},
doi = {10.1145/3649409.3691076},
abstract = {The task of decomposing a problem into sub-problems to build a solution, also formalized as planning in prior work, is a key skill for programming expertise. Improving the decomposition and planning skills of novices is shown to be a challenging goal for educators. Moreover, decomposing complex projects into smaller subtasks is an increasingly relevant skill with rapid developments in tools like large language models (LLMs). While there are many aspects of planning, one skill consistently observed in studies with experts is the ability to identify subtasks that can be solved via common code patterns. To support students in acquiring these skills, many researchers have explored explicit instruction about a set of common patterns in programs (i.e. programming plans). However, recent work implies that students may need additional support to fully benefit from such interventions. This panel aims to bring computing education researchers together to discuss the main challenges around teaching decomposition and planning using common patterns, the crucial factors for designing instruction for teaching these concepts, and the impact evolving technology like LLMs can have on these developments.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 2},
pages = {291–292},
numpages = {2},
keywords = {cs1, decomposition, large language models, programming plans},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@inproceedings{10.1145/3641554.3701791,
author = {Koutcheme, Charles and Dainese, Nicola and Sarsa, Sami and Hellas, Arto and Leinonen, Juho and Ashraf, Syed and Denny, Paul},
title = {Evaluating Language Models for Generating and Judging Programming Feedback},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701791},
doi = {10.1145/3641554.3701791},
abstract = {The emergence of large language models (LLMs) has transformed research and practice across a wide range of domains. Within the computing education research (CER) domain, LLMs have garnered significant attention, particularly in the context of learning programming. Much of the work on LLMs in CER, however, has focused on applying and evaluating proprietary models. In this article, we evaluate the efficiency of open-source LLMs in generating high-quality feedback for programming assignments and judging the quality of programming feedback, contrasting the results with proprietary models. Our evaluations on a dataset of students' submissions to introductory Python programming exercises suggest that state-of-the-art open-source LLMs are nearly on par with proprietary models in both generating and assessing programming feedback. Additionally, we demonstrate the efficiency of smaller LLMs in these tasks and highlight the wide range of LLMs accessible, even for free, to educators and practitioners.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {624–630},
numpages = {7},
keywords = {automatic evaluation, automatic feedback, generative ai, large language models, llm-as-a-judge, open source, programming feedback},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3649409.3691094,
author = {Feng, Ty and Liu, Sa and Ghosal, Dipak},
title = {CourseAssist: Pedagogically Appropriate AI Tutor for Computer Science Education},
year = {2024},
isbn = {9798400706042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649409.3691094},
doi = {10.1145/3649409.3691094},
abstract = {The growing enrollments in computer science courses and increase in class sizes necessitate scalable, automated tutoring solutions to adequately support student learning. While Large Language Models (LLMs) like GPT-4 have demonstrated potential in assisting students through question-answering, educators express concerns over student overreliance, miscomprehension of generated code, and the risk of inaccurate answers. Rather than banning these tools outright, we advocate for a constructive approach that harnesses the capabilities of AI while mitigating potential risks. This poster introduces CourseAssist, a novel LLM-based tutoring system tailored for computer science education. Unlike generic LLM systems, CourseAssist uses retrieval-augmented generation, user intent classification, and question decomposition to align AI responses with specific course materials and learning objectives, thereby ensuring pedagogical appropriateness of LLMs in educational settings. We evaluated CourseAssist against a baseline of GPT-4 using a dataset of 50 question-answer pairs from a programming languages course, focusing on the criteria of usefulness, accuracy, and pedagogical appropriateness. Evaluation results show that CourseAssist significantly outperforms the baseline, demonstrating its potential to serve as an effective learning assistant. We have also deployed CourseAssist in 6 computer science courses at a large public R1 research university reaching over 500 students. Interviews with 20 student users show that CourseAssist improves computer science instruction by increasing the accessibility of course-specific tutoring help and shortening the feedback loop on their programming assignments. Future work will include extensive pilot testing at more universities and exploring better collaborative relationships between students, educators, and AI that improve computer science learning experiences.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 2},
pages = {310–311},
numpages = {2},
keywords = {AI tutor, computer science education, intelligent tutoring systems, large language models, pedagogical appropriateness, question answering},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@inproceedings{10.1145/3626253.3635403,
author = {Li, Yi and Zhang, Riteng and Qu, Danni and Marques Samary, Ma\'{\i}ra},
title = {Mining Students' Mastery Levels from CS Placement Tests via LLMs},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635403},
doi = {10.1145/3626253.3635403},
abstract = {In higher education, introductory Computer Science (CS) programs offer a range of foundational courses. These encompass not only the standard CS1 and CS2 courses but may also include more specialized options like CS0 and CS1.5. In order to appropriately assign students to the suitable introductory courses, many institutions utilize placement tests, which assess students' pre-existing knowledge and skills. While most institutions rely on accuracy alone to make these determinations, there is often additional information concealed within the completed tests. This paper delves into the potential of Large Language Models (LLMs) to uncover this hidden information, particularly in gaining insights into how students perform in different concepts. Moreover, our framework has the flexibility to accommodate variations in curricula across different institutions, providing additional analytical perspectives. Initially, we built a concept inventory (CI) using the concepts covered in an institution's CS0, CS1, and CS2 curricula. Next, an LLM, specifically GPT 3.5, was applied to associate each question in the placement test with one or more concepts in the CI. Finally, the results of the placement tests were scrutinized, allowing the calculation of mastery levels in each concept for individual students. These mastery levels enable institutions to gauge a student's prior knowledge across various concepts simply by using a CS placement test. Additionally, we presented a case study demonstrating the application of this framework to 267 existing placement test results at Boston College.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1883},
numpages = {1},
keywords = {concept inventory, introductory computer science courses, large language models, placement test},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3593663.3593695,
author = {Dobslaw, Felix and Bergh, Peter},
title = {Experiences with Remote Examination Formats in Light of GPT-4},
year = {2023},
isbn = {9781450399562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593663.3593695},
doi = {10.1145/3593663.3593695},
abstract = {Sudden access to the rapidly improving large language model GPT by OpenAI forces educational institutions worldwide to revisit their exam procedures. In the pre-GPT era, we successfully applied oral and open-book home exams for two courses in the third year of our predominantly remote Software Engineering BSc program. We ask in this paper whether our current open-book exams are still viable or whether a move back to a legally compliant but less scalable oral exam is the only workable alternative. We further compare work-effort estimates between oral and open-book exams and report on differences in throughput and grade distribution over eight years to better understand the impact of examination format on the outcome. Examining GPT-4 on the most recent open-book exams showed that our current Artificial Intelligence and Reactive Programming exams are not GPT v4 proof. Three potential weaknesses of GPT are outlined. We also found that grade distributions have largely been unaffected by the examination format, opening up for a move to oral examinations only if needed. Throughput was higher for open-book exam course instances (73% vs 64%), while fail rates were too (12% vs 7%), with teacher workload increasing even for smaller classes. We also report on our experience regarding effort. Oral examinations are efficient for smaller groups but come with caveats regarding intensity and stress.},
booktitle = {Proceedings of the 5th European Conference on Software Engineering Education},
pages = {220–225},
numpages = {6},
keywords = {Software Engineering Education, Oral Examinations, Examination Formats, ChatGPT},
location = {Seeon/Bavaria, Germany},
series = {ECSEE '23}
}

@inproceedings{10.1145/3641554.3701872,
author = {McDanel, Bradley and Novak, Ed},
title = {Designing LLM-Resistant Programming Assignments: Insights and Strategies for CS Educators},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701872},
doi = {10.1145/3641554.3701872},
abstract = {The rapid advancement of Large Language Models (LLMs) like ChatGPT has raised concerns among computer science educators about how programming assignments should be adapted. This paper explores the capabilities of LLMs (GPT-3.5, GPT-4, and Claude Sonnet) in solving complete, multi-part CS homework assignments from the SIGCSE Nifty Assignments list. Through qualitative and quantitative analysis, we found that LLM performance varied significantly across different assignments and models, with Claude Sonnet consistently outperforming the others. The presence of starter code and test cases improved performance for advanced LLMs, while certain assignments, particularly those involving visual elements, proved challenging for all models. LLMs often disregarded assignment requirements, produced subtly incorrect code, and struggled with context-specific tasks. Based on these findings, we propose strategies for designing LLM-resistant assignments. Our work provides insights for instructors to evaluate and adapt their assignments in the age of AI, balancing the potential benefits of LLMs as learning tools with the need to ensure genuine student engagement and learning.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {756–762},
numpages = {7},
keywords = {ai-resistant assignments, assignment design, cs education, llm code generation, programming pedagogy},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3626252.3630854,
author = {Neyem, Andres and Sandoval Alcocer, Juan Pablo and Mendoza, Marcelo and Centellas-Claros, Leonardo and Gonzalez, Luis A. and Paredes-Robles, Carlos},
title = {Exploring the Impact of Generative AI for StandUp Report Recommendations in Software Capstone Project Development},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630854},
doi = {10.1145/3626252.3630854},
abstract = {StandUp Reports play an important role in capstone software engineering courses, facilitating progress tracking, obstacle identification, and team collaboration. However, despite their significance, students often grapple with the challenge of creating StandUp Reports that are clear, concise, and actionable. This paper investigates the impact of the use of generative AI in producing StandUp report recommendations, aiming to assist students in enhancing the quality and effectiveness of their reports. In a semester-long capstone course, 179 students participated in 16 real-world software development projects. They submitted weekly StandUp Reports with the assistance of an AI-powered Slack, which analyzed their initial reports and provided suggestions for enhancing them using both GPT-3.5 and the early access GPT-4 API. After each submitted report, students voluntarily answered a survey about usability and suggestion preference. Furthermore, we conducted a linguistic analysis of the recommendations made by the algorithms to gauge reading ease and comprehension complexity. Our findings indicate that the AI-based recommendation system helped students improve the overall quality of their StandUp Reports throughout the semester. Students expressed a high level of satisfaction with the tool and exhibited a strong willingness to continue using it in the future. The survey reveals that students perceived a slight improvement when using GPT-4 compared to GPT-3.5. Finally, a computational linguistic analysis performed on the recommendations demonstrates that both algorithms significantly improve the alignment between the generated texts and the students' educational level, thereby improving the quality of the original texts.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {951–957},
numpages = {7},
keywords = {capstone courses, chatgpt, generative ai, large language models, software engineering education},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3706599.3720282,
author = {Mhasakar, Manas and Baker-Ramos, Rachel and Carter, Benjamin and Helekahi-Kaiwi, Evyn-Bree and Hester, Josiah},
title = {"I Would Never Trust Anything Western": Kumu (Educator) Perspectives on Use of LLMs for Culturally Revitalizing CS Education in Hawaiian Schools},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3720282},
doi = {10.1145/3706599.3720282},
abstract = {As large language models (LLMs) become increasingly integrated into educational technology, their potential to assist in developing curricula has gained interest among educators. Despite this growing attention, their applicability in culturally responsive Indigenous educational settings like Hawai‘i’s public schools and Kaiapuni (immersion language) programs, remains understudied. Additionally, ‘undefinedlelo Hawai‘i, the Hawaiian language, as a low-resource language, poses unique challenges and concerns about cultural sensitivity and the reliability of generated content. Through surveys and interviews with kumu (educators), this study explores the perceived benefits and limitations of using LLMs for culturally revitalizing computer science (CS) education in Hawaiian public schools with Kaiapuni programs. Our findings highlight AI’s time-saving advantages while exposing challenges such as cultural misalignment and reliability concerns. We conclude with design recommendations for future AI tools to better align with Hawaiian cultural values and pedagogical practices, towards the broader goal of trustworthy, effective, and culturally grounded AI technologies.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {13},
numpages = {10},
keywords = {Culturally responsive pedagogy, Artificial Intelligence in education, Culturally-relevant CS, Hawaiian Immersion Language Schools, Large Language Models, Human-centered AI, Education technology, Indigenous knowledge, Low-resource languages},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3626252.3630803,
author = {Joshi, Ishika and Budhiraja, Ritvik and Dev, Harshal and Kadia, Jahnvi and Ataullah, Mohammad Osama and Mitra, Sayan and Akolekar, Harshal D. and Kumar, Dhruv},
title = {ChatGPT in the Classroom: An Analysis of Its Strengths and Weaknesses for Solving Undergraduate Computer Science Questions},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630803},
doi = {10.1145/3626252.3630803},
abstract = {This research paper aims to analyze the strengths and weaknesses associated with the utilization of ChatGPT as an educational tool in the context of undergraduate computer science education. ChatGPT's usage in tasks such as solving assignments and exams has the potential to undermine students' learning outcomes and compromise academic integrity. This study adopts a quantitative approach to demonstrate the notable unreliability of ChatGPT in providing accurate answers to a wide range of questions within the field of undergraduate computer science. While the majority of existing research has concentrated on assessing the performance of Large Language Models in handling programming assignments, our study adopts a more comprehensive approach. Specifically, we evaluate various types of questions such as true/false, multi-choice, multi-select, short answer, long answer, design-based, and coding-related questions. Our evaluation highlights the potential consequences of students excessively relying on ChatGPT for the completion of assignments and exams, including self-sabotage. We conclude with a discussion on how can students and instructors constructively use ChatGPT and related tools to enhance the quality of instruction and the overall student experience.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {625–631},
numpages = {7},
keywords = {chatgpt, computer science, education},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3626252.3630960,
author = {Nguyen, Ha and Allan, Vicki},
title = {Using GPT-4 to Provide Tiered, Formative Code Feedback},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630960},
doi = {10.1145/3626252.3630960},
abstract = {Large language models (LLMs) have shown promise in generating sensible code explanation and feedback in programming exercises. In this experience report, we discuss the process of using one of these models (OpenAI's GPT-4) to generate individualized feedback for students' Java code and pseudocode. We instructed GPT-4 to generate feedback for 113 submissions to four programming problems in an Algorithms and Data Structures class. We prompted the model with example feedback (few-shot learning) and instruction to (1) give feedback on conceptual understanding, syntax, and time complexity, and (2) suggest follow-up actions based on students' code or provide guiding questions. Overall, GPT-4 provided accurate feedback and successfully built on students' ideas in most submissions. Human evaluators (computer science instructors and tutors) rated GPT-4's hints as useful in guiding students' next steps. Model performance varied with programming problems but not submission quality. We reflect on where the model performed well and fell short, and discuss the potential of integrating LLM-generated, individualized feedback into computer science instruction.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {958–964},
numpages = {7},
keywords = {computer science education, feedback, large language models},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3689187.3709607,
author = {Clear, Tony and Cajander, \r{A}sa and Clear, Alison and McDermott, Roger and Daniels, Mats and Divitini, Monica and Forshaw, Matthew and Humble, Niklas and Kasinidou, Maria and Kleanthous, Styliani and Kultur, Can and Parvini, Ghazaleh and Polash, Mohammad and Zhu, Tingting},
title = {AI Integration in the IT Professional Workplace: A Scoping Review and Interview Study with Implications for Education and Professional Competencies},
year = {2025},
isbn = {9798400712081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689187.3709607},
doi = {10.1145/3689187.3709607},
abstract = {As Artificial Intelligence (AI) continues transforming workplaces globally, particularly within the Information Technology (IT) industry, understanding its impact on IT professionals and computing curricula is crucial. This research builds on joint work from two countries, addressing concerns about AI's increasing influence in IT sector workplaces and its implications for tertiary education. The study focuses on AI technologies such as generative AI (GenAI) and large language models (LLMs). It examines how they are perceived and adopted and their effects on workplace dynamics, task allocation, and human-system interaction.IT professionals, noted as early adopters of AI, offer valuable insights into the interplay between AI and work engagement, highlighting the significant competencies required for digital workplaces. This study employs a dual-method approach, combining a systematic and multi-vocal literature review and qualitative research methods. These included a thematic analysis of a set of 47 interviews conducted between March and May of 2024 with IT professionals in two countries (New Zealand and Sweden). The research aimed to understand the implications for computing students, education curricula, and the assessment of emerging professional competencies.The literature review found insufficient evidence addressing comprehensive AI practice methodologies, highlighting the need to both develop and regulate professional competencies for effective AI integration. Key interview findings revealed diverse levels of GenAI adoption, ranging from individual experimentation to institutional integration. Participants generally expressed positive attitudes toward the technology and were actively pursuing self-learning despite some concerns. The themes emerging from the interviews included AI's role in augmenting human tasks, privacy and security concerns, productivity enhancements, legal and ethical challenges, and the evolving need for new competencies in the workplace.The study underscores the critical role of competency frameworks in guiding professional development and ensuring preparedness for an AI-driven environment. Additionally, it highlights the need for educational institutions to adapt curricula to address these emerging demands effectively},
booktitle = {2024 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {34–67},
numpages = {34},
keywords = {artificial intelligence, computing competencies, computing curricula, generative ai, it profession, large language models},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3649217.3653587,
author = {Denny, Paul and Smith, David H. and Fowler, Max and Prather, James and Becker, Brett A. and Leinonen, Juho},
title = {Explaining Code with a Purpose: An Integrated Approach for Developing Code Comprehension and Prompting Skills},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653587},
doi = {10.1145/3649217.3653587},
abstract = {Reading, understanding and explaining code have traditionally been important skills for novices learning programming. As large language models (LLMs) become prevalent, these foundational skills are more important than ever given the increasing need to understand and evaluate model-generated code. Brand new skills are also needed, such as the ability to formulate clear prompts that can elicit intended code from an LLM. Thus, there is great interest in integrating pedagogical approaches for the development of both traditional coding competencies and the novel skills required to interact with LLMs. One effective way to develop and assess code comprehension ability is with "Explain in plain English'' (EiPE) questions, where students succinctly explain the purpose of a fragment of code. However, grading EiPE questions has always been difficult given the subjective nature of evaluating written explanations and this has stifled their uptake. In this paper, we explore a natural synergy between EiPE questions and code-generating LLMs to overcome this limitation. We propose using an LLM to generate code based on students' responses to EiPE questions -- not only enabling EiPE responses to be assessed automatically, but helping students develop essential code comprehension and prompt crafting skills in parallel. We investigate this idea in an introductory programming course and report student success in creating effective prompts for solving EiPE questions. We also examine student perceptions of this activity and how it influences their views on the use of LLMs for aiding and assessing learning.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {283–289},
numpages = {7},
keywords = {code comprehension, cs1, eipe, explain in plan english, introductory programming, large language models, llms, prompting},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3649217.3653568,
author = {del Carpio Gutierrez, Andre and Denny, Paul and Luxton-Reilly, Andrew},
title = {Automating Personalized Parsons Problems with Customized Contexts and Concepts},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653568},
doi = {10.1145/3649217.3653568},
abstract = {Parsons problems provide useful scaffolding for introductory programming students learning to write code. However, generating large numbers of high-quality Parsons problems that appeal to the diverse range of interests in a typical introductory course is a significant challenge for educators. Large language models (LLMs) may offer a solution, by allowing students to produce on-demand Parsons problems for topics covering the breadth of the introductory programming curriculum, and targeting thematic contexts that align with their personal interests. In this paper, we introduce PuzzleMakerPy, an educational tool that uses an LLM to generate unlimited contextualized drag-and-drop programming exercises in the form of Parsons Problems, which introductory programmers can use as a supplemental learning resource. We evaluated PuzzleMakerPy by deploying it in a large introductory programming course, and found that the ability to personalize the contextual framing used in problem descriptions was highly engaging for students, and being able to customize the programming topics was reported as being useful for their learning.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {688–694},
numpages = {7},
keywords = {cs education tools, cs1, large language models, parsons problems, personalized learning},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3627217.3627233,
author = {Balse, Rishabh and Kumar, Viraj and Prasad, Prajish and Warriem, Jayakrishnan Madathil},
title = {Evaluating the Quality of LLM-Generated Explanations for Logical Errors in CS1 Student Programs},
year = {2023},
isbn = {9798400708404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627217.3627233},
doi = {10.1145/3627217.3627233},
abstract = {When students in CS1 (Introductory Programming) write erroneous code, course staff can use automated tools to provide various types of helpful feedback. In this paper, we focus on syntactically correct student code containing logical errors. Tools that explain logical errors typically require course staff to invest greater effort than tools that detect such errors. To reduce this effort, prior work has investigated the use of Large Language Models (LLMs) such as GPT-3 to generate explanations. Unfortunately, these explanations can be incomplete or incorrect, and therefore unhelpful if presented to students directly. Nevertheless, LLM-generated explanations may be of adequate quality for Teaching Assistants (TAs) to efficiently craft helpful explanations on their basis. We evaluate the quality of explanations generated by an LLM (GPT-3.5-turbo) in two ways, for 30&nbsp;buggy student solutions across 6&nbsp;code-writing problems. First, in a study with 5&nbsp;undergraduate TAs, we compare TA perception of LLM-generated and peer-generated explanation quality. TAs were unaware which explanations were LLM-generated, but they found them to be comparable in quality to peer-generated explanations. Second, we performed a detailed manual analysis of LLM-generated explanations for all 30&nbsp;buggy solutions. We found at least one incorrect statement in 15/30 explanations (50%). However, in 28/30 cases (93%), the LLM-generated explanation correctly identified at least one logical error. Our results suggest that for large CS1 courses, TAs with adequate training to detect erroneous statements may be able to extract value from such explanations.},
booktitle = {Proceedings of the 16th Annual ACM India Compute Conference},
pages = {49–54},
numpages = {6},
keywords = {Explanation, GPT-3.5-Turbo, Large language models (LLMs), Logical Errors, Python Programming},
location = {Hyderabad, India},
series = {COMPUTE '23}
}

@inproceedings{10.1145/3649405.3659534,
author = {Prather, James and Leinonen, Juho and Kiesler, Natalie and Benario, Jamie Gorson and Lau, Sam and MacNeil, Stephen and Norouzi, Narges and Opel, Simone and Pettit, Virginia and Porter, Leo and Reeves, Brent N. and Savelka, Jaromir and Smith, David H. and Strickroth, Sven and Zingaro, Daniel},
title = {How Instructors Incorporate Generative AI into Teaching Computing},
year = {2024},
isbn = {9798400706035},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649405.3659534},
doi = {10.1145/3649405.3659534},
abstract = {Generative AI (GenAI) has seen great advancements in the past two years and the conversation around adoption is increasing. Widely available GenAI tools are disrupting classroom practices as they can write and explain code with minimal student prompting. While most acknowledge that there is no way to stop students from using such tools, a consensus has yet to form on how students should use them if they choose to do so. At the same time, researchers have begun to introduce new pedagogical tools that integrate GenAI into computing curricula. These new tools offer students personalized help or attempt to teach prompting skills without undercutting code comprehension. This working group aims to detail the current landscape of education-focused GenAI tools and teaching approaches, present gaps where new tools or approaches could appear, identify good practice-examples, and provide a guide for instructors to utilize GenAI as they continue to adapt to this new era.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 2},
pages = {771–772},
numpages = {2},
keywords = {artificial intelligence, generative AI, large language models, pedagogical practices, teaching computing},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3641555.3705132,
author = {Blasco, I\~{n}aki and Mochetti, Karina},
title = {Assessing the Influence of ChatGPT on Student Outcomes in a Models of Computing Course},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705132},
doi = {10.1145/3641555.3705132},
abstract = {This study investigates the impact of ChatGPT on student performance in a Models of Computing course, foundational for the computer science major. Analysing data from 11 pre-lecture quizzes across four terms, we found a decline in average quiz scores, particularly in the latest term. The results suggest a correlation between increased reliance on ChatGPT and decreased student performance, especially on challenging questions where the AI frequently struggled. These findings highlight both the benefits and challenges of integrating AI in education. Our ongoing research aims to explore this further across multiple courses, ultimately promoting responsible AI use to enhance learning outcomes.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1389–1390},
numpages = {2},
keywords = {computing education, llm, student performance},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3690712.3690720,
author = {Jelson, Andrew and Lee, Sang Won},
title = {An empirical study to understand how students use ChatGPT for writing essays and how it affects their ownership},
year = {2024},
isbn = {9798400710315},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3690712.3690720},
doi = {10.1145/3690712.3690720},
abstract = {As large language models (LLMs) become more powerful and ubiquitous, systems like ChatGPT are increasingly used by students to help them with writing tasks. To better understand how these tools are used, we investigate how students might use an LLM for essay writing, for example, to study the queries asked to ChatGPT and the responses that ChatGPT gives. To that end, we plan to conduct a user study that will record the user writing process and present them with the opportunity to use ChatGPT as an AI assistant. This study’s findings will help us understand how these tools are used and how practitioners — such as educators and essay readers — should consider writing education and evaluation based on essay writing.},
booktitle = {Proceedings of the Third Workshop on Intelligent and Interactive Writing Assistants},
pages = {26–30},
numpages = {5},
location = {Honolulu, HI, USA},
series = {In2Writing '24}
}

@inproceedings{10.1145/3639474.3640059,
author = {Fwa, Hua Leong},
title = {Experience Report: Identifying common misconceptions and errors of novice programmers with ChatGPT},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640059},
doi = {10.1145/3639474.3640059},
abstract = {Identifying the misconceptions of novice programmers is pertinent for informing instructors of the challenges faced by their students in learning computer programming. In the current literature, custom tools, test scripts were developed and, in most cases, manual effort to go through the individual codes were required to identify and categorize the errors latent within the students' code submissions. This entails investment of substantial effort and time from the instructors. In this study, we thus propose the use of ChatGPT in identifying and categorizing the errors. Using prompts that were seeded only with the student's code and the model code solution for questions from two lab tests, we were able to leverage on ChatGPT's natural language processing and knowledge representation capabilities to automatically collate frequencies of occurrence of the errors by error types. We then clustered the generated error descriptions for further insights into the misconceptions of the students. The results showed that although ChatGPT was not able to identify the errors perfectly, the achieved accuracy of 93.3% is sufficiently high for instructors to have an aggregated picture of the common errors of their students. To conclude, we have proposed a method for instructors to automatically collate the errors latent within the students' code submissions using ChatGPT. Notably, with the novel use of generated error descriptions, the instructors were able to have a more granular view of the misconceptions of their students, without the onerous effort of manually going through the students' codes.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {233–241},
numpages = {9},
keywords = {LLM, ChatGPT, misconception, programming, errors, cluster, prompts},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1145/3622780.3623648,
author = {Kuramitsu, Kimio and Obara, Yui and Sato, Miyu and Obara, Momoka},
title = {KOGI: A Seamless Integration of ChatGPT into Jupyter Environments for Programming Education},
year = {2023},
isbn = {9798400703904},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622780.3623648},
doi = {10.1145/3622780.3623648},
abstract = {The impact of ChatGPT has brought both anxiety and anticipation to schools and universities. Exploring a positive method to improve programming skills with ChatGPT is a new and pressing challenge.  
In pursuit of this goal, we have developed KOGI, a learning support system that integrates ChatGPT into the Jupyter environment. This paper demonstrates how KOGI enables students to receive timely advice from ChatGPT in response to errors and other questions they encounter.  

We immediately introduced KOGI in our two introductory courses: Algorithms and Data Science. The introduction of KOGI resulted in a significant decrease in the number of unresolved student errors. In addition, we report on student trends observed in the classroom regarding the type and frequency of help requested. Although our findings are preliminary, they are informative for programming instructors interested in using ChatGPT.},
booktitle = {Proceedings of the 2023 ACM SIGPLAN International Symposium on SPLASH-E},
pages = {50–59},
numpages = {10},
keywords = {programming education, classroom experience, LLM, ChatGPT},
location = {Cascais, Portugal},
series = {SPLASH-E 2023}
}

@inproceedings{10.1145/3576882.3617909,
author = {Santos, Eddie Antonio and Prasad, Prajish and Becker, Brett A.},
title = {Always Provide Context: The Effects of Code Context on Programming Error Message Enhancement},
year = {2023},
isbn = {9798400700484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576882.3617909},
doi = {10.1145/3576882.3617909},
abstract = {Programming error messages (PEMs) are notoriously difficult for novice programmers to utilise. Many efforts have been made to enhance PEMs such that they are reworded to explain problems in terms that novices can understand. However, the effectiveness of these efforts to enhance PEMs has been weak or inconclusive. This work seeks to determine the role that code context has on programming error message enhancement. Erroneous Java code written by novices was sampled from the Blackbox Mini dataset. The erroneous code was presented to expert raters with four different PEM variants: javac (control), Decaf -- an error message enhancing IDE -- and two variants generated using GPT-4: one that enhanced just the javac error message alone, and one that incorporates the code context in the prompt. We find that providing code context to LLMs increases the likelihood of correct explanations for underlying errors, produces more specific fixes for erroneous programs, and produces fixes that are more likely to be correct. In large language models, the community now has a resource that is capable of taking code context into account, to the benefit of novice programmers.},
booktitle = {Proceedings of the ACM Conference on Global Computing Education Vol 1},
pages = {147–153},
numpages = {7},
keywords = {Blackbox, BlueJ, CS1, GPT-4, Java, compiler error messages, computing education, debugging, feedback, large language models, novice programmers, programming error messages},
location = {Hyderabad, India},
series = {CompEd 2023}
}

@article{10.5555/3717781.3717797,
author = {Evans, Jacob and Goldschmidt, Cody and Zhang, Yilian},
title = {Evaluating the Cognitive Level of GPT Models in Mathematics},
year = {2024},
issue_date = {November 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {5},
issn = {1937-4771},
abstract = {The current trend of using AI-based applications in everyday life has gained momentum among the general public. The GPT model has been promoted as a math tutoring tool for K-12 students. We highly question this promotion and believe a thorough examination of the GPT model's capability in mathematical cognition is necessary before it can be considered a reliable tutoring tool. In this paper, we present our preliminary findings on the GPT model's cognitive ability in mathematics. The model exhibits a low level of mathematical cognition and lacks training in important areas of trigonometry. The GPT model has not reached a level of reliability required for tutoring tool. Guardrails must be implemented for further use. We have developed an efficient strategy that allows the GPT model to categorize problems based on topic, and this self-feedback can be used to guide its problem-solving process.},
journal = {J. Comput. Sci. Coll.},
month = nov,
pages = {117–126},
numpages = {10}
}

@inproceedings{10.1145/3636243.3636256,
author = {Doughty, Jacob and Wan, Zipiao and Bompelli, Anishka and Qayum, Jubahed and Wang, Taozhi and Zhang, Juran and Zheng, Yujia and Doyle, Aidan and Sridhar, Pragnya and Agarwal, Arav and Bogart, Christopher and Keylor, Eric and Kultur, Can and Savelka, Jaromir and Sakr, Majd},
title = {A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education},
year = {2024},
isbn = {9798400716195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636243.3636256},
doi = {10.1145/3636243.3636256},
abstract = {There is a constant need for educators to develop and maintain effective up-to-date assessments. While there is a growing body of research in computing education on utilizing large language models&nbsp;(LLMs) in generation and engagement with coding exercises, the use of LLMs for generating programming MCQs has not been extensively explored. We analyzed the capability of GPT-4 to produce multiple-choice questions (MCQs) aligned with specific learning objectives (LOs) from Python programming classes in higher education. Specifically, we developed an LLM-powered (GPT-4) system for generation of MCQs from high-level course context and module-level LOs. We evaluated 651 LLM-generated and 449 human-crafted MCQs aligned to 246 LOs from 6 Python courses. We found that GPT-4 was capable of producing MCQs with clear language, a single correct choice, and high-quality distractors. We also observed that the generated MCQs appeared to be well-aligned with the LOs. Our findings can be leveraged by educators wishing to take advantage of the state-of-the-art generative models to support MCQ authoring efforts.},
booktitle = {Proceedings of the 26th Australasian Computing Education Conference},
pages = {114–123},
numpages = {10},
keywords = {Assessments, Automated Content Generation, Automatic Generation, GPT-4, LLMs, LOs, Large Language Models, Learning Objectives, MCQs, Multiple-choice Questions},
location = {Sydney, NSW, Australia},
series = {ACE '24}
}

@article{10.5555/3737313.3737334,
author = {Fernandez, Amanda S. and Patrick, David and Gomez, Mauricio and Cornell, Kimberly A.},
title = {Incorporating LLM Activities into Established CS1 Curriculum: An Experience Report},
year = {2025},
issue_date = {April 2025},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {8},
issn = {1937-4771},
abstract = {Large Language Models (LLMs), including Gemini, CoPilot, and ChatGPT, have experienced significant growth in usage and adoption in recent years. As these models become more sophisticated, particularly in code generation capabilities, educators need to adapt their CS1 courses. In this experience report, we share observations we made while designing and teaching LLM activities for CS1 students at two academic institutions during the spring 2024 term. Drawing on recent research, our activities consist of four short 10-15 minute exercises that guide students in how to properly utilize LLMs within their CS1 coursework. These activities can be easily added to the existing CS1 course curriculum to supplement the existing course materials. Post-activity surveys indicated a positive impact on students' understanding of CS concepts and indicated enthusiasm for learning how to use LLMs safely in programming.},
journal = {J. Comput. Sci. Coll.},
month = may,
pages = {79–93},
numpages = {15}
}

@inproceedings{10.1145/3641555.3705166,
author = {Demirta\c{s}, Mehmet Arif and Zheng, Claire and Cunningham, Kathryn},
title = {Detecting Programming Plans in Open-ended Code Submissions},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705166},
doi = {10.1145/3641555.3705166},
abstract = {Open-ended code-writing exercises are commonly used in large-scale introductory programming courses, as they can be autograded against test cases. However, code writing requires many skills at once, from planning out a solution to applying the intricacies of syntax. As autograding only evaluates code correctness, feedback addressing each of these skills separately cannot be provided. In this work, we explore methods to detect which high-level patterns (i.e. programming plans) have been used in a submission, so learners can receive feedback on planning skills even when their code is not completely correct. Our preliminary results show that LLMs with few-shot prompting can detect the use of programming plans in 95% of correct and 86% of partially correct submissions. Incorporating LLMs into grading of open-ended programming exercises can enable more fine-grained feedback to students, even in cases where their code does not compile due to other errors.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1435–1436},
numpages = {2},
keywords = {autograding, large language models, programming plans},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3653666.3656065,
author = {Smith, Julie M.},
title = {"I'm Sorry, but I Can't Assist": Bias in Generative AI},
year = {2024},
isbn = {9798400706264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653666.3656065},
doi = {10.1145/3653666.3656065},
abstract = {Research Questions: (1) Is there a pattern of racial bias in student advising recommendations made by generative AI? (2) What safeguards can promote equity when using generative AI in high-stakes decision-making? Methodology: Using lists of names associated with various ethnic/racial groups, we asked ChatGPT and Claude AI for recommendations for colleges and majors for each student. Results: ChatGPT was more likely to recommend STEM majors to some student groups. ChatGPT did not show systematic bias in various metrics of school quality, but Claude AI did. There were also overall differences in the colleges recommended by Claude AI and ChatGPT. Implications: We provide cautions and recommendations for using generative AI in high-stakes tasks.},
booktitle = {Proceedings of the 2024 on RESPECT Annual Conference},
pages = {75–80},
numpages = {6},
keywords = {artificial intelligence, generative ai, large language models, quity, racism, student advising},
location = {Atlanta, GA, USA},
series = {RESPECT 2024}
}

@inproceedings{10.1145/3699538.3699546,
author = {Keuning, Hieke and Alpizar-Chacon, Isaac and Lykourentzou, Ioanna and Beehler, Lauren and K\"{o}ppe, Christian and de Jong, Imke and Sosnovsky, Sergey},
title = {Students' Perceptions and Use of Generative AI Tools for Programming Across Different Computing Courses},
year = {2024},
isbn = {9798400710384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3699538.3699546},
doi = {10.1145/3699538.3699546},
abstract = {Investigation of students’ perceptions and opinions on the use of generative artificial intelligence (GenAI) in education is a topic gaining much interest. Studies addressing this are typically conducted with large heterogeneous groups, at one moment in time. However, how students perceive and use GenAI tools can potentially depend on many factors, including their background knowledge, familiarity with the tools, and the learning goals and policies of the courses they are taking. In this study we explore how students following computing courses use GenAI for programming-related tasks across different programs and courses: Bachelor and Master, in courses in which learning programming is the learning goal, courses that require programming as a means to achieve another goal, and in courses in which programming is optional, but can be useful. We are also interested in changes over time, since GenAI capabilities are changing at a fast pace, and users are adopting GenAI increasingly. We conducted three consecutive surveys (fall ‘23, winter ‘23, and spring ‘24) among students of all computing programs of a large European research university. We asked questions on the use in education, ethics, and job prospects, and we included specific questions on the (dis)allowed use of GenAI tools in the courses they were taking at the time. We received 264 responses, which we quantitatively and qualitatively analyzed, to find out how students have employed GenAI tools across 59 different computing courses, and whether the opinion of an average student about these tools evolves over time. Our study contributes to the emerging discussion of how to differentiate GenAI use across different courses, and how to align its use with the learning goals of a computing course.},
booktitle = {Proceedings of the 24th Koli Calling International Conference on Computing Education Research},
articleno = {14},
numpages = {12},
keywords = {Generative AI, Large Language Models, Computing Education, Programming Courses},
location = {
},
series = {Koli Calling '24}
}

@inproceedings{10.1145/3613904.3642229,
author = {Chen, Liuqing and Xiao, Shuhong and Chen, Yunnong and Song, Yaxuan and Wu, Ruoyu and Sun, Lingyun},
title = {ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642229},
doi = {10.1145/3613904.3642229},
abstract = {As Computational Thinking (CT) continues to permeate younger age groups in K-12 education, established CT platforms such as Scratch face challenges in catering to these younger learners, particularly those in the elementary school (ages 6-12). Through formative investigation with Scratch experts, we uncover three key obstacles to children’s autonomous Scratch learning: artist’s block in project planning, bounded creativity in asset creation, and inadequate coding guidance during implementation. To address these barriers, we introduce ChatScratch, an AI-augmented system to facilitate autonomous programming learning for young children. ChatScratch employs structured interactive storyboards and visual cues to overcome artist’s block, integrates digital drawing and advanced image generation technologies to elevate creativity, and leverages Scratch-specialized Large Language Models (LLMs) for professional coding guidance. Our study shows that, compared to Scratch, ChatScratch efficiently fosters autonomous programming learning, and contributes to the creation of high-quality, personally meaningful Scratch projects for children.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {649},
numpages = {19},
keywords = {Children Aged 6-12, Computational Thinking, Large Language Model, Scratch},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3641554.3701864,
author = {Zamfirescu-Pereira, J.D. and Qi, Laryn and Hartmann, Bj\"{o}rn and DeNero, John and Norouzi, Narges},
title = {61A Bot Report: AI Assistants in CS1 Save Students Homework Time and Reduce Demands on Staff. (Now What?)},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701864},
doi = {10.1145/3641554.3701864},
abstract = {LLM-based chatbots enable students to get immediate, interactive help on homework assignments, but even a thoughtfully-designed bot may not serve all pedagogical goals. We report here on the development and deployment of a GPT-4-based interactive homework assistant ("61A Bot'') for students in a large CS1 course; over 2000 students made over 100,000 requests of our Bot across two semesters. Our assistant offers one-shot, contextual feedback within the command-line "autograder'' students use to test their code. Our Bot wraps student code in a custom prompt that supports our pedagogical goals and avoids providing solutions directly. Analyzing student feedback, questions, and autograder data, we find reductions in homework-related question rates in our course forum, as well as reductions in homework completion time when our Bot is available. For students in the 50th -80th percentile, reductions can exceed 30 minutes per assignment, up to 50% less time than students at the same percentile rank in prior semesters. Finally, we discuss these observations, potential impacts on student learning, and other potential costs and benefits of AI assistance in CS1.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1309–1315},
numpages = {7},
keywords = {ai assistant deployment, ai assistant evaluation, automated tutors, large language models},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3613904.3642773,
author = {Kazemitabaar, Majeed and Ye, Runlong and Wang, Xiaoning and Henley, Austin Zachary and Denny, Paul and Craig, Michelle and Grossman, Tovi},
title = {CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642773},
doi = {10.1145/3613904.3642773},
abstract = {Timely, personalized feedback is essential for students learning programming. LLM-powered tools like ChatGPT offer instant support, but reveal direct answers with code, which may hinder deep conceptual engagement. We developed CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions. CodeAid answers conceptual questions, generates pseudo-code with line-by-line explanations, and annotates student’s incorrect code with fix suggestions. We deployed CodeAid in a programming class of 700 students for a 12-week semester. A thematic analysis of 8,000 usages of CodeAid was performed, further enriched by weekly surveys, and 22 student interviews. We then interviewed eight programming educators to gain further insights. Our findings reveal four design considerations for future educational AI assistants: D1) exploiting AI’s unique benefits; D2) simplifying query formulation while promoting cognitive engagement; D3) avoiding direct responses while encouraging motivated learning; and D4) maintaining transparency and control for students to asses and steer AI responses.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {650},
numpages = {20},
keywords = {AI assistants, AI tutoring, class deployment, design guidelines, educational technology, generative AI, intelligent tutoring systems, large language models, programming education},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.5555/3715622.3715633,
author = {Zuo, Fei and Tompkins, Cody and Qian, Gang and Rhee, Junghwan and Qu, Xianshan and Yang, Bokai},
title = {ChatGPT as an Assembly Language Interpreter for Computing Education},
year = {2024},
issue_date = {October 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {2},
issn = {1937-4771},
abstract = {Assembly language is a low-level programming language useful for a number of important computing areas, such as hardware and embedded systems programming, computer architecture, reverse engineering, and malware analysis. In recent years, generative AI, enhanced by GPT technology, has been widely adopted in the IT industry as well as computing education. However, little work has been done to investigate the applicability of GPT to teaching assembly language. In this paper, we fill in the gap by providing an empirical study of GPT's ability to interpret assembly instructions. In particular, we manually evaluated GPT-4's per-instruction explanations of code segments for four different computer architectures, namely x86, x86-64, ARM, and AArch64. Our study shows that, while inconsistencies and rare errors do exist, GPT's interpretations are highly accurate in general, demonstrating a great potential for such tools to be applied in pedagogical practices for tutoring assembly language.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {73–82},
numpages = {10}
}

@article{10.1145/3705734,
author = {George, Amrita and Storey, Veda Catherine and Hong, Shuguang},
title = {Unraveling the Impact of ChatGPT as a Knowledge Anchor in Business Education},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3705734},
doi = {10.1145/3705734},
abstract = {The emergence of Large Language Models (LLM), such as ChatGPT, is considered a productivity revolution in many areas of business and society. For a classroom setting, especially, it would be useful to understand whether, and how, to incorporate ChatGPT, similar to any other productivity revolution technology, such as calculators or a Google search engine. Although there are concerns regarding the use of LLMs in business education, the positive or negative impact of LLM use is not well-understood. In this research, we examine the substitution and complementarity effects of using ChatGPT in business curricula on learning outcomes and well-being in a socially supportive learning environment. Specifically, we examine whether technology anchors impact students’ goal orientation, learning outcomes, and well-being by conducting an empirical study with students majoring in Information Systems. Our analysis reveals that a technology anchor (computer playfulness) can complement the effects of social support on learning outcomes, while enhancing well-being for simple tasks. Students’ well-being and learning outcomes are hindered by LLM use (specifically, the computer anxiety anchor), substituting social support for simple and difficult tasks. These findings have implications for educational institutions that are assessing how to incorporate LLMs into business curricula.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = feb,
articleno = {4},
numpages = {30},
keywords = {ChatGPT, Large language model (LLM), technology self-efficacy, computer anxiety, goal orientation, computer playfulness, social support, technology anchors, generative AI, knowledge anchor, OpenAI, technology anchors, artificial intelligence (AI), achievement theory}
}

@inproceedings{10.1145/3587102.3588792,
author = {Savelka, Jaromir and Agarwal, Arav and Bogart, Christopher and Song, Yifan and Sakr, Majd},
title = {Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588792},
doi = {10.1145/3587102.3588792},
abstract = {We evaluated the capability of generative pre-trained transformers (GPT), to pass assessments in introductory and intermediate Python programming courses at the postsecondary level. Discussions of potential uses (e.g., exercise generation, code explanation) and misuses (e.g., cheating) of this emerging technology in programming education have intensified, but to date there has not been a rigorous analysis of the models' capabilities in the realistic context of a full-fledged programming course with diverse set of assessment instruments. We evaluated GPT on three Python courses that employ assessments ranging from simple multiple-choice questions (no code involved) to complex programming projects with code bases distributed into multiple files (599 exercises overall). Further, we studied if and how successfully GPT models leverage feedback provided by an auto-grader. We found that the current models are not capable of passing the full spectrum of assessments typically involved in a Python programming course (&lt;70% on even entry-level modules). Yet, it is clear that a straightforward application of these easily accessible models could enable a learner to obtain a non-trivial portion of the overall available score (&gt;55%) in introductory and intermediate courses alike. While the models exhibit remarkable capabilities, including correcting solutions based on auto-grader's feedback, some limitations exist (e.g., poor handling of exercises requiring complex chains of reasoning steps). These findings can be leveraged by instructors wishing to adapt their assessments so that GPT becomes a valuable assistant for a learner as opposed to an end-to-end solution.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {117–123},
numpages = {7},
keywords = {AI code generation, GPT, GitHub copilot, alphacode, codex, generative pre-trained transformers, introductory and intermediate programming, programming knowledge assessment, python course},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3544548.3580919,
author = {Kazemitabaar, Majeed and Chow, Justin and Ma, Carl Ka To and Ericson, Barbara J. and Weintrop, David and Grossman, Tovi},
title = {Studying the effect of AI Code Generators on Supporting Novice Learners in Introductory Programming},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580919},
doi = {10.1145/3544548.3580919},
abstract = {AI code generators like OpenAI Codex have the potential to assist novice programmers by generating code from natural language descriptions, however, over-reliance might negatively impact learning and retention. To explore the implications that AI code generators have on introductory programming, we conducted a controlled experiment with 69 novices (ages 10-17). Learners worked on 45 Python code-authoring tasks, for which half of the learners had access to Codex, each followed by a code-modification task. Our results show that using Codex significantly increased code-authoring performance (1.15x increased completion rate and 1.8x higher scores) while not decreasing performance on manual code-modification tasks. Additionally, learners with access to Codex during the training phase performed slightly better on the evaluation post-tests conducted one week later, although this difference did not reach statistical significance. Of interest, learners with higher Scratch pre-test scores performed significantly better on retention post-tests, if they had prior access to Codex.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {455},
numpages = {23},
keywords = {AI Coding Assistants, AI-Assisted Pair-Programming, ChatGPT, Copilot, GPT-3, Introductory Programming, K-12 Computer Science Education, Large Language Models, OpenAI Codex},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3641554.3701800,
author = {Shah, Anshul and Chernova, Anya and Tomson, Elena and Porter, Leo and Griswold, William G. and Soosai Raj, Adalbert Gerald},
title = {Students' Use of GitHub Copilot for Working with Large Code Bases},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701800},
doi = {10.1145/3641554.3701800},
abstract = {Large language models (LLMs) are already heavily used by professional software engineers. An important skill for new university graduates to possess will be the ability to use such LLMs to effectively navigate and modify a large code base. While much of the prior work related to LLMs in computing education focuses on novice programmers learning to code, less work has focused on how upper-division students use and trust these tools, especially while working with large code bases. In this study, we taught students about various GitHub Copilot features, including Copilot chat, in an upper-division software engineering course and asked students to add a feature to a large code base using Copilot. Our analysis revealed a novel interaction pattern that we call one-shot prompting, in which students ask Copilot to implement the entire feature at once and spend the next few prompts asking Copilot to debug the code or asking Copilot to regenerate its incorrect response. Finally, students reported significantly more trust in the code comprehension features than code generation features of Copilot, perhaps due to the presence of trust affordances in the Copilot chat that are absent in the code generation features. Our study takes the first steps in understanding how upper-division students use Github Copilot so that our instruction can adequately prepare students for a career in software engineering.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1050–1056},
numpages = {7},
keywords = {github copilot, large code bases, program comprehension, trust},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3680533.3697064,
author = {Feng, Tony Haoran and Denny, Paul and W\"{u}nsche, Burkhard C. and Luxton-Reilly, Andrew and Whalley, Jacqueline},
title = {An Eye for an AI: Evaluating GPT-4o's Visual Perception Skills and Geometric Reasoning Skills Using Computer Graphics Questions},
year = {2024},
isbn = {9798400711367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3680533.3697064},
doi = {10.1145/3680533.3697064},
abstract = {CG (Computer Graphics) is a popular field of CS (Computer Science), but many students find this topic difficult due to it requiring a large number of skills, such as mathematics, programming, geometric reasoning, and creativity. Over the past few years, researchers have investigated ways to harness the power of GenAI (Generative Artificial Intelligence) to improve teaching. In CS, much of the research has focused on introductory computing. A recent study evaluating the performance of an LLM (Large Language Model), GPT-4 (text-only), on CG questions, indicated poor performance and reliance on detailed descriptions of image content, which often required considerable insight from the user to return reasonable results. So far, no studies have investigated the abilities of LMMs (Large Multimodal Models), or multimodal LLMs, to solve CG questions and how these abilities can be used to improve teaching.In this study, we construct two datasets of CG questions requiring varying degrees of visual perception skills and geometric reasoning skills, and evaluate the current state-of-the-art LMM, GPT-4o, on the two datasets. We find that although GPT-4o exhibits great potential in solving questions with visual information independently, major limitations still exist to the accuracy and quality of the generated results. We propose several novel approaches for CG educators to incorporate GenAI into CG teaching despite these limitations. We hope that our guidelines further encourage learning and engagement in CG classrooms.},
booktitle = {SIGGRAPH Asia 2024 Educator's Forum},
articleno = {5},
numpages = {8},
keywords = {Large Language Models, LLMs, Large Multimodal Models, LMMs, Visual Language Models, VLMs, Generative Artificial Intelligence, GenAI, GPT-4, GPT-4o, Visual Perception, Geometric Reasoning, Computer Graphics, Computing Education, Evaluation, Assessment},
location = {
},
series = {SA '24}
}

@inproceedings{10.1145/3686852.3686874,
author = {Wang, Ye Diana},
title = {ChatGPT-Assisted ABET Accreditation for BSIT Programs},
year = {2024},
isbn = {9798400711060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686852.3686874},
doi = {10.1145/3686852.3686874},
abstract = {This talk presents practical strategies for facilitating ABET accreditation for BSIT Programs using ChatGPT as a digital assistant.},
booktitle = {Proceedings of the 25th Annual Conference on Information Technology Education},
pages = {129},
numpages = {1},
keywords = {ABET Accreditation, Assessment, BSIT Programs, ChatGPT},
location = {El Paso, TX, USA},
series = {SIGITE '24}
}

@article{10.5555/3665609.3665618,
author = {Sharpe, James S. and Dougherty, Ryan E. and Smith, Sarah J.},
title = {Can ChatGPT Pass a CS1 Python Course?},
year = {2024},
issue_date = {April 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {8},
issn = {1937-4771},
abstract = {In this paper we determine whether an LLM-ChatGPT in this case-can successfully complete the assignments in our CS1 course as if it were a "real" student. Our study contains a two-stage approach, involving reprompts to the LLM in the cases of either not successfully completing the assignment, or using concepts that are more advanced than are taught in our course. We find that LLMs can in fact can either perfectly solve, or almost perfectly solve, every assignment in our CS1 course.},
journal = {J. Comput. Sci. Coll.},
month = apr,
pages = {128–142},
numpages = {15}
}

@inproceedings{10.1145/3637989.3638020,
author = {Freeman, Bradley and Aoki, Kumiko},
title = {ChatGPT in education: A comparative study of media framing in Japan and Malaysia},
year = {2024},
isbn = {9798400708732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637989.3638020},
doi = {10.1145/3637989.3638020},
abstract = {This study examined the media coverage of ChatGPT in the context of education in Japan and Malaysia. Through a thematic analysis of news articles, we identified and analyzed the dominant frames associated with the use of ChatGPT in education. The study found that three frames dominated the coverage: pedagogical, ethical, and assessment. The coverage highlighted the potential benefits of ChatGPT, such as personalized learning and improved assessment processes, as well as concerns around academic integrity, AI bias, and the impact of AI on society. The study also revealed differences in the tone and source usage between Malaysian and Japanese media coverage. The findings have important implications for the development and implementation of emerging educational technologies, emphasizing the need for responsible and ethical use of AI in education.},
booktitle = {Proceedings of the 2023 7th International Conference on Education and E-Learning},
pages = {26–32},
numpages = {7},
keywords = {AI in education, Academic integrity, Media framing, OpenAi},
location = {Tokyo, Japan},
series = {ICEEL '23}
}

@inproceedings{10.1145/3573051.3593393,
author = {Markel, Julia M. and Opferman, Steven G. and Landay, James A. and Piech, Chris},
title = {GPTeach: Interactive TA Training with GPT-based Students},
year = {2023},
isbn = {9798400700255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573051.3593393},
doi = {10.1145/3573051.3593393},
abstract = {Interactive and realistic teacher training is hard to scale. This is a key issue for learning at scale, as inadequate preparation can negatively impact both students and teachers. What if we could make the teacher training experience more engaging and, as a downstream effect, reduce the potential for harm that teachers-in-training could inflict on students? We present GPTeach, an interactive chat-based teacher training tool that allows novice teachers to practice with simulated students. We performed two studies to evaluate GPTeach: one think-aloud study and one A/B test between our tool and a baseline. Participants took the role of a teaching assistant conducting office hours with two GPT-simulated students. We found that our tool provides the opportunity for teachers to get valuable teaching practice without the pressures of affecting real students, allowing them to iterate their responses both during and across sessions. Additionally, participants enjoyed flexibility in tailoring their responses according to the varied personas, needs, and learning goals. In this paper, we provide quantitative results and qualitative observations to inform future work in this area. We conclude with a discussion of actionable design ideas for such systems, as well as other ways to use this tool for evaluating teachers and students. GPTeach has recently been deployed into the teacher training component of an online course with over 800 novice teachers.},
booktitle = {Proceedings of the Tenth ACM Conference on Learning @ Scale},
pages = {226–236},
numpages = {11},
keywords = {GPT-simulated students, scalable teacher training},
location = {Copenhagen, Denmark},
series = {L@S '23}
}

@inproceedings{10.1145/3587102.3588852,
author = {Balse, Rishabh and Valaboju, Bharath and Singhal, Shreya and Warriem, Jayakrishnan Madathil and Prasad, Prajish},
title = {Investigating the Potential of GPT-3 in Providing Feedback for Programming Assessments},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588852},
doi = {10.1145/3587102.3588852},
abstract = {Recent advances in artificial intelligence have led to the development of large language models (LLMs), which are able to generate text, images, and source code based on prompts provided by humans. In this paper, we explore the capabilities of an LLM - OpenAI's GPT-3 model to provide feedback for student written code. Specifically, we examine the feasibility of GPT-3 to check, critique and suggest changes to code written by learners in an online programming exam of an undergraduate Python programming course.We collected 1211 student code submissions from 7 questions asked in a programming exam, and provided the GPT-3 model with separate prompts to check, critique and provide suggestions on these submissions. We found that there was a high variability in the accuracy of the model's feedback for student submissions. Across questions, the range for accurately checking the correctness of the code was between 57% to 79%, between 41% to 77% for accurately critiquing code, and between 32% and 93% for suggesting appropriate changes to the code. We also found instances where the model generated incorrect and inconsistent feedback. These findings suggest that models like GPT-3 currently cannot be 'directly' used to provide feedback to students for programming assessments.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {292–298},
numpages = {7},
keywords = {GPT-3, evaluation, feedback, large language models (LLM), python programming},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3626252.3630875,
author = {Ishizue, Ryosuke and Sakamoto, Kazunori and Washizaki, Hironori and Fukazawa, Yoshiaki},
title = {Improved Program Repair Methods using Refactoring with GPT Models},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630875},
doi = {10.1145/3626252.3630875},
abstract = {Teachers often utilize automatic program repair methods to provide feedback on submitted student code using model answer code. A state-of-the-art tool is Refactory, which achieves a high repair success rate and small patch size (less code repair) by refactoring code to expand the variety of correct code samples that can be referenced. However, Refactory has two major limitations. First, it cannot fix code with syntax errors. Second, it has difficulty fixing code when there are few correct submissions. Herein we propose a new method that combines Refactory and OpenAI's GPT models to address these issues and conduct a performance measurement experiment. The experiment uses a dataset consisting of 5 programming assignment problems and almost 1,800 real-life incorrect Python program submissions from 361 students for an introductory programming course at a large public university. The proposed method improves the repair success rate by 1-21% when the set of correct code samples is sufficient and the patch size is smaller than Refactory alone in 16-45% of the cases. When there was no set of correct code samples at all (only the model answer code was used as a reference for repair), method improves the repair success rate by 1-43% and the patch size is smaller than Refactory alone in 42-68% of the cases.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {569–575},
numpages = {7},
keywords = {generative ai, program repair, programming assignment},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3631802.3631807,
author = {Jeuring, Johan and Groot, Roel and Keuning, Hieke},
title = {What Skills Do You Need When Developing Software Using ChatGPT? (Discussion Paper)},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631807},
doi = {10.1145/3631802.3631807},
abstract = {Since the release of LLM-based tools such as GitHub Copilot and ChatGPT the media and popular scientific literature, but also journals such as the Communications of the ACM, have been flooded with opinions how these tools will change programming. The opinions range from “machines will program themselves”, to “AI does not help programmers”. Of course, these statements are meant to to stir up a discussion, and should be taken with a grain of salt, but we argue that such unfounded statements are potentially harmful. Instead, we propose to investigate which skills are required to develop software using LLM-based tools. In this paper we report on an experiment in which we explore if Computational Thinking (CT) skills predict the ability to develop software using LLM-based tools. Our results show that the ability to develop software using LLM-based tools can indeed be predicted by the score on a CT assessment. There are many limitations to our experiment, and this paper is also a call to discuss how to approach, preferably experimentally, the question of which skills are required to develop software using LLM-based tools. We propose to rephrase this question to include by what kind of people/programmers, to develop what kind of software using what kind of LLM-based tools.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {38},
numpages = {6},
keywords = {ChatGPT, Computational thinking skills, LLM-based tools, Software development skills},
location = {Koli, Finland},
series = {Koli Calling '23}
}

@inproceedings{10.1145/3696230.3696247,
author = {Malaborbor, Rose Ann Caparas and Rivera, Vincent Sulit and Diloy, Marlon A. and De Luna, Leandro R. and Dela Cruz, Aira Leigh Y. and Velasco, Abigail T.},
title = {Enhancing Data Privacy Literacy through Conversational AI: A QandA-Based Approach to Educating Users on the Provisions of the Data Privacy Act of 2012},
year = {2024},
isbn = {9798400717574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696230.3696247},
doi = {10.1145/3696230.3696247},
abstract = {With the advent of the digital age, data privacy has become increasingly relevant, provoking widespread concern and attention. As technology facilitates increased communication and automation in various aspects of life, companies and organizations must protect personal information. The National Privacy Commission (NPC) enforces the Data Privacy Act of 2012, which protects individuals' privacy rights. Despite legislative efforts, public awareness of data privacy remains low, leaving individuals vulnerable to cybercrime. To address this gap, this study proposes the development of a Question Answering (Q&amp;A) system leveraging LangChain technology and llama cpp Large Language Models (LLMs) to educate individuals about the Data Privacy Act of 2012. It proposes a methodology that utilizes document embeddings, vector storage, and semantic search techniques to facilitate efficient Q&amp;A processing. As part of the performance evaluation, BLEU, METEOR, and ROUGE metrics are used. These metrics demonstrate varying degrees of alignment between candidate responses and reference texts. Results show varying levels of alignment between candidate responses and reference texts. This includes notable strengths in addressing certain queries, but also areas for improvement. Overall, the study underscores the importance of leveraging advanced technologies to educate individuals about data privacy rights and responsibilities.},
booktitle = {Proceedings of the 2024 8th International Conference on Digital Technology in Education (ICDTE)},
pages = {271–276},
numpages = {6},
keywords = {BLEU, LangChain technology, METEOR, Q&amp;A System, ROUGE, Retrieval-Augmented Generation (RAG) framework, llama cpp Large Language Models (LLMs)},
location = {
},
series = {ICDTE '24}
}

@inproceedings{10.1145/3587102.3588773,
author = {Denny, Paul and Becker, Brett A. and Leinonen, Juho and Prather, James},
title = {Chat Overflow: Artificially Intelligent Models for Computing Education - renAIssance or apocAIypse?},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588773},
doi = {10.1145/3587102.3588773},
abstract = {Recent breakthroughs in deep learning have led to the emergence of generative AI models that exhibit extraordinary performance at producing human-like outputs. Using only simple input prompts, it is possible to generate novel text, images, video, music, and source code, as well as tackle tasks such as answering questions and translating and summarising text.However, the potential for these models to impact computing education practice is only just beginning to be explored. For example, novices learning to code can now use free tools that automatically suggest solutions to programming exercises and assignments; yet these tools were not designed with novices in mind and little to nothing is known about how they will impact learning. Furthermore, much attention has focused on the immediate challenges these models present, such as academic integrity concerns. It seems that even in the AI-era a pending apocalypse sells better than a promising renaissance.Generative AI will likely play an increasing role in people's lives in the reasonably foreseeable future. Model performance seems set to continue accelerating while novel uses and new possibilities multiply. Given this, we should devote just as much effort to identifying and exploiting new opportunities as we do to identifying and mitigating challenges.In this talk, we begin by discussing several concrete and research-backed opportunities for computing educators. Many of these have already shown great promise in positively impacting current practice. We then discuss more short- to medium-term possibilities in areas such as student recruitment, and curricular changes. Finally - against our better judgement - we speculate over the longer-term, including rethinking the very fundamentals of the practice of teaching introductory and advanced computing courses. In these discussions we suggest potential research questions and directions. Although making remotely accurate predictions in such a fast-changing landscape is foolhardy, we believe that now is the time to explore and embrace opportunities to help make positive change in as many computing classrooms as possible.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {3–4},
numpages = {2},
keywords = {ai, artificial intelligence, chatgpt, computer programming, computer science education, computing education, copilot, deep learning, generative ai, large language models, llm, machine learning},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3641554.3701844,
author = {Yu, Zezhu and Liu, Suqing and Denny, Paul and Bergen, Andreas and Liut, Michael},
title = {Integrating Small Language Models with Retrieval-Augmented Generation in Computing Education: Key Takeaways, Setup, and Practical Insights},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701844},
doi = {10.1145/3641554.3701844},
abstract = {Leveraging a Large Language Model (LLM) for personalized learning in computing education is promising, yet cloud-based LLMs pose risks around data security and privacy. To address these concerns, we developed and deployed a locally stored Small Language Model (SLM) utilizing Retrieval-Augmented Generation (RAG) methods to support computing students' learning. Previous work has demonstrated that SLMs can match or surpass popular LLMs (gpt-3.5-turbo and gpt-4-32k) in handling conversational data from a CS1 course. We deployed SLMs with RAG (SLM + RAG) in a large course with more than 250 active students, fielding nearly 2,000 student questions, while evaluating data privacy, scalability, and feasibility of local deployments. This paper provides a comprehensive guide for deploying SLM + RAG systems, detailing model selection, vector database choice, embedding methods, and pipeline frameworks. We share practical insights from our deployment, including scalability concerns, accuracy versus context length trade-offs, guardrails and hallucination reduction, as well as data privacy maintenance. We address the "Impossible Triangle" in RAG systems, which states that achieving high accuracy, short context length, and low time consumption simultaneously is not feasible. Furthermore, our novel RAG framework, Intelligence Concentration (IC), categorizes information into multiple layers of abstraction within Milvus collections mitigating trade-offs and enabling educational assistants to deliver more relevant and personalized responses to students quickly.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1302–1308},
numpages = {7},
keywords = {computer science education, computing education, conversational agent, intelligence concentration, intelligent tutoring system, large language models, milvus, personalized ai agent, retrieval-augmented generation, small language models},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3649217.3653543,
author = {Bassner, Patrick and Frankford, Eduard and Krusche, Stephan},
title = {Iris: An AI-Driven Virtual Tutor for Computer Science Education},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653543},
doi = {10.1145/3649217.3653543},
abstract = {Integrating AI-driven tools in higher education is an emerging area with transformative potential. This paper introduces Iris, a chat-based virtual tutor integrated into the interactive learning platform Artemis that offers personalized, context-aware assistance in large-scale educational settings. Iris supports computer science students by guiding them through programming exercises and is designed to act as a tutor in a didactically meaningful way. Its calibrated assistance avoids revealing complete solutions, offering subtle hints or counter-questions to foster independent problem-solving skills. For each question, it issues multiple prompts in a Chain-of-Thought to GPT-3.5-Turbo. The prompts include a tutor role description and examples of meaningful answers through few-shot learning. Iris employs contextual awareness by accessing the problem statement, student code, and automated feedback to provide tailored advice. An empirical evaluation shows that students perceive Iris as effective because it understands their questions, provides relevant support, and contributes to the learning process. While students consider Iris a valuable tool for programming exercises and homework, they also feel confident solving programming tasks in computer-based exams without Iris. The findings underscore students' appreciation for Iris' immediate and personalized support, though students predominantly view it as a complement to, rather than a replacement for, human tutors. Nevertheless, Iris creates a space for students to ask questions without being judged by others.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {394–400},
numpages = {7},
keywords = {chatgpt, cs1, education technology, generative ai, interactive learning, large language models, programming exercises},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@article{10.5555/3715638.3715650,
author = {Kwan, Pak},
title = {Supercharging Python Scripting Education with ChatGPT! - How I Use ChatGPT in my Advanced Python Scripting Class},
year = {2024},
issue_date = {September 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {4},
issn = {1937-4771},
abstract = {In recent years, the integration of artificial intelligence (AI) technologies into education has emerged as a promising approach to enhance student learning experiences and outcomes. This tutorial aims to explore how ChatGPT [1] can be effectively utilized to teach advanced Python scripting at the college level. Through interactive demonstrations, discussions, case study and hands-on activities, participants will gain insights into the potential applications of ChatGPT in the classroom and learn practical strategies for integrating this cutting-edge technology into their curriculum.},
journal = {J. Comput. Sci. Coll.},
month = sep,
pages = {35–37},
numpages = {3}
}

@inproceedings{10.1145/3716640.3716649,
author = {Prather, James and Reeves, Brent N and Denny, Paul and Leinonen, Juho and MacNeil, Stephen and Luxton-Reilly, Andrew and Orvalho, Jo\~{a}o and Alipour, Amin and Alfageeh, Ali and Amarouche, Thezyrie and Kimmel, Bailey and Wright, Jared and Blake, Musa and Barbre, Gweneth},
title = {Breaking the Programming Language Barrier: Multilingual Prompting to Empower Non-Native English Learners},
year = {2025},
isbn = {9798400714252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716640.3716649},
doi = {10.1145/3716640.3716649},
abstract = {Non-native English speakers (NNES) face multiple barriers to learning programming. These barriers can be obvious, such as the fact that programming language syntax and instruction are often in English, or more subtle, such as being afraid to ask for help in a classroom full of native English speakers. However, these barriers are frustrating because many NNES students know more about programming than they can articulate in English. Advances in generative AI (GenAI) have the potential to break down these barriers because state of the art models can support interactions in multiple languages. Moreover, recent work has shown that GenAI can be highly accurate at code generation and explanation. In this paper, we provide the first exploration of NNES students prompting in their native languages (Arabic, Chinese, and Portuguese) to generate code to solve programming problems. Our results show that students are able to successfully use their native language to solve programming problems, but not without some difficulty specifying programming terminology and concepts. We discuss the challenges they faced, the implications for practice in the short term, and how this might transform computing education globally in the long term.},
booktitle = {Proceedings of the 27th Australasian Computing Education Conference},
pages = {74–84},
numpages = {11},
keywords = {AI; Artificial Intelligence; Automatic Code Generation; Codex; Copilot; CS1; GenAI; GitHub; GPT; GPT-4; ChatGPT; HCI; Introductory Programming; Large Language Models; LLM; Non-Native English Speakers; Novice Programming; OpenAI; Prompt Problems},
location = {
},
series = {ACE '25}
}

@article{10.5555/3722479.3722526,
author = {Xie, Jingnan},
title = {Improving Introductory Java Programming Education Through ChatGPT},
year = {2024},
issue_date = {October 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {3},
issn = {1937-4771},
abstract = {The realm of introductory computer science (CS) education is swiftly changing, as educators actively pursue inventive strategies to captivate and empower students. This manuscript introduces a fresh methodology for teaching CS1 or CS2 courses, concentrating specifically on the fundamental principles of Java programming. Harnessing the capabilities of ChatGPT, an AI language model, we delve into how integrating conversational AI into the classroom milieu can foster a more dynamic and tailored learning journey. By furnishing a platform for students to pose inquiries, seek elucidation, and promptly receive feedback, ChatGPT functions as a virtual mentor, complementing conventional teaching methodologies. We scrutinize the potential repercussions of this approach on student learning outcomes (SLOs) and juxtapose it with traditional classroom paradigms. Furthermore, we deliberate on the ramifications of employing AI in education and its contribution to molding the trajectory of introductory programming courses.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {140–150},
numpages = {11}
}

@inproceedings{10.1145/3649217.3653575,
author = {Smith, C. Estelle and Shiekh, Kylee and Cooreman, Hayden and Rahman, Sharfi and Zhu, Yifei and Siam, Md Kamrul and Ivanitskiy, Michael and Ahmed, Ahmed M. and Hallinan, Michael and Grisak, Alexander and Fierro, Gabe},
title = {Early Adoption of Generative Artificial Intelligence in Computing Education: Emergent Student Use Cases and Perspectives in 2023},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653575},
doi = {10.1145/3649217.3653575},
abstract = {Because of the rapid development and increasing public availability of Generative Artificial Intelligence (GenAI) models and tools, educational institutions and educators must immediately reckon with the impact of students using GenAI. There is limited prior research on computing students' use and perceptions of GenAI. In anticipation of future advances and evolutions of GenAI, we capture a snapshot of student attitudes towards and uses of yet emerging GenAI, in a period of time before university policies had reacted to these technologies. We surveyed all computer science majors in a small engineering-focused R1 university in order to: (1) capture a baseline assessment of how GenAI has been immediately adopted by aspiring computer scientists; (2) describe computing students' GenAI-related needs and concerns for their education and careers; and (3) discuss GenAI influences on CS pedagogy, curriculum, culture, and policy. We present an exploratory qualitative analysis of this data and discuss the impact of our findings on the emerging conversation around GenAI and education.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {3–9},
numpages = {7},
keywords = {ai literacy, code generator, education, generative artificial intelligence, image generator, interactive tutoring, large language model, policy, student experience, survey},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3626253.3631657,
author = {Akram, Bita and Leinonen, Juho and Norouzi, Narges and Prather, James and Zhang, Lisa},
title = {AI in Computing Education from Research to Practice},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3631657},
doi = {10.1145/3626253.3631657},
abstract = {The panel comprises a diverse set of Computing educators working on AI in education. The panelists will address four areas of AI in Computing education: 1) AI for introductory CS classrooms, 2) Investigating opportunities presented by LLMs, 3) LLM-based tool development, and 4) Ethics and inclusion in AI curriculum. The panel will share experiences and discuss opportunities and challenges in AI education with the community.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1521–1522},
numpages = {2},
keywords = {artificial intelligence, computing education, large language models},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3631700.3665227,
author = {Fenu, Gianni and Galici, Roberta and Marras, Mirko and Reforgiato, Diego},
title = {Exploring Student Interactions with AI in Programming Training},
year = {2024},
isbn = {9798400704666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631700.3665227},
doi = {10.1145/3631700.3665227},
abstract = {In recent years, the integration of artificial intelligence (AI) in education has collected significant attention due to its potential to revolutionize learning experiences and support student skill development. This study delves into the dynamics of student interactions with AI support within the domain of C programming education, with a specific focus on the utilization of ChatGPT, a conversational AI model, during training sessions. Through manual clustering analysis, this research unveils distinct patterns of student engagement, elucidating diverse problem-solving approaches and varying levels of interaction with ChatGPT. Our findings underscore the importance of acknowledging individual differences in learning strategies and preferences, highlighting the necessity for personalized educational interventions tailored to meet the diverse needs of learners. However, despite the strides made in AI-supported learning, gaps persist in the existing literature, particularly concerning our understanding of how students approach prompts and exercises when utilizing AI-driven educational tools. This research aims to address this gap by shedding light on the nuanced dynamics of student-AI interactions during training of C programming, offering insights into effective pedagogical strategies and instructional design principles for integrating AI technologies into educational settings. This study makes a significant contribution to the continuous endeavors of educators and AI developers by furthering the discussion on AI-facilitated learning. It aims to enhance student engagement, learning outcomes, and overall educational experiences through the integration of technology into learning environments.},
booktitle = {Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {555–560},
numpages = {6},
keywords = {AI Assistance, ChatGPT, Large Language Models, Learning Strategies, Learning Support Systems, Programming Education},
location = {Cagliari, Italy},
series = {UMAP Adjunct '24}
}

@inproceedings{10.1145/3641554.3701873,
author = {Thorgeirsson, Sverrir and Ewen, Tracy and Su, Zhendong},
title = {What Can Computer Science Educators Learn From the Failures of Top-Down Pedagogy?},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701873},
doi = {10.1145/3641554.3701873},
abstract = {While educational researchers in various disciplines are grappling with how to develop policies and pedagogical approaches that address the use of generative artificial intelligence, the challenge is particularly complex in computer science education where the new technology is changing the core of the field. In this paper, we take a look at the pedagogy of other subjects with a longer history than computer science and a more extensive body of educational research to collect insights on how this challenge can be met. We begin by drawing from recent neurological research to find domains that share cognitive commonalities with computer programming and then build upon comparisons that others have made to literacy and mathematics education. We then consider how the "reading wars" and "math wars" have shaped these fields, which we see as conflicts between less effective top-down pedagogy and more effective bottom-up pedagogy, and reflect on what would be comparable approaches in teaching computing. We find that approaches that make heavy use of large language models without teaching fundamentals can be compared to the top-down pedagogy of reading and mathematics and are likely to be ineffective on their own. Therefore, we advise against the exclusive use of such approaches with novices. However, we also acknowledge that the social science surrounding computer science education is complex and that effectiveness only tells a part of the story, with other factors such as engagement, motivation and social dynamics also being important.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1127–1133},
numpages = {7},
keywords = {bottom-up pedagogy, computer science education, generative artificial intelligence, large language models, literacy, math wars, phonics, position paper, reading, reading wars, top-down pedagogy, whole language},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3686852.3686886,
author = {Thorat, Sahil and Zheng, Yong and Jacob Varghese, Vivian and Volkova, Anette},
title = {Designing a FAQ Chatbot to Enhance Faculty Support},
year = {2024},
isbn = {9798400711060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686852.3686886},
doi = {10.1145/3686852.3686886},
abstract = {Grant applications constitute a fundamental responsibility of research faculty within a university setting. Faculty members frequently encounter queries pertaining to grant applications and award management, e.g., procedural guidelines and post-award issues. While FAQs offer general responses to common questions, they often fall short in addressing specific or complex issues unique to individual inquiries. In this paper, we propose and develop a FAQ chatbot utilizing cutting-edge information retrieval to facilitate faculty inquiries, enhance operational efficiency, and contribute to the advancement of research initiatives at our university.},
booktitle = {Proceedings of the 25th Annual Conference on Information Technology Education},
pages = {147–150},
numpages = {4},
keywords = {FAQ, chatbot, grant, information retrieval, large language models},
location = {El Paso, TX, USA},
series = {SIGITE '24}
}

@article{10.1145/3688090,
author = {Padiyath, Aadarsh},
title = {Do I Have a Say in This, or Has ChatGPT Already Decided for Me?},
year = {2024},
issue_date = {Fall 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/3688090},
doi = {10.1145/3688090},
abstract = {It's not just about LLMs, it's about us too.},
journal = {XRDS},
month = oct,
pages = {52–55},
numpages = {4}
}

@inproceedings{10.1145/3641555.3705107,
author = {Fox, Armando and Fern\'{a}ndez, Pablo and Leinonen, Juho and Parejo Maestre, Jos\'{e} Antonio},
title = {Using Generative AI to Scaffold the Teaching of Software Engineering Team Skills},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705107},
doi = {10.1145/3641555.3705107},
abstract = {Most of the attention on GenAI in computing education has focused on programming-centric tasks, such as code generation, giving feedback on code, or providing synthetic programming partners. Yet in advanced software engineering and project courses, interpersonal skills such as team meetings or customer interviews are equally important but difficult and instructor-intensive to teach realistically. GenAI presents the possibility of scaffolding the teaching of some of these practices by enabling exercises in which students develop the ability to investigate a topic by iteratively asking questions to find a solution. The goal is to create scenarios in which students train to interact with humans in real-world situations, simulating these interactions in a controlled, guided environment. These simulations could help students practice and refine ''soft skills,'' such as teamwork and interviewing, by mimicking the types of exchanges and problem-solving they would encounter in professional environments. This approach allows learners to engage in realistic communication exercises, improving their ability to handle complex, interpersonal tasks through repeated practice with AI-guided feedback. As an example, we envision examples that include requirements elicitation with customers, development team meetings, and discussion with potential investors, to name just a few.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1719},
numpages = {1},
keywords = {computing education, empirical studies, experimentation, generative artificial intelligence, large language models, natural language generation, requirements analysis, requirements elicitation},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3663384.3663393,
author = {Feldman, Molly Q and Anderson, Carolyn Jane},
title = {Non-Expert Programmers in the Generative AI Future},
year = {2024},
isbn = {9798400710179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663384.3663393},
doi = {10.1145/3663384.3663393},
abstract = {Generative AI is rapidly transforming the practice of programming. At the same time, our understanding of who writes programs, for what purposes, and how they program, has been evolving. By facilitating natural-language-to-code interactions, large language models for code have the potential to open up programming work to a broader range of workers. While existing work finds productivity benefits for expert programmers, interactions with non-experts are less well-studied. In this paper, we consider the future of programming for non-experts through a controlled study of 67 non-programmers. Our study reveals multiple barriers to effective use of large language models of code for non-experts, including several aspects of technical communication. Comparing our results to a prior study of beginning programmers illuminates the ways in which a traditional introductory programming class does and does not equip students to effectively work with generative AI. Drawing on our empirical findings, we lay out a vision for how to empower non-expert programmers to leverage generative AI for a more equitable future of programming.},
booktitle = {Proceedings of the 3rd Annual Meeting of the Symposium on Human-Computer Interaction for Work},
articleno = {15},
numpages = {19},
keywords = {CS1, Code LLMs, Generative AI, mixed methods, non-experts},
location = {Newcastle upon Tyne, United Kingdom},
series = {CHIWORK '24}
}

@inproceedings{10.1145/3593342.3593360,
author = {Rajabi, Parsa and Taghipour, Parnian and Cukierman, Diana and Doleck, Tenzin},
title = {Exploring ChatGPT’s impact on post-secondary education: A qualitative study},
year = {2023},
isbn = {9798400707896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593342.3593360},
doi = {10.1145/3593342.3593360},
abstract = {As Chat Generative Pre-trained Transformer (ChatGPT) gains traction, its impact on post-secondary education is increasingly being debated. This qualitative study explores the perception of students and faculty members at a research university in Canada regarding ChatGPT’s use in a post-secondary setting, focusing on how it could be incorporated and what ways instructors can respond to this technology. We present the summary of a discussion that took place in a two-hour focus group session with 40 participants from the computer science and engineering departments, and highlight issues surrounding plagiarism, assessment methods, and the appropriate use of ChatGPT. Findings suggest that students are likely to use ChatGPT, but there is a need for specific guidelines, more classroom assessments, and mandatory reporting of ChatGPT use. The study contributes to the emergent research on ChatGPT in higher education and emphasizes the importance of proactively addressing challenges and opportunities associated with ChatGPT adoption and use.},
booktitle = {Proceedings of the 25th Western Canadian Conference on Computing Education},
articleno = {9},
numpages = {6},
keywords = {post-secondary, higher education, education, conversational AI, assessment, ChatGPT, Artificial Intelligence in education},
location = {Vancouver, BC, Canada},
series = {WCCCE '23}
}

@article{10.1145/3737885,
author = {Brown, Neil C. C. and Weill-Tessier, Pierre and Leinonen, Juho and Denny, Paul and K\"{o}lling, Michael},
title = {Howzat? Appealing to Expert Judgement for Evaluating Human and AI Next-Step Hints for Novice Programmers},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3737885},
doi = {10.1145/3737885},
abstract = {Motivation: Students learning to program often reach states where they are stuck and can make no forward progress – but this may be outside the classroom where no instructor is available to help. In this situation, an automatically generated next-step hint can help them make forward progress and support their learning. It is important to know what makes a good hint or a bad hint, and how to generate good hints automatically in novice programming tools, for example using Large Language Models (LLMs).Method and participants: We recruited 44 Java educators from around the world to participate in an online study. We used a set of real student code states as hint-generation scenarios. Participants used a technique known as comparative judgement to rank a set of candidate next-step Java hints, which were generated by Large Language Models (LLMs) and by five human experienced educators. Participants ranked the hints without being told how they were generated. The hints were generated with no explicit detail given to the LLMs/humans on what the target task was. Participants then filled in a survey with follow-up questions. The ranks of the hints were analysed against a set of extracted hint characteristics using a random forest approach.Findings: We found that LLMs had considerable variation in generating high quality next-step hints for programming novices, with GPT-4 outperforming other models tested. When used with a well-designed prompt, GPT-4 outperformed human experts in generating pedagogically valuable hints. A multi-stage prompt was the most effective LLM prompt. According to a fitted random forest model, the two most important factors of a good hint were length (80–160 words being best), and reading level (US grade nine or below being best). Offering alternative approaches to solving the problem was considered bad, and we found no effect of sentiment.Conclusions: Automatic generation of these hints is immediately viable, given that LLMs outperformed humans – even when the students’ task is unknown. Hint length and reading level were more important than several pedagogical features of hints. The fact that it took a group of experts several rounds of experimentation and refinement to design a prompt that achieves this outcome suggests that students on their own are unlikely to be able to produce the same benefit. The prompting task, therefore, should be embedded in an expert-designed tool.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Educ.},
month = may,
keywords = {LLMs, AI, Java, Next-step hints, comparative judgement}
}

@inproceedings{10.1145/3626253.3635369,
author = {MacNeil, Stephen and Leinonen, Juho and Denny, Paul and Kiesler, Natalie and Hellas, Arto and Prather, James and Becker, Brett A. and Wermelinger, Michel and Reid, Karen},
title = {Discussing the Changing Landscape of Generative AI in Computing Education},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635369},
doi = {10.1145/3626253.3635369},
abstract = {In a previous Birds of a Feather discussion, we delved into the nascent applications of generative AI, contemplating its potential and speculating on future trajectories. Since then, the landscape has continued to evolve revealing the capabilities and limitations of these models. Despite this progress, the computing education research community still faces uncertainty around pivotal aspects such as (1) academic integrity and assessments, (2) curricular adaptations, (3) pedagogical strategies, and (4) the competencies students require to instill responsible use of these tools. The goal of this Birds of a Feather discussion is to unravel these pressing and persistent issues with computing educators and researchers, fostering a collaborative exploration of strategies to navigate the educational implications of advancing generative AI technologies. Aligned with this goal of building an inclusive learning community, our BoF is led by globally distributed leaders to facilitate multiple coordinated discussions that can lead to a broader conversation about the role of LLMs in CS education.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1916},
numpages = {1},
keywords = {academic integrity, assessment, computing education, curriculum, large language models, pedagogy},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3691720.3691728,
author = {Qin, Yuqun and Zeng, Lizhi and Wei, Jieshu and Hu, Yani and Wu, Wenli and Wang, Hui},
title = {Integrating ChatGPT into Human Morphology and Curriculum Ideology and Politic: Enhancing Learning and Engagement},
year = {2024},
isbn = {9798400710230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691720.3691728},
doi = {10.1145/3691720.3691728},
abstract = {In order to better play the role of ChatGPT in the teaching of human morphology and structure and cultivate more high-quality skilled nursing talents, this study combines ChatGPT and Xueyin Online for blended teaching from the perspective of Curriculum Ideology and Politics. A total of 102 students from Class 1 and Class 2 of 2023 were enrolled. We established a t-test model, and used SPSS for data measurement and statistical analysis. Nursing class 2 was the control group, and the traditional teaching method was adopted. The experimental group of nursing class 1, on the basis of the control group, combined with ChatGPT for online and offline blended teaching. Finally, the theoretical and experimental scores of the two groups were compared, the comprehensive literacy and satisfaction levels of the students was evaluated by questionnaire survey. The theoretical and practical scores of the experimental group were higher than those of the control group (P&lt;0.05). Both the comprehensive literacy evaluation score and satisfaction levels were better than that of the control group (P&lt;0.05). The online and offline hybrid teaching of ChatGPT based on the concept of "big ideology and politics" is significantly better than that of traditional teaching. ChatGPT provides teachers and students with a new type of AI-assisted tool to improve teaching effectiveness.},
booktitle = {Proceedings of the 2nd International Conference on Educational Knowledge and Informatization},
pages = {40–45},
numpages = {6},
location = {Shanghai, China},
series = {EKI '24}
}

@inproceedings{10.1145/3638067.3638100,
author = {Freire, Andr\'{e} Pimenta and Cardoso, Paula Christina Figueira and Salgado, Andr\'{e} de Lima},
title = {May We Consult ChatGPT in Our Human-Computer Interaction Written Exam? An Experience Report After a Professor Answered Yes},
year = {2024},
isbn = {9798400717154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638067.3638100},
doi = {10.1145/3638067.3638100},
abstract = {Using ChatGPT in education presents challenges for evaluating students. It requires distinguishing between original ideas and those generated by the model, assessing critical thinking skills, and gauging subject mastery accurately, which can impact fair assessment practices. The Human-Computer Interaction course described in this experience report has enabled consultation with textbooks, slides and other materials for over five years. This experience report describes reflections regarding using ChatGPT as a source of consultation in a written HCI exam in 2023. The paper describes experiences with analysis of the types of questions ChatGPT was able to solve immediately without mediation and the types of questions that could benefit from ChatGPT’s assistance without compromising the assessment of higher-level learning outcomes that professors want to analyse in teaching HCI. The paper uses Bloom’s taxonomy to analyse different questions and abilities to be evaluated and how they can be solved solely by using ChatGPT. The paper discusses questions that need mediation, previous lived experience in class and understanding of the knowledge acquired in class that cannot be answered directly by copying and pasting questions into ChatGPT. The discussions can raise reflections on the learning outcomes that can be assessed in HCI written exams and how professors should reflect upon their experiences and expectations for exams in the age of growing generative artificial intelligence resources.},
booktitle = {Proceedings of the XXII Brazilian Symposium on Human Factors in Computing Systems},
articleno = {6},
numpages = {11},
keywords = {ChatGPT, HCI education, evaluation, open-book exams},
location = {Macei\'{o}, Brazil},
series = {IHC '23}
}

@inproceedings{10.1145/3649217.3653541,
author = {Menezes, Tyler and Egherman, Lola and Garg, Nikhil},
title = {AI-Grading Standup Updates to Improve Project-Based Learning Outcomes},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653541},
doi = {10.1145/3649217.3653541},
abstract = {Integrating project-based learning (such as class projects, capstones, or internships) into a Computer Science degree helps students apply what they have learned in lectures and homework. Often, these projects involve group work, but ensuring all students are contributing and learning equally is a large challenge.This experience report describes how we built and used a chatbot to collect and publish brief answers to the "standup" questions used daily in the technology industry: "What did you do yesterday? What are you doing today? Is anything blocking you?". We share the rubric we used to grade these updates, as well as how we trained an AI tool to perform this work for us.This tool was used by several hundred students from US-based Career and Technical Colleges (CTCs) and non-R1 universities, who were largely underrepresented in Computer Science. We found that collecting and publishing standups reduced the number of students who did not significantly contribute. We also found that scoring the standups according to a rubric helped identify and reach out to any students who were still not contributing, which helped some students but not others. Finally, we found that an AI tool can be used to evaluate student updates at scale helping under-resourced schools and over-worked faculty to implement this program in their classrooms.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {17–23},
numpages = {7},
keywords = {AI, capstone, internships, large language models (LLM), project-based learning},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3641555.3705102,
author = {Novak, Ed and Ohmann, Peter and Reckinger, Scott and Reckinger, Shanon},
title = {Oral Exams in Computer Science Education Amidst ChatGPT Dependency},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705102},
doi = {10.1145/3641555.3705102},
abstract = {Oral exams provide a compelling alternative to traditional evaluation methods, expanding or replacing traditional written work. Interest in oral exams is growing rapidly in computer science (CS) education due to shifts to remote learning and concerns around AI-supported programming. This Birds of a Feather (BoF) session is broadly applicable to many in the CS education community, whether they have previously tried oral exams, have concerns about the use of oral exams in CS education, or are curious to hear more about how oral exams might work. The BoF session will provide a forum to discover and discuss previous approaches to oral exams, dive into common themes of interest in small groups, and collectively identify promising future directions for oral exams in CS courses.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1725},
numpages = {1},
keywords = {assessment, large language models, oral exams, remote learning},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3664934.3664946,
author = {Xiao, Qimin},
title = {ChatGPT as an Artificial Intelligence (AI) Writing Assistant for EFL Learners: An Exploratory Study of its Effects on English writing Proficiency},
year = {2024},
isbn = {9798400716409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664934.3664946},
doi = {10.1145/3664934.3664946},
abstract = {ChatGPT, a revolutionary Artificial Intelligence (AI) tool, has taken the world by storm after its release by OpenAI in 2022. Many researchers have attempted to explore what role ChatGPT can play as an AI assistant in teaching and to what extent ChatGPT can be utilized for English as a foreign language (EFL) learners. The present study made exploratory efforts in shedding lights on the effect of applying ChatGPT as an AI Writing Assistant in English writing classroom for EFL learners. The research was conducted with 51 EFL learners divided into a control group (n=25) and an experimental group (n=26). The control group was given traditional in-class instruction and after-class activities, while the experimental group was encouraged to use ChatGPT during pre-writing stage and after-writing stage for content planning, personized interaction and tailored feedback. To avoid misuse of this AI tool, students are encouraged to do the drafting on their own. During the experiment (a duration of 10 weeks), writing tasks were employed to collect available data. It was found that the experimental group exhibited better writing proficiency in terms of content, structure and language use, compared with that of the control group. Overall, this research highlights the potentiality of ChatGPT as a valuable AI tool for EFL learners to improve English writing proficiency.},
booktitle = {Proceedings of the 2024 9th International Conference on Information and Education Innovations},
pages = {51–56},
numpages = {6},
keywords = {Artificial Intelligence (AI) Writing Assistant, ChatGPT integration, EFL learners, Writing proficiency},
location = {Verbania, Italy},
series = {ICIEI '24}
}

@inproceedings{10.1145/3626252.3630958,
author = {Cambaz, Doga and Zhang, Xiaoling},
title = {Use of AI-driven Code Generation Models in Teaching and Learning Programming: a Systematic Literature Review},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630958},
doi = {10.1145/3626252.3630958},
abstract = {The recent emergence of LLM-based code generation models can potentially transform programming education. To pinpoint the current state of research on using LLM-based code generators to support the teaching and learning of programming, we conducted a systematic literature review of 21 papers published since 2018. The review focuses on (1) the teaching and learning practices in programming education that utilized LLM-based code generation models, (2) characteristics and (3) performance indicators of the models, and (4) aspects to consider when utilizing the models in programming education, including the risks and challenges. We found that the most commonly reported uses of LLM-based code generation models for teachers are generating assignments and evaluating student work, while for students, the models function as virtual tutors. We identified that the models exhibit accuracy limitations; generated content often contains minor errors that are manageable by instructors but pose risks for novice learners. Moreover, risks such as academic misconduct and over-reliance on the models are critical when considering integrating these models into education. Overall, LLM-based code generation models can be an assistive tool for both learners and instructors if the risks are mitigated.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {172–178},
numpages = {7},
keywords = {artificial intelligence in education, code generation models, large language models, programming education, systematic review},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3626253.3633433,
author = {Liu, Rongxin and Zenke, Carter and Lloyd, Doug and Malan, David J.},
title = {Teaching with AI (GPT)},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3633433},
doi = {10.1145/3626253.3633433},
abstract = {Teaching computer science at scale can be challenging. From our experience in CS50, Harvard University's introductory course, we've seen firsthand the impactful role that generative artificial intelligence can play in education. Recognizing its potential and stakes, we integrated OpenAI's GPT into our own teaching methodology. The goal was to emulate a 1:1 teacher-to-student ratio, incorporating "pedagogical guardrails" to maintain instructional integrity. The result was a personalized, AI-powered bot in the form of a friendly rubber duck aimed at delivering instructional responses and troubleshooting without giving outright solutions. We plan to share our journey and offer insights into responsibly harnessing AI in educational settings. Participants will gain hands-on experience working with GPT through OpenAI's APIs, understanding and crafting prompts, answering questions using embedding-based search, and finally, building their own AI chatbot. Ultimately, we'll not only share lessons learned from our own approach but also equip educators hands-on with the knowledge and tools with which they, too, can implement these technologies in their unique teaching environments.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1902},
numpages = {1},
keywords = {ai, artificial intelligence, chatgpt, ethics, generative ai, gpt, programming, prompt, prompt engineering},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3664934.3664945,
author = {Haq, Muhammad Zia Ul and Cao, Guangming and Abukhait, Rawan},
title = {Understanding Students'Attitudes and Behavioral Intentions Towards Using ChatGPT},
year = {2024},
isbn = {9798400716409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664934.3664945},
doi = {10.1145/3664934.3664945},
abstract = {While the release of ChatGPT has sparked heated polarized debates in the education sector, there is no academic research on students’ attitudes and behavioral intentions regarding ChatGPT usage. To fill this gap, we employed the integrated AI acceptance-avoidance model (IAAAM) to investigate the impact of positive and negative factors on students’ attitudes and intentions toward ChatGPT. Our empirical results indicate that IAAAM offers a comprehensive understanding and prediction of students’ attitudes and intentions related to ChatGPT. This study advances our conceptual and empirical understanding of students’ attitudes and intentions regarding using ChatGPT. It also provides valuable implications for education policymakers, educators, and students by highlighting the importance of maintaining a balanced approach to the use of ChatGPT.},
booktitle = {Proceedings of the 2024 9th International Conference on Information and Education Innovations},
pages = {44–50},
numpages = {7},
location = {Verbania, Italy},
series = {ICIEI '24}
}

@inproceedings{10.1145/3587102.3588827,
author = {Malinka, Kamil and Peres\'{\i}ni, Martin and Firc, Anton and Hujn\'{a}k, Ondrej and Janus, Filip},
title = {On the Educational Impact of ChatGPT: Is Artificial Intelligence Ready to Obtain a University Degree?},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588827},
doi = {10.1145/3587102.3588827},
abstract = {In late 2022, OpenAI released a new version of ChatGPT, a sophisticated natural language processing system capable of holding natural conversations while preserving and responding to the context of the discussion. ChatGPT has exceeded expectations in its abilities, leading to extensive considerations of its potential applications and misuse. In this work, we evaluate the influence of ChatGPT on university education, with a primary focus on computer security-oriented specialization. We gather data regarding the effectiveness and usability of this tool for completing exams, programming assignments, and term papers. We evaluate multiple levels of tool misuse, ranging from utilizing it as a consultant to simply copying its outputs. While we demonstrate how easily ChatGPT can be used to cheat, we also discuss the potentially significant benefits to the educational system. For instance, it might be used as an aid (assistant) to discuss problems encountered while solving an assignment or to speed up the learning process. Ultimately, we discuss how computer science higher education should adapt to tools like ChatGPT.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {47–53},
numpages = {7},
keywords = {ChatGPT, academic education, artificial intelligence, computer security, virtual assistant},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3649165.3690107,
author = {Lim, Yumi Chin Yin and Weng, Kai},
title = {Automated Coding Challenges Assembly Using Pre-trained Programming Language Models},
year = {2024},
isbn = {9798400705984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649165.3690107},
doi = {10.1145/3649165.3690107},
abstract = {In programming language courses, many online platforms already feature extensive question banks. When teachers prepare course exercises, they need to select several programming problems of equivalent difficulty from the vast existing question bank so that different students can answer different but similar questions. These chosen problems are then assembled into a question pool, from which each student is randomly assigned a programming problem to complete. However, manually selecting problems and forming the pool can be extremely time-consuming due to the sheer volume of available problems. To address this, we propose an Automated Coding Challenge Assembly strategy to help teachers automatically identify similar yet distinct problems from large question banks. By analyzing the similarities in the code submitted by students, our method enables the automatic assembly of programming problems with equivalent difficulty levels. Experimental results have shown that the programming problems selected by this automated method exhibit high relevance and consistent difficulty, providing practical proof of our algorithm's effectiveness.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1},
pages = {123–129},
numpages = {7},
keywords = {automated coding challenges assembly, code embedding, codet5+, large language model, similarity},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@inproceedings{10.1145/3695080.3695150,
author = {Zhang, Huichen},
title = {ChatGPT intervenes in the application analysis of higher education classrooms},
year = {2024},
isbn = {9798400710223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3695080.3695150},
doi = {10.1145/3695080.3695150},
abstract = {As the frontier of a new round of technological revolution, ChatGPT is playing an active role in creating intelligent teaching classrooms, improving students' self-directed learning ability, and reshaping the evaluation of classroom teaching in higher education. However, in the process of ChatGPT's intervention in higher education classrooms, there are many shortcomings, such as the difficulty in distinguishing between true and false answers, weakening students' ability to think independently, and diluting the relationship between teachers and students. Therefore, it is suggested to start with strategies such as promoting the change of teachers' teaching concepts, improving students' critical awareness, and strengthening the supervision of school rules, so as to provide a useful reference for ChatGPT to properly intervene in higher education classrooms in the future.},
booktitle = {Proceedings of the 2024 International Conference on Cloud Computing and Big Data},
pages = {409–413},
numpages = {5},
location = {Dali, China},
series = {ICCBD '24}
}

@inproceedings{10.1145/3699538.3699556,
author = {Birillo, Anastasiia and Artser, Elizaveta and Potriasaeva, Anna and Vlasov, Ilya and Dzialets, Katsiaryna and Golubev, Yaroslav and Gerasimov, Igor and Keuning, Hieke and Bryksin, Timofey},
title = {One Step at a Time: Combining LLMs and Static Analysis to Generate Next-Step Hints for Programming Tasks},
year = {2024},
isbn = {9798400710384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3699538.3699556},
doi = {10.1145/3699538.3699556},
abstract = {Students often struggle with solving programming problems when learning to code, especially when they have to do it online, with one of the most common disadvantages of working online being the lack of personalized help. This help can be provided as next-step hint generation, i.e., showing a student what specific small step they need to do next to get to the correct solution. There are many ways to generate such hints, with large language models (LLMs) being among the most actively studied right now. While LLMs constitute a promising technology for providing personalized help, combining them with other techniques, such as static analysis, can significantly improve the output quality. In this work, we utilize this idea and propose a novel system to provide both textual and code hints for programming tasks. The pipeline of the proposed approach uses a chain-of-thought prompting technique and consists of three distinct steps: (1) generating subgoals — a list of actions to proceed with the task from the current student’s solution, (2) generating the code to achieve the next subgoal, and (3) generating the text to describe this needed action. During the second step, we apply static analysis to the generated code to control its size and quality. The tool is implemented as a modification to the open-source JetBrains Academy plugin, supporting students in their in-IDE courses. To evaluate our approach, we propose a list of criteria for all steps in our pipeline and conduct two rounds of expert validation. Finally, we evaluate the next-step hints in a classroom with 14 students from two universities. Our results show that both forms of the hints — textual and code — were helpful for the students, and the proposed system helped them to proceed with the coding tasks.},
booktitle = {Proceedings of the 24th Koli Calling International Conference on Computing Education Research},
articleno = {9},
numpages = {12},
keywords = {Programming Education, in-IDE learning, LLMs, Generative AI, Next-Step Hints},
location = {
},
series = {Koli Calling '24}
}

@inproceedings{10.1145/3641555.3705282,
author = {\v{R}echt\'{a}\v{c}kov\'{a}, Anna and Maximova, Alexandra and Pitts, Griffin},
title = {Finding Misleading Identifiers in Novice Code Using LLMs},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705282},
doi = {10.1145/3641555.3705282},
abstract = {Clear, well-chosen names for variables and functions significantly enhance code readability and maintainability. In computer science education, teaching students to select appropriate identifiers is a critical task, especially in CS1. This study explores how large language models (LLMs) could assist in teaching this skill. While prior research has explored the use of LLMs in programming education, their precision and consistency in teaching code quality, particularly identifier selection, remains largely unexplored. For this purpose, this study investigated how well different LLMs can detect and report misleading identifiers. In a dataset of 33 code samples, we manually labeled misleading identifiers. On this dataset, we then tested five different LLMs on their ability to detect these misleading identifiers, measuring the overall accuracy, precision, recall, and f-score. Results revealed that the most successful model, GPT-4o, was able to correctly detect most of the manually flagged misleading variable names. However, it also tended to flag issues with variable identifiers in cases where the human evaluators would not, and refined prompting was not able to discourage this behavior.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1595–1596},
numpages = {2},
keywords = {automated feedback, code quality, misleading identifiers, novice programmers},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3641555.3705244,
author = {Yang, Yoonseok and Liu, Jack and Zamfirescu-Pereira, J.D. and DeNero, John},
title = {Pensieve Discuss: Scalable Small-Group CS Tutoring System with AI},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705244},
doi = {10.1145/3641555.3705244},
abstract = {Small-group tutoring in Computer Science (CS) is effective, but presents the challenge of providing a dedicated tutor for each group and encouraging collaboration among group members at scale. We present Pensieve Discuss, a software platform that integrates synchronous editing for scaffolded programming problems with online human and AI tutors, designed to improve student collaboration and experience during group tutoring sessions. Our semester-long deployment to CS61A at UC Berkeley demonstrated consistently high collaboration rates, positive feedback about the AI tutor's helpfulness, and increased satisfaction with the group tutoring experience. The use of our system was preferred over an interface lacking AI tutors and synchronous editing capabilities. Our experiences suggest that small-group tutoring sessions are an important avenue for future research in educational AI.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1669–1670},
numpages = {2},
keywords = {AI tutors, collaborative learning, large language models, small-group tutoring},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3613904.3642706,
author = {Nguyen, Sydney and Babe, Hannah McLean and Zi, Yangtian and Guha, Arjun and Anderson, Carolyn Jane and Feldman, Molly Q},
title = {How Beginning Programmers and Code LLMs (Mis)read Each Other},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642706},
doi = {10.1145/3613904.3642706},
abstract = {Generative AI models, specifically large language models (LLMs), have made strides towards the long-standing goal of text-to-code generation. This progress has invited numerous studies of user interaction. However, less is known about the struggles and strategies of non-experts, for whom each step of the text-to-code problem presents challenges: describing their intent in natural language, evaluating the correctness of generated code, and editing prompts when the generated code is incorrect. This paper presents a large-scale controlled study of how 120 beginning coders across three academic institutions approach writing and editing prompts. A novel experimental design allows us to target specific steps in the text-to-code process and reveals that beginners struggle with writing and editing prompts, even for problems at their skill level and when correctness is automatically determined. Our mixed-methods evaluation provides insight into student processes and perceptions with key implications for non-expert Code LLM use within and outside of education.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {651},
numpages = {26},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3637989.3638014,
author = {Wang, Jin and Cornely, Pierre-Richard},
title = {Addressing Academic Misconduct in the Age of ChatGPT: Strategies and Solutions},
year = {2024},
isbn = {9798400708732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637989.3638014},
doi = {10.1145/3637989.3638014},
abstract = {ChatGPT, developed by OpenAI, has emerged as a pivotal advancement in the realm of artificial intelligence, boasting capabilities that extend from answering factual queries to engaging in nuanced dialogue. While ChatGPT offers transformative potential across various sectors, its integration into the educational domain presents unique challenges—most notably, an escalation in the prevalence and complexity of academic misconduct. Students have begun to exploit this technology to complete assignments, fabricate essays, and even cheat during examinations, thereby undermining the core principles of educational integrity. This paper aims to offer a comprehensive examination of the academic implications of ChatGPT, focusing on the ethical dimensions and the evolving forms of misconduct enabled by this technology. Through a thorough review of existing literature, case studies, and expert opinions, we propose a multifaceted strategy for institutions to effectively combat this emergent form of academic dishonesty, aiming to strike a balance between technological advancement and academic integrity.},
booktitle = {Proceedings of the 2023 7th International Conference on Education and E-Learning},
pages = {19–25},
numpages = {7},
location = {Tokyo, Japan},
series = {ICEEL '23}
}

@inbook{10.5555/3716662.3716740,
author = {Locatelli, Marcelo Sartori and Miranda, Matheus Prado and Costa, Igor Joaquim da Silva and Prates, Matheus Torres and Thom\'{e}, Victor and Monteiro, Mateus Zaparoli and Lacerda, Tomas and Pagano, Adriana and Neto, Eduardo Rios and Meira, Wagner and Almeida, Virgilio},
title = {Examining the Behavior of LLM Architectures within the Framework of Standardized National Exams in Brazil},
year = {2025},
publisher = {AAAI Press},
abstract = {The Exame Nacional do Ensino M\'{e}dio (ENEM) is a pivotal test for Brazilian students, required for admission to a significant number of universities in Brazil. The test consists of four objective high-school level tests on Math, Humanities, Natural Sciences and Languages, and one writing essay. Students' answers to the test and to the accompanying socioeconomic status questionnaire are made public every year (albeit anonymized) due to transparency policies from the Brazilian Government. In the context of large language models (LLMs), these data lend themselves nicely to comparing different groups of humans with AI, as we can have access to human and machine answer distributions. We leverage these characteristics of the ENEM dataset and compare GPT-3.5 and 4, and MariTalk, a model trained using Portuguese data, to humans, aiming to ascertain how their answers relate to real societal groups and what that may reveal about the model biases. We divide the human groups by using socioeconomic status (SES), and compare their answer distribution with LLMs for each question and for the essay. We find no significant biases when comparing LLM performance to humans on the multiple-choice Brazilian Portuguese tests, as the distance between model and human answers is mostly determined by the human accuracy. A similar conclusion is found by looking at the generated text as, when analyzing the essays, we observe that human and LLM essays differ in a few key factors, one being the choice of words where model essays were easily separable from human ones. The texts also differ syntactically, with LLM generated essays exhibiting, on average, smaller sentences and less thought units, among other differences. These results suggest that, for Brazilian Portuguese in the ENEM context, LLM outputs represent no group of humans, being significantly different from the answers from Brazilian students across all tests. The appendices may be found at https://arxiv.org/abs/2408.05035.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {879–890},
numpages = {12}
}

@inproceedings{10.1145/3626253.3635608,
author = {Agarwal, Arav and Mittal, Karthik and Doyle, Aidan and Sridhar, Pragnya and Wan, Zipiao and Doughty, Jacob Arthur and Savelka, Jaromir and Sakr, Majd},
title = {Understanding the Role of Temperature in Diverse Question Generation by GPT-4},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635608},
doi = {10.1145/3626253.3635608},
abstract = {We conduct a preliminary study of the effect of GPT's temperature parameter on the diversity of GPT4-generated questions. We find that using higher temperature values leads to significantly higher diversity, with different temperatures exposing different types of similarity between generated sets of questions. We also demonstrate that diverse question generation is especially difficult for questions targeting lower levels of Bloom's Taxonomy.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1550–1551},
numpages = {2},
keywords = {automated content generation, automatic generation, course design automation, curricular development, gpt-4, large language models, learning objectives, llms},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3631802.3631848,
author = {Deriba, Fitsum Gizachew and Sanusi, Ismaila Temitayo and Sunday, Amos Oyelere},
title = {Enhancing Computer Programming Education using ChatGPT- A Mini Review},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631848},
doi = {10.1145/3631802.3631848},
abstract = {This paper aims to provide insights into how ChatGPT enhances computer programming education by synthesizing existing studies using rapid review. We analysed 13 articles published in 2023, where studies focused on different aspects of basic programming education. The results indicate that 21% of these studies demonstrate that ChatGPT served as a tool for code explanation and handling complex topics. However, 36% show that ChatGPT had difficulty answering non-text-based and code-related questions, revealing reliability and accuracy issues with these tools. Another 36% of the studies showed that blindly over-reliance on ChatGPT affected critical thinking, student creativity, and problem-solving skills in programming education. 46% of the studies indicated the need to provide clear guidelines and employ plagiarism-detection tools to instruct students effectively. We suggest that educators should adopt diverse approaches to integrating ChatGPT as an educational tool while highlighting ethical considerations and model limitations.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {45},
numpages = {2},
location = {Koli, Finland},
series = {Koli Calling '23}
}

@inproceedings{10.1145/3649217.3653558,
author = {Pang, Ashley and Vahid, Frank},
title = {ChatGPT and Cheat Detection in CS1 Using a Program Autograding System},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653558},
doi = {10.1145/3649217.3653558},
abstract = {We experimented with ChatGPT's ability to write programs in a CS1 class, and the ability of a popular tool to auto-detect ChatGPT-written programs. We found ChatGPT was proficient at generating correct programs from a mere copy-paste of the English programming assignment specifications. However, running ChatGPT for 10 programming assignments and acting as 20 different students, and using zyBook's APEX beta tool for academic integrity, we found: (1) ChatGPT-generated programs tend to use a programming style departing from the style taught in the textbook or by the instructor, and these "style anomalies" were automatically detected. (2) Although ChatGPT may for the same assignment generate a few different program solutions for different students, ChatGPT often generates highly-similar programs for different students, so if enough students in a class (e.g., 5 or more) use ChatGPT, their programs will likely be flagged by a similarity checker. (3) If students are required to do all programming in the autograder's IDE, then a student using ChatGPT ends up showing very little time relative to classmates, which is automatically flagged. (4) Manually, we observed that if a student consistently uses ChatGPT to submit programs, the programming style may vary across programs, something normal students don't do; automation of style inconsistency detection was recently added to APEX. In short, while there will no doubt be an arms race between AI-generated programs and automatic detection of AI-generated programs, currently students using ChatGPT for multiple CS1 programs can be detected by automated tools such as zyBooks' APEX.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {367–373},
numpages = {7},
keywords = {CS1, ChatGPT, academic integrity, cheat detection, large language models, plagiarism, similarity checking, style anomalies},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@article{10.1145/3589649,
author = {Zhai, Xiaoming},
title = {ChatGPT for Next Generation Science Learning},
year = {2023},
issue_date = {Spring 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {3},
issn = {1528-4972},
url = {https://doi.org/10.1145/3589649},
doi = {10.1145/3589649},
abstract = {This article pilots ChatGPT in tackling the most challenging part of science learning and found it successful in automation of assessment development, grading, learning guidance, and recommendation of learning materials.},
journal = {XRDS},
month = apr,
pages = {42–46},
numpages = {5}
}

@inproceedings{10.1145/3652988.3673941,
author = {Dai, Laduona and Jung, Merel M. and \v{S}af\'{a}\v{r} Postma, Marie and van der Loo, Janneke and Louwerse, Max M.},
title = {LittleGenius: Co-Designing a GPT-4 Enhanced VR Pedagogical Framework with Teachers for Primary Education},
year = {2024},
isbn = {9798400706257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652988.3673941},
doi = {10.1145/3652988.3673941},
abstract = {This study introduces LittleGenius, a VR tool integrated with GPT-4 for primary education. Combining GPT-4’s advanced natural language processing with VR technology, LittleGenius creates an engaging learning environment. Users interact with an astronaut floating outside the International Space Station. A specialized GPT prompt was developed to foster deeper engagement through knowledge construction. The system enables natural speech interaction using Microsoft Azure’s Speech Cognitive Services. Nine in-service teachers evaluated the design, giving positive feedback, especially on the interactive astronaut agent’s engaging questions that could deepen student understanding. Future development will expand content and customize the system for diverse communication preferences and learning styles.},
booktitle = {Proceedings of the 24th ACM International Conference on Intelligent Virtual Agents},
articleno = {36},
numpages = {4},
keywords = {GPT-4, K-12 Education, VR Agent},
location = {GLASGOW, United Kingdom},
series = {IVA '24}
}

@inproceedings{10.1145/3617650.3624946,
author = {Balse, Rishabh and Prasad, Prajish and Warriem, Jayakrishnan Madathil},
title = {Exploring the Potential of GPT-4 in Automated Mentoring for Programming Courses},
year = {2023},
isbn = {9798400703744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617650.3624946},
doi = {10.1145/3617650.3624946},
abstract = {This research proposes an AI-assisted mentoring system for programming education, leveraging the advanced capabilities of OpenAI's GPT-4. We aim to validate students' pseudocode or algorithmic approaches to Python programming problems within the context of a Tier-1 institution in India, where the high student-to-mentor ratio presents unique challenges. The proposed system aspires to alleviate the pressures of the current mentoring system, providing a more accessible, responsive, and effective educational support system.},
booktitle = {Proceedings of the ACM Conference on Global Computing Education Vol 2},
pages = {191},
numpages = {1},
keywords = {python programming education, large language models, automated programming mentoring, GPT-4},
location = {Hyderabad, India},
series = {CompEd 2023}
}

@inproceedings{10.1145/3649405.3659527,
author = {Clear, Tony and Cajander, \r{A}sa and Clear, Alison and Mcdermott, Roger and Bergqvist, Andreas and Daniels, Mats and Divitini, Monica and Forshaw, Matthew and Humble, Niklas and Kasinidou, Maria and Kleanthous, Styliani and Kultur, Can and Parvini, Ghazaleh and Polash, Mohammad and Zhu, Tingting},
title = {A Plan for a Joint Study into the Impacts of AI on Professional Competencies of IT Professionals and Implications for Computing Students},
year = {2024},
isbn = {9798400706035},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649405.3659527},
doi = {10.1145/3649405.3659527},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 2},
pages = {757–758},
numpages = {2},
keywords = {artificial intelligence, computing competencies, computing curricula, generative ai, it profession, large language models},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3585059.3611424,
author = {Wang, Ye Diana},
title = {ChatGPT-Proofing a Web Development Course},
year = {2023},
isbn = {9798400701306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585059.3611424},
doi = {10.1145/3585059.3611424},
abstract = {This talk discusses an effective and efficient process that a BSIT program has undergone for transitioning to the new ABET Student Outcomes and some preliminary results, which may benefit other computing programs in preparation for an ABET accreditation visit in 2019 or beyond.},
booktitle = {Proceedings of the 24th Annual Conference on Information Technology Education},
pages = {199–200},
numpages = {2},
keywords = {ABET Accreditation, New Criterion 3, Student Outcomes},
location = {Marietta, GA, USA},
series = {SIGITE '23}
}

@inproceedings{10.5555/3712729.3712990,
author = {Leathrum, James F. and Shen, Yuzhong and Sosonkina, Masha},
title = {Investigating the Use of Generative AI in M&amp;S Education},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {Large Language Models (LLMs) are rapidly creating a place for themselves in society. There are numerous reports, both good and bad, of their use in business, academia, government and society. While some organizations are trying to limit, or eliminate, their use, it appears that it is inevitable they will become a common "tool". In education, there is a fear that students will not acquire critical thinking in the future, but we argue that LLMs will become a tool to assist students with critical thinking, giving guidance, feedback, and assessment. This paper investigates how the current state of LLMs can be integrated into modeling and simulation (M&amp;S) education. Example cases for modeling and simulation development are presented showing how an LLM can assist M&amp;S design and education in anticipation of LLMs becoming a common tool for M&amp;S practitioners. Current limitations are also highlighted, and where possible, short-term solutions are proposed.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3142–3153},
numpages = {12},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@article{10.5555/3715602.3715619,
author = {Hong, Alexander and Hong, Gongbing},
title = {The Effectiveness of Coding LLMs and the Challenges in Teaching CS1/2: A Case Study},
year = {2024},
issue_date = {October 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {1},
issn = {1937-4771},
abstract = {This paper presents a case study that evaluates the effectiveness of coding Large Language Models (LLMs) in introductory computer science courses at the university level. The study assesses six different AI-powered code generators. The evaluation focuses on the accuracy of these AI code generators in solving ten programming problems from a set of problems that instructors at Duke University can assign to students for weekly completion. The results demonstrate the effectiveness of coding LLMs in solving these problems.Based on the findings, the paper discusses the challenges faced by the computer science education community and potential strategies to address them. The advent of coding LLMs poses significant challenges to traditional teaching and learning methods in computer science. These challenges include the need for strategies to mitigate any negative impact of LLMs on the learning process. At the same time, these code LLMs also offer tremendous opportunities for enhancing teaching and learning.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {122–131},
numpages = {10}
}

@inproceedings{10.1145/3641555.3705227,
author = {Hou, Xinying and Wu, Zihan and Wang, Xu and Ericson, Barbara J.},
title = {Personalized Parsons Puzzles as Scaffolding Enhance Practice Engagement Over Just Showing LLM-Powered Solutions},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705227},
doi = {10.1145/3641555.3705227},
abstract = {As generative AI products could generate code and assist students with programming learning seamlessly, integrating AI into programming education contexts has driven much attention. However, one emerging concern is that students might get answers without learning from the LLM-generated content. In this work, we deployed the LLM-powered personalized Parsons puzzles as scaffolding to write-code practice in a Python learning classroom (PC condition) and conducted an 80-minute randomized between-subjects study. Both conditions received the same practice problems. The only difference was that when requesting help, the control condition showed students a complete solution (CC condition), simulating the most traditional LLM output. Results indicated that students who received personalized Parsons puzzles as scaffolding engaged in practicing significantly longer than those who received complete solutions when struggling.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1483–1484},
numpages = {2},
keywords = {GPT, LLM, active learning, generative AI, parsons problems},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3649409.3691092,
author = {de Miranda, Fabio and Ferrao, Rafael Corsi and Soler, Diego Pavan and Vieira Graglia, Marcelo Augusto},
title = {LLM-based Individual Contribution Summarization in Software Projects},
year = {2024},
isbn = {9798400706042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649409.3691092},
doi = {10.1145/3649409.3691092},
abstract = {This work in progress is about preliminary results in using a Large Language Model (LLM) to summarize individual student contributions in open-ended software projects. Projects for industry clients are good real-world learning opportunities. Though, if the scope is open and defined based on external clients' needs, each group's project will look unique, what makes a challenge for grading and regular feedback. Distributed code version control systems such as Git and resources such as Git classroom help, but it is still burdensome to have professors and TAs looking at the repositories with a frequency that enables useful, timely feedback for the students. We prototyped a method of summarizing each student's contributions to a project's Git repository using an LLM, indicating how to preprocess and break down repository data in order to get better responses from the system. Each student's contributions were extracted using Pydriller. This technique was tested during a 3-week full-time software development sprint in a class of 28 students. Preliminary results indicate a general agreement of students and faculty with the synthesized summaries and an increase in students' awareness of individual responsibilities within the teams and an improvement in engagement among less active members.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 2},
pages = {307–308},
numpages = {2},
keywords = {project assessment, software engineering education, teamwork},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@article{10.1145/3688094,
author = {Ng, Lynette},
title = {LLMS In Education},
year = {2024},
issue_date = {Fall 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/3688094},
doi = {10.1145/3688094},
journal = {XRDS},
month = oct,
pages = {66–70},
numpages = {5}
}

@inproceedings{10.1145/3545945.3569830,
author = {Wermelinger, Michel},
title = {Using GitHub Copilot to Solve Simple Programming Problems},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569830},
doi = {10.1145/3545945.3569830},
abstract = {The teaching and assessment of introductory programming involves writing code that solves a problem described by text. Previous research found that OpenAI's Codex, a natural language machine learning model trained on billions of lines of code, performs well on many programming problems, often generating correct and readable Python code. GitHub's version of Codex, Copilot, is freely available to students. This raises pedagogic and academic integrity concerns. Educators need to know what Copilot is capable of, in order to adapt their teaching to AI-powered programming assistants. Previous research evaluated the most performant Codex model quantitatively, e.g. how many problems have at least one correct suggestion that passes all tests. Here I evaluate Copilot instead, to see if and how it differs from Codex, and look qualitatively at the generated suggestions, to understand the limitations of Copilot. I also report on the experience of using Copilot for other activities asked of students in programming courses: explaining code, generating tests and fixing bugs. The paper concludes with a discussion of the implications of the observed capabilities for the teaching of programming.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {172–178},
numpages = {7},
keywords = {academic integrity, code explanation, code generation, introductory programming, novice programming, openai codex, programming exercises, programming patterns, test generation},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3610969.3610982,
author = {Mahon, Joyce and Mac Namee, Brian and Becker, Brett A.},
title = {No More Pencils No More Books: Capabilities of Generative AI on Irish and UK Computer Science School Leaving Examinations},
year = {2023},
isbn = {9798400708763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610969.3610982},
doi = {10.1145/3610969.3610982},
abstract = {We investigate the capabilities of ChatGPT (GPT-4) on second-level (high-school) computer science examinations: the UK A-Level and Irish Leaving Certificate. Both are national, government-set / approved, and centrally assessed examinations. We also evaluate performance differences in exams made publicly available before and after the ChatGPT knowledge cutoff date, and investigate what types of question ChatGPT struggles with. We find that ChatGPT is capable of achieving very high marks on both exams and that the performance difference before and after the knowledge cutoff date are minimal. We also observe that ChatGPT struggles with questions involving symbols or images, which can be mitigated when in-text information ‘fills in the gaps’. Additionally, GPT-4 performance can be negatively impacted when an initial inaccurate answer leads to further inaccuracies in subsequent parts of the same question. Finally, the element of choice on the Leaving Certificate is a significant advantage in achieving a high grade. Notably, there are minimal occurrences of hallucinations in answers and few errors in solutions not involving images. These results reveal several strengths and weaknesses of these exams in terms of how generative AI performs on them and have implications for exam design, the construction of marking schemes, and could also shift the focus of what is examined and how.},
booktitle = {Proceedings of the 2023 Conference on United Kingdom &amp; Ireland Computing Education Research},
articleno = {2},
numpages = {7},
keywords = {second-level, school, high school, examinations, UK, Leaving Certificate, LCCS, K-12, Ireland, Generative AI, GPT-4, ChatGPT, Artificial Intelligence, A-Level},
location = {Swansea, Wales Uk},
series = {UKICER '23}
}

@inproceedings{10.1145/3641555.3705208,
author = {Weber, Jason Lee and Park, Hyunjun and Song, Daniel J. and Apillanes, Jared and Martinez Neda, Barbara and Wong-Ma, Jennifer and Gago-Masague, Sergio},
title = {Investigating Autograder Usage in the Post- Pandemic and LLM Era},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705208},
doi = {10.1145/3641555.3705208},
abstract = {This work investigates the impact of Large Language Models (LLMs) and the COVID-19 pandemic on student behavior with autograder systems in three programming-heavy courses. We examine whether the release of LLMs like ChatGPT and GitHub Copilot, along with post-pandemic effects, has modified student interactions with autograders. Using data from student submissions over five years, totalling over 4,500 students across over 420,000 submissions, we analyze trends in submission behaviors before and after these events. Our methodology involves tracking submission patterns, focusing on timing, frequency, and score.Contrary to expectations, our findings reveal that metrics remain relatively consistent in the post-ChatGPT and post-pandemic era. Despite yearly fluctuations, no significant shift in student behaviors is attributable to these changes. Students continue to rely on a combination of manual debugging and autograder feedback without noticeable changes in their problem-solving approach.These findings highlight the resilience of the educational practices in these courses and suggest that integrating LLMs into mid-level CS curriculum may not necessitate the significant paradigm shift previously envisioned. Future work should extend these analyses to courses with different structures to determine if these results are generalizable. If not, the specific course aspects contributing to our observed ChatGPT and pandemic resilience should be identified.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1653–1654},
numpages = {2},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3657604.3662046,
author = {Chen, Binglin and Lewis, Colleen M. and West, Matthew and Zilles, Craig},
title = {Plagiarism in the Age of Generative AI: Cheating Method Change and Learning Loss in an Intro to CS Course},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3662046},
doi = {10.1145/3657604.3662046},
abstract = {Background: ChatGPT became widespread in early 2023 and enabled the broader public to use powerful generative AI, creating a new means for students to complete course assessments.  Purpose: In this paper, we explored the degree to which generative AI impacted the frequency and nature of cheating in a large introductory programming course. We also estimate the learning impact of students choosing to submit plagiarized work rather than their own work.  Methods: We identified a collection of markers that we believe are indicative of plagiarism in this course. We compare the estimated prevalence of cheating in the semesters before and during which ChatGPT became widely available. We use linear regression to estimate the impact of students' patterns of cheating on their final exam performance. Findings: The patterns associated with these plagiarism markers suggest that the quantity of plagiarism increased with the advent of generative AI, and we see evidence of a shift from online plagiarism hubs (e.g., Chegg, CourseHero) to ChatGPT. In addition, we observe statistically significant learning losses proportional to the amount of presumed plagiarism, but there is no statistical difference on the proportionality between semesters.  Implications: Our findings suggest that unproctored exams become increasingly insecure and care needs to be taken to ensure the validity of summative assessments. More importantly, our results suggest that generative AI can be detrimental to students' learning. It seems necessary for educators to reduce the benefit of students using generative AI for counterproductive purposes.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {75–85},
numpages = {11},
keywords = {cheating, cs 1, generative ai, llm, plagiarism detection},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@inproceedings{10.1145/3641554.3701974,
author = {P?durean, Victor-Alexandru and Denny, Paul and Singla, Adish},
title = {BugSpotter: Automated Generation of Code Debugging Exercises},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701974},
doi = {10.1145/3641554.3701974},
abstract = {Debugging is an essential skill when learning to program, yet its instruction and emphasis often vary widely across introductory courses. In the era of code-generating large language models (LLMs), the ability for students to reason about code and identify errors is increasingly important. However, students frequently resort to trial-and-error methods to resolve bugs without fully understanding the underlying issues. Developing the ability to identify and hypothesize the cause of bugs is crucial but can be time-consuming to teach effectively through traditional means. This paper introduces BugSpotter, an innovative tool that leverages an LLM to generate buggy code from a problem description and verify the synthesized bugs via a test suite. Students interact with BugSpotter by designing failing test cases, where the buggy code's output differs from the expected result as defined by the problem specification. This not only provides opportunities for students to enhance their debugging skills, but also to practice reading and understanding problem specifications. We deployed BugSpotter in a large classroom setting and compared the debugging exercises it generated to exercises hand-crafted by an instructor for the same problems. We found that the LLM-generated exercises produced by BugSpotter varied in difficulty and were well-matched to the problem specifications. Importantly, the LLM-generated exercises were comparable to those manually created by instructors with respect to student performance, suggesting that BugSpotter could be an effective and efficient aid for learning debugging.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {896–902},
numpages = {7},
keywords = {bugspotter, debugging, exercise generation, generative ai, llms, programming education, test cases},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3706599.3720291,
author = {Jamie, Pooriya and HajiHashemi, Reyhaneh and Alipour, Sharareh},
title = {Utilizing ChatGPT in a Data Structures and Algorithms Course: A Teaching Assistant's Perspective},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3720291},
doi = {10.1145/3706599.3720291},
abstract = {Integrating large language models (LLMs) like ChatGPT into computer science education offers transformative potential for complex courses such as data structures and algorithms (DSA). This study examines ChatGPT as a supplementary tool for teaching assistants (TAs), guided by structured prompts and human oversight, to enhance instruction and student outcomes. A controlled experiment compared traditional TA-led instruction with a hybrid approach where TAs used ChatGPT-4o and ChatGPT o1 to generate exercises, clarify concepts, and provide feedback. Structured prompts emphasized problem decomposition, real-world context, and code examples, enabling tailored support while mitigating over-reliance on AI. Results demonstrated the hybrid approach’s efficacy, with students in the ChatGPT-assisted group scoring 16.50 points higher on average and excelling in advanced topics. However, ChatGPT’s limitations necessitated TA verification. This framework highlights the dual role of LLMs: augmenting TA efficiency while ensuring accuracy through human oversight, offering a scalable solution for human-AI collaboration in education.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {567},
numpages = {7},
keywords = {LLMs, ChatGPT, Teaching Assistant, Data Structures and Algorithms Course, Education},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3641554.3701972,
author = {Ahmed, Umair Z. and Sahai, Shubham and Leong, Ben and Karkare, Amey},
title = {Feasibility Study of Augmenting Teaching Assistants with AI for CS1 Programming Feedback},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701972},
doi = {10.1145/3641554.3701972},
abstract = {With the increasing adoption of Large Language Models (LLMs), there are proposals to replace human Teaching Assistants (TAs) with LLM-based AI agents for providing feedback to students. In this paper, we explore a new hybrid model where human TAs receive AI-generated feedback for CS1 programming exercises, which they can then review and modify as needed. We conducted a large-scale randomized intervention with 185 CS1 undergraduate students, comparing the efficacy of this hybrid approach against manual feedback and direct AI-generated feedback.Our initial hypothesis predicted that AI-augmented feedback would improve TA efficiency and increase the accuracy of guidance to students. However, our findings revealed mixed results. Although students perceived improvements in feedback quality, the hybrid model did not consistently translate to better student performance. We also observed complacency among some TAs who over-relied on LLM generated feedback and failed to identify and correct inaccuracies. These results suggest that augmenting human tutors with AI may not always result in improved teaching outcomes, and further research is needed to ensure it is truly effective.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {11–17},
numpages = {7},
keywords = {cs1, gpt, hint, llm, programming, randomized trial, ta},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.5555/3643142.3643420,
author = {Tolk, Andreas and Barry, Philip and Loper, Margaret L. and Rabadi, Ghaith and Scherer, William T. and Yilmaz, Levent},
title = {Chances and Challenges of Chatgpt and Similar Models for Education in M&amp;S},
year = {2024},
isbn = {9798350369663},
publisher = {IEEE Press},
abstract = {This position paper summarizes the inputs of a group of experts from academia and industry presenting their view on chances and challenges of using ChatGPT within Modeling and Simulation education. The experts also address the need to evaluate continuous education as well as education of faculty members to address scholastic challenges and opportunities while meeting the expectation of industry. Generally, the use of ChatGPT is encouraged, but it needs to be embedded into an updated curriculum with more emphasis on validity constraints, systems thinking, and ethics.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3332–3346},
numpages = {15},
location = {San Antonio, Texas, USA},
series = {WSC '23}
}

@inproceedings{10.1145/3568812.3603487,
author = {Akram, Bita and Magooda, Ahmed},
title = {Analysis of Students’ Problem-Solving Behavior when Using Copilot for Open-Ended Programming Projects},
year = {2023},
isbn = {9781450399753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568812.3603487},
doi = {10.1145/3568812.3603487},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2},
pages = {32},
numpages = {1},
keywords = {CS1, Copilot, generative AI in CS education, introductory programming classrooms},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3649217.3653595,
author = {Cucuiat, Veronica and Waite, Jane},
title = {Feedback Literacy: Holistic Analysis of Secondary Educators' Views of LLM Explanations of Program Error Messages},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653595},
doi = {10.1145/3649217.3653595},
abstract = {The implications of using large language model (LLM) tools for learning to program at secondary school level are largely unknown, and yet there is pressure for teachers to engage with these. To start addressing this gap, we investigated: RQ1: What are secondary educators' views on the potential classroom use of LLM program error message explanations? RQ2: In what ways can a feedback literacy perspective support the analysis of educators' views of potential classroom use of LLM program error message explanations? The responses of eight expert secondary school educators were gathered during a semi-structured, activity-based interview and qualitatively analysed. Fifteen themes were derived from their commentary, of which ten corresponded to enhanced program error message (PEM) guidelines. Yet, all themes correlated to feedback literacy theory, providing a more holistic view. The analysis revealed that educators preferred LLM explanations to guide and develop understanding rather than tell, that students should be supported to make judgements and action LLM-generated feedback. Combining PEM guideline and feedback literacy findings, we suggest augmented IDEs should be designed with educators and students in mind, and teacher professional development (PD) is needed. Research is needed to compare our findings with a wider range of educators and investigate what feedback literacy means for resource design, PD, and classroom practice in secondary and undergraduate contexts.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {192–198},
numpages = {7},
keywords = {AI, IDE, K-12 education, ML, feedback literacy},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3632621.3671429,
author = {Potriasaeva, Anna and Dzialets, Katsiaryna and Golubev, Yaroslav and Birillo, Anastasiia},
title = {Using a Low-Code Environment to Teach Programming in the Era of LLMs},
year = {2024},
isbn = {9798400704765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632621.3671429},
doi = {10.1145/3632621.3671429},
abstract = {LLMs change the landscape of software engineering, and the question arises: “How can we combine LLMs with traditional teaching approaches in computer science?”. In this work, we propose to teach students in a low-code environment of code generation, developing not only their coding but also decomposition and prompting skills.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 2},
pages = {542–543},
numpages = {2},
keywords = {Generative AI, LLMs, MOOC, Programming Education},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@article{10.1145/3685680,
author = {Annapureddy, Ravinithesh and Fornaroli, Alessandro and Gatica-Perez, Daniel},
title = {Generative AI Literacy: Twelve Defining Competencies},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
url = {https://doi.org/10.1145/3685680},
doi = {10.1145/3685680},
abstract = {This article introduces a competency-based model for generative artificial intelligence (AI) literacy covering essential skills and knowledge areas necessary to interact with generative AI. The competencies range from foundational AI literacy to prompt engineering and programming skills, including ethical and legal considerations. These 12 competencies offer a framework for individuals, policymakers, government officials, and educators looking to navigate and take advantage of the potential of generative AI responsibly. Embedding these competencies into educational programs and professional training initiatives can equip individuals to become responsible and informed users and creators of generative AI. The competencies follow a logical progression and serve as a roadmap for individuals seeking to become familiar with generative AI and for researchers and policymakers to develop assessments, educational programs, guidelines, and regulations.},
journal = {Digit. Gov.: Res. Pract.},
month = feb,
articleno = {13},
numpages = {21},
keywords = {Generative AI literacy, AI literacy, data literacy, generative AI, prompt engineering, AI competencies, AI skills}
}

@inproceedings{10.1145/3613904.3642332,
author = {Belghith, Yasmine and Mahdavi Goloujeh, Atefeh and Magerko, Brian and Long, Duri and Mcklin, Tom and Roberts, Jessica},
title = {Testing, Socializing, Exploring: Characterizing Middle Schoolers’ Approaches to and Conceptions of ChatGPT},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642332},
doi = {10.1145/3613904.3642332},
abstract = {As generative AI rapidly enters everyday life, educational interventions for teaching about AI need to cater to how young people, in particular middle schoolers who are at a critical age for reasoning skills and identity formation, conceptualize and interact with AI. We conducted nine focus groups with 24 middle school students to elicit their interests, conceptions of, and approaches to a popular generative AI tool, ChatGPT. We highlight a) personally and culturally-relevant topics to this population, b) three distinct approaches in students’ open-ended interactions with ChatGPT: AI testing-oriented, AI socializing-oriented, and content exploring-oriented, and 3) an improved understanding of youths’ conceptions and misconceptions of generative AI. While misconceptions highlight gaps in understanding what generative AI is and how it works, most learners show interest in learning about what AI is and what it can do. We discuss the implications of these conceptions for designing AI literacy interventions in museums.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {276},
numpages = {17},
keywords = {AI literacy, ChatGPT, Child-AI Interaction, Conceptions of AI, Conversational Agents (CAs), Generative AI, Informal Learning, Large Language Models (LLMs)},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3633053.3633057,
author = {Petrovska, Olga and Clift, Lee and Moller, Faron and Pearsall, Rebecca},
title = {Incorporating Generative AI into Software Development Education},
year = {2024},
isbn = {9798400709326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3633053.3633057},
doi = {10.1145/3633053.3633057},
abstract = {This paper explores how Generative AI can be incorporated into software development education. We present examples of formative and summative assessments, which explore various aspects of ChatGPT, including its coding capabilities, its ability to construct arguments as well as ethical issues of using ChatGPT and similar tools in education and the workplace. Our work is inspired by the insights from surveys that show that the learners on our Degree Apprenticeship Programme have a great interest in learning about and exploiting emerging AI technology. Similarly, our industrial partners have a clear interest for their employees to be formally prepared to use GenAI in their software engineering roles. In this vein, it is proposed that embedding the use of GenAI tools in a careful and creative way - by developing assessments which encourage learners to critically evaluate AI output - can be beneficial in helping learners understand the subject material being taught without the risk of the AI tools “doing the homework”.},
booktitle = {Proceedings of the 8th Conference on Computing Education Practice},
pages = {37–40},
numpages = {4},
keywords = {apprenticeship, assessment, education, generative AI, software engineering},
location = {Durham, United Kingdom},
series = {CEP '24}
}

@inproceedings{10.1145/3636243.3636248,
author = {Hou, Irene and Mettille, Sophia and Man, Owen and Li, Zhuo and Zastudil, Cynthia and MacNeil, Stephen},
title = {The Effects of Generative AI on Computing Students’ Help-Seeking Preferences},
year = {2024},
isbn = {9798400716195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636243.3636248},
doi = {10.1145/3636243.3636248},
abstract = {Help-seeking is a critical way that students learn new concepts, acquire new skills, and get unstuck when problem-solving in their computing courses. The recent proliferation of generative AI tools, such as ChatGPT, offers students a new source of help that is always available on-demand. However, it is unclear how this new resource compares to existing help-seeking resources along dimensions of perceived quality, latency, and trustworthiness. In this paper, we investigate the help-seeking preferences and experiences of computing students now that generative AI tools are available to them. We collected survey data (n=47) and conducted interviews (n=8) with computing students. Our results suggest that although these models are being rapidly adopted, they have not yet fully eclipsed traditional help resources. The help-seeking resources that students rely on continue to vary depending on the task and other factors. Finally, we observed preliminary evidence about how help-seeking with generative AI is a skill that needs to be developed, with disproportionate benefits for those who are better able to harness the capabilities of LLMs. We discuss potential implications for integrating generative AI into computing classrooms and the future of help-seeking in the era of generative AI.},
booktitle = {Proceedings of the 26th Australasian Computing Education Conference},
pages = {39–48},
numpages = {10},
keywords = {ChatGPT, Generative AI, computing education, help-seeking},
location = {Sydney, NSW, Australia},
series = {ACE '24}
}

@inproceedings{10.1145/3677619.3678131,
author = {Morales-Navarro, Luis and Gao, Phillip and Yang, Eric and Kafai, Yasmin B},
title = {"It's smart and it's stupid:" Youth's conflicting perspectives on LLMs' language comprehension and ethics},
year = {2024},
isbn = {9798400710056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677619.3678131},
doi = {10.1145/3677619.3678131},
abstract = {While experts disagree on whether large pretrained language models (LLMs) understand human language, youth use, play, and experiment with LLM-powered applications every day. Yet little attention has been given to young people’s perspectives on LLMs’ capacity for understanding and the ethical implications of synthetic texts produced by these systems. We conducted a participatory design session using big paper methods with 18 14-15-year-olds in which they (1) interacted with ChatGPT (GPT 3.5) and (2) prompted it with commonsense questions. We examined youths’ big papers, prompts, and conversations during the activity. Our analysis shows that youth had conflicting views on GPTs’ language comprehension and task completion, as well as synthetic text’s ethical implications with regards to misinformation, privacy, and safety. We discuss how these findings could inform the design of learning activities.},
booktitle = {Proceedings of the 19th WiPSCE Conference on Primary and Secondary Computing Education Research},
articleno = {36},
numpages = {2},
keywords = {artificial intelligence, computing education, ethics, k-12, machine learning},
location = {Munich, Germany},
series = {WiPSCE '24}
}

@inproceedings{10.1145/3626252.3630773,
author = {Woodrow, Juliette and Malik, Ali and Piech, Chris},
title = {AI Teaches the Art of Elegant Coding: Timely, Fair, and Helpful Style Feedback in a Global Course},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630773},
doi = {10.1145/3626252.3630773},
abstract = {Teaching students how to write code that is elegant, reusable, and comprehensible is a fundamental part of CS1 education. However, providing this "style feedback" in a timely manner has proven difficult to scale. In this paper, we present our experience deploying a novel, real-time style feedback tool in Code in Place, a large-scale online CS1 course. Our tool is based on the latest breakthroughs in large-language models (LLMs) and was carefully designed to be safe and helpful for students. We used our Real-Time Style Feedback tool (RTSF) in a class with over 8,000 diverse students from across the globe and ran a randomized control trial to understand its benefits. We show that students who received style feedback in real-time were five times more likely to view and engage with their feedback compared to students who received delayed feedback. Moreover, those who viewed feedback were more likely to make significant style-related edits to their code, with over 79% of these edits directly incorporating their feedback. We also discuss the practicality and dangers of LLM-based tools for feedback, investigating the quality of the feedback generated, LLM limitations, and techniques for consistency, standardization, and safeguarding against demographic bias, all of which are crucial for a tool utilized by students.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1442–1448},
numpages = {7},
keywords = {cs1, deployed at scale, gpt, llms, real time, style feedback},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3711403.3711435,
author = {Yang, Qi},
title = {A Review of the Role and Impact of Generative Artificial Intelligence on Education},
year = {2025},
isbn = {9798400717468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711403.3711435},
doi = {10.1145/3711403.3711435},
abstract = {In order to ensure quality development in the age of intelligence, it is crucial to integrate intelligent technology with education. Artificial Intelligence (AI) and Generative Artificial Intelligence (GAI) are disruptive technologies in the area of education. While online education brings significant advantages in enhancing educational quality, promoting educational equity, and improving educational efficiency, it has also raised concerns among scholars around the world regarding students' moral ethics, cultivation of emotional values, technological dependence, thinking deprivation, privacy, and policy making. Using Cite Space software to analyze more than 50 articles from core journals in the field of educational technology at home and abroad, this paper comprehensively summarizes the role and impact of generative AI in education up to 2023, suggests the limitations of generative AI in empowering education at present, and predicts the direction scholars will tend to research in this field in the future.},
booktitle = {Proceedings of the 2024 7th International Conference on Educational Technology Management},
pages = {121–127},
numpages = {7},
keywords = {ChatGPT, Education informatization, Generative artificial intelligence},
location = {
},
series = {ICETM '24}
}

@inproceedings{10.5555/3712729.3712987,
author = {Shin, Jinnie and Cruz-Castro, Laura and Yang, Zhenlin and Castelblanco, Gabriel and Aggarwal, Ashish and Leite, Walter L. and Carroll, Bruce F.},
title = {Understanding Optimal Interactions between Students and a Chatbot during a Programming Task},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {This study explores integrating Large Language Models (LLMs) into computer science education by examining undergraduate interactions with a GPT-4-based chatbot during a formative assignment in an introductory course. We aim to delineate optimal help-seeking behaviors and ascertain if effective problem-navigating strategies correlate with improved learning outcomes. Using descriptive statistics and Structural Topic Modeling (STM), we analyze the types of questions posed and their connection to task completion success. Findings reveal a positive association between the number of attempts and help requests, indicating more engaged students seek assistance. STM analysis shows high-ability students address abstract concepts early, while lower-ability students focus on syntax-related issues. These insights underscore the need to evaluate interaction behaviors to optimize chatbot use in education, leading to proposed guidelines to enhance chatbot utilization, promoting responsible use and maximizing educational advantages.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3106–3117},
numpages = {12},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@inproceedings{10.1145/3639474.3640084,
author = {Sa\u{g}lam, Timur and Hahner, Sebastian and Schmid, Larissa and Burger, Erik},
title = {Automated Detection of AI-Obfuscated Plagiarism in Modeling Assignments},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640084},
doi = {10.1145/3639474.3640084},
abstract = {Plagiarism is a widespread problem in computer science education, exacerbated by the impracticability of manual inspection in large courses. Even worse, tools based on large language models like ChatGPT have made it easier than ever to obfuscate plagiarized solutions. Additionally, most plagiarism detectors only apply to code, and only a few approaches exist for modeling assignments, which lack broad resilience to obfuscation attacks. This paper presents a novel approach for automated plagiarism detection in modeling assignments that combines automated analysis with human inspection. We evaluate our approach with real-world assignments and plagiarism obfuscated by ChatGPT. Our results show that we achieve a significantly higher detection rate for AI-generated attacks and a broader resilience than the state-of-the-art.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {297–308},
numpages = {12},
keywords = {plagiarism detection, obfuscation, ChatGPT, artificial intelligence},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@article{10.1145/3624720,
author = {Denny, Paul and Prather, James and Becker, Brett A. and Finnie-Ansley, James and Hellas, Arto and Leinonen, Juho and Luxton-Reilly, Andrew and Reeves, Brent N. and Santos, Eddie Antonio and Sarsa, Sami},
title = {Computing Education in the Era of Generative AI},
year = {2024},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {67},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/3624720},
doi = {10.1145/3624720},
abstract = {Challenges and opportunities faced by computing educators and students adapting to LLMs capable of generating accurate source code from natural-language problem descriptions.},
journal = {Commun. ACM},
month = jan,
pages = {56–67},
numpages = {12}
}

@inproceedings{10.1145/3649217.3653574,
author = {Denny, Paul and MacNeil, Stephen and Savelka, Jaromir and Porter, Leo and Luxton-Reilly, Andrew},
title = {Desirable Characteristics for AI Teaching Assistants in Programming Education},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653574},
doi = {10.1145/3649217.3653574},
abstract = {Providing timely and personalized feedback to large numbers of students is a long-standing challenge in programming courses. Relying on human teaching assistants (TAs) has been extensively studied, revealing a number of potential shortcomings. These include inequitable access for students with low confidence when needing support, as well as situations where TAs provide direct solutions without helping students to develop their own problem-solving skills. With the advent of powerful large language models (LLMs), digital teaching assistants configured for programming contexts have emerged as an appealing and scalable way to provide instant, equitable, round-the-clock support. Although digital TAs can provide a variety of help for programming tasks, from high-level problem solving advice to direct solution generation, the effectiveness of such tools depends on their ability to promote meaningful learning experiences. If students find the guardrails implemented in digital TAs too constraining, or if other expectations are not met, they may seek assistance in ways that do not help them learn. Thus, it is essential to identify the features that students believe make digital teaching assistants valuable. We deployed an LLM-powered digital assistant in an introductory programming course and collected student feedback (n=813) on the characteristics of the tool they perceived to be most important. Our results highlight that students value such tools for their ability to provide instant, engaging support, particularly during peak times such as before assessment deadlines. They also expressed a strong preference for features that enable them to retain autonomy in their learning journey, such as scaffolding that helps to guide them through problem-solving steps rather than simply being shown direct solutions.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {408–414},
numpages = {7},
keywords = {ai tutors, automated tutors, digital tas, feedback, llms},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3660650.3660657,
author = {Roberts, Jordan and Mohamed, Abdallah},
title = {Generative AI in CS Education: Literature Review through a SWOT Lens},
year = {2024},
isbn = {9798400709975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660650.3660657},
doi = {10.1145/3660650.3660657},
abstract = {The rapid growth of generative artificial intelligence (AI) models introduced challenges for educators, students and administrators across the academic sphere related to how to manage and regulate these tools. While some oppose their use, many researchers have begun to approach the topic of educational AI use from a different perspective. Despite being in its early stages; this field of research has produced notable insights into the capabilities and limitations of models like ChatGPT. This paper utilizes a SWOT analysis framework to analyze and consolidate existing literature, with a specific focus on Computer Science education. Through the analysis of this literature, we have created a set of use cases and guidelines to aid in the future development of strategies and tools within this field. Our findings indicate that while some concerns are valid, such as AI's ability to generate plagiarized work, we identified several promising avenues and opportunities for careful integration of this technology into education.},
booktitle = {Proceedings of the 26th Western Canadian Conference on Computing Education},
articleno = {10},
numpages = {6},
location = {Kelowna, BC, Canada},
series = {WCCCE '24}
}

@inproceedings{10.1145/3626252.3630828,
author = {Prasad, Prajish and Sane, Aamod},
title = {A Self-Regulated Learning Framework using Generative AI and its Application in CS Educational Intervention Design},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630828},
doi = {10.1145/3626252.3630828},
abstract = {Self-regulation refers to the ability to plan, monitor, control and reflect on one's problem-solving process. Prior research has shown that self-regulated learning (SRL) strategies help improve novice performance in solving programming problems. However, with the advent of LLM tools like ChatGPT, novices can generate fairly accurate code by just providing the problem prompt, and hence may forego applying essential self-regulation strategies such as planning and reflection to solve the problem. In this position paper, we discuss challenges and opportunities that generative AI technologies pose for novices' self-regulation strategies in the context of programming problem solving. We believe that the key challenge facing educators is that such technologies may hamper novices' ability to regulate their programming problem solving process.On the other hand, these technologies also open up the possibility to design new interventions that promote better SRL strategies in learners. We draw on generic and domain-specific self-regulated learning theories as the basis of our work, and propose an SRL framework that incorporates use of generative AI tools in programming problem solving. We illustrate how the proposed framework guides exploration of the design space of interventions that integrate generative AI in CS education.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1070–1076},
numpages = {7},
keywords = {chatgpt, generative ai, llm, metacognition, pair programming, pair thinking, self-regulated learning, self-regulation, srl},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3706598.3713513,
author = {Bodonhelyi, Anna and Thaqi, Enkeleda and \"{O}zdel, S\"{u}leyman and Bozkir, Efe and Kasneci, Enkelejda},
title = {From Passive Watching to Active Learning: Empowering Proactive Participation in Digital Classrooms with AI Video Assistant},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713513},
doi = {10.1145/3706598.3713513},
abstract = {In online education, innovative tools are crucial for enhancing learning outcomes. SAM (Study with AI Mentor) is an advanced platform that integrates educational videos with a context-aware chat interface powered by large language models. SAM encourages students to ask questions and explore unclear concepts in real time, offering personalized, context-specific assistance, including explanations of formulas, slides, and images. We evaluated SAM in two studies: one with 25 university students and another with 80 crowdsourced participants, using pre- and post-knowledge tests to compare a group using SAM and a control group. The results demonstrated that SAM users achieved greater knowledge gains specifically for younger learners and individuals in flexible working environments, such as students, supported by a 97.6% accuracy rate in the chatbot’s responses. Participants also provided positive feedback on SAM’s usability and effectiveness. SAM’s proactive approach to learning not only enhances learning outcomes but also empowers students to take full ownership of their educational experience, representing a promising future direction for online learning tools.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {16},
numpages = {21},
keywords = {E-Learning, Real-Time Assistant, AI tutor, ChatGPT, User Study},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3724504.3724537,
author = {Zhang, Wen-di and Dou, Huan-xin},
title = {Generation and Evaluation of International Chinese Teaching Resources by Generative Artificial Intelligence},
year = {2025},
isbn = {9798400711732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3724504.3724537},
doi = {10.1145/3724504.3724537},
abstract = {Generative artificial intelligence has set off a new round of intelligent revolution and promoted the reform and development of the education industry. The development of international Chinese education also requires the digitalization and intelligence of international Chinese teaching resources. In this regard, this article utilizes the technology of ChatGPT platform to integrate teaching resources, constructs an artificial intelligence teaching resource generation framework consisting of demand analysis, intelligent generation, and quality assessment modules, as well as a quality evolution model of artificial intelligence international Chinese teaching resources. Based on this framework and resource quality evolution model, an experiment on the generation of artificial intelligence teaching resources was carried out, and inspections and evaluations were conducted from the perspectives of natural language processing technology, learners, and teachers. The results show that the teaching resources generated by artificial intelligence pass the inspection of natural language understanding technology and have good quality; learners and teachers are optimistic about the application of teaching resources in teaching and believe that most of these resources have reached a usable state; learners' overall experience in using teaching resources is positive and they believe that these resources can promote learning in many aspects. The application of artificial intelligence in generating teaching resources in this article helps to optimize the construction mode of international Chinese teaching resources and promote the high-quality development of international Chinese education.},
booktitle = {Proceedings of the 2024 2nd International Conference on Information Education and Artificial Intelligence},
pages = {187–192},
numpages = {6},
keywords = {Artificial Intelligence, ChatGPT, International Chinese Education, Teaching Resources},
location = {
},
series = {ICIEAI '24}
}

@article{10.1145/3688092,
author = {He, Zhongxuan},
title = {The Journey of LLMs in Education},
year = {2024},
issue_date = {Fall 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/3688092},
doi = {10.1145/3688092},
journal = {XRDS},
month = oct,
pages = {61},
numpages = {1}
}

@inproceedings{10.1145/3641554.3701853,
author = {Filcik, Daniel and Sobiesk, Edward and Matthews, Suzanne J.},
title = {Fostering Creativity: Student-Generative AI Teaming in an Open-Ended CS0 Assignment},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701853},
doi = {10.1145/3641554.3701853},
abstract = {The increasing ubiquity of web-based generative artificial intelligence technologies necessitates that all students experience teaming with such technologies -- exploring their strengths and limitations and learning how to create synergy with them. To aid in this effort, we designed an open-ended generative AI project for the freshmen taking our general-education introduction to computing course. Students were required to team with generative AI to create something beyond what they alone (or the AI alone) could accomplish. Upon completion, students submitted a short written critical analysis documenting their experiences and presented a three-minute demonstration of their project in class. Despite limited course coverage of AI and generative AI prior to this project, we were impressed by the creativity and sophistication of the submitted final products as well as the breadth of generative AI tools explored. Student reflections on the experience illustrated numerous insights into the strengths and limitations of the tools they employed. Our results underscore that students can learn about the benefits and limitations of generative AI in as little as a single assignment and that covering such topics need not require extensive amounts of course time and resources.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {339–345},
numpages = {7},
keywords = {computing education, cs0, final project, freshmen, generative artificial intelligence, human-ai teaming},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.1145/3685235.3685237,
author = {Deng, Xuefei (Nancy) and Joshi, K.D.},
title = {Promoting Ethical Use of Generative AI in Education},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {3},
issn = {0095-0033},
url = {https://doi.org/10.1145/3685235.3685237},
doi = {10.1145/3685235.3685237},
abstract = {Generative artificial intelligence (AI) represents a crucial subset of AI models characterized by their ability to generate new content based on user input, showing vast potential to transform learning and teaching. However, educators have raised ethical concerns, particularly regarding the adverse effect on students' learning if students simply parrot generative AI-generated content without engaging in critical analysis or original thought. Moreover, there exists the potential of generative AI to perpetuate existing biases in training data. This editorial discusses three major concerns in generative AI use in education and proposes questions (on task-AI fit and people-AI fit) and approaches to address the ethical considerations by adopting five principles of AI ethics. The editorial also discusses developing a classroom AI use policy as one governance mechanism for promoting ethical use of AI. As generative AI technology continues to evolve, so must our educational practices. The editorial ends with a call for readers (educators) to collaboratively define the terms of engagement with generative AI in educational settings and to begin this discourse by sharing insights and experiences with promoting ethical use of generative AI.},
journal = {SIGMIS Database},
month = jul,
pages = {6–11},
numpages = {6},
keywords = {ai ethics, ai use policy, biases, ethical use of ai, generative ai, higher education, normalization of mediocrity, plagiarism, prompt engineering}
}

@inproceedings{10.1145/3639474.3640068,
author = {Pan, Wei Hung and Chok, Ming Jie and Wong, Jonathan Leong Shan and Shin, Yung Xin and Poon, Yeong Shian and Yang, Zhou and Chong, Chun Yong and Lo, David and Lim, Mei Kuan},
title = {Assessing AI Detectors in Identifying AI-Generated Code: Implications for Education},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640068},
doi = {10.1145/3639474.3640068},
abstract = {Educators are increasingly concerned about the usage of Large Language Models (LLMs) such as ChatGPT in programming education, particularly regarding the potential exploitation of imperfections in Artificial Intelligence Generated Content (AIGC) Detectors for academic misconduct.In this paper, we present an empirical study where the LLM is examined for its attempts to bypass detection by AIGC Detectors. This is achieved by generating code in response to a given question using different variants. We collected a dataset comprising 5,069 samples, with each sample consisting of a textual description of a coding problem and its corresponding human-written Python solution codes. These samples were obtained from various sources, including 80 from Quescol, 3,264 from Kaggle, and 1,725 from Leet-Code. From the dataset, we created 13 sets of code problem variant prompts, which were used to instruct ChatGPT to generate the outputs. Subsequently, we assessed the performance of five AIGC detectors. Our results demonstrate that existing AIGC Detectors perform poorly in distinguishing between human-written code and AI-generated code.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {1–11},
numpages = {11},
keywords = {software engineering education, AI-generated code, AI-generated code detection},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@article{10.5555/3715622.3715630,
author = {Lindoo, Ed and Lotfy, Mohamed},
title = {Generative AI and its Impact on the CS Classroom and Programmers},
year = {2024},
issue_date = {October 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {2},
issn = {1937-4771},
abstract = {As the integration of generative artificial intelligence (AI) in educational settings becomes more widespread, students, teachers, and educational institutions face the challenge of utilizing these technologies in a responsible manner. The responsible use of generative AI can help CS and IT students develop critical thinking, enhance their learning experience, facilitate the learning process, can assist in understanding code concepts, programming skills, and/or enhancing the programming knowledge. The aim of this investigation is on how students might utilize, and potentially abuse, generative AI. In this paper we provide examples of how generative AI can be used to generate code modules. We discuss the use of generative AI in programming classes as well as its impact on the future of programming and programmers.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {35–50},
numpages = {16}
}

@inproceedings{10.1145/3652620.3687805,
author = {Netz, Lukas and Reimer, Jan and Rumpe, Bernhard},
title = {Using Grammar Masking to Ensure Syntactic Validity in LLM-based Modeling Tasks},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687805},
doi = {10.1145/3652620.3687805},
abstract = {Low-code development platforms (LCDPs) are becoming increasingly important in industry, which confronts us in academic teaching with the challenge of educating students in the basic principles, critical engagement, and evaluation of LCDPs. This leads us to the question, how to teach the usage of different LCDPs during an university course. The short time frame of university-level courses makes it challenging to teach more than only one LCDP. In our teaching approach, students use two different LCDPs and create a web-application with both of them. Firstly, we require the students to define a target application with common modeling languages, next they use the first LCDP, at about half the time they switch to the second LCDP and present their findings of the differences in methodology and development processes at the end. We discuss this approach, show survey results from the participants, and explain lessons learned. This concept allows students critical engagement with LCDPs and model-driven software engineering. Supervisors get an insight into the learnability of each LCDP and how novices adapt to different domain-specific languages and their notations.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {115–122},
numpages = {8},
keywords = {low-code development platforms, education, university-level courses, model-driven software engineering, problem-based learning},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@article{10.5555/3729857.3729868,
author = {Bandi, Ajay},
title = {Pedagogical Evaluation of Generative AI Course for Technologists},
year = {2025},
issue_date = {April 2025},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {6},
issn = {1937-4771},
abstract = {Generative AI is a transformative technology that impacts various fields, including software development, data analytics, and cybersecurity. To address this, we have designed and developed a Generative AI course for technologists, integrating foundational knowledge of various Gen AI architecture models with hands-on practical experience using Python libraries, including HuggingFace. This paper discusses the detailed course structure and assessments. A pedagogical evaluation approach is followed to identify the challenges encountered in the course and how to overcome them. The results demonstrate that the Generative AI Course for Technologists effectively equips students with technical expertise and critical thinking skills through a balanced combination of theoretical concepts and practical exercises, such as chatbot development and prompt engineering. The course addresses challenges like hardware limitations and API integration by proposing future improvements, including a dedicated Python module and access to cloud-based GPU tools, ensuring learners are well-prepared to navigate and ethically apply Generative AI in real-world contexts.},
journal = {J. Comput. Sci. Coll.},
month = apr,
pages = {99–110},
numpages = {12}
}

@inproceedings{10.1145/3716640.3716658,
author = {Feng, Tony Haoran and Luxton-Reilly, Andrew and W\"{u}nsche, Burkhard C and Denny, Paul},
title = {From Automation to Cognition: Redefining the Roles of Educators and Generative AI in Computing Education},
year = {2025},
isbn = {9798400714252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716640.3716658},
doi = {10.1145/3716640.3716658},
abstract = {Generative Artificial Intelligence (GenAI) offers numerous opportunities to revolutionise teaching and learning in Computing Education (CE). However, educators have expressed concerns that students may over-rely on GenAI and use these tools to generate solutions without engaging in the learning process. While substantial research has explored GenAI use in CE, and many Computer Science (CS) educators have expressed their opinions and suggestions on the subject, there remains little consensus on implementing curricula and assessment changes.In this paper, we describe our experiences with using GenAI in CS-focused educational settings and the changes we have implemented accordingly in our teaching in recent years since the popularisation of GenAI. From our experiences, we propose two primary actions for the CE community: 1) redesign take-home assignments to incorporate GenAI use and assess students on their process of using GenAI to solve a task rather than simply on the final product; 2) redefine the role of educators to emphasise metacognitive aspects of learning, such as critical thinking and self-evaluation. This paper presents and discusses these stances and outlines several practical methods to implement these strategies in CS classrooms. Then, we advocate for more research addressing the concrete impacts of GenAI on CE, especially those evaluating the validity and effectiveness of new teaching practices.},
booktitle = {Proceedings of the 27th Australasian Computing Education Conference},
pages = {164–171},
numpages = {8},
keywords = {Generative Artificial Intelligence, GenAI, Strategy, Assignments, Metacognition, Assessments},
location = {
},
series = {ACE '25}
}

@inproceedings{10.1109/SC41406.2024.00076,
author = {Giordani, Jeremiah and Xu, Ziyang and Colby, Ella and Ning, August and Godala, Bhargav Reddy and Chaturvedi, Ishita and Zhu, Shaowei and Chon, Yebin and Chan, Greg and Tan, Zujun and Collier, Galen and Halverson, Jonathan D. and Deiana, Enrico Armenio and Liang, Jasper and Sossai, Federico and Su, Yian and Patel, Atmn and Pham, Bangyen and Greiner, Nathan and Campanoni, Simone and August, David I.},
title = {Revisiting Computation for Research: Practices and Trends},
year = {2024},
isbn = {9798350352917},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC41406.2024.00076},
doi = {10.1109/SC41406.2024.00076},
abstract = {In the field of computational science, effectively supporting researchers necessitates a deep understanding of how they utilize computational resources. Building upon a decade-old survey that explored the practices and challenges of research computation, this study aims to bridge the understanding gap between providers of computational resources and researchers who rely on them. This study revisits key survey questions and gathers feedback on open-ended topics from over a hundred interviews. Quantitative analyses of present and past results illuminate the landscape of research computation. Qualitative analyses, including careful use of large language models, highlight trends and challenges with concrete evidence. Given the rapid evolution of computational science, this paper offers a toolkit with methodologies and insights to simplify future research and ensure ongoing examination of the landscape. This study, with its findings and toolkit, guides enhancements to computational systems, deepens understanding of user needs, and streamlines reassessment of the computational landscape.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {70},
numpages = {14},
location = {Atlanta, GA, USA},
series = {SC '24}
}

@inproceedings{10.1145/3641555.3705064,
author = {Erez, Yael and Ayali, Lilach and Hazzan, Orit},
title = {Evolution of Students' Attitudes Towards the Use of Generative AI Tools in a CS1 Course: Implications for Instructors},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705064},
doi = {10.1145/3641555.3705064},
abstract = {Recent advancements in large language model-based generative artificial intelligence (GenAI) tools have transformed computer science education, presenting both opportunities and challenges. A study investigating students' attitudes toward these tools was conducted during an Introduction to Computer Science course. The target of the study was to gauge students' evolving attitudes toward using GenAI tools in the course, before, during and after ChatGPT was gradually assimilated into homework assignments. The study refers to three phases: preliminary phase, assimilation phase, and calibration stage, which currently takes place. Findings show that, in the preliminary phase, students appreciated the efficiency of GenAI tools offered but were concerned about developing a dependency on these tools and about ''cheating''. Findings from the assimilation phase indicate that consistent, guided exposure to GenAI tools positively shifted students' views, alleviating initial concerns and promoting a positive attitude toward using GenAI tools in the course. The targets of the calibration phase are: a) to examine how to leverage independent learning by formulating clear guidelines that can build trust in the technology and help overcome concerns regarding reliability and credibility; b) to check how GenAI can help students in a Introduction to Computer Science course acquire skills such as critical thinking and code comprehension. The study offers insights for educators on the integration of GenAI tools into computer science courses to enhance learning while maintaining academic integrity.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1740},
numpages = {1},
keywords = {critical thinking, cs1, generative ai, introduction to computer science, mixed methods, program comprehension, skills, students' attitudes},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.5555/3637068.3637089,
author = {Cerkez, Paul S. and Hummel, Joseph Edward and Mejias, Marlon and Pruitt, William},
title = {ChatGPT: To Use or Not to Use, That is the Question: Panel Discussion},
year = {2023},
issue_date = {November 2023},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {5},
issn = {1937-4771},
abstract = {ChatGPT, from OpenAI (AI - artificial intelligence), and the many similar Large Language Models (LLM) appear to have taken the world by storm with some for it, some against it. In simple terms, these products are a great tool for the experienced domain user, however, precisely because of their capability, there is a lot of controversy surrounding student's use.},
journal = {J. Comput. Sci. Coll.},
month = nov,
pages = {175–176},
numpages = {2}
}

@inproceedings{10.1145/3641555.3705272,
author = {Hooper, Kerrie and Lunn, Stephanie Jill},
title = {Traversing New Horizons: An Exploration of Educational Policies on Generative AI},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705272},
doi = {10.1145/3641555.3705272},
abstract = {Understanding how tertiary academic institutions approach the integration of generative AI (GAI) into their course policies is crucial since AI technologies are rapidly transforming society. AI is being used and applied across sectors and industries, and it is important to do so with regard to ethics. This exploratory study sought to examine how GAI policies were discussed across academic institutions. The policies were analyzed using NLP techniques and utilized existing publicly available datasets, which consisted of a collection of over 100 university policies and syllabi policies. Unsupervised clustering techniques were applied to analyze patterns in how different institutions may express their policies and best practices. These findings illuminate how universities and colleges may approach topics and challenges around AI, and specifically GAI.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1479–1480},
numpages = {2},
keywords = {NLP analysis, academic policies, generative AI},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3626252.3630881,
author = {Katuka, Gloria Ashiya and Chakraburty, Srijita and Lee, Hyejeong and Dhama, Sunny and Earle-Randell, Toni and Celepkolu, Mehmet and Boyer, Kristy Elizabeth and Glazewski, Krista and Hmelo-Silver, Cindy and Mcklin, Tom},
title = {Integrating Natural Language Processing in Middle School Science Classrooms: An Experience Report},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630881},
doi = {10.1145/3626252.3630881},
abstract = {With the increasing prevalence of large language models (LLMs) such as ChatGPT, there is a growing need to integrate natural language processing (NLP) into K-12 education to better prepare young learners for the future AI landscape. NLP, a sub-field of AI that serves as the foundation of LLMs and many advanced AI applications, holds the potential to enrich learning in core subjects in K-12 classrooms. In this experience report, we present our efforts to integrate NLP into science classrooms with 98 middle school students across two US states, aiming to increase students' experience and engagement with NLP models through textual data analyses and visualizations. We designed learning activities, developed an NLP-based interactive visualization platform, and facilitated classroom learning in close collaboration with middle school science teachers. This experience report aims to contribute to the growing body of work on integrating NLP into K-12 education by providing insights and practical guidelines for practitioners, researchers, and curriculum designers.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {639–645},
numpages = {7},
keywords = {middle school science classrooms, natural language processing, nlp and ai learning, nlp+science},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@article{10.5555/3722479.3722506,
author = {Liao, Weidong and Guzide, Osman},
title = {Enhancing Undergraduate Computing Education with LMMs and ChatGPT-4o},
year = {2024},
issue_date = {October 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {3},
issn = {1937-4771},
abstract = {Large Language Models (LLMs) and ChatGPT have significantly impacted programming practices and computer science education. The rapid advancements in natural language processing, recurrent neural networks, and Transformer architectures have captured the attention of students and educators alike. These tools aid students in brainstorming, coding, analyzing code, and writing reports. Although concerns about cheating and plagiarism persist, these tools also provide educators with novel ways to create and assess assignments. Despite some hesitancy among educators to integrate these AI tools into the classroom, the advert and development of Large MultiModal Models (LMMs), the enhancement of LLMs that can deal with multimedia inputs and outputs, illustrates a significant evolution in generative AI capabilities.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {62},
numpages = {1}
}

@article{10.5555/3715602.3715609,
author = {Weiss, Richard and Mache, Jens},
title = {Cybersecurity Exercises in the Age of LLMs},
year = {2024},
issue_date = {October 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {1},
issn = {1937-4771},
abstract = {In this tutorial, we will introduce a cybersecurity education framework for developing polymorphic hands-on exercises. Many faculty readily acknowledge the importance of cybersecurity in the Computer Science curriculum, but there are still barriers to integrating it into existing courses. One of those barriers is the fact that in most courses, the current content fills the entire term. Another issues is that faculty don't have time and expertise to create new content that would fit well with their current content and style. The third problem is that exercises created should be resistant to solution by LLMs. We have developed cybersecurity exercises that combine two principles: environment specificity and polymorphism. Environment specificity means that the solutions to the exercise should depend on the local environment (LLMs don't have access to that information). In this context, polymorphism means that they can be easily modified each time that the class is taught.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {25–27},
numpages = {3}
}

@inproceedings{10.1145/3627217.3627238,
author = {Singhal, Shreya and Kumar, Viraj},
title = {Creating Thorough Tests for AI-Generated Code is Hard},
year = {2023},
isbn = {9798400708404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627217.3627238},
doi = {10.1145/3627217.3627238},
abstract = {Before implementing a function, programmers are encouraged to write a suite of test cases that specify its intended behaviour on several inputs. A suite of tests is thorough if any buggy implementation fails at least one of these tests. We posit that as the proportion of code generated by Large Language Models (LLMs) grows, so must the ability of students to create test suites that are thorough enough to detect subtle bugs in such code. Our paper makes two contributions. First, we demonstrate how difficult it can be to create thorough tests for LLM-generated code by evaluating 27&nbsp;test suites from a public dataset (EvalPlus). Second, by identifying deficiencies in these test suites, we propose strategies for improving the ability of students to develop thorough test suites for LLM-generated code.},
booktitle = {Proceedings of the 16th Annual ACM India Compute Conference},
pages = {108–111},
numpages = {4},
location = {Hyderabad, India},
series = {COMPUTE '23}
}

@inproceedings{10.1145/3641554.3701829,
author = {Liu, Runda and Chen, Shengqi and Chen, Jiajie and Niu, Songjie and Ma, Yuchun and Tang, Xiaofeng},
title = {Iterative Design of a Teaching Assistant Training Program in Computer Science Using the Agile Method},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701829},
doi = {10.1145/3641554.3701829},
abstract = {Facing soaring enrollment and disruptive educational technologies, computing education increasingly relies on the contributions of teaching assistants (TAs), hence the critical importance of high-quality TA training. However, the design and implementation of TA training in computer science face substantial barriers, such as the lack of experienced TA trainers and the scarcity of relevant training materials.This experience report describes the design and implementation of a peer-led computer science TA training program that began in 2022 and has since undergone three iterations, inspired by the approach of agile software development. The current program consists of 10 sessions, organized to serve TAs in three respective stages of professional development. The iterations involved updating and enrichment of the syllabus, transitioning from lecture-centered to discussion-centered training, and discussions of emerging topics in computing education such as the use of large language models (LLMs). Participant feedback showed that TAs approved the iterative design of the training, while identifying areas for further improvement. We summarize lessons learned from the iterative process, reflect on the role of peer TA trainers, and discuss plans for future iterations.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {680–686},
numpages = {7},
keywords = {agile, ta training, teaching assistant},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3626252.3630842,
author = {Amoozadeh, Matin and Daniels, David and Nam, Daye and Kumar, Aayush and Chen, Stella and Hilton, Michael and Srinivasa Ragavan, Sruti and Alipour, Mohammad Amin},
title = {Trust in Generative AI among Students: An exploratory study},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630842},
doi = {10.1145/3626252.3630842},
abstract = {Generative Artificial Intelligence (GenAI) systems have experienced exponential growth in the last couple of years. These systems offer exciting capabilities for CS Education (CSEd), such as generating programs, that students can well utilize for their learning. Among the many dimensions that might affect the effective adoption of GenAI for CSEd, in this paper, we investigate students' trust. Trust in GenAI influences the extent to which students adopt GenAI, in turn affecting their learning. In this paper, we present results from a survey of 253 students at two large universities to understand how much they trust GenAI tools and their feedback on how GenAI impacts their performance in CS courses. Our results show that students have different levels of trust in GenAI. We also observe different levels of confidence and motivation, highlighting the need for further understanding of factors impacting trust.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {67–73},
numpages = {7},
keywords = {generative ai, novice programmers, trust},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3632621.3671434,
author = {Parthasarathy, P D and Lakshmi, T G and Indra, R and Spruha, Satavlekar and Joshi, Swaroop},
title = {Digital Conscience: Investigating the State of Ethics in CS Curricula in India},
year = {2024},
isbn = {9798400704765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632621.3671434},
doi = {10.1145/3632621.3671434},
abstract = {Contemporary computer science graduates enter a world where numerous profound ethical dilemmas confronting society revolve around computing. Navigating the complexities of today’s technological landscape requires a profound consideration of ethical questions. Deliberations on the kinds of data to be gathered or avoided, the ethical dimensions of algorithmic decision-making, and the role of digital platforms in safeguarding democratic principles are at the forefront of discussions. Additionally, addressing biases in large language models (LLMs), ensuring responsible AI practices, and evaluating the technology industry’s contributions to climate change are pivotal aspects.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 2},
pages = {549–550},
numpages = {2},
keywords = {CS Curricula, CSEthics, Computing Education in Global South, India, responsible computing},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@inproceedings{10.1145/3641032.3641055,
author = {Faccia, Alessio and Ridon, Manjeet and Beebeejaun, Zeenat and Mosteanu, Narcisa Mosteanu Roxana},
title = {Advancements and Challenges of Generative AI in Higher Educational Content Creation A Technical Perspective},
year = {2024},
isbn = {9798400709173},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641032.3641055},
doi = {10.1145/3641032.3641055},
abstract = {Generative Artificial Intelligence (AI) has witnessed remarkable advancements, igniting interest in various domains, including Higher Education. This research paper explores the impacts and challenges of integrating Generative AI in content creation within Higher Education. We utilise a literature review and case study approach to gain insights into the potential benefits and complexities of implementing Generative AI in educational settings. Specific research questions are formulated to investigate the influence of Generative AI on content creation efficiency, productivity, quality, and adaptability. The paper also highlights ethical considerations and the evolving role of educators in the AI-driven educational landscape. Furthermore, the research paper examines the practical applications of Generative AI tools such as OpenAI GPT, GPT-Neo, Hugging Face's Transformers Library, Cognii, MosaChat-AI, TeacherMatic, and OpenAI Codex in Higher Education content creation. This comprehensive analysis aims to provide educators, instructional designers, and policymakers with valuable insights and concrete examples of how Generative AI can be leveraged to create personalised learning materials, improve assessment strategies, and enhance the overall educational experience for students pursuing advanced technical subjects. The culmination of this research presents a vision for a future where Generative AI, thoughtfully implemented and ethically managed, empowers educational institutions to meet the diverse and evolving needs of learners in the digital era.},
booktitle = {Proceedings of the 2023 8th International Conference on Information Systems Engineering},
pages = {48–54},
numpages = {7},
keywords = {Applications, Chat GPT, Generative AI, Higher Education},
location = {Bangkok, Thailand},
series = {ICISE '23}
}

@inproceedings{10.1145/3627217.3627234,
author = {Pawagi, Mrigank and Kumar, Viraj},
title = {GuardRails: Automated Suggestions for Clarifying Ambiguous Purpose Statements},
year = {2023},
isbn = {9798400708404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627217.3627234},
doi = {10.1145/3627217.3627234},
abstract = {Before implementing a function, programmers are encouraged to write a purpose statement i.e., a short, natural-language explanation of what the function computes. A purpose statement may be ambiguous i.e., it may fail to specify the intended behaviour when two or more inequivalent computations are plausible on certain inputs. Our paper makes four contributions. First, we propose a novel heuristic that suggests such inputs using Large Language Models (LLMs). Using these suggestions, the programmer may choose to clarify the purpose statement (e.g., by providing a functional example that specifies the intended behaviour on such an input). Second, to assess the quality of inputs suggested by our heuristic, and to facilitate future research, we create an open dataset of purpose statements with known ambiguities. Third, we compare our heuristic against GitHub Copilot’s Chat feature, which can suggest similar inputs when prompted to generate unit tests. Fourth, we provide an open-source implementation of our heuristic as an extension to Visual Studio Code for the Python programming language, where purpose statements and functional examples are specified as docstrings and doctests respectively. We believe that this tool will be particularly helpful to novice programmers and instructors.},
booktitle = {Proceedings of the 16th Annual ACM India Compute Conference},
pages = {55–60},
numpages = {6},
keywords = {CS1, function design, purpose statement},
location = {Hyderabad, India},
series = {COMPUTE '23}
}

@inproceedings{10.1145/3674399.3674426,
author = {Dong, Dong and Liang, Yue},
title = {Grading Programming Assignments by Summarization},
year = {2024},
isbn = {9798400710117},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674399.3674426},
doi = {10.1145/3674399.3674426},
abstract = {Grading programming assignments manually is a big burden for instructors who teach programming languages for university students due to complexity and subjectivity. The black test approach adopted by online judge systems can only outputs either an answer is correct or incorrect. This study proposes a Large Language Model (LLM) approach to automatically grade answers from students for programming assignments. A LLM mode formed by coder-decoder architecture is utilized to generate summarization from source code, then the summarization is compared to the textual assignment description by semantic similarity. Finally, the output is converted to five-score rating. CodeBERT and a Transformer model serve as coder and decoder respectively. The semantic similarity is computed by MiniLM-L6. The validation test shows that the accuracy of the suggested approach reaches 0.92.},
booktitle = {Proceedings of the ACM Turing Award Celebration Conference - China 2024},
pages = {53–58},
numpages = {6},
keywords = {CodeBERT, automatic grading, programming assignment assessment, source code summarization},
location = {Changsha, China},
series = {ACM-TURC '24}
}

@inproceedings{10.1145/3641554.3701953,
author = {Deb, Debzani and Taylor, Greg and Betz, Scott and Maddux, Bao Anh T. and Ebert, C. Edward and Richardson, Flourice W. and Couto, Jeanine Lino S. and Jarrett, Michael S. and Madjd-Sadjadi, Zagros},
title = {Enhancing University Curricula with Integrated AI Ethics Education: A Comprehensive Approach},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701953},
doi = {10.1145/3641554.3701953},
abstract = {As AI technologies become more prevalent, it is crucial for students to develop responsible, ethical, and proactive AI engagement skills. Recent educational initiatives have focused on enhancing CS and engineering students' AI ethics education but have largely overlooked integrating these concepts across other disciplines. This paper presents and assesses a pioneering initiative that integrates AI ethics into university curricula through a collaborative framework between CS and domain educators. We introduced 1-3 week AI ethics modules in seven diverse courses from Art to Chemistry, incorporating case studies and hands-on activities using chat- or image-based Large Language Models (LLMs). Student surveys indicated significant gains in confidence regarding AI ethics discussions, application of principles, and reasoning skills. Our approach advocates for utilizing structured frameworks and faculty collaboration in embedding AI ethics into university curricula, enhancing students' practical skills and ethical understanding across diverse professional settings.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {248–254},
numpages = {7},
keywords = {ai ethics, computing education, ethics education, inclusive computing curricula and pedagogy, non-majors},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3641555.3705178,
author = {Eikmeier, Nicole and Perlmutter, Leah},
title = {Experiences Teaching A Course On Algorithms, Ethics, and Society},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705178},
doi = {10.1145/3641555.3705178},
abstract = {It is essential for CS students to graduate with competence about ethics and societal impacts of technology. We designed and taught a new reading discussion course, at Grinnell College, Algorithms, Ethics, and Society, for advanced undergraduate students who have completed CS1 and CS2. Course topics included Identity in Computing, Tech Ethics, Algorithms Informing Policies, Large Language Models, Networks and Social Media, Health Applications, and Robotics. We encountered some challenges with the discussion format, which we addressed by upholding class norms, employing discussion techniques learned from humanities and social science colleagues, and being open to learn from our mistakes.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1449–1450},
numpages = {2},
keywords = {computer science education, computing and society, technology ethics},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3722237.3722260,
author = {Wu, Yanan and Zeng, Xiaoping and Lin, Qibei},
title = {Generative AI Integrated Educational Model for User-Centered Design},
year = {2025},
isbn = {9798400712692},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3722237.3722260},
doi = {10.1145/3722237.3722260},
abstract = {The advent of artificial intelligence (AI) has profoundly transformed the educational landscape. Many educators are exploring how AI tools can enhance learning instructional programs. However, there is less focus on how its application within design education—particularly when teaching user-centered design. This study developed an educational model utilizing AI for user-centered design curriculum. Based on design thinking theory, this model integrates ChatGPT and Midjourney into the divergent and convergent design phases to facilitate the workflow. The empirical research showed that educational model can foster students’ creativity and problem-solving skills. The findings highlight the efficacy of AI integration in curricula design and instructional practices.},
booktitle = {Proceedings of the 2024 3rd International Conference on Artificial Intelligence and Education},
pages = {129–135},
numpages = {7},
keywords = {Generative AI, design education, design thinking, instructional design, user-centered design},
location = {
},
series = {ICAIE '24}
}

@proceedings{10.1145/3689535,
title = {UKICER '24: Proceedings of the 2024 Conference on United Kingdom &amp; Ireland Computing Education Research},
year = {2024},
isbn = {9798400711770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Manchester, United Kingdom}
}

@inproceedings{10.1145/3702163.3702188,
author = {Schefer-Wenzl, Sigrid and Vogl, Christoph and Peiris, Sahani and Miladinovic, Igor},
title = {Exploring the Adoption of Generative AI Tools in Computer Science Education: A Student Survey},
year = {2025},
isbn = {9798400717819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702163.3702188},
doi = {10.1145/3702163.3702188},
abstract = {The integration of generative AI tools into education has the potential to revolutionize learning experiences, particularly in computer science. This paper explores the adoption and utilization of generative AI tools among computer science students at the University of Applied Sciences Campus Vienna in Austria through a comprehensive survey. The study aims to understand the extent to which AI tools like ChatGPT are integrated into students' academic routines, their perceptions of these tools, and the challenges and opportunities they present. The survey results indicate a high level of acceptance and frequent use of AI tools for tasks such as programming, exam preparation, and generating simplified explanations. However, concerns about the accuracy of AI-generated content and the potential impact on critical thinking skills were also highlighted. The findings underscore the need for clear institutional guidelines and ethical considerations in the use of AI tools in education. This paper contributes to the growing body of literature on AI in education and provides insights for educators and policymakers to enhance the responsible integration of AI technologies in computer science curricula.},
booktitle = {Proceedings of the 2024 16th International Conference on Education Technology and Computers},
pages = {173–178},
numpages = {6},
keywords = {Artificial Intelligence, Computer Science Education, Generative AI Tools, Higher Education},
location = {
},
series = {ICETC '24}
}

@inproceedings{10.1145/3649165.3690125,
author = {Kerslake, Chris and Denny, Paul and Smith, David H. and Prather, James and Leinonen, Juho and Luxton-Reilly, Andrew and MacNeil, Stephen},
title = {Integrating Natural Language Prompting Tasks in Introductory Programming Courses},
year = {2024},
isbn = {9798400705984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649165.3690125},
doi = {10.1145/3649165.3690125},
abstract = {Introductory programming courses often emphasize mastering syntax and basic constructs before progressing to more complex and interesting programs. This bottom-up approach can be frustrating for novices, shifting the focus away from problem solving and potentially making computing less appealing to a broad range of students. The rise of generative AI for code production could partially address these issues by fostering new skills via interaction with AI models, including constructing high-level prompts and evaluating code that is automatically generated. In this experience report, we explore the inclusion of two prompt-focused activities in an introductory course, implemented across four labs in a six-week module. The first requires students to solve computational problems by writing natural language prompts, emphasizing problem-solving over syntax. The second involves students crafting prompts to generate code equivalent to provided fragments, to foster an understanding of the relationship between prompts and code. Most of the students in the course had reported finding programming difficult to learn, often citing frustrations with syntax and debugging. We found that self-reported difficulty with learning programming had a strong inverse relationship with performance on traditional programming assessments such as tests and projects, as expected. However, performance on the natural language tasks was less strongly related to self-reported difficulty, suggesting they may target different skills. Learning how to communicate with AI coding models is becoming an important skill, and natural language prompting tasks may appeal to a broad range of students.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1},
pages = {88–94},
numpages = {7},
keywords = {cs1, eipe, explain in plain english, introductory programming, llm, natural language prompting, prompt engineering},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@inproceedings{10.1145/3649405.3659517,
author = {Glassey, Richard and Baltatzis, Alexander},
title = {Active Repos: Integrating Generative AI Workflows into GitHub},
year = {2024},
isbn = {9798400706035},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649405.3659517},
doi = {10.1145/3649405.3659517},
abstract = {The aim of this work is to describe a simple and cost effective way to integrate generative AI into GitHub to support course specific scenarios. We are motivated by helping teachers realise their creative AI use cases in spite of technical barriers and also to ensure that students have a blessed and fair way to access AI services without needing to sign-up, prompt or pay. First we will describe a scenario that we have implemented for our own CS1 course, then we will describe the technical requirements for implementation. We finish off with our early thoughts on where these types of scenarios might be heading in terms of supporting computing education.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 2},
pages = {777–778},
numpages = {2},
keywords = {CS1, GitHub actions, automation, generative AI},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3643834.3661587,
author = {Long, Tao and Gero, Katy Ilonka and Chilton, Lydia B},
title = {Not Just Novelty: A Longitudinal Study on Utility and Customization of an AI Workflow},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3661587},
doi = {10.1145/3643834.3661587},
abstract = {Generative AI brings novel and impressive abilities to help people in everyday tasks. There are many AI workflows that solve real and complex problems by chaining AI outputs together with human interaction. Although there is an undeniable lure of AI, it is uncertain how useful generative AI workflows are after the novelty wears off. Additionally, workflows built with generative AI have the potential to be easily customized to fit users’ individual needs, but do users take advantage of this? We conducted a three-week longitudinal study with 12 users to understand the familiarization and customization of generative AI tools for science communication. Our study revealed that there exists a familiarization phase, during which users were exploring the novel capabilities of the workflow and discovering which aspects they found useful. After this phase, users understood the workflow and were able to anticipate the outputs. Surprisingly, after familiarization the perceived utility of the system was rated higher than before, indicating that the perceived utility of AI is not just a novelty effect. The increase in benefits mainly comes from end-users’ ability to customize prompts, and thus potentially appropriate the system to their own needs. This points to a future where generative AI systems can allow us to design for appropriation.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {782–803},
numpages = {22},
keywords = {AI chains, LLMs, customization, familiarization, generative AI, longitudinal user experience, mental model, novelty, ownership, scaffolding, science communication, technology appropriation, workflow},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1145/3626253.3633409,
author = {Hazzan, Orit and Erez, Yael},
title = {Generative AI in Computer Science Education},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3633409},
doi = {10.1145/3626253.3633409},
abstract = {Generative AI has the potential to become disruptive technology for computer science education. Therefore, computer science educators must be familiar with the threats they should deal with and with the opportunities that generative-AI opens for the computer science education community. In the workshop, we explore the integration of several generative-AI tools and applications in computer science education. Activities include lesson design, code development, test design and assessment. We address the students' and the educators' perspectives. In addition, we explore computer science practices and soft skills to be applied with these tools as well as immediate and future applications and implications for computer science education and for the society. AT the end of the workshop, the participants will be able to use these generative AI tools in their daily educational computer science activities and beyond.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1899},
numpages = {1},
keywords = {ai, assessment, computer science education, curriculum design, disruptive technology, generative ai, skills},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3699538.3699567,
author = {Amoozadeh, Matin and Nam, Daye and Prol, Daniel and Alfageeh, Ali and Prather, James and Hilton, Michael and Srinivasa Ragavan, Sruti and Alipour, Amin},
title = {Student-AI Interaction: A Case Study of CS1 students},
year = {2024},
isbn = {9798400710384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3699538.3699567},
doi = {10.1145/3699538.3699567},
abstract = {Generative artificial intelligence tools (Generative AI), such as ChatGPT, allow users to interact with them in intuitive ways (e.g., conversational) and receive (mostly) good-quality answers. In education, such systems can support students’ learning objectives by providing accessible explanations and examples even when students pose vague queries. But, they also encourage undesired help-seeking behaviors, such as by providing solutions to the students’ homework. Therefore, it is important to better understand how students approach such tools and the potential issues such approaches might present for the learners.In this paper, we present a case study for understanding student-AI collaboration to solve programming tasks in the CS1 introductory programming course. To this end, we recruited a gender-balanced majority non-white set of 15 CS1 students at the University of Houston, a large public university in the US. We observed them solving programming tasks. We used a mixed-method approach to study their interactions as they tackled Python programming tasks, focusing on when and why they used ChatGPT for problem-solving. We analyze and classify the questions submitted by the 15 participants to ChatGPT. Additionally, we analyzed user interaction patterns, their reactions to ChatGPT’s responses, and the potential impacts of Generative AI on their perception of self-efficacy.Our results suggest that, in about a third of the cases, the student attempted to complete the task by submitting the full description of the tasks to ChatGPT without making any effort on their own. We also observed that few students verified their solutions. We discuss the potential implications of these results.},
booktitle = {Proceedings of the 24th Koli Calling International Conference on Computing Education Research},
articleno = {13},
numpages = {13},
keywords = {Generative Artificial Intelligence, Human-AI Interaction, Self-regulation, CS1, User study, Novice programmers},
location = {
},
series = {Koli Calling '24}
}

@inproceedings{10.1145/3711403.3711410,
author = {Wen, Jiacun and Lin, Yi and Si, Nian},
title = {Behavioral Analysis of Classroom Interactions Supported by Generative Artificial Intelligence},
year = {2025},
isbn = {9798400717468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711403.3711410},
doi = {10.1145/3711403.3711410},
abstract = {Generative artificial intelligence, represented by Chatgpt, has been developing rapidly because of its superiority in form and process, covering almost all industries. In order to comply with the development of technology, some classroom teaching also incorporates it to build a generative artificial intelligence classroom. The classroom interaction behavior has an important reference value to help teachers reconstruct the teaching design and reform the teaching mode. The purpose of this paper is to derive significant behavioral sequence characteristics by coding and recording the actual video of generative artificial intelligence classrooms and analyzing the classroom interaction behaviors using lag sequence analysis. The study shows that the teacher-student interaction in the generative artificial intelligence classroom is more active, and the students' active participation in the classroom is very high, which will further promote the generative artificial intelligence classroom and realize the deep integration of the new technology and the classroom.},
booktitle = {Proceedings of the 2024 7th International Conference on Educational Technology Management},
pages = {49–54},
numpages = {6},
keywords = {Classroom interactive behavior, Generative artificial intelligence, lagged series analysis},
location = {
},
series = {ICETM '24}
}

@inproceedings{10.1145/3677045.3685439,
author = {R\"{o}nnberg, Niklas and B\"{o}r\"{u}tecene, Ahmet},
title = {Use of Generative AI for Fictional Field Studies in Design Courses},
year = {2024},
isbn = {9798400709654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677045.3685439},
doi = {10.1145/3677045.3685439},
abstract = {In this paper, we present how we used generative AI (GenAI) as a pedagogical tool for students taking a course in tangible interaction design. In this course, the students design different physical-digital objects (PDOs) to learn designing, sketching and prototyping with code and hardware. However, due to the short course duration these PDOs are not evaluated or explored with any kind of field or user study. Therefore we gave the students the exercise of doing user interviews with GenAI to explore their design ideas further. With this paper, we contribute a description and the outcomes of this approach, and highlight the pedagogical implications for student learning.},
booktitle = {Adjunct Proceedings of the 2024 Nordic Conference on Human-Computer Interaction},
articleno = {23},
numpages = {5},
keywords = {Design, Education, Field study, Generative AI, User interview},
location = {Uppsala, Sweden},
series = {NordiCHI '24 Adjunct}
}

@inproceedings{10.1145/3626253.3635483,
author = {Lee Solano, Lorenzo and Renzella, Jake and Vassar, Alexandra},
title = {DCC Sidekick: Helping Novices Solve Programming Errors Through a Conversational Explanation Interface},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635483},
doi = {10.1145/3626253.3635483},
abstract = {Students in introductory computing courses often lack the experience required to effectively identify and resolve errors in their code. For such students, Programming Error Messages (PEMs) are often the first indication of an error, and could provide valuable debugging guidance. However, in many cases, such as with standard C compiler implementations, PEMs are largely unsuitable for novices. Confusing, misleading, and filled with terse language and jargon, these messages instead act as an additional source of difficulty.In this paper, we present DCC Sidekick, which integrates the Debugging C Compiler (DCC) with a Large Language Model (LLM) in a web-based dashboard to produce contextual, accurate guidance conducive to student learning. This dashboard is directly accessible from the output of the compiler, and provides a bird's-eye-view of the program source, compiler output, and a conversational AI interface to help unravel cryptic error messages. We aim to deploy DCC Sidekick to a C-based CS1 cohort at a large higher education institution to investigate how novice students utilise the conversational explanation interface during debugging activities. In this work, we present our experience designing and building DCC Sidekick.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1714–1715},
numpages = {2},
keywords = {ai in education, compiler error messages, cs1, error message enhancement, generative ai},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3702212.3702214,
author = {Clift, Lee and Petrovska, Olga},
title = {Learning without Limits: Analysing the Usage of Generative AI in a Summative Assessment},
year = {2025},
isbn = {9798400711725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702212.3702214},
doi = {10.1145/3702212.3702214},
abstract = {This paper explores how Generative AI (GenAI) can be introduced within summative assessment components in software engineering education. We present an example of an assessment which allows learners to use GenAI in a freeform, constructionist manner, as part of a large, software development project. This work is inspired by previously executed AI-focused assessments and surveys, which explicitly indicate that learners on an Applied Software Engineering Degree Apprenticeship Programme want to formally learn how to use GenAI tools when programming and their employers want to see these skills from graduates. The learning outcome of the assignment was for learners to explore a typical developmental pipeline as a solo developer, moving from design to development to finished product. Learners were marked exclusively on their end product and understanding of application components, not the written code itself, resulting in an assessment where the end product and project were prioritised over foundational code (which was adequately assessed in other components). The results show that all learners used GenAI to some extent during their project, and in all cases, they found it beneficial for large programming tasks. Learners were generally able to produce a larger, more comprehensive and more ambitious project, compared to previous years. It is proposed that removing the barrier to GenAI - and demystifying it - can encourage a constructionist approach to its use, and normalise it as a potential tool for programming.},
booktitle = {Proceedings of the 9th Conference on Computing Education Practice},
pages = {5–8},
numpages = {4},
keywords = {GenAI, software engineering, education, apprenticeship},
location = {
},
series = {CEP '25}
}

@inproceedings{10.1145/3702212.3702223,
author = {Petrovska, Olga and Pearsall, Rebecca and Clift, Lee},
title = {Assessing Software Engineering Students' Analytical Skills in the Era of Generative AI},
year = {2025},
isbn = {9798400711725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702212.3702223},
doi = {10.1145/3702212.3702223},
abstract = {This poster showcases an assessment designed to develop and evaluate software engineering students’ code analysis skills. We demonstrate how students approached code analysis tasks when given multiple code samples created by a human and various AI tools.},
booktitle = {Proceedings of the 9th Conference on Computing Education Practice},
pages = {34},
numpages = {1},
keywords = {generative AI, software engineering, education, apprenticeship},
location = {
},
series = {CEP '25}
}

@inproceedings{10.1145/3686852.3687075,
author = {Beaton, Catherine and Weeden, Elissa and Zilora, Stephen},
title = {Instructional Approaches Complementing the Use of Generative Artificial Intelligence in Higher Education},
year = {2024},
isbn = {9798400711060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686852.3687075},
doi = {10.1145/3686852.3687075},
abstract = {The explosion of generative artificial intelligence (AI) has created a level of chaos in higher education as both students and faculty try to determine its utility and how best to incorporate it into the learning process. Students may view generative AI as a means to an end of achieving a perfect grade, skipping important elements of the learning process, or they may view it as an opportunity to expand their creative efforts. Faculty may view it as a tool students use to circumvent plagiarism detection, may feel it potentially minimizes the role of faculty in the classroom, or they may view it as an opportunity to avail of a supplement to existing activities and assignments. Ultimately, faculty are faced with maintaining academic integrity and reinforcing the need and importance of the learning process. This paper explores the combination of three approaches: peer-supported incremental learning, master/apprentice model, and growth mindset as a way for faculty to guide appropriate student use of generative AI, while also maintaining the integrity of the learning process.},
booktitle = {Proceedings of the 25th Annual Conference on Information Technology Education},
pages = {62–67},
numpages = {6},
keywords = {Artificial intelligence, Growth mindset, Master/Apprentice model, Peer-supported incremental learning},
location = {El Paso, TX, USA},
series = {SIGITE '24}
}

@inproceedings{10.1145/3641554.3701957,
author = {Basit, Nada and Floryan, Mark and Hott, John R. and Huo, Allen and Le, Jackson and Zheng, Ivan},
title = {ASCI: AI-Smart Classroom Initiative},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701957},
doi = {10.1145/3641554.3701957},
abstract = {The Artificial Intelligence Smart Classroom Initiative (ASCI) presents a re-imagined set of online course tools, designed primarily to support growing computer science classes. The system has four primary tools: an office hours queue, an automatic student grouping algorithm, a course-specific local large-language model (LLM), and administration tools for detecting students and TAs that need support. These tools interoperate to improve the quality of one another (e.g., LLM conversations support students directly in the office hours queue) and are enhanced by synchronizing data from multiple external sources such as Piazza, Gradescope, and Canvas. The system has been deployed in multiple courses over the past three semesters: initially as a FIFO queue, then supporting manual grouping and smart grouping of office hour attendees, and recently including LLM support. Preliminary results indicate that students who were grouped using the tool were more likely to return to the queue more than twice as often (on average) than those who were not. However, while grouping in office hours has the potential to decrease student wait times, teaching assistants and students tend to favor one-on-one meetings over group meetings. This might be improved in the future with updates to the software, TA training, and incorporation of other supporting tools (e.g., LLM technology). The other, newer, tools will be more thoroughly evaluated in future semesters.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {81–87},
numpages = {7},
keywords = {computer science education, cosine similarity, group formation, office hours},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3626252.3630937,
author = {Grover, Shuchi},
title = {Teaching AI to K-12 Learners: Lessons, Issues, and Guidance},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630937},
doi = {10.1145/3626252.3630937},
abstract = {There is growing recognition of the need to teach artificial intelli- gence (AI) and machine learning (ML) at the school level. This push acknowledges the meteoric growth in the range and diversity of ap- plications of ML in all industries and everyday consumer products, with Large Language Models (LLMs) being only the latest and most compelling example yet. Efforts to bring AI, especially ML educa- tion to school learners are being propelled by substantial industry interest, research efforts, as well as technological developments that make sophisticated ML tools readily available to learners of all ages. These early efforts span a variety of learning goals captured by the AI4K12 "big ideas" framework and employ a plurality of pedagogies.This paper provides a sense for the current state of the field, shares lessons learned from early K-12 AI education as well as CS education efforts that can be leveraged, highlights issues that must be addressed in designing for teaching AI in K-12, and provides guidance for future K-12 AI education efforts and tackle what to many feels like "the next new thing".},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {422–428},
numpages = {7},
keywords = {artificial intelligence, k-12 ai education, k-12 cs education, machine learning},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3657604.3662030,
author = {Moore, Steven and Schmucker, Robin and Mitchell, Tom and Stamper, John},
title = {Automated Generation and Tagging of Knowledge Components from Multiple-Choice Questions},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3662030},
doi = {10.1145/3657604.3662030},
abstract = {Knowledge Components (KCs) linked to assessments enhance the measurement of student learning, enrich analytics, and facilitate adaptivity. However, generating and linking KCs to assessment items requires significant effort and domain-specific knowledge. To streamline this process for higher-education courses, we employed GPT-4 to generate KCs for multiple-choice questions (MCQs) in Chemistry and E-Learning. We analyzed discrepancies between the KCs generated by the Large Language Model (LLM) and those made by humans through evaluation from three domain experts in each subject area. This evaluation aimed to determine whether, in instances of non-matching KCs, evaluators showed a preference for the LLM-generated KCs over their human-created counterparts. We also developed an ontology induction algorithm to cluster questions that assess similar KCs based on their content. Our most effective LLM strategy accurately matched KCs for 56% of Chemistry and 35% of E-Learning MCQs, with even higher success when considering the top five KC suggestions. Human evaluators favored LLM-generated KCs, choosing them over human-assigned ones approximately two-thirds of the time, a preference that was statistically significant across both domains. Our clustering algorithm successfully grouped questions by their underlying KCs without needing explicit labels or contextual information. This research advances the automation of KC generation and classification for assessment items, alleviating the need for student data or predefined KC labels.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {122–133},
numpages = {12},
keywords = {concept labeling, knowledge component, knowledge labeling, learning engineering, multiple-choice question},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@inproceedings{10.1145/3640544.3645215,
author = {Laney, Mason and Dewan, Prasun},
title = {Human-AI Collaboration in a Student Discussion Forum},
year = {2024},
isbn = {9798400705090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640544.3645215},
doi = {10.1145/3640544.3645215},
abstract = {The recent public releases of AI tools such as ChatGPT have forced computer science educators to reconsider how they teach. These tools have demonstrated considerable ability to generate code and answer conceptual questions, rendering them incredibly useful for completing CS coursework. While overreliance on AI tools could hinder students’ learning, we believe they have the potential to be a helpful resource for both students and instructors alike. We propose a novel system for instructor-mediated GPT interaction in a class discussion board. By automatically generating draft responses to student forum posts, GPT can help Teaching Assistants (TAs) respond to student questions in a more timely manner, giving students an avenue to receive fast, quality feedback on their solutions without turning to ChatGPT directly. Additionally, since they are involved in the process, instructors can ensure that the information students receive is accurate, and can provide students with incremental hints that encourage them to engage critically with the material, rather than just copying an AI-generated snippet of code. We utilize Piazza—a popular educational forum where TAs help students via text exchanges—as a venue for GPT-assisted TA responses to student questions. These student questions are sent to GPT-4 alongside assignment instructions and a customizable prompt, both of which are stored in editable instructor-only Piazza posts. We demonstrate an initial implementation of this system, and provide examples of student questions that highlight its benefits.},
booktitle = {Companion Proceedings of the 29th International Conference on Intelligent User Interfaces},
pages = {74–77},
numpages = {4},
location = {Greenville, SC, USA},
series = {IUI '24 Companion}
}

@inproceedings{10.1145/3674399.3674423,
author = {Xu, Ke and Yi, Hanxiao and Xu, Zichen and Wu, Dan},
title = {Data-driven Contribution-based Disciplinary Assessment System},
year = {2024},
isbn = {9798400710117},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674399.3674423},
doi = {10.1145/3674399.3674423},
abstract = {A scientific disciplinary assessment system is crucial for nurturing high-quality disciplines within Computer Science. Computer Science Education (CSE) emphasizes the need for a scientific and comprehensive assessment method that guides the development of the discipline, with a particular focus on practical contributions. However, traditional assessment systems tend to prioritize the theoretical outcomes. Moreover, data expansion demands significant effort and time from educational professionals, making it challenging to conduct a thorough evaluation of the disciplines. To tackle these issues, we introduce a data-driven, contribution-based disciplinary assessment system. This system takes into account both theoretical and practical contributions to provide a holistic evaluation. Our proposed system employs a contribution-based assessment approach to establish a correct evaluative direction, steering discipline construction to align with societal needs. It also incorporates intelligent algorithms and a Large Language Model (LLM), leveraging their substantial computational power in the evaluation process. This integration alleviates the workload of educational professionals by automating the collection and analysis of information. The paper outlines a detailed implementation plan that integrates contribution evaluation theory with intelligent technologies, aiming to foster the ongoing advancement of CSE education.},
booktitle = {Proceedings of the ACM Turing Award Celebration Conference - China 2024},
pages = {42–47},
numpages = {6},
keywords = {Big Data-driven, Contribution-Based Evaluation Method, Disciplinary assessment},
location = {Changsha, China},
series = {ACM-TURC '24}
}

@inproceedings{10.1145/3649217.3653596,
author = {Apiola, Mikko and Vartiainen, Henriikka and Tedre, Matti},
title = {First Year CS Students Exploring And Identifying Biases and Social Injustices in Text-to-Image Generative AI},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653596},
doi = {10.1145/3649217.3653596},
abstract = {Generative AI is a recent breakthrough in AI. While it has become a hot topic in computing education research (CER), much of the recent research has focused on e.g. issues of plagiarism or academic integrity. One problem spot with Generative AI is its susceptibility to various kinds of algorithmic bias. In this study, we collected data from an introductory computing course, where students experimented with text-to-image generative models and reflected on their generated image sets, in terms of biases, related harms, and possible fixes. Data were collected in Fall 2023 (pilot data in Fall 2022). Data included reports from 163 students. The results show (1) a variety of bias types observed by students related to gender, ethnicity, age, as well as a variety of bias types not observed by students, (2) two major types of attributions for the source of bias: bias caused by biases in the society and bias caused by data or algorithms, and (3) a number of potential harms associated with the biases, as well as attributions of those harms in specific contexts and use cases.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {485–491},
numpages = {7},
keywords = {bias, critical computing education, generative ai, social injustice},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3649409.3691073,
author = {Barendsen, Erik and Lonati, Violetta and Quille, Keith and Altin, Rukiye and Divitini, Monica and Hooshangi, Sara and Karnalim, Oscar and Kiesler, Natalie and Melton, Madison and Suero Montero, Calkin and Morpurgo, Anna},
title = {AI in and for K-12 Informatics Education. Life after Generative AI.},
year = {2024},
isbn = {9798400706042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649409.3691073},
doi = {10.1145/3649409.3691073},
abstract = {The use and adoption of Generative AI (GenAI) has revolutionised various sectors, including computing education. However, this narrow focus comes at a cost to the wider AI in and for educational research. This working group aims to explore current trends and explore multiple sources of information to identify areas of AI research in K-12 informatics education that are being underserved but needed in the post-GenAI AI era. Our research focuses on three areas: curriculum, teacher-professional learning and policy. The denouement of this aims to identify trends and shortfalls for AI in and for K-12 informatics education. We will systematically review the current literature to identify themes and emerging trends in AI education at K-12. This will be done under two facets, curricula and teacher-professional learning. In addition, we will conduct interviews and surveys with educators and AI experts. Next, we will examine the current policy (such as the European AI Act, and European Commission guidelines on the use of AI and data in education and training as well as international counterparts). Policies are often developed by both educators and experts in the domain, thus providing a source of topics or areas that may be added to our findings. Finally, by synthesising insights from educators, AI experts, and policymakers, as well as the literature and policy, our working group seeks to highlight possible future trends and shortfalls.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 2},
pages = {279–280},
numpages = {2},
keywords = {AI, GenAI, K-12, curricula, generative AI, informatics},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@inproceedings{10.1145/3626252.3630764,
author = {Wang, Sierra and Mitchell, John and Piech, Chris},
title = {A Large Scale RCT on Effective Error Messages in CS1},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630764},
doi = {10.1145/3626252.3630764},
abstract = {In this paper, we evaluate the most effective error message types through a large-scale randomized controlled trial conducted in an open-access, online introductory computer science course with 8,762 students from 146 countries. We assess existing error message enhancement strategies, as well as two novel approaches of our own: (1) generating error messages using OpenAI's GPT in real time and (2) constructing error messages that incorporate the course discussion forum. By examining students' direct responses to error messages, and their behavior throughout the course, we quantitatively evaluate the immediate and longer term efficacy of different error message types. We find that students using GPT generated error messages repeat an error 23.1% less often in the subsequent attempt, and resolve an error in 34.8% fewer additional attempts, compared to students using standard error messages. We also perform an analysis across various demographics to understand any disparities in the impact of different error message types. Our results find no significant difference in the effectiveness of GPT generated error messages for students from varying socioeconomic and demographic backgrounds. Our findings underscore GPT generated error messages as the most helpful error message type, especially as a universally effective intervention across demographics.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1395–1401},
numpages = {7},
keywords = {cs1, error messages, gpt, llm, randomized control trial},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3626253.3635595,
author = {Hamerski, Patti C.},
title = {Generative AI as a Resource for Creativity in Computational Physics},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635595},
doi = {10.1145/3626253.3635595},
abstract = {Generative artificial intelligence (gen-AI) has become ubiquitous in daily life, including classroom environments where students are using it to assist them on their coursework. Given the widespread use of this tool and the lack of knowledge over how it can support learning, there is a need for educators to have a framework for using it in the classroom and teaching their students usage strategies that are beneficial for learning. One pathway forward is through creativity, a process crucial for learning and also connected to the act of using gen-AI. This poster demonstrates the results of a study designed to provide an in-depth view on how creativity intersects with gen-AI usage in a computational physics course. In the course, students learn about computing tools during group-based, open-ended computational physics activities. Students are often tasked with using gen-AI to explore and help make decisions. The findings demonstrate a connection between using gen-AI and engaging in creative processes, and the implications point to strategies for supporting student usage of gen-AI.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1666–1667},
numpages = {2},
keywords = {computational science, creativity, curriculum design, generative ai},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3641554.3701859,
author = {Gorson Benario, Jamie and Marroquin, Jenn and Chan, Monica M. and Holmes, Ernest D.V. and Mejia, Daniel},
title = {Unlocking Potential with Generative AI Instruction: Investigating Mid-level Software Development Student Perceptions, Behavior, and Adoption},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701859},
doi = {10.1145/3641554.3701859},
abstract = {Generative AI tools are rapidly evolving and impacting many domains, including programming. Computer Science (CS) instructors must address student access to these tools. While some advocate to ban the tools entirely, others suggest embracing them so that students develop the skills for utilizing the tools safely and responsibly. Studies indicate positive impacts, as well as cautions, on student outcomes when these tools are integrated into courses. We studied the impact of incorporating instruction on industry-standard generative AI tools into a mid-level software development course with students from 16 Minority Serving Institutions. 89% of student participants used generative AI tools prior to the course without any formal instruction. After formal instruction, students most frequently used generative AI tools for explaining concepts and learning new things. Students generally reported positive viewpoints on their ability to learn to program and learn problem-solving skills while using generative AI tools. Finally, we found that students: reported to understand their code when they work with generative AI tools, are critical about the outputs that generative AI tools provide, and check outputs of generative AI tools to ensure accuracy.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {395–401},
numpages = {7},
keywords = {cs education, generative ai, llms in cs education, minority serving institutions},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3593342.3593348,
author = {Naringrekar, Pranjal Dilip and Akhmetov, Ildar and Stroulia, Eleni},
title = {Generating CS1 Coding Questions using OpenAI},
year = {2023},
isbn = {9798400707896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593342.3593348},
doi = {10.1145/3593342.3593348},
abstract = {In CS1, to assess student knowledge, instructors prepare exam questions that often include code snippets. Due to the significant amount of time and effort required to create high-quality exam questions, instructors often only produce a single version of the exam. This results in all students receiving the same set of questions, which raises the possibility of plagiarism. In this paper, we propose a tool that allows computing science educators to generate a number of variations of a given code snippet, where the pedagogical intent of the code remains the same, but the code is mutated.},
booktitle = {Proceedings of the 25th Western Canadian Conference on Computing Education},
articleno = {11},
numpages = {2},
keywords = {coding questions, code generation, CS1},
location = {Vancouver, BC, Canada},
series = {WCCCE '23}
}

@inproceedings{10.1145/3670013.3670058,
author = {Torrato, Janette B. and Pillar, Genevieve A. and Robledo, Dave Arthur R. and Aguja, Socorro E. and Prudente, Maricar S.},
title = {Knowledge, Attitudes, and Practices on ChatGPT:Perspectives from Students and Teachers of De La Salle Santiago Zobel School},
year = {2024},
isbn = {9798400717062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670013.3670058},
doi = {10.1145/3670013.3670058},
abstract = {Abstract. Chat Generative Pre-trained Transformer (ChatGPT) is an artificial intelligence (AI) system that is gaining popularity among students and teachers. However, in the basic education level, the use of ChatGPT is still an ongoing discussion, particularly on how to regulate its use. This study endeavors to describe the perspectives of teachers and students about ChatGPT in terms of their knowledge, attitudes, and practices to better appreciate its role in advancing teaching and learning. This descriptive survey involved a total of (N=187) respondents, including students from Grade 6 (n=70), Grade 10 (n=38), and Grade 12 (n=18) and teachers (n=61). The newly developed 39-item Knowledge, Attitudes, Practices on ChatGPT Questionnaire (KAP-CQ39) with a Cronbach α = 0.91 was used as the primary instrument in understanding and leveraging the academic potential of this AI system. Open-ended questions on the advantages and disadvantages of using ChatGPT are also included in the questionnaire. KAP-CQ39 was administered online using Google Forms. Data culled from the survey was analyzed using an online open-source program referred to as Jeffreys's Amazing Statistics Program (JASP). Results revealed no significant difference in perspectives between teachers (mean=11.88) and students (mean=11.46) knowledge on ChatGPT F (2,169) =2.66, p=0.104. Similarly, attitudes towards the educational use of ChatGPT showed that both teachers and students hold positive attitudes. Demographic factors contributing to the differences in teachers’ perspectives on the educational use of ChatGPT were sex, years of teaching experience, and specialization. For the students, the demographic factors did not contribute to the differences in their perspectives. Generally, in terms of practices, responses provided valuable insights into how ChatGPT can be better designed and implemented for teaching and learning. Thus, policy implications were drawn relative to the efficient use of ChatGPT.},
booktitle = {Proceedings of the 2024 15th International Conference on E-Education, E-Business, E-Management and E-Learning},
pages = {107–116},
numpages = {10},
location = {Fukuoka-shi, Japan},
series = {IC4E '24}
}

@inproceedings{10.1145/3657604.3664682,
author = {Pitts, Griffin and Marcus, Viktoria and Motamedi, Sanaz},
title = {A Proposed Model of Learners' Acceptance and Trust of Pedagogical Conversational AI},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3664682},
doi = {10.1145/3657604.3664682},
abstract = {Conversational AI (C-AI), like OpenAI's ChatGPT [38] or Google's Gemini [1], has seen a surge in development in recent years, driven by advancements in large language models. C-AI has the unique capability to instantaneously communicate with others using vast and contextual knowledge, providing personalized assistance tailored to individual needs. While the specific applications and advantages of conversational technologies are still being explored, prior research has noted the potential for conversational agents to serve in pedagogical settings, such as teaching agents, collaborative partners, or motivational tools [13,30]. The successful development and implementation of pedagogical C-AI relies on an understanding of learners' perceptions, trust, and overall acceptance of C-AI. There is a need for a comprehensive understanding of the factors influencing learners' trust and acceptance of this emerging technology.Grounded in the theories proposed by the Technology Acceptance Model (TAM) [16], the Unified Theory of Acceptance and Use of Technology (UTAUT) [47], and Mayer, Davis and Schoorman's model of organizational trust [35], this paper proposes a model of learners' acceptance and trust for C-AI. The proposed model integrates factors relating to learners' perceptions and trust of C-AI, and additionally, four groups of moderating variables (e.g. individual characteristics). The proposed model hypothesizes that learners' perceptions and trust significantly influence their acceptance and behavioral intention to use C-AI. The relationships between learners' perceptions, trust, and acceptance are additionally proposed to be influenced by four groups of moderating variables: individual characteristics, AI characteristics, facilitating conditions, and subjective norms.To validate the proposed model, we plan to collect survey data and leverage structural equation modeling (SEM) techniques. We will specifically evaluate the fitness of the proposed model through chi-square tests, RMSEA, CFI, and SRMR metrics. This study can provide valuable insights toward the scalable adoption and usage of C-AI in learning environments. Understanding the structural relationships between the factors that influence learners' acceptance and trust of pedagogical C-AI will be crucial for designing and deploying these technologies in ways that foster learners' engagement, self-efficacy, academic interest, and ultimately lead to positive learning outcomes.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {427–432},
numpages = {6},
keywords = {ai in education, ai trust, conversational agents, conversational ai, human-ai interaction, intelligent agents, pedagogical agents, technology acceptance model},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@inproceedings{10.1145/3568812.3603469,
author = {Amoozadeh, Matin and Daniels, David and Chen, Stella and Nam, Daye and Kumar, Aayush and Hilton, Michael and Alipour, Mohammad Amin and Ragavan, Sruti Srinivasa},
title = {Towards Characterizing Trust in Generative Artificial Intelligence among Students},
year = {2023},
isbn = {9781450399753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568812.3603469},
doi = {10.1145/3568812.3603469},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2},
pages = {3–4},
numpages = {2},
keywords = {Generative AI, Novice programmers, Trust},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3626252.3630887,
author = {Malik, Ali and Woodrow, Juliette and Piech, Chris},
title = {Learners Teaching Novices: An Uplifting Alternative Assessment},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630887},
doi = {10.1145/3626252.3630887},
abstract = {We propose and carry-out a novel method of formative assessment called Assessment via Teaching (AVT), in which learners demonstrate their understanding of CS1 topics by tutoring more novice students. AVT has powerful benefits over traditional forms of assessment: it is centered around service to others and is highly rewarding for the learners who teach. Moreover, teaching greatly improves the learners' own understanding of the material and has a huge positive impact on novices, who receive free 1:1 tutoring. Lastly, this form of assessment is naturally difficult to cheat---a critical property for assessments in the era of large-language models. We use AVT in a randomised control trial with learners in a CS1 course at an R1 university. The learners provide tutoring sessions to more novice students taking a lagged online version of the same course. We show that learners who do an AVT session before the course exam performed 20 to 30 percentage points better than the class average on several questions. Moreover, compared to students who did a practice exam, the AVT learners enjoyed their experience more and were twice as likely to study for their teaching session. We believe AVT is a scalable and uplifting method for formative assessment that could one day replace traditional exams.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {785–791},
numpages = {7},
keywords = {formative assessment, learning at scale, online courses, peer teaching, student-led teaching, studying strategies},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3633083.3633099,
author = {Stone, Irene},
title = {Exploring the Research Gap: Generative AI and Learning of Python Programming among Post-Primary Students},
year = {2023},
isbn = {9798400716461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3633083.3633099},
doi = {10.1145/3633083.3633099},
abstract = {The introduction of Leaving Certificate Computer Science (LCCS) in Ireland in 2018 signifies a notable advancement in post-primary education. Moreover, developments in generative Artificial Intelligence (GAI) in education, are gaining prominence, yet we do not understand its value or how best to implement it in post-primary educational settings. Despite a growing international body of research in this area, my scoping review highlights that many aspects of these topics have yet to be explored, particularly in the context of post-primary students in Ireland. My study will begin to bridge this gap by exploring how a purposeful sample of LCCS post-primary students in Ireland engage with GAI tools, such as ChatGPT, during their initial experiences learning Python programming. These findings, when approached through the lens of Human-Centred Artificial Intelligence (HCAI), can help enhance pedagogical strategies and lead to improved learning experiences for students.},
booktitle = {Proceedings of the 2023 Conference on Human Centered Artificial Intelligence: Education and Practice},
pages = {51},
numpages = {1},
location = {Dublin, Ireland},
series = {HCAIep '23}
}

@proceedings{10.1145/3716640,
title = {ACE '25: Proceedings of the 27th Australasian Computing Education Conference},
year = {2025},
isbn = {9798400714252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3723178.3723268,
author = {Sadat Shanto, Shakib and Ahmed, Zishan and Jony, Akinul Islam},
title = {Generative AI for Programming Education: Can ChatGPT Facilitate the Acquisition of Fundamental Programming Skills for Novices?},
year = {2025},
isbn = {9798400713828},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3723178.3723268},
doi = {10.1145/3723178.3723268},
abstract = {Modern Generative AI (GAI) systems like ChatGPT have sparked much interest in their potential to revolutionize programming education, especially for beginners. However, the existing empirical data regarding the effectiveness of technologies like ChatGPT as autonomous programming tutors is presently limited. The present study investigates the capacity of ChatGPT to facilitate the acquisition of fundamental programming skills for novice programmers without human assistance. This study puts forth a conceptual framework (APEC - Adaptive Programming Education via ChatGPT) that integrates both bottom-up and top-down approaches, incorporating ChatGPT as the principal instructor for the study of programming. An empirical study was undertaken to assess the usefulness of ChatGPT as a tool for teaching novice programmers a new programming language. This empirical study was conducted on 20 undergraduate students. To provide an expert assessment of the quality of the responses, a survey was conducted with three programming experts proficient in Python. The survey findings indicate that ChatGPT is proficient in explaining core principles such as variables, data types, and control statements through conversational exchanges, adopting an intelligent and logical methodology. Nevertheless, certain constraints arise when dealing with increasingly complex topics.},
booktitle = {Proceedings of the 3rd International Conference on Computing Advancements},
pages = {685–692},
numpages = {8},
keywords = {Generative AI, ChatGPT, Programming Education, Educational Technology, Higher Education},
location = {
},
series = {ICCA '24}
}

@inproceedings{10.1145/3699538.3699591,
author = {Keuning, Hieke and Luxton-Reilly, Andrew and Ott, Claudia and Petersen, Andrew and Kiesler, Natalie},
title = {Goodbye Hello World - Research Questions for a Future CS1 Curriculum},
year = {2024},
isbn = {9798400710384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3699538.3699591},
doi = {10.1145/3699538.3699591},
abstract = {Generative AI (GenAI) is currently capable of generating correct code for introductory level programming problems, and its performance is improving. We believe that this capability can be leveraged to improve student motivation, broaden students’ understanding of software development, and engage them in more authentic learning. We defined a set of assumptions about GenAI’s future capabilities (e.g., the ability to generate small pieces of code and to compose these pieces of code via user prompts) and engaged in a backcasting exercise to identify what else is needed to develop a CS1 course that places GenAI in a central role. Undertaking this thought experiment immediately revealed that aspects of the software development process usually reserved for later in the curriculum, such as requirements elicitation and design, could be introduced earlier in the process. With GenAI tools bearing the load of generating correct code snippets, students could focus on higher-level software design and construction skills and practice them in an authentic environment. Our thought experiment identified a set of questions that need to be addressed for such a course to actually exist, including questions about student preparation, and the ability of students to decompose problems effectively and to resolve problems that arise when integrating pieces of code. We also identified questions related to the design of a GenAI centered course, such as the impact on student motivation of using GenAI instead of engaging directly with code, the extent to which social learning theories apply to interactions with GenAI, and how existing pedagogies can integrate GenAI tools.},
booktitle = {Proceedings of the 24th Koli Calling International Conference on Computing Education Research},
articleno = {27},
numpages = {2},
keywords = {Computing education, CS1, Generative AI, Mastery Learning, LLMs},
location = {
},
series = {Koli Calling '24}
}

@inproceedings{10.1145/3702386.3702391,
author = {Zhang, Li and Li, Xiaohua and Kong, Xiangdan},
title = {Investigating AI-Integrated Ecological Civilization Education: Opportunities and Challenges in a Qualitative Study},
year = {2025},
isbn = {9798400710131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702386.3702391},
doi = {10.1145/3702386.3702391},
abstract = {This study aims to explore the opportunities and challenges of integrating artificial intelligence (AI) into ecological civilization education through a qualitative research approach. A 12-week ecological civilization course was developed using the flipped classroom model, leveraging ChatGPT to support instructional activities. The study involved interviews with eight students (four sophomores and four juniors) to investigate their usage, attitudes, and feedback on AI tools. The findings indicate that ChatGPT significantly enhances learning outcomes, improves assignment and project quality, and boosts creativity and innovation. However, students also highlighted issues with information accuracy and practical application challenges. Based on these findings, the study offers recommendations for optimizing AI tool usage in future courses and emphasizes the importance of AI-human collaboration in solving complex problems.},
booktitle = {Proceedings of the 2024 International Conference on Artificial Intelligence and Teacher Education},
pages = {70–75},
numpages = {6},
keywords = {Artificial Intelligence, ChatGPT, Ecological Civilization Education, Learning Outcomes, Student Feedback},
location = {
},
series = {ICAITE '24}
}

@inproceedings{10.1145/3626253.3635524,
author = {Bevilacqua, Joey and Chiodini, Luca and Moreno Santos, Igor and Hauswirth, Matthias},
title = {Using Notional Machines to Automatically Assess Students' Comprehension of Their Own Code},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635524},
doi = {10.1145/3626253.3635524},
abstract = {Code comprehension has been shown to be challenging and important for a positive learning outcome. Students don't always understand the code they write. This has been exacerbated by the advent of large language models that automatically generate code that may or may not be correct. Now students don't just have to understand their own code, but they have to be able to critically analyze automatically generated code as well. To help students with code comprehension, instructors often use notional machines. Notional machines are used not only by instructors to explain code, but also in activities or exam questions given to students. Traditionally, these questions involve code that was not written by students. However, asking questions to students about their own code (Questions on Learners' Code, QLCs) has been shown to strengthen their code comprehension. This poster presents an approach to combine notional machines and QLCs to automatically generate personalized questions about learners' code based on notional machines. Our aim is to understand whether notional machine-based QLCs are effective. We conducted a pilot study with 67 students to test our approach, and we plan to conduct a comprehensive empirical evaluation to study its effectiveness.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1572–1573},
numpages = {2},
keywords = {assessment, code comprehension, expressions, notional machines, programmming},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3632634.3655868,
author = {Sumner, Mary and Van Slyke, Craig and Niederman, Fred and Galletta, Dennis},
title = {Panel: Using Generative AI in Teaching and Learning.},
year = {2024},
isbn = {9798400704772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632634.3655868},
doi = {10.1145/3632634.3655868},
booktitle = {Proceedings of the 2024 Computers and People Research Conference},
articleno = {37},
numpages = {2},
location = {Murfreesboro, TN, USA},
series = {SIGMIS-CPR '24}
}

@inproceedings{10.1145/3587103.3594155,
author = {Wermelinger, Michel},
title = {Checking Conformance to a Subset of the Python Language},
year = {2023},
isbn = {9798400701399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587103.3594155},
doi = {10.1145/3587103.3594155},
abstract = {Introductory courses usually only teach a small subset of a programming language and its library, in order to focus on the general concepts rather than overwhelm students with the syntactic, semantic and API minutiae of a particular language.This paper presents courseware that checks if a program only uses the subset of the Python language and library defined by the instructor. This allows to automatically check that programming examples, exercises and assessments only use the taught constructs. It also helps detect student code with advanced constructs, possibly copied from Q&amp;A sites or generated by large language models.The tool is easy to install, configure and use. It also checks Python code in Jupyter notebooks, a popular format for interactive textbooks and assessment handouts.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2},
pages = {573–574},
numpages = {2},
keywords = {academic integrity, code checking, introductory programming, novice programming, programming exercises},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@article{10.1145/3715112,
author = {Betz, Stefanie and Penzenstadler, Birgit},
title = {With Great Power Comes Great Responsibility: The Role of Software Engineers},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3715112},
doi = {10.1145/3715112},
abstract = {The landscape of Software Engineering evolves rapidly amidst digital transformation and the ascendancy of AI, leading to profound shifts in the role and responsibilities of Software Engineers. This evolution encompasses both immediate changes, such as the adoption of Large Language Model-based approaches to coding, and deeper shifts driven by the profound societal and environmental impacts of technology. Despite the urgency, there persists a lag in adapting to these evolving roles. This roadmap article proposes 10 research challenges to develop a new generation of Software Engineers equipped to navigate the technical and social complexities as well as ethical considerations inherent in their evolving profession. Furthermore, the challenges target role definition, integration of AI, education transformation, standards evolution, and impact assessment to equip future Software Engineers to skillfully and responsibly handle the obstacles within their transforming discipline.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {136},
numpages = {21},
keywords = {Sustainability, Responsibility, Roles, Ethics}
}

@inproceedings{10.1145/3706598.3713832,
author = {Jain, Yoshee and Demirtas, Mehmet Arif and Cunningham, Kathryn Irene},
title = {PLAID: Supporting Computing Instructors to Identify Domain-Specific Programming Plans at Scale},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713832},
doi = {10.1145/3706598.3713832},
abstract = {Pedagogical approaches focusing on stereotypical code solutions, known as programming plans, can increase problem-solving ability and motivate diverse learners. However, plan-focused pedagogies are rarely used beyond introductory programming. Our formative study (N=10 educators) showed that identifying plans is a tedious process. To advance plan-focused pedagogies in application-focused domains, we created an LLM-powered pipeline that automates the effortful parts of educators’ plan identification process by providing use-case-driven program examples and candidate plans. In design workshops (N=7 educators), we identified design goals to maximize instructors’ efficiency in plan identification by optimizing interaction with this LLM-generated content. Our resulting tool, PLAID, enables instructors to access a corpus of relevant programs to inspire plan identification, compare code snippets to assist plan refinement, and facilitates them in structuring code snippets into plans. We evaluated PLAID in a within-subjects user study (N=12 educators) and found that PLAID led to lower cognitive demand and increased productivity compared to the state-of-the-art. Educators found PLAID beneficial for generating instructional material. Thus, our findings suggest that human-in-the-loop approaches hold promise for supporting plan-focused pedagogies at scale.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {52},
numpages = {21},
keywords = {programming plan, programming pattern, pattern identification, instructor support},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3605468.3609775,
author = {Philbin, Carrie Anne},
title = {Impact of Generative AI on K-12 Students’ Perceptions of Computing: A Research Proposal},
year = {2023},
isbn = {9798400708510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605468.3609775},
doi = {10.1145/3605468.3609775},
abstract = {The rapid progress of generative artificial intelligence (AI) is fundamentally reshaping traditional perspectives on knowledge and skills, with profound implications for computing education. This necessitates a thorough examination of the relevance and timeliness of computing as a subject, especially for K-12 students who are making critical decisions about their future qualifications. This abstract proposes an empirical research study that aims to explore the effects of integrating generative AI in the creation of digital artefacts on K-12 students’ perceptions of the value of computing, as well as their understanding of ownership and achievement. Constructive discussions regarding the outlined approach are encouraged.},
booktitle = {Proceedings of the 18th WiPSCE Conference on Primary and Secondary Computing Education Research},
articleno = {28},
numpages = {2},
keywords = {Artificial Intelligence education, Creative computing, Generative AI, K-12 education, Student perceptions},
location = {Cambridge, United Kingdom},
series = {WiPSCE '23}
}

@inproceedings{10.1145/3594671.3594685,
author = {Tanimoto, Steven L.},
title = {Five Futures with AI Coding Agents},
year = {2023},
isbn = {9798400707551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594671.3594685},
doi = {10.1145/3594671.3594685},
abstract = {Many computer programmers are beginning to use computational agents to help them develop software. This article raises questions about the nature of programmer-to-agent relationships. The author’s intent is to foster thought that will help human programmers best prepare for such relationships and perhaps design the relationships, ultimately keeping their jobs and improving their programming experience.},
booktitle = {Companion Proceedings of the 7th International Conference on the Art, Science, and Engineering of Programming},
pages = {32–38},
numpages = {7},
keywords = {verification, trust, soloing, software development, programming experience, pair programming, liveness, live coding, interactive intelligent development environment, future, flow, collaboration, coding service, coding agents, code golf, artificial intelligence, Copilot, ChatGPT},
location = {Tokyo, Japan},
series = {Programming '23}
}

@inproceedings{10.1145/3699538.3699571,
author = {Birillo, Anastasiia},
title = {Bringing Industry-Grade Code Quality and Practices into Software Engineering Education (Doctoral Consortium)},
year = {2024},
isbn = {9798400710384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3699538.3699571},
doi = {10.1145/3699538.3699571},
abstract = {Using professional development tools and practices is an essential part of being a programmer. However, beginners often struggle with professional tools. In this work, we ask the question: “How can we adapt professional programming tools to improve software engineering education?” and aim to find efficient ways to solve this problem.},
booktitle = {Proceedings of the 24th Koli Calling International Conference on Computing Education Research},
articleno = {33},
numpages = {2},
keywords = {Code Quality Assessment, Code Formatting, LLMs, Generative AI, Next-Step Hints},
location = {
},
series = {Koli Calling '24}
}

@inproceedings{10.1145/3639856.3639904,
author = {Agarwal, Ishika and Sehgal, Shradha and Goyal, Varun and Sonawane, Prathamesh},
title = {QuickAns: A Virtual Teaching Assisstant},
year = {2024},
isbn = {9798400716492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639856.3639904},
doi = {10.1145/3639856.3639904},
abstract = {QuickAns is a virtual teaching assistant designed to assist course staff who use Campuswire as their Q&amp;A platform. It reads Campuswire posts from digest emails and sends a potential answer to the course staff. At this stage, the course staff can review the answer for any logistical issues and answer a student’s question in a matter of minutes.},
booktitle = {Proceedings of the Third International Conference on AI-ML Systems},
articleno = {48},
numpages = {2},
keywords = {Education, LLMs, Teaching Aid, Vector Database},
location = {Bangalore, India},
series = {AIMLSystems '23}
}

@inproceedings{10.1145/3658549.3658566,
author = {Ho, Chia-Ling and Liu, Xin-Ying and Qiu, Yu-Wei and Yang, Shih-Yang},
title = {Research on Innovative Applications and Impacts of Using Generative AI for User Interface Design in Programming Courses},
year = {2024},
isbn = {9798400709180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658549.3658566},
doi = {10.1145/3658549.3658566},
abstract = {Generative Artificial Intelligence (GAI) has become a hot topic nowadays, as its powerful content generation models enable users to instantly create everything from digital media products to coding examples through simple text queries, providing more possibilities in the field of education. This study aims to investigate the impact of Generative AI intervention in teaching App Inventor programming courses, analyzing the differences between UI materials designed by traditional teachers based on their professional knowledge and experience, and UI materials created by Generative AI in classroom teaching. The study also evaluates the impact of Generative AI on students' learning outcomes and motivation through satisfaction and Technology Acceptance Model (TAM) questionnaires. The results indicate that UI materials produced through Generative AI effectively enhance students' satisfaction with the course and their acceptance of new technologies. Compared to traditional teaching methods, Generative AI significantly saves teachers' time and effort in designing materials while simultaneously improving teaching efficiency and quality.},
booktitle = {Proceedings of the 2024 International Conference on Information Technology, Data Science, and Optimization},
pages = {68–72},
numpages = {5},
keywords = {Generative artificial intelligence, Intelligent assistant, Learning effectiveness, Programming course, User interface design},
location = {Taipei, Taiwan},
series = {I-DO '24}
}

@inproceedings{10.1145/3610969.3611132,
author = {Petrovska, Olga and Clift, Lee and Moller, Faron},
title = {Generative AI in Software Development Education: Insights from a Degree Apprenticeship Programme},
year = {2023},
isbn = {9798400708763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610969.3611132},
doi = {10.1145/3610969.3611132},
abstract = {We describe insights gained from incorporating ChatGPT into assignments for our Software Engineering Degree Apprenticeship programme, including attitudes expressed by the learners and their employers regarding our approach.},
booktitle = {Proceedings of the 2023 Conference on United Kingdom &amp; Ireland Computing Education Research},
articleno = {19},
numpages = {1},
keywords = {Software Engineering, Generative AI, Education, Apprenticeships},
location = {Swansea, Wales Uk},
series = {UKICER '23}
}

@inproceedings{10.1145/3641555.3704762,
author = {Birillo, Anastasiia and Keuning, Hieke and Migut, Gosia and Dzialets, Katsiaryna and Golubev, Yaroslav},
title = {Creating in-IDE Programming Courses},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3704762},
doi = {10.1145/3641555.3704762},
abstract = {The in-IDE learning format represents a novel way of teaching programming to students entirely within an industry-grade IDE, allowing them to learn both the language and the necessary tooling at the same time. In this tutorial, we will teach the audience everything they need to know to create in-IDE courses and analyze how the students are working in them. In the first part of the tutorial, the audience will get to know the JetBrains Academy plugin that allows creating courses for IntelliJ-based IDEs such as IntelliJ IDEA and PyCharm. The participants will develop their own simple courses with theory, programming tasks, and quizzes, as well as employ some LLM-based features like automatic test generation. In the second part, we will learn how to use another plugin to collect code snapshots and the usage of IDE features of students when they are solving the tasks. Finally, the participants will solve tasks in their own course while using the data gathering plugin, and we will show them how to process and analyze the collected data. As the outcome of the tutorial, the audience will know how to create in-IDE courses, track the students' performance and analyze it, and will already have their own simple course and a dataset that can be expanded or used for further research.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1767},
numpages = {1},
keywords = {In-IDE learning, JetBrains academy, LLMs, MOOCs, activity tracking, course creation, generative AI, programming education, programming exercises},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@proceedings{10.1145/3657604,
title = {L@S '24: Proceedings of the Eleventh ACM Conference on Learning @ Scale},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to present the Proceedings of the Eleventh Annual ACM Conference on Learning at Scale, L@S 2024, held July 18-20, 2024 at Georgia Tech in Atlanta, Georgia, USA.The Learning at Scale conference was created by the Association for Computing Machinery (ACM), inspired by the emergence of Massive Open Online Courses (MOOCs) and the accompanying shift in thinking about education. During the last few years, new opportunities for scaling up learning have emerged, like hybrid learning environments combining online and face-to-face, and informal learning enabled by all sorts of platforms (e.g., gamified language learning, citizen science communities, and collaborative programming communities). In the recent two years, the unprecedented development of generative AI has brought profound opportunities to scale the teaching and learning experiences, with the goal of enhancing learning for the increasingly diverse group of learners in both formal and informal contexts. L@S has evolved along with these emergent massive learning scenarios and opportunities and is today one of the most prominent venues for discussion of the highest quality of research on how learning and teaching can be transformed at scale, in diverse learning environments.The theme of L@S 2024 is Scaling Learning in the Age of AI. Rapid advances in AI have created new opportunities but also challenges for the Learning@Scale community. The advances in generative AI show potential to enhance pedagogical practices and the efficacy of learning at scale. This has led to an unprecedented level of interest in employing generative AI for scaling tutoring and feedback. The prevalence of such tools calls for new practices and understanding on how AI-based methods should be designed and developed to enhance the experiences and outcomes of teachers and learners.Learning@Scale 2024 solicits empirical and theoretical papers on, but not limited to, the following topics (in no particular order): 1) Instruction at scale: studies that examine how teachers and educators scale their instructions, what aspects of instruction could be scaled effectively, and which of these instructional strategies are the most effective for learning. 2) Interventions at scale: studies that examine the effects of interventions on student learning and performance when implemented at scale. We welcome studies that use both qualitative and quantitative methods. 3) The use of generative AI to scale learning: studies that investigate stakeholders' experiences with generative AI, students' and teachers' interactions with generative AI, and the potentials and limitations of using generative AI in education. 4) Systems and tools to support learning at scale: research that designs and develops systems and tools to support learning at scale. For example, this involves scaling learning through web-based systems, MOOCs, visualization, intelligent tutoring systems, gamification, immersive techniques (AR/VR/MR), mobile technologies, tangible interfaces, and various other technologies. 5) The evaluation of existing learning at scale systems and online learning environments using but not limited to the above-mentioned technologies. 6) Methods and algorithms that model learner behavior: research that contributes methods, algorithms, and pipelines that process large student data to enhance learning at scale. 7) Scaling learning in informal contexts: studies that explore how people take advantage of online environments to pursue their interests informally. 8) Review and synthesis of existing literature related to learning at scale. 9) Empirical studies and interventions that address equity, trust, algorithmic transparency and explainability, fairness and bias when using AI in education. 10) Research that addresses accessibility in learning at scale contexts. 11) Design and deployment of learning at scale systems for learners from underrepresented groups.},
location = {Atlanta, GA, USA}
}

@inproceedings{10.1145/3678610.3678630,
author = {Mei-seung, Cheng},
title = {Navigating the “Cooked” Data: A Framework for Understanding GenAI's Impact on Academic Writing and Learning},
year = {2024},
isbn = {9798400716799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678610.3678630},
doi = {10.1145/3678610.3678630},
abstract = {This article explores the integration of Generative AI (GenAI) technologies, such as ChatGPT, Bard, and LaMDA, in academic writing classrooms, examining both their potential to transform learning and the challenges they present. Building on Activity Theory, the study assesses the transformation of students' roles, the writing assistant tool, and the rules and division of labor within the academic community after technology integration. We argue that GenAI, while offering powerful potential for personalized feedback and learning, disrupts traditional educational dynamics. This raises critical questions about student roles, data integrity, and the evolving responsibilities of teachers. We propose eleven research questions to guide future investigations. These questions emphasize the need for a nuanced understanding of how GenAI impacts the learning experience and its implications for academic integrity. We also highlight the ethical considerations surrounding its use. This work aims to contribute to the ongoing conversation surrounding AI in education, promoting a more comprehensive understanding of the opportunities and challenges presented by this transformative technology.},
booktitle = {Proceedings of the 2024 10th International Conference on E-Society, e-Learning and e-Technologies (ICSLT)},
pages = {76–81},
numpages = {6},
keywords = {Academic Integrity, Activity Theory, Generative AI in education, Technology in higher education},
location = {
},
series = {ICSLT '24}
}

@inproceedings{10.1145/3641555.3705025,
author = {Diaz, Marc and Karp, Dustin and Tuli, Prayuj and Kapoor, Amanpreet},
title = {Edugator: An AI-enabled Tool for Creating and Delivering Interactive Computing Content},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705025},
doi = {10.1145/3641555.3705025},
abstract = {Edugator is a browser-based, AI-enabled tool designed to help instructors of introductory computing courses create and deliver interactive educational content. It streamlines the content authoring process by incorporating generative AI models into both the creation and delivery stages. Instructors can create bespoke interactive computing lessons and programming problems by providing a prompt and a few clicks. They can also author templates and test cases in programming languages such as C++, Java, C, and Python. Additionally, instructors can validate programming problems by running them against an auto-generated solution, allowing them to refine the problems before releasing it to students, preventing misinformation or ambiguity. Students can complete lessons and solve programming problems in a browser-based text editor receiving immediate feedback. They can also interact with a large language model-powered AI chatbot that scaffolds a student on how to approach the problem without giving out solutions. Edugator is built using modern web frameworks and the goal of the tool is to accelerate the adoption of automated assessment tools by minimizing the challenges instructors face with such tools. It also supports Learning Tools Interoperability (LTI), allowing seamless integration with learning management systems (LMS). The demo will provide an overview of Edugator's features, including authoring programming problems and lessons using AI or remixing existing problems obtained from test banks, LTI integration, and AI-chatbot. More information about the tool can be found at https://edugator.app/ and https://github.com/edugatorlabs/resources},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1732},
numpages = {1},
keywords = {ai tutor, automated assessment tool, generative ai, introductory programming, learning by doing},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3605098.3636180,
author = {Aguilar, Stephen J and Wang, Changzhao},
title = {Duty vs. Consequence: Exploring Teachers' Assessment of the Ethical Dimensions of Generative AI Technologies},
year = {2024},
isbn = {9798400702433},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605098.3636180},
doi = {10.1145/3605098.3636180},
abstract = {This study examines how K-12 teachers (n=248) in the United States evaluated multiple ethical propositions grounded within classic philosophic distinction of deontology and consequentialism. Notably, we observed gender differences in ethical evaluations, with women scoring higher in several deontological propositions. Furthermore, teachers' attitudes significantly predicted their stances on consequentialist propositions, while self-efficacy and anxiety were related to both consequentialist and deontological perspectives.},
booktitle = {Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing},
pages = {106–108},
numpages = {3},
keywords = {ethics, generative AI, deontology, consequentialism, teachers, pedagogy},
location = {Avila, Spain},
series = {SAC '24}
}

@proceedings{10.1145/3649409,
title = {SIGCSE Virtual 2024: Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 2},
year = {2024},
isbn = {9798400706042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of SIGCSE Virtual 2024 Steering, Organization, and Program Committees, we would like to welcome you to this wonderful event. SIGCSE Virtual 2024, 1st ACM Virtual Global Computing Education Conference is now a reality after over a year of work by all the committee members. We like to send our special thanks to the SIGCSE Board and ACM for their continued support, encouragement and facilitation.One of the major goals of SIGCSE Virtual is to promote an inclusive and easily accessible conference to all interested in CS education research and practice. The hope is to allow those who are not able to easily travel to SIGCSE conferences to participate virtually from around the world. For this reason, the core of the conference follows all other SIGCSE conferences by providing papers, panels, posters/lightning talks, working groups, and doctoral consortium sessions dedicated to CS education research and practice.The conference has different themes based on the global aspects of CS education while considering regional circumstances. The sessions are offered considering time-zone constraints. The online program adjusts to time zones.Several different activities are provided besides the technical sessions by conference sponsors as well as for social engagements. All these activities are included in the program.},
location = {Virtual Event, NC, USA}
}

@inproceedings{10.1145/3660650.3660668,
author = {Rajabi, Parsa},
title = {Experience Report: Adopting AI-Usage Policy in Software Engineering Education},
year = {2024},
isbn = {9798400709975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660650.3660668},
doi = {10.1145/3660650.3660668},
abstract = {This report examines the introduction of an AI-usage policy within a Software Engineering course, aiming to overcome the challenges of incorporating generative AI (genAI) tools in academic settings. As the debate around the impact of technologies like ChatGPT in education continues, this policy represents a proactive stance, addressing both the opportunities and risks associated with AI tool usage. With N=86 students, this course implemented a policy that promotes responsible AI use through guidelines and an "AI-usage disclosure" form for coursework submissions. This approach sought to improve AI literacy, ensure academic integrity, and mitigate potential academic misconduct cases. Despite challenges, including adherence to AI disclosures and the evolving definition of AI tools, the policy promoted a more inclusive learning environment and encouraged a deeper understanding of AI’s role and limitations in computer science education. The findings highlight the need for ongoing policy revisions to adapt to technological advancements, emphasizing the pilot as an essential step towards integrating AI responsibly in educational contexts.},
booktitle = {Proceedings of the 26th Western Canadian Conference on Computing Education},
articleno = {19},
numpages = {2},
keywords = {AI in Education, AI-usage Policy, Academic Integrity, ChatGPT, Software Engineering Education},
location = {Kelowna, BC, Canada},
series = {WCCCE '24}
}

@inproceedings{10.1145/3689535.3689537,
author = {Stone, Irene},
title = {Investigating the Use of ChatGPT to Support the Learning of Python Programming Among Upper Secondary School Students: A Design-Based Research Study},
year = {2024},
isbn = {9798400711770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689535.3689537},
doi = {10.1145/3689535.3689537},
abstract = {This study investigates how ChatGPT can be used to support the learning of Python programming among upper second-level students in an Irish classroom. It addresses critical gaps in the literature, such as the lack of research at secondary level, the need for human-centered studies conducted over time, and the absence of guidelines for integrating ChatGPT into introductory programming education. Employing a design-based research methodology, this study aims to understand student engagement with ChatGPT and investigates how to support their use of prompts when learning to program. The research involves students as co-creators alongside their teacher, who is also the researcher, in developing a pedagogical framework that integrates ChatGPT into Python programming education.},
booktitle = {Proceedings of the 2024 Conference on United Kingdom &amp; Ireland Computing Education Research},
articleno = {11},
numpages = {1},
keywords = {AI, CS1, ChatGPT, LLMs, artificial intelligence, design-based research, generative AI, human-centered, novice programming, pedagogical practices, programming, python, student-centered},
location = {Manchester, United Kingdom},
series = {UKICER '24}
}

@proceedings{10.1145/3636243,
title = {ACE '24: Proceedings of the 26th Australasian Computing Education Conference},
year = {2024},
isbn = {9798400716195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@proceedings{10.1145/3717867,
title = {Websci '25: Proceedings of the 17th ACM Web Science Conference 2025},
year = {2025},
isbn = {9798400714832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3626253.3635380,
author = {Veilleux, Nanette and Bates, Rebecca and Goldsmith, Judy and Summet, Valerie},
title = {Mentoring, AI, and the End of Affirmative Action: Connecting with SIGCSE Reads},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635380},
doi = {10.1145/3626253.3635380},
abstract = {This Birds of a Feather will begin with a high-level overview of the SIGCSE Reads 2024 books and then quickly move to discussion about mentoring students in the era of large language models and ChatGPT, including how students may value the curriculum differently, how learning outcomes may change, and how we can support students and alumni/ae as they work with rapidly changing job and learning expectations. We expect that many of the sessions at SIGCSE will address the radical shifts in learning outcomes and curricular changes due to LLMs. We will not focus on the particulars of these changes, but rather on mentoring in this time with Sister Resisters: Mentoring Black Women on Campus by Janie Victoria Ward and Tracy L. Robinson-Wood as a resource. How do we guide our students through the curriculum upheaval triggered by shifting learning outcomes? How do we help them prepare for the new instantiation of computer science?  This BOF is the primary session for SIGCSE Reads. We encourage discussion of this year's fiction works The Lifecycle of Software Objects by Ted Chiang and "Dolly" by Elizabeth Bear, as well as past Reads, throughout the conference.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1922},
numpages = {1},
keywords = {computing education, diversity in computing, mentoring, science fiction},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3490100.3516473,
author = {Suh, Sangho and An, Pengcheng},
title = {Leveraging Generative Conversational AI to Develop a Creative Learning Environment for Computational Thinking},
year = {2022},
isbn = {9781450391450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490100.3516473},
doi = {10.1145/3490100.3516473},
abstract = {We explore how generative conversational AI can assist students’ learning, creative, and sensemaking process in a visual programming environment where users can create comics from code. The process of visualizing code in terms of comics involves mapping programming language (code) to natural language (story) and then to visual language (of comics). While this process requires users to brainstorm code examples, metaphors, and story ideas, the recent development in generative models introduces an exciting opportunity for learners to harness their creative superpower and researchers to advance our understanding of how generative conversational AI can augment our intelligence in creative learning contexts. We provide an overview of our system and discuss interaction scenarios to demonstrate ways we can partner with generative conversational AI in the context of learning computer programming.},
booktitle = {Companion Proceedings of the 27th International Conference on Intelligent User Interfaces},
pages = {73–76},
numpages = {4},
keywords = {coding strip, comics, generative conversational AI, visual programming environment},
location = {Helsinki, Finland},
series = {IUI '22 Companion}
}

@article{10.5555/3665609.3665633,
author = {Liu, Sa and Grey, Brian and Watkins, Ryan and Chu, Chad and Grim, Phillip and McManus, Thomas},
title = {Assessing Risks, Challenges and Opportunities of Generative AI in Computer Programming Education --- Lightning Talk},
year = {2024},
issue_date = {April 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {8},
issn = {1937-4771},
abstract = {Artificial Intelligence (AI) has the potential to transform the education sector by enhancing teaching and learning experiences. According to Sal Khan, founder of Khan Academy, AI is about to start "the biggest positive transformation that education has ever seen"1 by making high-quality personalized tutoring available (tuition free) to everyone on the planet. Given AI's, and more specifically Generative AI's (GAI), rapidly developing capabilities (e.g., to provide tailored feedback, ask questions of students, give examples and non-examples, and offer general learning support), incorporating GAI into programming education has the potential to enhance student engagement and learning outcomes. At the same time, they identified challenges in using GAI, such as its inability to answer some questions and its tendency to provide incorrect or incomplete responses. Students also report an increase in anxiety surrounding GAI and its potential effects on future professional opportunities. Outside of the classroom there is likewise an increasing prevalence of GAI in computational professions, making it crucial to equip students with the necessary knowledge and skills to effectively, responsibly, and ethically utilize GAI. Rather than avoiding the use of GAI in the classroom, in this study we aim to investigate the pros and cons of leveraging GAI's capabilities to offer personalized guidance and assistance to students as they learn programming. By doing this research, we are learning to create more interactive and engaging learning experiences that better equip students with the skills and knowledge needed to succeed in the field of programming. This project, which is currently being conducted, was designed to address this research question: To what extent does the incorporation of GAI impact students' engagement, motivation, and achievement, particularly with the material in Intro to Programming courses and their chosen STEM field of study? It is utilizing case studies that focus on the integration of GAI into computer programming education. The team has 1) developed a series of GAI-supported teaching modules specifically designed to improve problem-solving skills in programming tasks among undergraduate students; and 2) is in the process of analyzing student feedback on GAI integration in computer programming education. This project offers an important exploration into the intersection of GAI and programming education, with the expectation that results will provide useful guidance for programming instructors who are adapting their instructional strategies for the emerging role of GAI in programming. The team will briefly present the status of the research and early insights from the project, and then engage with the audience on how lessons learned from this work can pragmatically shape programming courses in their institutions. Quick tips, takeaways, and prompting strategies will be shared throughout this interactive lighting talk.},
journal = {J. Comput. Sci. Coll.},
month = apr,
pages = {210–211},
numpages = {2}
}

@proceedings{10.1145/3641555,
title = {SIGCSETS 2025: Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 56th annual SIGCSE Technical Symposium on Computer Science Education (SIGCSE TS 2025)! For the first time since 1978, our symposium is being held in the "Steel City" of Pittsburgh, Pennsylvania, at the intersection of the Allegheny, Ohio, and Monongahela rivers. We are looking forward to four days of productive and informative presentations, vibrant and engaging discussions, and an overall wonderful experience with our SIGCSE community members. We are confident that our program of events provides meaningful and productive experiences for all.Our theme for this year is "Leading the Transformation". Our theme reflects the role of the computer science education community in adapting educational practice to new technologies and challenges. With advances in Artificial Intelligence (AI) transforming both academia and the workplace, the computer science education community has a unique opportunity to help shape the future use and application of computing. Our program this year is quite diverse and offers something for everyone, so please take time to peruse the schedule and choose the sessions which appeal to you. Pittsburgh is also an exciting city with lots to see and do, so you are encouraged to enjoy all the city has to offer.The format of the 2025 Technical Symposium is similar to 2024 in many ways. We will once again have a program that extends into Saturday afternoon, including papers and the Nifty Assignment session after lunch. We will also again have three Birds-of-a-Feather sessions, two on Thursday evening and one during lunch on Friday. For online attendees, we will continue to offer streaming of keynotes, the Nifty Assignment session, the First-Timers Lunch presentation, and a small set of paper, panel, and special sessions.This year we received almost 1200 submissions. Submission statistics for all of the Technical Symposium's tracks can be found in the table that follows. Papers were submitted to one of three tracks (Computing Education Research, Experience Reports and Tools, Position and Curricula Initiatives) with reviewing tailored to each track. Each paper submission was reviewed by at least three reviewers, with a substantial proportion of papers receiving four (or more) reviews, plus a meta review. We sincerely appreciate the work of the more than 800 reviewers and 112 Associate Program Chairs who contributed to the creation of this years' program. Their reviews helped us decide which submissions were accepted while also providing detailed feedback that allowed authors to further improve the final versions of their submissions.},
location = {Pittsburgh, PA, USA}
}

@inproceedings{10.1145/3641555.3705201,
author = {Bejarano, Andres and Dickey, Ethan and Setsma, Rhianna},
title = {Implementing the AI-Lab Framework: Enhancing Introductory Programming Education for CS Majors},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705201},
doi = {10.1145/3641555.3705201},
abstract = {The advent of generative AI tools presents novel opportunities and challenges in computer science education, particularly in introductory programming courses. This study explores the implementation of AI-Lab, a framework designed to guide students in the effective and ethical use of generative AI, in this case ChatGPT, in academic settings without compromising skill development. Conducted during Spring 2024, our use of the intervention targeted over 500 Computer Science and Data Science majors enrolled in their major-specific Data Structures and Algorithms courses. The AI-Lab framework enabled students to develop both conceptual questions and c++ and Python programs by interacting with ChatGPT and iteratively correcting its errors. Focus groups and post-intervention surveys revealed a generally positive experience. Students appreciated the ability to leverage AI for tasks outside their major, recognizing the value of understanding correct solutions through AI-assisted programming. Moreover, the guided use of generative AI by professors alleviated concerns regarding academic dishonesty, fostering a supportive learning environment. Despite these benefits, students expressed awareness of the potential drawbacks of over-reliance on AI, noting the risk of impeding their professional growth. Nevertheless, they acknowledged the practical utility of AI for non-major related tasks. This study highlights the importance of incorporating structured AI training in curricula to balance skill development and ethical AI usage, offering insights for broader applications in higher education.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1383–1384},
numpages = {2},
keywords = {ai lab, ai-assisted programming, ai-lab framework, ethical ai usage, generative ai in education, skill development with ai, structured ai training},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.5555/3636517.3636522,
author = {Crandall, Aaron S. and Sprint, Gina and Fischer, Bryan},
title = {Generative Pre-Trained Transformer (GPT) Models as a Code Review Feedback Tool in Computer Science Programs},
year = {2023},
issue_date = {October 2023},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {1},
issn = {1937-4771},
abstract = {Undergraduate computer science and software engineering students benefit significantly from in-depth reviews of their code early and often in their courses. Performing these reviews is time-consuming for teaching assistants and professors to complete, consequently impacting the timeliness and consistency of the provided feedback. When code feedback is not delivered close to the time of authorship, the utility of the review for students is diminished. Prior work with Automatic Static Analysis Tools has shown promise at using artificial intelligence to automate code reviews, with some success integrating them into classroom environments. To leverage new advances in Generative Pre-Trained Transformer (GPT) models, this work reports on an Automatic Review Tool (ART) to provide timely, automatically generated code reviews. ART was evaluated in a second-semester computer science course by integrating ART into the course's Github-based assignment submission system. A cohort of student volunteers (N = 74) read the ART reviews and provided feedback using a survey spanning two of their course assignments. The results of this pilot study show that students perceived ART was successful at detecting defects and offering style-based suggestions, and students were receptive to receiving future automated reviews of their work.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {38–47},
numpages = {10}
}

@proceedings{10.1145/3649165,
title = {SIGCSE Virtual 2024: Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1},
year = {2024},
isbn = {9798400705984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {On behalf of SIGCSE Virtual 2024 Steering, Organization, and Program Committees, we would like to welcome you to this wonderful event. SIGCSE Virtual 2024, 1st ACM Virtual Global Computing Education Conference is now a reality after over a year of work by all the committee members. We like to send our special thanks to the SIGCSE Board and ACM for their continued support, encouragement and facilitation.One of the major goals of SIGCSE Virtual is to promote an inclusive and easily accessible conference to all interested in CS education research and practice. The hope is to allow those who are not able to easily travel to SIGCSE conferences to participate virtually from around the world. For this reason, the core of the conference follows all other SIGCSE conferences by providing papers, panels, posters/lightning talks, working groups, and doctoral consortium sessions dedicated to CS education research and practice.The conference has different themes based on the global aspects of CS education while considering regional circumstances. The sessions are offered considering time-zone constraints. The online program adjusts to time zones.Several different activities are provided besides the technical sessions by conference sponsors as well as for social engagements. All these activities are included in the program.},
location = {Virtual Event, NC, USA}
}

@inproceedings{10.1145/3641555.3705048,
author = {Phelps, Victoria and Ball, Michael and Garcia, Dan and Garcia, Yuan},
title = {Snap! 10 --- From Blocks to AI: Empowering Learning with Custom Primitives and Machine Learning},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705048},
doi = {10.1145/3641555.3705048},
abstract = {This year's Snap! 10 release marks a major leap forward, bringing advanced machine learning capabilities directly into the hands of students and educators. Version 10 introduces support for building ML models, including single-layer perceptrons, making it easier than ever for students to explore AI concepts within a block-based environment. These updates are accompanied by the ability to define Snap! primitives using Snap! blocks themselves, empowering users to deeply customize and extend the language.Building on the rich foundation of previous versions, Snap! 10 also includes hundreds of improvements aimed at enhancing both the classroom and the individual learning experience. These include quality-of-life updates such as new debugging tools, dynamic runtime access via the ''this'' reporter, and expanded support for working with dictionaries and APIs.In this demo, we'll showcase the exciting new features in Snap! 10, focusing on how they can be used to engage students in advanced topics like machine learning, data science, and computational thinking. Attendees will learn how to leverage these tools to build custom experiences that meet the evolving needs of their classrooms.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1737},
numpages = {1},
keywords = {ai, block-based programming, llm, metaprogramming, programming paradigms, snap!},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@proceedings{10.1145/3649217,
title = {ITiCSE 2024: Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 29th annual conference on Innovation and Technology in Computer Science Education (ITiCSE 2024), hosted by Universit\`{a} degli Studi di Milano in Milan, Italy.ITiCSE 2024 will take place from Friday July 5 to Wednesday July 10. The conference program includes a keynote address, paper sessions, a panel, tips, techniques &amp; courseware demonstrations, posters, a doctoral consortium, and working group presentations. Working groups meet July 5-7 and will submit draft reports before the conference begins on July 8.The submissions to ITiCSE 2024 were reviewed by 446 researchers and practitioners from computing education and related fields, including 44 program committee members and 402 reviewers. Thanks to their outstanding effort and commitment, every submission received a metareview and most received at least three reviews, providing authors of all submissions with constructive feedback. Although no review process is flawless, we are confident that this effort led to a vibrant conference program, capturing multiple voices and perspectives in the field.},
location = {Milan, Italy}
}

@proceedings{10.1145/3623762,
title = {ITiCSE-WGR '23: Proceedings of the 2023 Working Group Reports on Innovation and Technology in Computer Science Education},
year = {2023},
isbn = {9798400704055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {In these proceedings, we present papers from the Working Groups that worked in the context of the 28th Annual Conference on Innovation &amp; Technology in Computer Science Education (ITiCSE), held in Turku Finland, and hosted by University of Turku from the 10th to the 12th of July 2023.The concept of Working Groups has been a unique feature of the ITiCSE conference series since its inception, with CompEd adopting the Working Group practice in 2019. A Working Group typically comprises 5 to 10 researchers who work together on a project related to computing education. Working Groups provide a wonderful opportunity to work intensively on a topic of interest with an international group of computing education researchers. This unique experience is one that, in our opinion, each Computer Science Educator should strive to participate in at least once.In 2023, 13 proposals for Working Groups were received and six Working Groups were selected by the Working Group chairs to recruit members and proceed for ITiCSE 2023. There were over 100 member applications to Working Groups, with 67 being accepted across the six Working Groups.},
location = {Turku, Finland}
}

@proceedings{10.1145/3641554,
title = {SIGCSETS 2025: Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 56th annual SIGCSE Technical Symposium on Computer Science Education (SIGCSE TS 2025)! For the first time since 1978, our symposium is being held in the "Steel City" of Pittsburgh, Pennsylvania, at the intersection of the Allegheny, Ohio, and Monongahela rivers. We are looking forward to four days of productive and informative presentations, vibrant and engaging discussions, and an overall wonderful experience with our SIGCSE community members. We are confident that our program of events provides meaningful and productive experiences for all.Our theme for this year is "Leading the Transformation". Our theme reflects the role of the computer science education community in adapting educational practice to new technologies and challenges. With advances in Artificial Intelligence (AI) transforming both academia and the workplace, the computer science education community has a unique opportunity to help shape the future use and application of computing. Our program this year is quite diverse and offers something for everyone, so please take time to peruse the schedule and choose the sessions which appeal to you. Pittsburgh is also an exciting city with lots to see and do, so you are encouraged to enjoy all the city has to offer.The format of the 2025 Technical Symposium is similar to 2024 in many ways. We will once again have a program that extends into Saturday afternoon, including papers and the Nifty Assignment session after lunch. We will also again have three Birds-of-a-Feather sessions, two on Thursday evening and one during lunch on Friday. For online attendees, we will continue to offer streaming of keynotes, the Nifty Assignment session, the First-Timers Lunch presentation, and a small set of paper, panel, and special sessions.This year we received almost 1200 submissions. Submission statistics for all of the Technical Symposium's tracks can be found in the table that follows. Papers were submitted to one of three tracks (Computing Education Research, Experience Reports and Tools, Position and Curricula Initiatives) with reviewing tailored to each track. Each paper submission was reviewed by at least three reviewers, with a substantial proportion of papers receiving four (or more) reviews, plus a meta review. We sincerely appreciate the work of the more than 800 reviewers and 112 Associate Program Chairs who contributed to the creation of this years' program. Their reviews helped us decide which submissions were accepted while also providing detailed feedback that allowed authors to further improve the final versions of their submissions.},
location = {Pittsburgh, PA, USA}
}

@inproceedings{10.1145/3699538.3699580,
author = {Kiesler, Natalie and Scholz, Ingo and Albrecht, Jens and Stappert, Friedhelm and Wienkop, Uwe},
title = {Novice Learners of Programming and Generative AI - Prior Knowledge Matters},
year = {2024},
isbn = {9798400710384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3699538.3699580},
doi = {10.1145/3699538.3699580},
abstract = {With the broad availability of Generative AI (GenAI), introductory programming education is starting to change. At Nuremberg Tech, we observed the doubling of failure rates to approximately 50% in the first semester course “Procedural Programming” across students of all study programs. Due to these exam results in winter 2023/24, we conducted a pilot study to gather students’ use of GenAI tools, their exam results, and prior programming education and experience. The results imply significant differences of students’ use of GenAI tools depending on their prior programming education. We will therefore extend the investigation in winter term 2024/25.},
booktitle = {Proceedings of the 24th Koli Calling International Conference on Computing Education Research},
articleno = {51},
numpages = {2},
keywords = {GenAI, student success, programming education, introductory programming, use pattern},
location = {
},
series = {Koli Calling '24}
}

@proceedings{10.1145/3686852,
title = {SIGITE '24: Proceedings of the 25th Annual Conference on Information Technology Education},
year = {2024},
isbn = {9798400711060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {El Paso, TX, USA}
}

@inproceedings{10.1145/3653666.3656075,
author = {Okolo, Chinasa T.},
title = {Beyond AI Hype: A Hands-on Workshop Series for Enhancing AI Literacy in Middle and High School Students},
year = {2024},
isbn = {9798400706264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653666.3656075},
doi = {10.1145/3653666.3656075},
abstract = {The increasing usage of AI in high-stakes decision-making underscores a pressing need for various stakeholders to understand AI, learn how to identify AI-generated content, and become aware of its societal risks. We detail outcomes from engaging underrepresented secondary school students in a 5-day workshop series consisting of brief lectures, hands-on activities, and short research assignments. We find that the workshop improved students' knowledge about AI and the ethical implications of using these technologies. Our work highlights policy implications and outlines actionable efforts needed to advance AI literacy, with the workshop content being developed into an open-source AI literacy curriculum.},
booktitle = {Proceedings of the 2024 on RESPECT Annual Conference},
pages = {86–93},
numpages = {8},
keywords = {ai literacy, ai pedagogy, computing education, equity, generative ai, human-centered ai, technology ethics},
location = {Atlanta, GA, USA},
series = {RESPECT 2024}
}

@article{10.5555/3717781.3717801,
author = {Crews, Thad and Erickson, John and Wu, Tong},
title = {Exploring Faculty and Student Perspectives on GenAI in Higher Education},
year = {2024},
issue_date = {November 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {5},
issn = {1937-4771},
abstract = {This study explores the growing impact of GenAI tools in higher education. The study involves a repeated cross-sectional survey of faculty and students to identify valuable insights into evolving patterns and preferences regarding the impact of ChatGPT and other AI tools in higher education. Results are reported with insights for faculty and policy makers.},
journal = {J. Comput. Sci. Coll.},
month = nov,
pages = {159–170},
numpages = {12}
}

@article{10.1145/3715920.3715922,
author = {Neumann, Marion and Rosenthal, Stephanie and Stevens, Justin and Lugo, Rachel and Agnew, William and Wein, Shira},
title = {EAAI-24 Blue Sky Ideas in Artificial Intelligence Education from the AAAI/ACM SIGAI New and Future AI Educator Program},
year = {2025},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {4},
url = {https://doi.org/10.1145/3715920.3715922},
doi = {10.1145/3715920.3715922},
abstract = {The 14th Symposium on Educational Advances in Artificial Intelligence (EAAI-24), cochaired by Marion Neumann and Stephanie Rosenthal, continued the tradition of the AAAI/ACM SIGAI New and Future AI Educator (NFAIED) Program to support the training of early-career university faculty, secondary school faculty, and future educators (PhD candidates or postdocs who intend a career in academia).This paper is a collection of the "blue sky" essays of the 2024 NFAIED awardees, intended to help motivate discussion around various current and important issues in AI education.},
journal = {AI Matters},
month = mar,
pages = {4–8},
numpages = {5}
}

@inproceedings{10.1145/3649217.3653615,
author = {Gardella, Nicholas and Pettit, Raymond and Riggs, Sara L.},
title = {Performance, Workload, Emotion, and Self-Efficacy of Novice Programmers Using AI Code Generation},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653615},
doi = {10.1145/3649217.3653615},
abstract = {Artificial Intelligence-driven Development Environments (AIDEs) offer developers revolutionary computer programming assistance. There is great potential in incorporating AIDEs into Computer Science education; however, the effects of these tools should be fully examined before doing so. Here, a within-subjects study was conducted to compare the programming performance, workload, emotion, and self-efficacy of seventeen novices coding with and without use of the GitHub Copilot AIDE under time pressure. Results showed that using the AIDE significantly increased programming efficiency and reduced effort and mental workload but did not significantly impact emotion or self-efficacy. However, participants' performance improved with more experience using the AI, and their self-efficacy followed. The results suggest that students who try AIDEs will likely be tempted to use them for time-sensitive work. There is no evidence that providing AIDEs will aid struggling students, but there is a clear need for students to practice with AI to become competent and confident using it.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {290–296},
numpages = {7},
keywords = {ai code generators, artificial intelligence-driven development environment, computer science education, cs1, generative ai, github copilot, introductory programming, novice programmers},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3598579.3689377,
author = {Bouvier, Dennis J and Lovellette, Ellie and Santos, Eddie Antonio and Becker, Brett A. and Dasigi, Venu G. and Forden, Jack and Glebova, Olga and Joshi, Swaroop and Kurkovsky, Stan and Russell, Se\'{a}n},
title = {Teaching Programming Error Message Understanding},
year = {2024},
isbn = {9798400702228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3598579.3689377},
doi = {10.1145/3598579.3689377},
abstract = {About a decade ago there was a sharp increase in the number of research publications focusing on programming error messages. The majority of this work focused on improving or "enhancing'' compiler error messages to make them easier for humans, especially novice programmers, to interpret and use more effectively. Little research has been published on explicitly teaching students how to effectively utilize error messages, or leveraging programming error messages as learning opportunities. However, there is no shortage of research showing that error and warning messages present barriers to students learning to program. Additionally, unhelpful and/or unfriendly error messages can cause frustration and negative emotions. We operate on the premise that not utilizing programming error and warning messages effectively, or at all, increases the difficulty of learning to program, and the undesired effects that this is known to have. As compiler messages vary by programming language and/or development environment, both in message and presentation, lessons on interpreting them are not typically included in mainstream educational materials. We believe this gap can be filled and that students can learn to use error messages to their advantage. Further, we believe that teaching students how to read and use error messages can have a significant positive impact on the learning experience for novice programmers. This work presents research on teaching novice programmers how to use programming error messages. Reported here are results from (a) a global survey of computing instructors investigating teaching the use of error / warning messages to novice programmers, (b) a multi-national pilot survey of 345 students about their experience with and attitudes toward error messages, (c) a search for existing online resources created and used by computing faculty, as well as (d) results of an empirical study with 1061 student participants testing an example lesson on using programming language use. The data show our single class-period example lesson makes a difference in student attitude toward programming error messages and their ability to use the messages.},
booktitle = {Working Group Reports on 2023 ACM Conference on Global Computing Education},
pages = {1–30},
numpages = {30},
keywords = {computer error messages, computing education, error messages, novice programmers, programming error messages, runtime errors, warning messages},
location = {Hyderabad, India},
series = {CompEd 2023}
}

@proceedings{10.1145/3632620,
title = {ICER '24: Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1},
year = {2024},
isbn = {9798400704758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1145/3632620.3671108,
author = {Pawagi, Mrigank and Kumar, Viraj},
title = {Probeable Problems for Beginner-level Programming-with-AI Contests},
year = {2024},
isbn = {9798400704758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632620.3671108},
doi = {10.1145/3632620.3671108},
abstract = {To broaden participation, competitive programming contests may include beginner-level problems that do not require knowledge of advanced Computer Science concepts (e.g., algorithms and data structures). However, since most participants have easy access to AI code-generation tools, these problems often become trivial to solve. For beginner-friendly programming contests that do not prohibit the use of AI tools, we propose Probeable Problems: code writing tasks that provide (1)&nbsp;a problem specification that deliberately omits certain details, and (2)&nbsp;a mechanism to probe for these details by asking clarifying questions and receiving immediate feedback. To evaluate our proposal, we conducted a 2-hour programming contest for undergraduate Computer Science students from multiple institutions, where each student was an active member of their institution’s ACM student chapter. The contest comprised of six Probeable Problems for which a popular code-generation tools (e.g., GitHub Copilot) were unable to generate accurate solutions due to the absence of details. Students were permitted to work individually or in groups, and were free to use AI tools. We obtained consent from 26&nbsp;groups (67&nbsp;students) to use their submissions for research. To determine whether Probeable Problems are suitable for such contests, we analyze the extent to which the code submitted by these groups identifies missing details.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1},
pages = {166–176},
numpages = {11},
keywords = {Ambiguity, CS1, Code specifications, Code writing},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@inproceedings{10.1145/3605098.3636160,
author = {Hassany, Mohammad and Ke, Jiaze and Brusilovsky, Peter and Lekshmi Narayanan, Arun Balajiee and Akhuseyinoglu, Kamil},
title = {Authoring Worked Examples for JAVA Programming with Human AI Collaboration},
year = {2024},
isbn = {9798400702433},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605098.3636160},
doi = {10.1145/3605098.3636160},
abstract = {Worked examples are among the most popular types of learning content in programming classes. However, instructors rarely have time to provide line-by-line explanations for a large number of examples typically used in a programming class. In this paper, we explore and assess a human-AI collaboration approach to authoring worked examples for Java programming. We introduce an authoring system for creating Java worked examples that generates a starting version of code explanations and presents it to the instructor to edit if necessary. We also present a study that assesses the quality of explanations created with this approach.},
booktitle = {Proceedings of the 39th ACM/SIGAPP Symposium on Applied Computing},
pages = {101–103},
numpages = {3},
keywords = {code examples, authoring tool, human-AI collaboration},
location = {Avila, Spain},
series = {SAC '24}
}

@proceedings{10.1145/3702163,
title = {ICETC '24: Proceedings of the 2024 16th International Conference on Education Technology and Computers},
year = {2024},
isbn = {9798400717819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@article{10.1145/3656478,
author = {Gooch, Daniel and Waugh, Kevin and Richards, Mike and Slaymaker, Mark and Woodthorpe, John},
title = {Exploring the Profile of University Assessments Flagged as Containing AI-Generated Material},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {2153-2184},
url = {https://doi.org/10.1145/3656478},
doi = {10.1145/3656478},
journal = {ACM Inroads},
month = may,
pages = {39–47},
numpages = {9}
}

@inproceedings{10.1145/3639474.3640065,
author = {Tao, Yida and Chen, Wenyan and Ye, Qingyang and Zhao, Yao},
title = {Beyond Functional Correctness: An Exploratory Study on the Time Efficiency of Programming Assignments},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640065},
doi = {10.1145/3639474.3640065},
abstract = {Practical programming assignments are critical parts of programming courses in Computer Science education. Students are expected to translate programming concepts learned from lectures into executable implementations that solve the tasks outlined in the assignments. These implementations are primarily assessed based on their functional correctness, ensuring that students' code produces the expected output when provided with specific inputs.However, functional correctness is not the only metric that evaluates the quality of programs. Runtime efficiency is a metric that is less frequently evaluated in programming courses, yet it holds significant importance in the context of professional software development. To investigate this gap and its potential ramifications, we conducted a large-scale empirical study on the time efficiency of 250 programming assignments that are evaluated solely on functional correctness. The results demonstrate that students' programming assignments exhibit significant variance in terms of execution time. We further identified 27 recurring inefficient code patterns from these assignments, and observed that most of the inefficient patterns can be optimized by automated tools such as PMD, IntelliJ IDEA and ChatGPT. Our findings provide actionable guidelines for educators to enhance the organization and integration of code performance topics throughout the programming course curriculum.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {320–330},
numpages = {11},
keywords = {programming assignment, code performance, tool support},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@proceedings{10.1145/3603287,
title = {ACMSE '24: Proceedings of the 2024 ACM Southeast Conference},
year = {2024},
isbn = {9798400702372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are pleased to welcome you to the 2024 ACM Southeast Conference (ACMSE 2024) sponsored by ACM and the College of Computing and Software Engineering (CCSE) at Kennesaw State University, Marietta, Georgia, USA. ACMSE 2024 continues the ACM Southeast Conference tradition of participation in all areas of computing disciplines. We hope this conference will be an excellent opportunity to share current and future hot research trends amongst researchers from around the world.},
location = {Marietta, GA, USA}
}

@proceedings{10.1145/3598579,
title = {CompEd 2023: Working Group Reports on 2023 ACM Conference on Global Computing Education},
year = {2024},
isbn = {9798400702228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the Working Group proceedings of the ACM Global Computing Education Conference (CompEd), held in Hyderabad, India, hosted by IIIT Hyderabad, from 6th December to the 9th December 2023.As stated in previous Working Group proceedings "the concept of Working Groups has been a unique feature of the ITiCSE conference series since its inception, with CompEd adopting the Working Group practice in 2019. A Working Group typically comprises 5 to 10 researchers who work together on a project related to computing education. Working Groups provide a wonderful opportunity to work intensively on a topic of interest with an international group of computing education researchers. This unique experience is one that, in our opinion, each Computer Science Educator should strive to participate in at least once."},
location = {Hyderabad, India}
}

@proceedings{10.1145/3699538,
title = {Koli Calling '24: Proceedings of the 24th Koli Calling International Conference on Computing Education Research},
year = {2024},
isbn = {9798400710384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3649217.3653626,
author = {Denzler, Benjamin and Vahid, Frank and Pang, Ashley and Salloum, Mariam},
title = {Style Anomalies Can Suggest Cheating in CS1 Programs},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653626},
doi = {10.1145/3649217.3653626},
abstract = {Student cheating on at-home programming assignments is a well- known problem. A key contributor is externally-obtained solutions from websites, contractors, and recently generative AI. In our experience, such externally-obtained solutions often use coding styles that depart from a class' style, which we call "style anomalies," such as using untaught or advanced constructs like pointers or ternary operators, or having different indenting or brace usage from the class style. We developed a tool to auto-count style anomalies. For six labs across four terms in 2021-2022, and 50 sampled students per lab, we found 18% of submissions on average had unusually-high style anomaly counts. Importantly, 8% of submissions on average had a high style anomaly count but were not flagged by a similarity checker, meaning 8% of submissions are suspicious but might have been missed if using similarity checking alone. We repeated a similar analysis for Spring 2023 when generative AI (ChatGPT) was gaining popularity, and the numbers rose to 26% and 18%, respectively. Detailed investigations by instructors led to a majority (but not all) high style anomaly submissions being deemed cheating. Even for high-similarity submissions, counting style anomalies can help instructors focus investigations on the most-likely cheating cases, and can strengthen cases sent to student conduct offices. With the rise of externally-obtained solutions from websites, contractors, and generative AI, counting style anomalies may become an increasingly important complement to similarity checking; in fact, it is now the primary cheat-detection tool in our CS1 at a large state university, with similarity secondary.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {381–387},
numpages = {7},
keywords = {cheating, cs1, plagiarism, program autograders, program style},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@proceedings{10.1145/3626253,
title = {SIGCSE 2024: Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 55th annual SIGCSE Technical Symposium on Computer Science Education (SIGCSE TS 2024)! This year, we have returned to Portland, Oregon. We hope that, like us, you are looking forward to a highly productive and engaging symposium that provides ample opportunity to renew old relationships, build new connections, and learn about the latest advances in our field. While we are sure that there will be a few surprises along the way, we hope and expect that we won't experience anything nearly as disruptive as the opening days of the pandemic, which occurred when we last tried to gather here in 2020.Our theme for this year's symposium is "Blazing New Trails in CS Education." This broad theme captures the exceptional work being performed by this community to enhance our teaching, improve our assessments, attract diverse students, and all of the other laudable projects, initiatives, and undertakings that affect positive change. The breadth of the program is substantial - there truly should be something for everyone. In fact, your biggest challenge may be deciding which session to attend in each time slot because there is so much going on! We know that many of you want to attend as many sessions as possible while you are here in Portland, but we encourage you to also find a little bit of time for yourself so that you leave Portland refreshed, renewed and encouraged, rather than exhausted or burnt out.},
location = {Portland, OR, USA}
}

@proceedings{10.1145/3723010,
title = {ECSEE '25: Proceedings of the 6th European Conference on Software Engineering Education},
year = {2025},
isbn = {9798400712821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3706599.3720032,
author = {Ko, Eunhye Grace and Nanayakkara, Shaini and Huff, Earl W},
title = {"We need to avail ourselves of [GenAI] to enhance knowledge distribution": Empowering Older Adults through GenAI Literacy},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3720032},
doi = {10.1145/3706599.3720032},
abstract = {As generative AI (GenAI) becomes increasingly ubiquitous, it is crucial to equip users, particularly vulnerable populations like older adults (65+), with the knowledge to understand its benefits and potential risks. Older adults often face greater reservations about adopting emerging technologies and require tailored literacy support. Using a mixed methods approach, this study examines strategies for delivering GenAI literacy to older adults through a chatbot named Litti, evaluating its impact on their Al literacy (knowledge, safety, and ethical use). The quantitative data showed a trend toward improved AI literacy, though the results were not statistically significant. However, the qualitative interviews revealed diverse levels of familiarity with generative AI, along with a strong desire to learn more. Qualitative findings also show that although Litti provided a positive learning experience, it did not significantly enhance participants’ trust or sense of safety regarding GenAI. This exploratory case study highlights the challenges and opportunities in designing AI literacy education for the rapidly growing older adult population.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {34},
numpages = {7},
keywords = {generative artificial intelligence, AI literacy, older adults},
location = {
},
series = {CHI EA '25}
}

@proceedings{10.1145/3700297,
title = {ISAIE '24: Proceedings of the 2024 International Symposium on Artificial Intelligence for Education},
year = {2024},
isbn = {9798400707100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3689187,
title = {ITiCSE 2024: 2024 Working Group Reports on Innovation and Technology in Computer Science Education},
year = {2025},
isbn = {9798400712081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {In these proceedings, we present papers from the Working Groups that worked in the context of the 29th Annual Conference on Innovation &amp; Technology in Computer Science Education (ITiCSE), held in Milan Italy, and hosted by Universit\`{a} degli Studi di Milano from the 8th to the 10th of July 2024.The concept of Working Groups has been a unique feature of the ITiCSE conference series since its inception. Working Groups are now part of CompEd and SIGCSE Virtual. An ITiCSE Working Group is typically composed of 8-12 researchers who work together for about nine months on a research project related to computing education. Working Groups provide a wonderful opportunity to work intensively on a topic of interest with an international group of computing education researchers. This unique experience is one that, in our opinion, each Computer Science Educator should strive to participate in at least once.In 2024, 13 proposals for Working Groups were received and ten Working Groups were selected by the Working Group chairs to recruit members and proceed for ITiCSE 2024. There were 166 member applications to Working Groups, with 134 being accepted including 33 Working Group leaders across the ten Working Groups. This is a record number of both Working Groups and Working Group members at a single conference.Working Groups began their work virtually from March up until the beginning of the ITiCSE conference. Their work included intensive collaboration on-site for the three days prior to the conference. A draft report was then submitted on the Sunday prior to the conference; a few weeks after the conference, the Working Groups submitted their final report for peer review.If the report was accepted for publication, the groups revised it based on the reviewers' comments and suggestions. This dedicated ITiCSE Working Group proceedings volume presents the final camera-ready version of the reports. We are glad that all ten papers were selected for publication in these proceedings for the ACM Digital Library.},
location = {Milan, Italy}
}

@proceedings{10.1145/3722237,
title = {ICAIE '24: Proceedings of the 2024 3rd International Conference on Artificial Intelligence and Education},
year = {2024},
isbn = {9798400712692},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3628516.3659367,
author = {Kruis, Joost and Pera, Maria Soledad and Napel, Zo\"{e} ten and Landoni, Monica and Murgia, Emiliana and Huibers, Theo and Feskens, Remco},
title = {Toward Personalised Learning Experiences: Beyond Prompt Engineering},
year = {2024},
isbn = {9798400704420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628516.3659367},
doi = {10.1145/3628516.3659367},
abstract = {We discuss the foundation of a collaborative effort to explore AI’s role in supporting (teachers and) children in their learning experiences. We integrate principles of educational psychology, AI, and HCI, and align with best practices in education while undertaking a human-centered focus on design and development that puts the student at the centre and keeps the expert-in-the-loop. Initially, we study assessment items—questions or tasks tied to a learning target. These items vary in complexity, serve as indicators of students’ grasp of specific concepts and spotlight areas where support may be needed. This preliminary analysis will help us outline a framework to guide the design and evaluation of AI technology for K-12 education. Such a framework would ensure that assessment item generation technology goes beyond the current one-dimensional approach by incorporating multifaceted, adaptable perspectives that consider the variegated landscape of learners’ needs, subject matter complexities, and pedagogical goals.},
booktitle = {Proceedings of the 23rd Annual ACM Interaction Design and Children Conference},
pages = {644–649},
numpages = {6},
keywords = {Generative AI, Human-centered Design, Learning},
location = {Delft, Netherlands},
series = {IDC '24}
}

@proceedings{10.1145/3626252,
title = {SIGCSE 2024: Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 55th annual SIGCSE Technical Symposium on Computer Science Education (SIGCSE TS 2024)! This year, we have returned to Portland, Oregon. We hope that, like us, you are looking forward to a highly productive and engaging symposium that provides ample opportunity to renew old relationships, build new connections, and learn about the latest advances in our field. While we are sure that there will be a few surprises along the way, we hope and expect that we won't experience anything nearly as disruptive as the opening days of the pandemic, which occurred when we last tried to gather here in 2020.Our theme for this year's symposium is "Blazing New Trails in CS Education." This broad theme captures the exceptional work being performed by this community to enhance our teaching, improve our assessments, attract diverse students, and all of the other laudable projects, initiatives, and undertakings that affect positive change. The breadth of the program is substantial - there truly should be something for everyone. In fact, your biggest challenge may be deciding which session to attend in each time slot because there is so much going on! We know that many of you want to attend as many sessions as possible while you are here in Portland, but we encourage you to also find a little bit of time for yourself so that you leave Portland refreshed, renewed and encouraged, rather than exhausted or burnt out.},
location = {Portland, OR, USA}
}

@inproceedings{10.1145/3650212.3680326,
author = {Xie, Linna and Li, Chongmin and Pei, Yu and Zhang, Tian and Pan, Minxue},
title = {BRAFAR: Bidirectional Refactoring, Alignment, Fault Localization, and Repair for Programming Assignments},
year = {2024},
isbn = {9798400706127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650212.3680326},
doi = {10.1145/3650212.3680326},
abstract = {The problem of automated feedback generation for introductory programming assignments (IPAs) has attracted significant attention with the increasing demand for programming education. While existing approaches, like Refactory, that employ the ”block-by-block” repair strategy have produced promising results, they suffer from two limitations.                                                                                                                                                                                                                                                                		First, Refactory randomly applies refactoring and mutation operations to correct and buggy programs, respectively, to align their control-flow structures (CFSs), which, however, has a relatively low success rate and often complicates the original repairing tasks.                                                                                                                                                                                                                                                                 		Second, Refactory generates repairs for each basic block of the buggy program when its semantics differs from the counterpart in the correct program, which, however, ignores the different roles that basic blocks play in the programs and often produces unnecessary repairs.                                                                                                                                                                                                                                                                		To overcome these limitations, we propose the Brafar approach to feedback generation for IPAs.                                                                                                                                                                                                                                                                 		The core innovation of Brafar lies in its novel bidirectional refactoring algorithm and coarse-to-fine fault localization.                                                                                                                                                                                                                                                                		The former aligns the CFSs of buggy and correct programs by applying semantics-preserving refactoring operations to both programs in a guided manner,                                                                                                                                                                                                                                                                		while the latter identifies basic blocks that truly need repairs based on the semantics of their enclosing statements and themselves.                                                                                                                                                                                                                                                                		In our experimental evaluation on 1783 real-life incorrect student submissions from a publicly available dataset, Brafar significantly outperformed Refactory and Clara, generating correct repairs for more incorrect programs with smaller patch sizes in a shorter time.},
booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {856–868},
numpages = {13},
keywords = {Program Repair, Programming Education, Software Refactoring},
location = {Vienna, Austria},
series = {ISSTA 2024}
}

@inproceedings{10.1145/3689535.3689552,
author = {Whyte, Robert and Kirby, Diana and Sentance, Sue},
title = {Secondary Students' Emerging Conceptions of AI: Understanding AI Applications, Models, Engines and Implications},
year = {2024},
isbn = {9798400711770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689535.3689552},
doi = {10.1145/3689535.3689552},
abstract = {Despite the growing use of AI technologies, there is limited empirical evidence of secondary students’ understanding of AI. Gaining insights into ways in which this might be influenced by their everyday engagement with these tools can support the teaching of AI, including the design of educational resources. The SEAME framework proposes four dimensions for thinking about AI: (i) the social and ethical (SE) implications of AI; (ii) its use in applications (A); (iii) how models (M) are trained; and (iv) the underlying engines (E) used. In this paper, we investigate students’ emerging conceptions of AI and how these can be understood through classification using an appropriate framework. Drawing on data from 474 secondary and college students (11–18), we found that students mostly focused on the applications of AI, followed by its social and ethical implications. We found evidence that students held some primarily accurate conceptions of how AI works, such as how models are trained using large datasets and how human behaviours are simulated (e.g. talking, problem-solving). Specific conceptions relating to generative AI tools were also observed, including the ability to generate content and response to prompts. Conversely, we found students held naive conceptions relating to AI systems having agency or emotions. We argue that efforts to promote accurate conceptions of AI should build on students’ conceptions and take into account how language and representation are used (e.g. anthropomorphism). The results of this study have implications for educators, resource developers and researchers.},
booktitle = {Proceedings of the 2024 Conference on United Kingdom &amp; Ireland Computing Education Research},
articleno = {3},
numpages = {7},
keywords = {K-12 education, artificial intelligence, computing education, conceptions, machine learning},
location = {Manchester, United Kingdom},
series = {UKICER '24}
}

@proceedings{10.1145/3711403,
title = {ICETM '24: Proceedings of the 2024 7th International Conference on Educational Technology Management},
year = {2024},
isbn = {9798400717468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3724504,
title = {ICIEAI '24: Proceedings of the 2024 2nd International Conference on Information Education and Artificial Intelligence},
year = {2024},
isbn = {9798400711732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3631802,
title = {Koli Calling '23: Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
year = {2023},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Koli, Finland}
}

@article{10.1145/3688083,
author = {Aher, Gati},
title = {From Artificial Mentors to Simulated Subjects: Using Artificial Intelligence to Support Agency in Student-Driven Project-Based Learning},
year = {2024},
issue_date = {Fall 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/3688083},
doi = {10.1145/3688083},
abstract = {Turning passions into projects is difficult. Could AI tools provide students with the interruption, suspension, and sustenance they need to want to stay with the difficulty?},
journal = {XRDS},
month = oct,
pages = {20–25},
numpages = {6}
}

@inproceedings{10.1145/3711403.3711450,
author = {Boubaker, Anis and Fang, Ying},
title = {Automated Generation of Challenge Questions for Student Code Evaluation Using Abstract Syntax Tree Embeddings and RAG: An Exploratory Study},
year = {2025},
isbn = {9798400717468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711403.3711450},
doi = {10.1145/3711403.3711450},
abstract = {This paper presents an exploratory study on detecting learning gaps in student-submitted code by generating automated challenge questions. The proposed method compares the abstract syntax trees (ASTs) of student code with those of class-taught examples using embeddings and retrieval-augmented generation (RAG). The approach identifies the most structurally deviant sections of student code and generates challenge questions targeting advanced, un-taught coding techniques, such as function pointers and variadic functions. The evaluation, conducted on real-world C programming assignments, demonstrates the effectiveness of the selection process and the quality of generated questions. This work highlights the potential for using structural analysis and automated challenge questions generation to improve student assessment in coding education.},
booktitle = {Proceedings of the 2024 7th International Conference on Educational Technology Management},
pages = {277–282},
numpages = {6},
keywords = {Code review, educational challenge questions, abstract syntax trees, retrieval augmented generation},
location = {
},
series = {ICETM '24}
}

@article{10.1145/3636341.3636351,
author = {Bauer, Christine and Carterette, Ben and Ferro, Nicola and Fuhr, Norbert and Beel, Joeran and Breuer, Timo and Clarke, Charles L. A. and Crescenzi, Anita and Demartini, Gianluca and Di Nunzio, Giorgio Maria and Dietz, Laura and Faggioli, Guglielmo and Ferwerda, Bruce and Fr\"{o}be, Maik and Hagen, Matthias and Hanbury, Allan and Hauff, Claudia and Jannach, Dietmar and Kando, Noriko and Kanoulas, Evangelos and Knijnenburg, Bart P. and Kruschwitz, Udo and Li, Meijie and Maistro, Maria and Michiels, Lien and Papenmeier, Andrea and Potthast, Martin and Rosso, Paolo and Said, Alan and Schaer, Philipp and Seifert, Christin and Spina, Damiano and Stein, Benno and Tintarev, Nava and Urbano, Juli\'{a}n and Wachsmuth, Henning and Willemsen, Martijn C. and Zobel, Justin},
title = {Report on the Dagstuhl Seminar on Frontiers of Information Access Experimentation for Research and Education},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/3636341.3636351},
doi = {10.1145/3636341.3636351},
abstract = {This report documents the program and the outcomes of Dagstuhl Seminar 23031 "Frontiers of Information Access Experimentation for Research and Education", which brought together 38 participants from 12 countries. The seminar addressed technology-enhanced information access (information retrieval, recommender systems, natural language processing) and specifically focused on developing more responsible experimental practices leading to more valid results, both for research as well as for scientific education.The seminar featured a series of long and short talks delivered by participants, who helped in setting a common ground and in letting emerge topics of interest to be explored as the main output of the seminar. This led to the definition of five groups which investigated challenges, opportunities, and next steps in the following areas: reality check, i.e. conducting real-world studies, human-machine-collaborative relevance judgment frameworks, overcoming methodological challenges in information retrieval and recommender systems through awareness and education, results-blind reviewing, and guidance for authors.Date: 15--20 January 2023.Website: https://www.dagstuhl.de/23031.},
journal = {SIGIR Forum},
month = dec,
articleno = {7},
numpages = {28}
}

@article{10.1145/3673428,
author = {Shein, Esther},
title = {The Impact of AI on Computer Science Education},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {67},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/3673428},
doi = {10.1145/3673428},
abstract = {Understanding why “working hard and struggling is … an important way of learning.”},
journal = {Commun. ACM},
month = aug,
pages = {13–15},
numpages = {3}
}

@article{10.1145/3689215,
author = {Zlotnikova, Irina and Hlomani, Hlomani},
title = {GenAI in the Context of African Universities: A Crisis of Tertiary Education or Its New Dawn?},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689215},
doi = {10.1145/3689215},
abstract = {The rapid progression of generative artificial intelligence (GenAI) tools has raised significant interest and concern in academia. Instances of students submitting AI-generated assignments prompt investigations into implications for teaching, learning, and academic integrity. Recent publications highlight concerns such as a lack of conceptual understanding, threats to academic integrity, and disruptions to traditional assessment methods. While recognizing benefits like automated scoring and personalized learning, authors stress the responsible use of GenAI, emphasizing the educator's role in guiding students. This commentary identifies opportunities and threats of GenAI in African university contexts. Opportunities include increased operational efficiency, content generation, automated assessment, recognition of accessibility needs, overcoming language barriers, and accelerated research. However, these tools require human correction and cautious consideration of job displacement concerns. Threats encompass job displacement, privacy and security issues, threats to academic integrity, hallucinations/confabulations of GenAI, access and infrastructure challenges, technological overemphasis, lack of customization for local needs and cultural contexts, dependency on external providers, and unaffordable costs. The need for robust guidelines that balance technological advances with traditional teaching methods in African universities is emphasized. Given digital transformation initiatives like the African Union's Agenda 2063 and Botswana's SmartBots strategy, integrating GenAI could shape the future of African tertiary education. Proactive policies should address ethical concerns, ensure access, and make GenAI tools available, requiring a collaborative effort to navigate its impact responsibly.},
note = {Just Accepted},
journal = {Digit. Gov.: Res. Pract.},
month = aug,
keywords = {Generative artificial intelligence, universities, African countries}
}

@proceedings{10.1145/3708394,
title = {AIFE '24: Proceeding of the 2024 International Conference on Artificial Intelligence and Future Education},
year = {2024},
isbn = {9798400710650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3576882,
title = {CompEd 2023: Proceedings of the ACM Conference on Global Computing Education Vol 1},
year = {2023},
isbn = {9798400700484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome participants to the 2nd ACM Global Conference on Computing Education (ACM CompEd 2023) being held in Hyderabad, India, 7th-9th December, 2023 with the Working Groups meetings being held on 5th and 6th December 2023.ACM CompEd is a recent addition to the list of ACM sponsored conferences devoted to research in all aspects of computing education, including education at the school and college levels. The Hyderabad edition is only the second in this promising series. The long hiatus due to Covid-19 pushed this conference by two years, but we are glad that it is finally here!This edition of ACM CompEd partly overlaps with COMPUTE 2023, ACM India's flagship conference on Computing Education. Having the two conferences adjacent to each other is a great way to build synergy between the Indian computing education community and the global community of computing education researchers.},
location = {Hyderabad, India}
}

@proceedings{10.1145/3585059,
title = {SIGITE '23: Proceedings of the 24th Annual Conference on Information Technology Education},
year = {2023},
isbn = {9798400701306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Marietta, GA, USA}
}

@inproceedings{10.1145/3626252.3630755,
author = {Shah, Anshul and Yu, Jerry and Tong, Thanh and Soosai Raj, Adalbert Gerald},
title = {Working with Large Code Bases: A Cognitive Apprenticeship Approach to Teaching Software Engineering},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630755},
doi = {10.1145/3626252.3630755},
abstract = {Prior work has highlighted the gap between industry expectations for recent university graduates and the abilities those recent graduates possess. These works have even specifically recommended that students be given the opportunity to work on large, pre-existing code bases in their undergraduate career. This paper presents our experience teaching a newly-created course calledWorking with Large Code Bases. Guided by a Cognitive Apprenticeship approach to provide an authentic classroom experience that emphasizes the implicit processes and techniques involved in real-world software engineering, the course serves as a practical introduction to the skills and workflow involved in navigating and understanding a large code base. The goal of this experience report is to provide the motivation for key course design decisions, an overview of the course content, and a detailed description of key course components. We present student feedback indicating improved confidence in navigating a large code base and course outcomes related to specific tools and techniques students used in the course. Finally, we provide the full set of course materials we used and actionable recommendations for instructors to administer this course at their own institution, even with limited TA support.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1209–1215},
numpages = {7},
keywords = {cognitive apprenticeship, large code bases, program comprehension, project-based learning},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3702163.3702180,
author = {Pe\~{n}afiel, Myriam Guadalupe and V\'{a}squez-Pe\~{n}afiel, Mar\'{\i}a-Stefanie and Pe\~{n}afiel, Diego Alberto V\'{a}squez},
title = {The Role of Artificial Intelligence Tools in Knowledge Generation: Implications for Education},
year = {2025},
isbn = {9798400717819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702163.3702180},
doi = {10.1145/3702163.3702180},
abstract = {The research delves into the transformative potential of artificial intelligence (AI) tools in knowledge generation and their pivotal role in the educational process. It illuminates which AI tools can revolutionize educational activities, harnessing their power through the myriad AI tools available for diverse educational functions and the most popular AI tools that amplify student learning. Insights from both public and private higher education institutions, encompassing students from various semesters, have unveiled significant revelations. A key finding was the students' awareness of the available AI tools and their preferences. These findings underscore the imperative of effectively integrating diverse AI tools within the educational environment to bolster knowledge generation among students, thereby highlighting areas for improvement and potential in educational practices related to AI.},
booktitle = {Proceedings of the 2024 16th International Conference on Education Technology and Computers},
pages = {117–124},
numpages = {8},
keywords = {AI, AI tools, Artificial intelligence, Knowledge Generation, Mathematics of computing, education},
location = {
},
series = {ICETC '24}
}

@inproceedings{10.1145/3686852.3687073,
author = {Servin, Christian and Karichev, Nadia V. and Pagel, Myshie},
title = {Unfolding Programming: How to Use AI Tools in Introductory Computing Courses},
year = {2024},
isbn = {9798400711060},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686852.3687073},
doi = {10.1145/3686852.3687073},
abstract = {Artificial Intelligence (AI) generative tools, commonly referred to as AI-based tools, have become integral in various computing domains, including education. The widespread adoption of these tools has raised concerns among educators, spanning from issues related to plagiarism and comprehension gaps to potential threats to student identity. Consequently, educators are grappling with how to adapt their courses and incorporate AI technologies into their curriculum and pedagogical approaches. In addition to navigating challenges associated with AI regulations, educators face the compounded difficulty of addressing post-pandemic issues, such as students displaying diminished effort and professionalism in the classroom. The convergence of these two challenges creates a complex scenario that intertwines technical and professional considerations. Within the Computer Science Fundamentals course, commonly referred to as CS 1, the learning process revolves around comprehending programming through a sequential understanding of steps, as each concept builds upon the preceding one. This investigation centers on the CS 1 curriculum within an American two-year program, commonly known as a community college. The objective is to address a problem by leveraging an AI tool within team settings. The study assesses both problem-solving capabilities and the effectiveness of teamwork, providing recommendations to guide students in the proper utilization of AI tools. The emphasis is on fostering contextual relevance and collaborative work within the generative learning process.},
booktitle = {Proceedings of the 25th Annual Conference on Information Technology Education},
pages = {49–55},
numpages = {7},
keywords = {ai-tools, community colleges, prompt programming, two-year},
location = {El Paso, TX, USA},
series = {SIGITE '24}
}

@proceedings{10.1145/3660650,
title = {WCCCE '24: Proceedings of the 26th Western Canadian Conference on Computing Education},
year = {2024},
isbn = {9798400709975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Kelowna, BC, Canada}
}

@article{10.1145/3687131,
author = {Brodley, Carla and Barr, Valerie and Gunter, Elsa and Guzdial, Mark and Libeskind-Hadas, Ran and Manaris, Bill},
title = {ACM 2023: CS + X---Challenges and Opportunities in Developing Interdisciplinary-Computing Curricula},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2153-2184},
url = {https://doi.org/10.1145/3687131},
doi = {10.1145/3687131},
journal = {ACM Inroads},
month = aug,
pages = {42–50},
numpages = {9}
}

@article{10.5555/3729849.3729853,
author = {Garcia, Yuan and Ngo, Jenny and Lin, Florence Rui and Dodds, Zachary},
title = {Adaptable Metrics to Inform Introductory CS},
year = {2025},
issue_date = {April 2025},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {9},
issn = {1937-4771},
abstract = {Metrics have long been used to assess and guide successful software projects. Traditionally these metrics have measured software's professional rather than its educational suitability. This work proposes six adaptable, reproducible pedagogical metrics. With these metrics, we track an Introductory CS course's capstone projects, 2018--2024. The results suggest both year-over-year evolution and a more sudden, LLM-correlated impact on students' relationship with their early computing work. We have begun adapting our curriculum to these signals, and we foresee future refinements and broader applications to metrics-based reproducible curricular assessment.},
journal = {J. Comput. Sci. Coll.},
month = apr,
pages = {34–42},
numpages = {9}
}

@inproceedings{10.1145/3632634.3655852,
author = {Aladi, Clement Chimezie},
title = {IT Higher Education Teachers and Trust in AI-Enabled Ed-Tech: Implications for Adoption of AI in Higher Education},
year = {2024},
isbn = {9798400704772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632634.3655852},
doi = {10.1145/3632634.3655852},
abstract = {The integration of Artificial Intelligence (AI) in higher education encounters a myriad of inhibiting factors, notably the conspicuous absence of transparency, reliability issues, and ethical concerns. This problem has substantially impeded the assimilation of generative AI-enabled Educational Technology (Ed-Tech) within the higher education domain, unlike other fields such as finance, health, and management. The prevailing sentiment among higher education practitioners remains wavering, with differing opinions on whether to permit AI comprehensively, impose complete restrictions, or allow minimal integration into academic courses. This pilot study endeavors to elucidate the nuanced determinants influencing cognitive trust of Information Technology (IT) Higher Education instructors in AI-enabled educational Technology. The implications of this trust, or lack thereof, on the broader adoption of AI in higher education, constitute a focal point of investigation in this scholarly investigation.},
booktitle = {Proceedings of the 2024 Computers and People Research Conference},
articleno = {19},
numpages = {16},
keywords = {AI-enabled, EdTech, Educational Technology, Higher Education, Keywords: AI, Trust},
location = {Murfreesboro, TN, USA},
series = {SIGMIS-CPR '24}
}

@inproceedings{10.1145/3660650.3660662,
author = {Goddard, Quinn and Moton, Nathan and Hudson, Jonathan and He, Helen Ai},
title = {A Chatbot Won't Judge Me: An Exploratory Study of Self-disclosing Chatbots in Introductory Computer Science Classes},
year = {2024},
isbn = {9798400709975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660650.3660662},
doi = {10.1145/3660650.3660662},
abstract = {Students in introductory Computer Science (CS) courses sometimes struggle with learning course content, but feel these struggles are uniquely theirs. To foster a more inclusive CS culture and normalize challenges in the learning process, we designed a conversational agent (“chatbot”) that self-discloses information about the chatbot’s own imaginary struggles with learning course material. Inspired by previous work in the mental health domain where humans reciprocated disclosure when a chatbot disclosed sensitive information, our goal was to promote student self-disclosure of learning challenges and to help students feel less alone. To inform design, we first conducted three focus groups with CS students on themes of identity and belonging. Based on these findings, we designed a self-disclosing chatbot (“Mibi”) and deployed it in a pilot summer course (40 students) and a larger course (460 students) in the fall semester of 2023. Our work is the first real-world deployment of a chatbot in higher education for promoting student wellbeing, rather than assisting with practical course content. We highlight findings from this exploratory study, sharing how students engaged with Mibi, where it succeeded, where it has room to grow, and how that can inform future iterations of this promising new classroom companion for student mental health.},
booktitle = {Proceedings of the 26th Western Canadian Conference on Computing Education},
articleno = {9},
numpages = {7},
keywords = {CS1/CS2, Chatbot, Computer Science, Mental well-being, Qualitative, Self-Disclosure},
location = {Kelowna, BC, Canada},
series = {WCCCE '24}
}

@inproceedings{10.1145/3641554.3701883,
author = {Haji Amin Shirazi, Shirin and Pang, Ashley and Knight, Allan and Salloum, Mariam and Vahid, Frank},
title = {Midterm Exam Outliers Efficiently Highlight Potential Cheaters on Programming Assignments},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701883},
doi = {10.1145/3641554.3701883},
abstract = {The ubiquitous use of online tools, contractors and homework sites, has made plagiarism a concerning topic in computer science education. With the introduction of ChatGPT, it poses a threat now more than ever. Many cheating detection tools, such as similarity checkers and style anomaly checkers, help instructors decide whether a student has plagiarized. However, these are not scalable to large classes. Similarity tools can produce high rates of suspected cheating and thus ineffectively use an instructor's time in weeding out the actual cheating cases, especially in the early weeks of CS courses where programs can be small and student solutions can be very similar. We developed a new approach using outlier detection to filter inconsistent performers based on their lab scores throughout the course and their midterm exam scores. Instructors can then manually analyze a manageable amount of students even with large class sizes. We performed our experiment on two large course offerings of CS1 (a total of 177 students) using our algorithm and compared it to a manual analysis performed by an experienced CS1 instructor. The detection approach identified 11 students in the first offering (Winter 2019) and 12 students in the second offering (Spring 2023). With an average precision of 83%, our tool produces a list of concerning students with high precision. This significantly helps teachers efficiently allocate their time and pursue cheating early in the term in order to address and prevent further issues.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {437–442},
numpages = {6},
keywords = {academic integrity, cs1, plagiarism, programming},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3561833.3561843,
author = {Raman, Arun and Kumar, Viraj},
title = {Programming Pedagogy and Assessment in the Era of AI/ML: A Position Paper},
year = {2022},
isbn = {9781450397759},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3561833.3561843},
doi = {10.1145/3561833.3561843},
abstract = {A growing set of tasks that once depended critically on human cognition can now be effectively accomplished by automated systems, either independently or in partnership with humans. This trend is being accelerated by advances in Artificial Intelligence and Machine Learning, and it forces educators to consider whether tasks emphasized by existing curricula will remain relevant once their students graduate. For introductory programming (CS1), existing curricula emphasize the task of code writing. We survey recent research on automated systems for writing code, some of which are as proficient as undergraduates on CS1-level programming tasks. Further, these tools are rapidly improving and some of them are available freely for students. Thus, we posit that pedagogy and assessment for code writing requires rethinking. We examine the components of the code-writing task using a six-step framework proposed in the literature, and we identify the impact of automated systems at each step. In response to this impact, we propose changes to CS1 pedagogy and assessment.},
booktitle = {Proceedings of the 15th Annual ACM India Compute Conference},
pages = {29–34},
numpages = {6},
keywords = {pedagogy and assessment, code writing, CS1},
location = {Jaipur, India},
series = {COMPUTE '22}
}

@inproceedings{10.1145/3641555.3705252,
author = {Tadimalla, Sri Yash and Maher, Mary Lou},
title = {Sociotechnical AI Education Course Design for CS Majors and Non-Majors},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705252},
doi = {10.1145/3641555.3705252},
abstract = {As generative AI increasingly integrates into society and education, the number of institutions implementing AI usage policies and offering introductory AI courses is rising. These introductory AI courses mustn't replicate the "gateway/weed-out" phenomenon observed in introductory computer science courses like CS1 and CS2. Literature in computer science education suggests that interventions such as summer camps, bridge courses, and socio-technical courses have improved the sense of belonging and retention among students from underrepresented groups, thereby broadening participation in computer science. Building on previous work to create a socio-technical curriculum for all ages and education levels, this paper presents a course for teaching introductory AI concepts that adopts a socio-technical approach, complete with weekly activities and content designed for broad access. The course has been taught as a 1-credit general education course, primarily for freshmen and first-year students from various majors, and a 3-credit course for CS majors at all levels.This paper provides a curriculum and resources to teach a socio-technical introductory AI course. This approach is important because it not only democratizes AI education across diverse student backgrounds but also equips all students with the critical socio-technical multidisciplinary perspective necessary to navigate and shape the future ethical landscape of AI technology.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1631–1632},
numpages = {2},
keywords = {AI curriculum, AI education, intro to AI, socio-technical AI literacy},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.1145/3700773,
author = {Holland-Minkley, Amanda and Barnard, Jakob E. and Barr, Valerie and Braught, Grant and Davis, Janet and Reed, David and Schmitt, Karl and Tartaro, Andrea and Teresco, James D.},
title = {CS2023: Computer Science Curriculum Guidelines: A New Liberal Arts Perspective},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {2153-2184},
url = {https://doi.org/10.1145/3700773},
doi = {10.1145/3700773},
journal = {ACM Inroads},
month = feb,
pages = {40–52},
numpages = {13}
}

@proceedings{10.1145/3696230,
title = {ICDTE '24: Proceedings of the 2024 8th International Conference on Digital Technology in Education (ICDTE)},
year = {2024},
isbn = {9798400717574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3617650,
title = {CompEd 2023: Proceedings of the ACM Conference on Global Computing Education Vol 2},
year = {2023},
isbn = {9798400703744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome participants to the 2nd ACM Global Conference on Computing Education (ACM CompEd 2023) being held in Hyderabad, India, 7th-9th December, 2023 with the Working Groups meetings being held on 5th and 6th December 2023.ACM CompEd is a recent addition to the list of ACM sponsored conferences devoted to research in all aspects of computing education, including education at the school and college levels. The Hyderabad edition is only the second in this promising series. The long hiatus due to Covid-19 pushed this conference by two years, but we are glad that it is finally here!This edition of ACM CompEd partly overlaps with COMPUTE 2023, ACM India's flagship conference on Computing Education. Having the two conferences adjacent to each other is a great way to build synergy between the Indian computing education community and the global community of computing education researchers.},
location = {Hyderabad, India}
}

@article{10.1145/3722449.3722458,
author = {Vos, David and Rus, Clara and Ernst, Marina},
title = {Report on the 15th European Summer School in Information Retrieval (ESSIR 2024)},
year = {2025},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/3722449.3722458},
doi = {10.1145/3722449.3722458},
abstract = {The 15th European Summer School in Information Retrieval (ESSIR 2024) was held in Amsterdam from 1 to 5 July 2024 at the University of Amsterdam. ESSIR consisted of a week of lectures by top invited experts in the Information Retrieval field. This year, the ESSIR program consisted of 17 lectures, a symposium (FDIA), a hackathon, and four social events. Special to this year's ESSIR were the organization of the hackathon, a fully hybrid setup, the low registration fee with possible support for students, and the high number of social events. This report outlines the summer school, which attracted a record-high number of 131 participants, including speakers, volunteers, and offline and online attendees.Date: 1--5 July 2024.Website: https://2024.essir.eu.},
journal = {SIGIR Forum},
month = mar,
pages = {1–9},
numpages = {9}
}

@inproceedings{10.1145/3723010.3723019,
author = {Maisch, Robin and Hagel, Nathan and Bartel, Alexander},
title = {Towards Robust Plagiarism Detection in Programming Education: Introducing Tolerant Token Matching Techniques to Counter Novel Obfuscation Methods},
year = {2025},
isbn = {9798400712821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3723010.3723019},
doi = {10.1145/3723010.3723019},
abstract = {With the rise of AI-generated code, programming courses face new challenges in detecting code plagiarism. Traditional methods struggle against obfuscation techniques that modify code structure through statement insertion and deletion. To address this, we propose a novel approach based on tolerant token matching designed to enhance resilience against such attacks. We evaluate our method through three experiments on a real-life dataset with AI-obfuscated plagiarisms. The results show that our approach increased the median similarity gap between originals and plagiarisms by 1 to 6 percentage points.},
booktitle = {Proceedings of the 6th European Conference on Software Engineering Education},
pages = {11–19},
numpages = {9},
keywords = {Software Plagiarism Detection, Source Code Plagiarism Detection, Plagiarism Obfuscation, Obfuscation Attacks, Code Normalization, Tokenization, Computer Science Education},
location = {
},
series = {ECSEE '25}
}

@inproceedings{10.1145/3593342.3593349,
author = {Kendon, Tyson and Wu, Leanne and Aycock, John},
title = {AI-Generated Code Not Considered Harmful},
year = {2023},
isbn = {9798400707896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593342.3593349},
doi = {10.1145/3593342.3593349},
abstract = {Recent developments in AI-generated code are merely the latest in a series of challenges to traditional computer science education. AI code generators, along with the plethora of available code on the Internet and sites that facilitate contract cheating, are a striking contrast to the heroic notion of programmers toiling away to create artisanal code from whole cloth. We need not interpret this to mean that more, potentially automated, policing of student assignments is necessary: automated policing of student work is already fraught with complications and ethical concerns. We argue that instructors should instead reconsider assessment design in their pedagogy in light of recent developments, with a focus on how students build knowledge, practice skills, and develop processes. How can these new tools support students and the way they learn, and support the way that computer scientists will work in the years to come? This is an opportunity to revisit how computer science is taught, how it is assessed, how we think about and present academic integrity, and the role of the computer scientist in general.},
booktitle = {Proceedings of the 25th Western Canadian Conference on Computing Education},
articleno = {3},
numpages = {7},
keywords = {tool-generated code, copy-paste, contract cheating, assessments, academic integrity, AI-generated code},
location = {Vancouver, BC, Canada},
series = {WCCCE '23}
}

@inproceedings{10.1145/3576882.3617916,
author = {Agarwal, Nimisha and Kumar, Viraj and Raman, Arun and Karkare, Amey},
title = {A Bug's New Life: Creating Refute Questions from Filtered CS1 Student Code Snapshots},
year = {2023},
isbn = {9798400700484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576882.3617916},
doi = {10.1145/3576882.3617916},
abstract = {In an introductory programming (CS1) context, a Refute question asks students for a counter-example which proves that a given code fragment is an incorrect solution for a given task. Such a question can be used as an assessment item to (formatively) develop or (summatively) demonstrate a student's abilities to comprehend the task and the code well enough to recognize a mismatch. These abilities assume greater significance with the emergence of generative AI technologies capable of writing code that is plausible (at least to novice programmers) but not always correct.Instructors must address three concerns while designing an effective Refute question, each influenced by their specific teaching-learning context: (1) Is the task comprehensible? (2) Is the incorrect code a plausible solution for the task? (3) Is the complexity of finding a counter-example acceptable? While the first concern can often be addressed by reusing tasks from previous code writing questions, addressing the latter concerns may require substantial instructor effort. We therefore investigate whether concerns (2) and (3) can be addressed by buggy student solutions for the corresponding code writing question from a previous course offering. For 6 code writing questions (from a Fall 2015 C programming course), our automated evaluation system logged 13,847 snapshots of executable student code, of which 10,574 were buggy (i.e., they failed at least one instructor-supplied test case). Code selected randomly from this pool rarely addresses these concerns, and manual selection is infeasible. Our paper makes three contributions. First, we propose an automated mechanism to filter this pool to a more manageable number of snapshots from which appropriate code can be selected manually. Second, we evaluate our semi-automated mechanism with respect to concerns (2) and (3) by surveying a diverse set of 56 experienced participants (instructors, tutors, and teaching assistants). Third, we use this mechanism to seed a public repository of Refute questions and provide a template to create additional questions using a public resource (CodeCheck).},
booktitle = {Proceedings of the ACM Conference on Global Computing Education Vol 1},
pages = {7–14},
numpages = {8},
keywords = {CS1, assessment, refute questions},
location = {Hyderabad, India},
series = {CompEd 2023}
}

@proceedings{10.1145/3637989,
title = {ICEEL '23: Proceedings of the 2023 7th International Conference on Education and E-Learning},
year = {2023},
isbn = {9798400708732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@proceedings{10.1145/3607505,
title = {CSET '23: Proceedings of the 16th Cyber Security Experimentation and Test Workshop},
year = {2023},
isbn = {9798400707889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Marina del Rey, CA, USA}
}

@article{10.5555/3665609.3665621,
author = {Jonas, Michael},
title = {Mitigating Use of Artificial Intelligence in Student Assignments},
year = {2024},
issue_date = {April 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {8},
issn = {1937-4771},
abstract = {With the proliferation of advanced Artificial Intelligent systems trained on robust language models, it is becoming more difficult to discern acts of plagiarism in the classroom. Though some of this falls under the academic misconduct policy of an institution, it can be confusing to students as to what qualifies as proper use. Students use tools like Grammarly, to improve their writing, and find more advanced AI tools, such as ChatGPT as an additional resource. For faculty to simply ban the use of these tools, creates an unworkable model. Embracing them can also be problematic as not all educational material benefits from a flipped approach. In this paper we discuss changing the dynamic by applying targeted assessments that incrementally address elements in an assignment, to affirm the work that students submitted and discourage use of improper tools that don't assist in learning.},
journal = {J. Comput. Sci. Coll.},
month = apr,
pages = {173–181},
numpages = {9}
}

@article{10.1145/3688081,
author = {Schotanus, Frank},
title = {Classroom Technology, AI, and How They Came to Exist Together},
year = {2024},
issue_date = {Fall 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/3688081},
doi = {10.1145/3688081},
journal = {XRDS},
month = oct,
pages = {11},
numpages = {1}
}

@inproceedings{10.1145/3641554.3701831,
author = {Wen, Elliott and Ma, Sean and Denny, Paul and Tempero, Ewan and Weber, Gerald and Yue, Zongcheng},
title = {KernelVM: Teaching Linux Kernel Programming through a Browser-Based Virtual Machine},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701831},
doi = {10.1145/3641554.3701831},
abstract = {Providing students with hands-on experience in kernel programming within a real-world operating system is highly beneficial in an Operating Systems (OS) course for teaching core operating system concepts and developing practical skills. However, accessing suitable devices for such hands-on experimentation poses significant challenges. Traditional solutions involve hosting virtual machines on cloud platforms, which are expensive and do not scale well with increasing student numbers. Additionally, many students' personal devices, such as Macs or iPads, have limited support for running Linux, creating further barriers. In this paper, we introduce KernelVM, a novel cost-effective platform that offers students a Linux virtual machine with full superuser access and pre-configured kernel programming toolchains. KernelVM is accessible via any modern browser on any device. It performs all computations locally within the user's browser, thus eliminating cloud computing costs. KernelVM provides a robust learning environment by incorporating interactive virtual hardware components and an automatic evaluation system, supporting a wide range of tasks, including multi-threaded cryptographic kernel modules and Linux drivers for hardware interaction. We detail the design of KernelVM, and describe our experiences incorporating it for the first time into an OS course with 159 undergraduate students. We found that KernelVM was instrumental in improving the quality and efficiency of hands-on learning experiences, with students reporting increased satisfaction and engagement due to the immediate feedback and the ability to experiment in a risk-free environment. Our experience suggests that KernelVM not only addresses the logistical challenges of kernel programming education, but it helps foster a highly interactive and engaging learning experience.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1204–1210},
numpages = {7},
keywords = {computer science education, linux kernel programming, virtual machine, web-based education},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.1145/3635471,
author = {Barr, Valerie and Brodley, Carla E. and P\'{e}rez-Qui\~{n}ones, Manuel},
title = {Visualizing Progress in Broadening Participation in Computing: The Value of Context},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {67},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/3635471},
doi = {10.1145/3635471},
abstract = {To improve diversity and BPC analysis and assessment, institutions should examine cohort-based data, report intersectional data as a norm, and consider university demographic context.},
journal = {Commun. ACM},
month = jul,
pages = {46–55},
numpages = {10}
}

@inproceedings{10.1145/3641555.3705144,
author = {Gupta, Ishita and Bridgman, Maya and Wang, Sierra and Mitchell, John},
title = {Coding Pathfinder: A Platform for Creative, Self-Guided Mastery in Programming},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705144},
doi = {10.1145/3641555.3705144},
abstract = {We present Coding Pathfinder, a platform to help non-programmers learn to code for a specific purpose. This paper explores how we can scaffold generative AI to provide structure and ensure mastery in informal learning settings, introducing a new approach to coding education. In the current iteration of Pathfinder, a user describes the coding task that they are working on. After collecting some details and scoping the project, Pathfinder identifies the skills that the user will master upon successful completion of the project. It then assesses which of the skills our users already has, and designs a personalised learning journey. The guided journey consists of instructions, explanations, tasks and videos. We also incorporate a chat feature so users can ask questions and engage as if they are working with a tutor.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1465–1466},
numpages = {2},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.1145/3563968,
author = {Barr, Valerie},
title = {What must all post-secondary students learn about computing?},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/3563968},
doi = {10.1145/3563968},
abstract = {Expanding students' understanding of computing's potential.},
journal = {Commun. ACM},
month = oct,
pages = {29–31},
numpages = {3}
}

@article{10.1145/3688082,
author = {Nie, Allen},
title = {On the Promising Path of Making Education Effective for Every Student},
year = {2024},
issue_date = {Fall 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/3688082},
doi = {10.1145/3688082},
abstract = {Traditional classroom teaching is an example of scaling education with a one-size-fits-all strategy, but modern machine learning algorithms are promised to adapt and personalize for each student. Can we succeed and what are the harms and benefits of introducing AI into the classroom?},
journal = {XRDS},
month = oct,
pages = {14–19},
numpages = {6}
}

@inproceedings{10.1145/3657604.3664658,
author = {Slama, Rachel and Toutziaridi, Amalia Christina and Reich, Justin},
title = {Three Paradoxes to Reconcile to Promote Safe, Fair, and Trustworthy AI in Education},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3664658},
doi = {10.1145/3657604.3664658},
abstract = {Incorporating recordings of teacher-student conversations into the training of LLMs has the potential to improve AI tools. Although AI developers are encouraged to put "humans in the loop" of their AI safety protocols, educators do not typically drive the data collection or design and development processes underpinning new technologies. To gather insight into privacy concerns, the adequacy of safety procedures, and potential benefits of recording and aggregating data at scale to inform more intelligent tutors, we interviewed a pilot sample of teachers and administrators using a scenario-based, semi-structured interview protocol. Our preliminary findings reveal three "paradoxes" for the field to resolve to promote safe, fair, and trustworthy AI. We conclude with recommendations for education stakeholders to reconcile these paradoxes and advance the science of learning.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {295–299},
numpages = {5},
keywords = {education, human-centered design, responsible AI, teacher perspectives, tutoring},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@article{10.1145/3709616.3709620,
author = {Parra, Esteban and Aponte, Jairo},
title = {Report from the Summer School on Software Engineering andArtificial Intelligence},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3709616.3709620},
doi = {10.1145/3709616.3709620},
abstract = {This report summarizes the curriculum and academic outcomes of the Summer School on Software Engineering and Artificial Intelligence (AI) held at the Universidad de Los Andes in Bogot´a, Colombia. The summer school offered an in-depth introduction to the fields of Machine Learning (ML), Artificial Intelligence (AI), and Natural Language Processing (NLP); their role and applications in Software Engineering (SE) and the software development process. The feedback we received from the participants indicates that the program successfully enhanced their knowledge and the skills needed for them to navigate the role of AI in the current landscape of software engineering.The students of the summer school were engaged in the development of a full software system using AI-based tools as part of the development process. We found that the project was successful in providing the students with experience regarding how to incorporate AI-based tools as part of their software development process but not all students showed the same level of proficiency when leveraging AI tools.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {12–14},
numpages = {3}
}

@inproceedings{10.1145/3699538.3699558,
author = {H\"{o}per, Lukas and Schulte, Carsten},
title = {New Perspectives on the Future of Computing Education: Teaching and Learning Explanatory Models},
year = {2024},
isbn = {9798400710384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3699538.3699558},
doi = {10.1145/3699538.3699558},
abstract = {This paper introduces the explanatory model approach to address challenges in computing education arising from rapid technological developments and paradigm shifts, particularly regarding artificial intelligence and machine learning. Traditional approaches in computing education aim to teach basic concepts derived from the computer science discipline as they are in order to support students’ understanding of these concepts and digital technologies that implement these concepts. This approach is challenged in topics like machine learning, where the ground truth of the inner workings and the behaviors of these technologies is not so clear, making rethinking approaches in computing education necessary. The explanatory model approach suggests that students learn models about computational concepts and digital artifacts that help them understand, explain, and reason about digital technologies. While drawing on the notion of models in science and science education, this approach emphasizes learning and using explanatory models as a focal point in computing classes. Doing so may help students make use of these models as tools and enable them to reflect on and critique different models in various contexts. Additionally, this paper discusses how making explanatory models explicit in research can enrich computing education research and our discourses and describes avenues for researching explanatory models as different perspectives on computational concepts.},
booktitle = {Proceedings of the 24th Koli Calling International Conference on Computing Education Research},
articleno = {22},
numpages = {8},
keywords = {K-12, computing education, explanatory models, artificial intelligence, computational concepts},
location = {
},
series = {Koli Calling '24}
}

@proceedings{10.1145/3632621,
title = {ICER '24: Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 2},
year = {2024},
isbn = {9798400704765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1145/3613904.3642807,
author = {Zheng, Chengbo and Yuan, Kangyu and Guo, Bingcan and Hadi Mogavi, Reza and Peng, Zhenhui and Ma, Shuai and Ma, Xiaojuan},
title = {Charting the Future of AI in Project-Based Learning: A Co-Design Exploration with Students},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642807},
doi = {10.1145/3613904.3642807},
abstract = {Students’ increasing use of Artificial Intelligence (AI) presents new challenges for assessing their mastery of knowledge and skills in project-based learning (PBL). This paper introduces a co-design study to explore the potential of students’ AI usage data as a novel material for PBL assessment. We conducted workshops with 18 college students, encouraging them to speculate an alternative world where they could freely employ AI in PBL while needing to report this process to assess their skills and contributions. Our workshops yielded various scenarios of students’ use of AI in PBL and ways of analyzing such usage grounded by students’ vision of how educational goals may transform. We also found that students with different attitudes toward AI exhibited distinct preferences in how to analyze and understand their use of AI. Based on these findings, we discuss future research opportunities on student-AI interactions and understanding AI-enhanced learning.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {94},
numpages = {19},
keywords = {AI for education, co-design, generative AI, project-based learning, qualitative study},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3706599.3719953,
author = {Li, Charlotte and Long, Duri},
title = {Understanding Journalistic Practices Surrounding AI: Towards the Design of Support Resources for Journalists to Foster Public AI Literacy},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719953},
doi = {10.1145/3706599.3719953},
abstract = {AI literacy refers to an important set of skills for thinking critically about AI for the general public, and media plays a significant role in shaping public understanding and perception of AI. Research has called for the design of socio-technical systems for journalism practitioners to foster AI literacy in media. In this paper, we conduct an interview study investigating how journalists produce news articles about AI based on a qualitative text analysis of recently published news articles about AI. Our analyses synthesize the current landscape of journalistic practices for producing AI news and surface journalism-specific considerations for communicating AI to the public. We recommend opportunities for supporting journalists with design interventions at different stages of the news production cycle. Our findings have implications for both designers of support tools for journalists, and the computational journalism community.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {550},
numpages = {9},
keywords = {AI Literacy, Computational Journalism, Design},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3641554.3701961,
author = {Brown, Cameron and Cruz Castro, Laura},
title = {Coordinate: A Virtual Classroom Management Tool For Large Computer Science Courses Using Discord},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701961},
doi = {10.1145/3641554.3701961},
abstract = {Effective classroom management is becoming increasingly challenging with the growing size of core computer science classrooms. However, classroom management remains a critical component of instruction in higher education. Successful classroom management ensures a structured, respectful environment that maximizes student engagement and learning outcomes. To address this, we present Coordinate, a novel educational tool based on Discord, a widely used free voice, video, and text chat application. Coordinate leverages Discord's interface, extensibility, and familiarity with young people. With the innovative blending between communication and logistics, this tool addresses the need to reduce instructors' time on classroom management tasks. Coordinate automatically manages several integral classroom offerings, including office hour queues, student assignment extension requests, teaching team performance, student feedback, and Q&amp;As. Alongside these features, we present details about the tool architecture, implementation, and deployment. Our tool has been deployed throughout a growing number of classes at a large state university to thousands of students. We present the perception of four distinct computer science instructors and their students. Overall, students are satisfied with the tool and find it valuable and easy to use. At the same time, instructors believe it can be a critical component of their classrooms, significantly reducing their time spent on class management and allowing them to focus on other necessary tasks. We also discuss the challenges and opportunities of using the Discord platform in an educational context. Our findings suggest that Coordinate can be a valuable tool for classroom management in higher education.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {165–171},
numpages = {7},
keywords = {classroom management, discord, higher education, software tools},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3585059.3611413,
author = {Soale, Jenifer and Collins, Tracy},
title = {Harnessing the Disruption of New Technologies to Maintain Effective Assessment Strategies in Information Technology Education},
year = {2023},
isbn = {9798400701306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585059.3611413},
doi = {10.1145/3585059.3611413},
abstract = {Technology tools have always been readily available to help students learn. From calculators to search engines to Artificial Intelligence (AI) tools, it has always been a challenge for educators to maintain assessments that effectively assess student learning on their merit. Adopting a continuous improvement mindset, reflecting on student outcomes, and utilizing research in assessment development can assist in this process. This paper will make recommendations on how to develop assessments in the world of AI that will promote and effectively assess student learning in Information Technology education.},
booktitle = {Proceedings of the 24th Annual Conference on Information Technology Education},
pages = {176–177},
numpages = {2},
keywords = {Academic Integrity, Artificial Intelligence, Information Technology Education, Student Assessment},
location = {Marietta, GA, USA},
series = {SIGITE '23}
}

@proceedings{10.1145/3691720,
title = {EKI '24: Proceedings of the 2nd International Conference on Educational Knowledge and Informatization},
year = {2024},
isbn = {9798400710230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Shanghai, China}
}

@inproceedings{10.1145/3641237.3691645,
author = {Liddle, Daniel and Grant, Carrie},
title = {AI Literacy as a Potential Mechanism of Stratification},
year = {2024},
isbn = {9798400705199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641237.3691645},
doi = {10.1145/3641237.3691645},
abstract = {Drawing on data from two parallel IRB-approved studies, this paper examines how students incorporate AI-generated text in their TPC writing assignments. In both studies, students were coached through the process of using AI tools for specific writing tasks, including an emphasis that AI-generated text should be scrutinized for rhetorical, stylistic, and factual errors. By looking at examples of students’ work and reflections of students’ experience, the authors identify differences in AI use between successful and unsuccessful writing samples. The authors consider how these differences could challenge the notion that AI might “level the playing field” of writing skill levels. The authors conclude with four AI literacies that will be necessary to prevent further stratification of student-writers’ skills.},
booktitle = {Proceedings of the 42nd ACM International Conference on Design of Communication},
pages = {3–10},
numpages = {8},
keywords = {AI literacy, Artificial Intelligence, instructional design, style},
location = {Fairfax, VA, USA},
series = {SIGDOC '24}
}

@article{10.5555/3715638.3715656,
author = {Roll, James},
title = {AI as a Learning Tool for Introductory Programming},
year = {2024},
issue_date = {September 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {4},
issn = {1937-4771},
abstract = {The goal of this assignment is to introduce introductory programming students to using generative AI tools like Claude and ChatGPT to help them in learning introductory programming. Students are shown how they can use AI tools to help explain basic programming concepts, decode cryptic error messages, explain why a program isn't working, and find syntax errors in and suggest fixes. Students are also encouraged to avoid using AI Tools to fully write programs at this point in their education, and introduced to the limitations generative AI tools for programming. This version of the assignment was written for an introductory Java programming course, but could easily be adapted to other programming languages.},
journal = {J. Comput. Sci. Coll.},
month = sep,
pages = {45},
numpages = {1}
}

@inproceedings{10.1145/3633083.3633220,
author = {Shaka, Martha and Carraro, Diego and Brown, Kenneth N.},
title = {Personalised Programming Education with Knowledge Tracing},
year = {2023},
isbn = {9798400716461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3633083.3633220},
doi = {10.1145/3633083.3633220},
abstract = {In traditional programming education, addressing diverse student needs and providing effective and scalable learning experiences is challenging. Conventional methods struggle to adapt to varying learning styles and offer personalised feedback. AI-based Programming Tools (AIPTs) have shown promise in automating feedback, simplifying programming concepts, and guiding students. Their widespread adoption is hindered by limitations related to accuracy, explanation, and personalisation. Conversely, AIPTs tailored for expert programmers, such as ChatGPT and Copilot, have gained popularity for their productivity-enhancing capabilities, but they still fall short in terms of personalisation, neglecting individual students’ unique knowledge and skills. Our research aims to leverage AI to create AIPTs that offer personalised feedback through adaptive learning, accommodating diverse student backgrounds and proficiency levels. In particular, we explore using Knowledge Tracing (KT) to anticipate specific syntax errors in programming assignments, addressing the challenges novices face in acquiring syntactical knowledge. The findings suggest the KT’s potential to transform programming education by enabling timely interventions for students dealing with specific errors or misconceptions, automating personalised feedback, and informing tailored instructional strategies.},
booktitle = {Proceedings of the 2023 Conference on Human Centered Artificial Intelligence: Education and Practice},
pages = {47},
numpages = {1},
keywords = {Automated Feedback, Knowledge Tracing, Personalisation, Programming Assignments, Syntax Errors},
location = {Dublin, Ireland},
series = {HCAIep '23}
}

@article{10.1145/3717402.3717404,
author = {Lonati, Violetta and Monga, Mattia and Barendsen, Erik},
title = {ITiCSE 2024 Recap},
year = {2025},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0097-8418},
url = {https://doi.org/10.1145/3717402.3717404},
doi = {10.1145/3717402.3717404},
abstract = {The 29th annual ACM conference on Innovation and Technology in Computer Science Education (ITiCSE) was held in Milan, Italy, July 5-10, 2024. Despite some last-minute changes due to an ongoing student protest at the University of Milan, we can consider this edition a success. A total of 345 participants attended ITiCSE, coming from 36 different countries},
journal = {SIGCSE Bull.},
month = feb,
pages = {4–5},
numpages = {2}
}

@inproceedings{10.1145/3641555.3705266,
author = {Hou, Irene and Nguyen, Hannah Vy and Man, Owen and MacNeil, Stephen},
title = {The Evolving Usage of GenAI by Computing Students},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705266},
doi = {10.1145/3641555.3705266},
abstract = {Help-seeking is a critical aspect of learning and problem-solving for computing students. Recent research has shown that many students are aware of generative AI (GenAI) tools; however, there are gaps in the extent and effectiveness of how students use them. With over two years of widespread GenAI usage, it is crucial to understand whether students' help-seeking behaviors with these tools have evolved and how. This paper presents findings from a repeated cross-sectional survey conducted among computing students across North American universities ( n=95 ). Our results indicate shifts in GenAI usage patterns. In 2023, 34.1% of students ( n=47 ) reported never using ChatGPT for help, ranking it fourth after online searches, peer support, and class forums. By 2024, this figure dropped sharply to 6.3% ( n=48 ), with ChatGPT nearly matching online search as the most commonly used help resource. Despite this growing prevalence, there has been a decline in students' hourly and daily usage of GenAI tools, which may be attributed to a common tendency to underestimate usage frequency. These findings offer new insights into the evolving role of GenAI in computing education, highlighting its increasing acceptance and solidifying its position as a key help resource.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1481–1482},
numpages = {2},
keywords = {chatgpt, computing education, generative ai, help-seeking},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@proceedings{10.1145/3696271,
title = {MLMI '24: Proceedings of the 2024 7th International Conference on Machine Learning and Machine Intelligence (MLMI)},
year = {2024},
isbn = {9798400717833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3689492.3689806,
author = {Gordon, Colin S.},
title = {The Linguistics of Programming},
year = {2024},
isbn = {9798400712159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689492.3689806},
doi = {10.1145/3689492.3689806},
abstract = {Research in programming languages and software engineering are broadly concerned with the study of aspects of computer programs: their syntactic structure, the relationship between form and meaning (semantics), empirical properties of how they are constructed and deployed, and more. We could equally well apply this description to the range of ways in which linguistics studies the form, meaning, and use of natural language. We argue that despite some notable examples of PL and SE research drawing on ideas from natural language processing, there are still a wealth of concepts, techniques, and conceptual framings originating in linguistics which would be of use to PL and SE research. Moreover we show that beyond mere parallels, there are cases where linguistics research has complementary methodologies, may help explain or predict study outcomes, or offer new perspectives on established research areas in PL and SE. Broadly, we argue that researchers across PL and SE are investigating close cousins of problems actively studied for years by linguists, and familiarity with linguistics research seems likely to bear fruit for many PL and SE researchers.},
booktitle = {Proceedings of the 2024 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {162–182},
numpages = {21},
keywords = {Linguistics, programming languages, software engineering},
location = {Pasadena, CA, USA},
series = {Onward! '24}
}

@article{10.1145/3716860,
author = {Ko, Shao-Heng and Stephens-Martinez, Kristin},
title = {Rethinking Computing Students’ Help Resource Utilization through Sequentiality},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {1},
url = {https://doi.org/10.1145/3716860},
doi = {10.1145/3716860},
abstract = {Background. Academic help-seeking benefits students’ achievement, but existing literature either studies important factors in students’ selection of all help resources via self-reported surveys or studies their help-seeking behavior in one or two separate help resources via actual help-seeking records. Little is known about whether computing students’ approaches and behavior match, and not much is understood about how they transition sequentially from one help resource to another.Objectives. We aim to study post-secondary computing students’ academic help-seeking approach and behavior. Specifically, we seek to investigate students’ self-reported orders of resource usage and whether these approaches match with students’ actual utilization of help resources. We also examine frequent patterns emerging from students’ chronological help-seeking records in course-affiliated help resources.Context and Study Method. We surveyed students’ self-reported orders of resource usage across 12 offerings of seven courses at two institutions, then analyzed their responses using various help resource dimensions identified by existing works. From two of these courses (an introduction to programming course and a data science course, 11 offerings), we obtained students’ help-seeking records in all course-affiliated help resources, along with code autograder records. We then compared students’ reported orders in these two courses against their actions in the records. Finally, we mined sequences of student help-seeking events from these two courses to reveal frequent sequential patterns.Findings. Students’ reported orders of help resource usage form a progression of clusters where resources in each cluster are more similar to each other by help resource dimensions than to resources outside of their cluster. This progression partially confirms phenomena and decision factors reported by existing literature, but no factor/dimension alone can explain the entire progression. We found students’ actual help-seeking records did not deviate much from their self-reported orders. Mining of the sequential records revealed that help-seeking from course-affiliated human resources led to measurable progress more often than not, and students’ usage of consulting/office hours (mainly run by undergraduate teaching assistants) itself was the best indicator for future usage within the lifespan of the same assignment.Implications. Our results demonstrate that computing students’ help resource selection/utilization is a sophisticated process that should be modeled and analyzed with sufficient awareness of its inherent sequentiality. We identify future research directions through this preliminary analysis, which can lead to a better understanding of computing students’ help-seeking behavior and better resource utilization/management in large-scale instructional contexts.},
journal = {ACM Trans. Comput. Educ.},
month = apr,
articleno = {7},
numpages = {34},
keywords = {Computing Education, Help-Seeking}
}

@article{10.1145/3717402.3717403,
author = {Dorodchi, Mohsen and Gal-Ezer, Judith and Cooper, Stephen},
title = {Introducing SIGCSE Virtual},
year = {2025},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0097-8418},
url = {https://doi.org/10.1145/3717402.3717403},
doi = {10.1145/3717402.3717403},
abstract = {SIGCSE Virtual 2024, the First ACM Virtual Global Computing Education Conference, will be held fully online from December 5 to 8, 2024. The conference has different themes based on the global aspects of CS education, while considering regional circumstances, and the sessions are offered considering time-zone constraints.},
journal = {SIGCSE Bull.},
month = feb,
pages = {2–4},
numpages = {3}
}

@inproceedings{10.1145/3631802.3631818,
author = {Linhuber, Matthias and Bernius, Jan Philip and Krusche, Stephan},
title = {Constructive Alignment in Modern Computing Education: An Open-Source Computer-Based Examination System},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631818},
doi = {10.1145/3631802.3631818},
abstract = {Large-scale paper-based examinations (PBEs) in computing education frequently emphasize rote memorization, thereby misaligning instructional objectives with assessment techniques. Such incongruities hinder the preparation of students for real world challenges in both industry and academia by inadequately evaluating higher-order cognitive abilities. Often, educators are deterred from implementing comprehensive skills assessment due to the perceived complexity and resource-intensive grading processes involved. To mitigate these limitations, this paper introduces an exam mode as an integral feature of the open-source learning platform Artemis. Designed for both local and cloud-based deployment, this exam mode incorporates anti-cheating protocols, automates the grading of diverse exercise types, and features double-blind manual grading to ensure assessment integrity. It fosters the evaluation of complex cognitive skills while substantially reducing the administrative load on faculty. This paper substantiates the effectiveness of the Artemis&nbsp; exam mode through widespread institutional adoption, demonstrated by over 50 successful computer-based examinations (CBEs). An in-depth case study involving 1,700 undergraduate software engineering students offers key insights, best practices, and lessons learned. This research not only pioneers the documentation of a secure, scalable, and reliable exam system at an institutional scale but also marks a seminal contribution to modernizing assessment strategies in computing education, with a particular focus on constructive alignment.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {7},
numpages = {11},
keywords = {Automated Assessment, Automated Grading, Continuous Integration, Exam Mode, Instant Feedback, Online Editor, Open-Source Learning Platform, Plagiarism Checks, Programming Exercises, Scalability., Version Control},
location = {Koli, Finland},
series = {Koli Calling '23}
}

@proceedings{10.1145/3651890,
title = {ACM SIGCOMM '24: Proceedings of the ACM SIGCOMM 2024 Conference},
year = {2024},
isbn = {9798400706141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@proceedings{10.1145/3633053,
title = {CEP '24: Proceedings of the 8th Conference on Computing Education Practice},
year = {2024},
isbn = {9798400709326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Durham, United Kingdom}
}

@inbook{10.1145/3718491.3718558,
author = {Pan, Shengwu and Zeng, Qi and Li, Mingjiang and Yan, Na},
title = {Global and Domestic Perspectives on AI in Education: A Knowledge Mapping Analysis Using CiteSpace},
year = {2025},
isbn = {9798400710865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3718491.3718558},
abstract = {This study utilized CiteSpace to perform an extensive knowledge mapping analysis of 826 Chinese articles and 488 international publications on artificial intelligence in education, obtained from the China National Knowledge Infrastructure (CNKI) and Web of Science databases, spanning the years 2014 to 2024. The research employed visualization methods, such as co-citation analysis and network algorithms, to comprehensively assess research hotspots, key topics, and emerging trends in the field, both domestically and internationally. Research indicates that overseas studies prioritize the incorporation of AI technology in education and the development of theoretical frameworks, whereas domestic research focuses on the practical use of AI in curriculum development and educational administration. Despite these divergent perspectives, both international and domestic research entities share a mutual interest in the use and advancement of AI in education, especially in relation to personalized learning and educational equity. To promote AI in global education, Future research should prioritize AI-driven digital transformation and standardization in education, Interdisciplinary collaboration and leveraging big data analytics will enhance innovation and decision-making, Addressing ethical issues, privacy, and societal acceptance is crucial for equitable and sustainable educational development.},
booktitle = {Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum},
pages = {404–415},
numpages = {12}
}

@article{10.1145/3717825,
author = {Kugler, Logan},
title = {Computer Science Under Trump},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0001-0782},
url = {https://doi.org/10.1145/3717825},
doi = {10.1145/3717825},
abstract = {Experts say that computer science could look a lot different under the new administration.},
note = {Online First},
journal = {Commun. ACM},
month = feb,
numpages = {4}
}

@proceedings{10.1145/3677619,
title = {WiPSCE '24: Proceedings of the 19th WiPSCE Conference on Primary and Secondary Computing Education Research},
year = {2024},
isbn = {9798400710056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Munich, Germany}
}

@proceedings{10.1145/3610969,
title = {UKICER '23: Proceedings of the 2023 Conference on United Kingdom &amp; Ireland Computing Education Research},
year = {2023},
isbn = {9798400708763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Swansea, Wales Uk}
}

@inproceedings{10.1145/3641554.3701924,
author = {Blumenthal, Richard and Blumenthal, Johanna},
title = {Moving What's in the CS Curriculum Forward: A Proposition to Address Ten Wicked Curricular Issues},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701924},
doi = {10.1145/3641554.3701924},
abstract = {Every decade, a Steering Committee convenes tasked with creating new computer science curricular recommendations. As they work, they are faced with a plethora of wicked curricular issues and a short timeline to resolve these issues. A systemic literature review of the ten years preceding CS2023 reveals a small number of publications focused on curricular issues. Intentional discussion among the education community is ideal for addressing these wicked issues. Towards this end, ten unsettled representative wicked curricular issues faced by the CS2023 Steering Committee are introduce with the aim of motivating the community to engage in increased and intentional formal discussions regarding these issues. Preserving these discussions will improve the next curriculum. A framework for structuring such discussions is also offered.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {137–143},
numpages = {7},
keywords = {computer science, cs2023, curricular issues, curriculum},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.5555/3729849.3729850,
author = {Kerney, William},
title = {Treachery and Deceit: Detecting and Dissuading AI Cheating},
year = {2025},
issue_date = {April 2025},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {9},
issn = {1937-4771},
abstract = {Last semester, 75% of the author's data structures students were caught cheating at least once, with Generative AI technologies being the most common means by which they cheated. While it may be tempting to move back to in-person pen-and-paper evaluations to ensure students have retained material, the author has found is possible to detect and discourage the use of cheating via various tricky methods. Finally, the author looks at attempts by students to conceal their use of AI in cheating, and how successful off the shelf AI detection tools are at finding the use of AI in coding assignments before and after being rewritten by hand.},
journal = {J. Comput. Sci. Coll.},
month = apr,
pages = {10–17},
numpages = {8}
}

@inproceedings{10.1145/3641399.3641442,
author = {Agarwal, Nimisha},
title = {Finding and Investigating Buggy Codes to Make CS1 Learning Efficient},
year = {2024},
isbn = {9798400717673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641399.3641442},
doi = {10.1145/3641399.3641442},
abstract = {In an introductory programming course, students make several logical errors and struggle to fix those errors. This PhD research focuses on creating tools and methods that can aid students in the learning process and help them to find such errors more efficiently.},
booktitle = {Proceedings of the 17th Innovations in Software Engineering Conference},
articleno = {24},
numpages = {3},
keywords = {CS1, Refute questions, automated feedback, error localization, targeted test cases, test case generation},
location = {Bangalore, India},
series = {ISEC '24}
}

@proceedings{10.1145/3653666,
title = {RESPECT 2024: Proceedings of the 2024 on RESPECT Annual Conference},
year = {2024},
isbn = {9798400706264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to RESPECT 2024! The Conference on Research in Equity and Sustained Participation in Engineering, Computing, and Technology is the premier venue for research on equity, inclusion, and justice in computing and computing education. This volume contains the papers presented at RESPECT 2024, the ninth edition of this conference, held on May 16-17, 2024 at the Georgia Tech Conference Center and Hotel in Atlanta, Georgia.As researchers, especially those of us focused on equity, freedom, and justice, our job is to give language to and make meaning of the joy, trauma, and unwavering spirit of the most vulnerable and marginalized among us. We don't do this simply to shed light but to influence change. We stand when others are forced to sit and speak when others are silenced. For many of us, the past several years have rendered our usual tools ineffective, and often we find ourselves seated, scared to stand, and essentially silenced as we search for new language to describe old problems. We increasingly, and rightfully, feel frustrate... powerles... angry. Unfortunately, we sometimes allow these feelings to lead us to inaction. However, as the Black feminist scholar Audre Lorde wrote in her masterpiece, Sister Outsider, anger, directed in productive ways, can be transformative.},
location = {Atlanta, GA, USA}
}

@inproceedings{10.1145/3641554.3701799,
author = {Koitz-Hristov, Roxane},
title = {Peer Code Review Methods: An Experience Report from a Data Structures and Algorithms Course},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701799},
doi = {10.1145/3641554.3701799},
abstract = {Peer code review is a key practice in professional software development, and its integration into computer science education can provide valuable learning experiences for students. However, few reports compare different peer code review methods within a single educational context. This experience report shares insights from implementing various review types-individual, team, and pair code reviews-in a first-year Data Structures and Algorithms course in a bachelor's degree program. Throughout the semester, students took an active role in their learning by completing three programming assignments, each followed by a different peer review method. Feedback was collected through questionnaires to capture the students' perceptions of their data structure knowledge, programming skills, and overall learning experience. Our report outlines the design of the different review learning activities, provides insights into the students' opinions on the review techniques, and reflects on the challenges and successes we encountered. As each method offers unique benefits, we believe that incorporating a variety of peer code review methods can enhance the overall learning experience in computer science courses.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {610–616},
numpages = {7},
keywords = {code review, computer science education, data structures and algorithms, higher education, peer code review, peer review},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3689535.3689543,
author = {Addo, Salomey Afua and Sentance, Sue},
title = {Exploring Computing Teachers' Readiness to Teach AI in Secondary Schools},
year = {2024},
isbn = {9798400711770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689535.3689543},
doi = {10.1145/3689535.3689543},
abstract = {Artificial intelligence (AI) is significantly impacting how we live, and the increased capabilities of generative AI applications have positioned AI firmly in the public domain. There is a growing interest in what AI might look like as a subject within the K-12 curriculum, whilst research on teachers’ readiness for teaching AI is as yet limited. This paper describes a qualitative study investigating teachers’ readiness to teach AI in secondary education. The interview study involved eight computing teachers with varying teaching experiences. We used reflexive thematic analysis for themes development. Findings suggest several indicators of teachers’ readiness, including attitudes, prior AI experience, professional development, and access to quality resources. This paper contributes to ongoing debates about how to best support teachers to be ready to teach AI effectively at the school level.},
booktitle = {Proceedings of the 2024 Conference on United Kingdom &amp; Ireland Computing Education Research},
articleno = {17},
numpages = {1},
keywords = {K-12 education, artificial intelligence, computing education, teacher readiness},
location = {Manchester, United Kingdom},
series = {UKICER '24}
}

@inproceedings{10.1145/3712464.3712518,
author = {Li, Kunyan and Yu, Shuxuan and Wang, Shu and Li, Jiayi and Li, Rushuang and Wei, Jinwu},
title = {An AI-Driven Training System Architecture for Specialized Operations},
year = {2025},
isbn = {9798400710636},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3712464.3712518},
doi = {10.1145/3712464.3712518},
abstract = {This paper introduces an AI-driven training system architecture tailored for specialized operations within high-complexity industries. Leveraging the advancements in artificial intelligence, particularly large models, the system offers a structured, four-layer architecture encompassing Device, Data, Model, and Application Layers. These layers integrate advanced AI capabilities to simulate real-world environments, deliver personalized training, and optimize performance in critical operations. Experimental results highlight the system's ability to reduce operator response times, improve success rates, and enhance operational safety and efficiency. This research presents a transformative approach to training personnel, aiming to elevate the safety and effectiveness of industrial operations.},
booktitle = {Proceedings of the 2024 4th International Conference on Signal Processing and Communication Technology},
pages = {310–315},
numpages = {6},
keywords = {AI-Driven Training, Highly complex industries, Specialized operations},
location = {
},
series = {SPCT '24}
}

@proceedings{10.1145/3593342,
title = {WCCCE '23: Proceedings of the 25th Western Canadian Conference on Computing Education},
year = {2023},
isbn = {9798400707896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Vancouver, BC, Canada}
}

@inproceedings{10.1145/3617650.3624948,
author = {Kalluri, Balaji and Prasad, Prajish and Sharma, Prakrati},
title = {Making CS Education Relevant to the 21st Century: Blending Critical Thinking into an Introductory Programming Course},
year = {2023},
isbn = {9798400703744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617650.3624948},
doi = {10.1145/3617650.3624948},
abstract = {In this poster, we describe a theoretical model aimed at developing future human computational thinking in students. We describe a blended pedagogy drawing approaches from multiple disciplines such as social sciences, design, ethics, and computing to develop a new undergraduate introductory programming course. We describe how we introduced this pedagogy in an introductory programming course, taught in a university in India and summarise our preliminary findings.},
booktitle = {Proceedings of the ACM Conference on Global Computing Education Vol 2},
pages = {194},
numpages = {1},
keywords = {21st-century skills, CS1, Python programming, critical thinking},
location = {Hyderabad, India},
series = {CompEd 2023}
}

@proceedings{10.1145/3669947,
title = {ICEDS '24: Proceedings of the 2024 5th International Conference on Education Development and Studies},
year = {2024},
isbn = {9798400718083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cambridge, United Kingdom}
}

@article{10.1145/3688091,
author = {Seehorn, Ellie and Herskovitz, Jaylin},
title = {VISIONS of Accessibility: Human-AI Lab (HAIL), University of Michigan},
year = {2024},
issue_date = {Fall 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/3688091},
doi = {10.1145/3688091},
journal = {XRDS},
month = oct,
pages = {60–61},
numpages = {2}
}

@article{10.1145/3717512.3717513,
author = {Uhlig, Steve},
title = {The October 2024 Issue},
year = {2025},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/3717512.3717513},
doi = {10.1145/3717512.3717513},
abstract = {This October 2024 issue contains three technical papers, one of which is of a rather educational nature and can be considered both an educational contribution and a technical paper.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = feb,
pages = {1},
numpages = {1}
}

@inproceedings{10.1145/3641554.3701912,
author = {Li, Miranda and Malik, Ali and Piech, Chris},
title = {Fostering and Understanding Diverse Interpersonal Connections in a Massive Online CS1 Course},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701912},
doi = {10.1145/3641554.3701912},
abstract = {Forming social relationships is critical to student success and well-being, but is one of the first aspects to be neglected in the design of massive online courses. We present our experience deploying an in-course networking tool that enabled 1,600+ learners and teachers in a massive online CS1 course to form 2,000+ connections with other individuals. We discuss how social preferences and networking goals vary by demographics, economic factors, course goals, and course role. Contrary to usual online social behavior, users in our network sent more out-group requests than a random baseline by role (2.04x), gender (1.1x), and developing vs. developed country (1.07x). We highlight differences between developing vs. developed country users: developing country users send 2.5x requests and make, on average, 1.78x as many connections as those from developed countries. From a randomized control trial we find that random recommendations increase the volume of sent requests by 44.48% and promote cross-group requests across developing vs. developed countries (+28.9%), age (+15.1%), and gender (+8.6%). Ultimately we show that integrating socialization as a core feature of online CS1 classrooms can help support people from all backgrounds in achieving their diverse educational goals, which often extend well beyond improving coding proficiency.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {666–672},
numpages = {7},
keywords = {mooc, online learning, social network analysis, social presence},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3702163.3702465,
author = {Kim, Junwhan},
title = {On Transdisciplinary Research through Data Science and Engineering Education},
year = {2025},
isbn = {9798400717819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702163.3702465},
doi = {10.1145/3702163.3702465},
abstract = {This paper explores the role of Data Science and Engineering (DSE) education in fostering transdisciplinary research and innovation. By combining data science’s analytical capabilities with the infrastructure-building focus of data engineering, DSE offers powerful tools for addressing complex challenges across diverse fields such as finance, agriculture, law, and engineering. However, researchers outside traditional DSE domains often lack the expertise to manage and leverage data effectively. To bridge this gap, we have developed four courses—data science, data engineering, advanced DSE, and vision AI—at the University of the District of Columbia, designed to equip students from various disciplines with the necessary skills to apply DSE techniques in their respective fields. This paper highlights the research conducted by students in these courses, emphasizing the importance of transdisciplinary collaboration in advancing scientific discovery and technological innovation.},
booktitle = {Proceedings of the 2024 16th International Conference on Education Technology and Computers},
pages = {523–528},
numpages = {6},
keywords = {Transdisciplinary Research, Data Science and Engineering Education},
location = {
},
series = {ICETC '24}
}

@inproceedings{10.1145/3632620.3671112,
author = {Skripchuk, James and Bacher, John and Price, Thomas},
title = {An Investigation of the Drivers of Novice Programmers' Intentions to Use Web Search and GenAI},
year = {2024},
isbn = {9798400704758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632620.3671112},
doi = {10.1145/3632620.3671112},
abstract = {External help resources are frequently used by novice programmers solving classwork in undergraduate computing courses. Traditionally, these tools consisted of web resources such as tutorial websites and Q&amp;A forums. With the rise of Generative AI (GenAI), there has been increasing concern and research about how external resources should be used in the classroom. However, little work has directly contrasted student beliefs and perceptions of web resources with GenAI, has grounded these beliefs in prior psychological theory, and has investigated how demographic factors and student backgrounds influence these beliefs and intentions. We administered a vignette-style survey across two courses required for a CS major at an R1 University, a freshman (n = 152) and senior capstone course (n = 44). Students responded to likert questions aiming to measure behavioral factors related to these tools, such as intention to use, perceived attitudes, peer perceptions, and their own perceived tool competency. We primarily investigate the results of an introductory course, finding that novices have a wide range of opinions on both resources, but overall find them slightly useful and have a tendency to prefer web-search. We compare this with seniors, who have more positive perceptions of these tools, and discuss possible reasons and implications for this difference. We constructed two path models to investigate which factors strongly influence novices’ intention to use resources and find the primary factor to be their general attitudes in how these tools will result in a positive or negative outcome (e.g. perceived benefits, justifiability). We also measure the effects of student background on intention to use these resources. Finally, we discuss implications and suggestions on how instructors can use this information to approach, address, and influence resource usage in their classrooms.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1},
pages = {487–501},
numpages = {15},
keywords = {CS Education, GenAI, Help-seeking, student perspectives, web-search},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@inproceedings{10.1145/3587102.3588794,
author = {Ouh, Eng Lieh and Gan, Benjamin Kok Siew and Jin Shim, Kyong and Wlodkowski, Swavek},
title = {ChatGPT, Can You Generate Solutions for my Coding Exercises? An Evaluation on its Effectiveness in an undergraduate Java Programming Course.},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588794},
doi = {10.1145/3587102.3588794},
abstract = {In this study, we assess the efficacy of employing the ChatGPT language model to generate solutions for coding exercises within an undergraduate Java programming course. ChatGPT, a large-scale, deep learning-driven natural language processing model, is capable of producing programming code based on textual input. Our evaluation involves analyzing ChatGPT-generated solutions for 80 diverse programming exercises and comparing them to the correct solutions. Our findings indicate that ChatGPT accurately generates Java programming solutions, which are characterized by high readability and well-structured organization. Additionally, the model can produce alternative, memory-efficient solutions. However, as a natural language processing model, ChatGPT struggles with coding exercises containing non-textual descriptions or class files, leading to invalid solutions. In conclusion, ChatGPT holds potential as a valuable tool for students seeking to overcome programming challenges and explore alternative approaches to solving coding problems. By understanding its limitations, educators can design coding exercises that minimize the potential for misuse as a cheating aid while maintaining their validity as assessment tools.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {54–60},
numpages = {7},
keywords = {Java, computer science education, object-oriented, programming},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3649217.3653573,
author = {Buffardi, Kevin and Brooks, JoAna and Alexander, David},
title = {Designing a CURE for CS1},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653573},
doi = {10.1145/3649217.3653573},
abstract = {Course-based Undergraduate Research Experience (CURE) is a pedagogy for engaging an entire class in the scientific exploration of real research problems with unknown solutions that have impact beyond the classroom. Since CUREs originated in biological sciences, there are unique challenges to adapting the CURE model for computer science. We designed a CURE by aligning its principles with entrepreneurial mindset (EM) and software development practices by applying them in a programming project. This experience report introduces the design and adoption of a CURE project for an introductory programming class (CS1).The CURE project aims to innovate color selection for digital visualizations by facilitating easier interpretation and improved accessibility for people with color vision deficiencies. We designed the "color vision project" to engage students in developing novel algorithms for issues in color accessibility, which replaced traditional, auto-graded programming projects. The project includes multiple deliverables for students to demonstrate authentic software development practices while incrementally adapting to new discoveries.Over three semesters, we gathered surveys (n=62) to gauge students' connections with scientific practices and entrepreneurial mindsets. Survey responses indicate that students identified with both CURE and EM. We also offer reflections on continuing to improve CURE implementations in computer science and give recommendations on how to mitigate the misuse of artificial intelligence in CURE projects.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {660–666},
numpages = {7},
keywords = {accessibility, color blindness, cs1, cure, entrepreneurial mindset, entrepreneurship, image manipulation},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3630106.3659045,
author = {Engelmann, Severin and Choksi, Madiha Zahrah and Wang, Angelina and Fiesler, Casey},
title = {Visions of a Discipline: Analyzing Introductory AI Courses on YouTube},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3659045},
doi = {10.1145/3630106.3659045},
abstract = {Education plays an indispensable role in fostering societal well-being and is widely regarded as one of the most influential factors in shaping the future of generations to come. As artificial intelligence (AI) becomes more deeply integrated into our daily lives and the workforce, educational institutions at all levels are directing their focus on resources that cater to AI education. Yet, informal education, including online learning on social media platforms like YouTube, plays an increasingly significant role for both students and the general public. Offering greater accessibility compared to formal education, millions of individuals use YouTube for educational resources on AI today. Due to the substantial societal impact of AI, it is crucial for introductory AI courses to meaningfully address the ethical implications associated with AI. Our work investigates the current landscape of introductory AI courses on YouTube, and the potential for introducing ethics in this context. We qualitatively analyze the 20 most watched introductory AI courses on YouTube, coding a total of 92.2 hours of educational content viewed by close to 50 million people. We find that these introductory AI courses do not meaningfully engage with ethical or societal challenges of AI (RQ1). When defining and framing AI, introductory AI courses foreground excitement around AI’s transformative role in society, over-exaggerate AI’s current and future abilities, and anthropomorphize AI (RQ2). In teaching AI, we see a widespread reliance on corporate AI tools and frameworks as well as a prioritization on a hands-on approach to learning rather than on conceptual foundations (RQ3). In promoting key AI practices, introductory AI courses abstract away entirely the socio-technical nature of AI classification and prediction, for example by favoring data quantity over data quality (RQ4). Given the power of openly available introductory courses to shape enduring beliefs around AI and its field at the onset of a learning journey, we extend our analysis with recommendations that aim to integrate ethical reflections into introductory AI courses. We recommend that introductory AI courses should (1) highlight ethical challenges of AI to present a more balanced perspective, (2) raise ethical issues explicitly relevant to the technical concepts discussed and (3) nurture a sense of accountability in future AI developers.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2400–2420},
numpages = {21},
keywords = {Computer science education, accountability in computer science, artificial intelligence ethics, ethics in computer science education},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@article{10.1145/3703162,
author = {Connolly, Randy},
title = {Public Computing Intellectuals in the Age of AI Crisis},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
url = {https://doi.org/10.1145/3703162},
doi = {10.1145/3703162},
abstract = {The belief that AI technology is on the cusp of causing a generalized social crisis became a popular one in 2023. While there was no doubt an element of hype and exaggeration to some of these accounts, they do reflect the fact that there are troubling ramifications to this technology stack. This conjunction of shared concerns about social, political, and personal futures presaged by current developments in AI presents the academic discipline of computing with a renewed opportunity for self-examination and reconfiguration. This position article endeavors to do so in four sections. The first section explores what is at stake for computing in the narrative of an AI crisis. The second section articulates possible educational responses to this crisis and advocates for a broader analytic focus on power relations. The third section presents a novel characterization of academic computing’s field of practice, one which includes not only the discipline’s usual instrumental forms of practice but reflexive practice as well. This reflexive dimension integrates both the critical and public functions of the discipline as equal intellectual partners and a necessary component of any contemporary academic field. The final section will advocate for a conceptual archetype—the Public Computer Intellectual and its less conspicuous but still essential cousin, the Almost-Public Computer Intellectual—as a way of practically imagining the expanded possibilities of academic practice in our discipline, one that provides both self-critique and an outward-facing orientation toward the public good. It will argue that the computer education research community can play a vital role in this regard. Recommendations for pedagogical change within computing to develop more reflexive capabilities are also provided.},
journal = {ACM Trans. Comput. Educ.},
month = dec,
articleno = {53},
numpages = {26},
keywords = {AI, ethics, social issues, crisis, intellectuals, critique, public good, critical theory, social theory}
}

@article{10.1145/3643646,
author = {Pias, Marcelo and Cuadros-Vargas, Ernesto and Duran, Rodrigo},
title = {Computer Science Education in Latin America and the Caribbean},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {2153-2184},
url = {https://doi.org/10.1145/3643646},
doi = {10.1145/3643646},
journal = {ACM Inroads},
month = feb,
pages = {38–47},
numpages = {10}
}

@article{10.1145/3585060.3585062,
author = {Trim, Michelle},
title = {Faking it and Breaking it: Responsible AI is needed Now},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {3},
issn = {0095-2737},
url = {https://doi.org/10.1145/3585060.3585062},
doi = {10.1145/3585060.3585062},
abstract = {Frauds, fakes, forgeries… these are not new problems for the public to contend with. Purveyors of designer goods such as handbags and shoes have developed keen skills to spot clever knockoffs, though some copies are so good that their authenticity seems less important to folks buying them to wear. Forging currency is an incredibly serious crime, and yet it is the crux of many movies and stories. In antiquities and in art, making forgeries gets a different sort of respect as doing so effectively requires great skill akin to that practiced by the original artisans themselves. In academia, as well as politics [8], plagiarism is one of the worst offenses one can commit because faking one's intellectual achievements [7] threatens the attainment of credentials and offices while getting away with plagiarism can threaten the institution's reputation. So, with so much practice living life negotiating the real from the fake, why are the latest advancements in machine-based image and text generation so concerning? One reason might have to do with computing's penchant for outsourcing messy, qualitative evaluation methods to external, seemingly neutral metrics. Like all sciences, computing must constantly grapple with the myth of objectivity [15].},
journal = {SIGCAS Comput. Soc.},
month = feb,
pages = {7–9},
numpages = {3}
}

@inproceedings{10.1145/3700297.3700315,
author = {Qu, Ting and Yang, Zuguo},
title = {Overview of Artificial Intelligence Applications in Educational Research},
year = {2024},
isbn = {9798400707100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3700297.3700315},
doi = {10.1145/3700297.3700315},
abstract = {As artificial intelligence technology evolves, the big AI model will spark a change in the education sector. Revealing the trends in the artificial intelligence application in educational research is significant for understanding the interdisciplinary research frontiers of artificial intelligence and education. This study collected data from the InCites and Web of Science through forward citations and presented new experimental results using scientometrics and visualization tools such as VOSviewer, CiteSpace, Pajek, and Scimago Graphica. It has been found that applying AI in the domain of education is currently undergoing rapid development; research power mainly comes from China, the United States, Spain, the United Kingdom and Australia. The main research hotspots include machine learning, higher education, learning analytics, and educational data mining. The important foundational and frontier literature in this field can be summarized into four themes: academic prediction and early dropout prevention, learning analytics and data mining, incremental learning, and learning performance. Both generative artificial intelligence and explainable artificial intelligence (XAI) are recent developments in the realm of education research.},
booktitle = {Proceedings of the 2024 International Symposium on Artificial Intelligence for Education},
pages = {101–108},
numpages = {8},
keywords = {artificial intelligence, bibliometrics, data analytics, education, research frontiers},
location = {
},
series = {ISAIE '24}
}

@inproceedings{10.1145/3706468.3706528,
author = {Fang, Yu and Huang, Shihong and Ogan, Amy},
title = {A Cross-Cultural Confusion Model for Detecting and Evaluating Students’ Confusion In a Large Classroom},
year = {2025},
isbn = {9798400707018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706468.3706528},
doi = {10.1145/3706468.3706528},
abstract = {In traditional lecture delivery setting, it is very challenging to identify which part of the lecture material that students are struggling with. One approach to identify difficult concepts is to capture students’ confusion during class time. However, most existing confusion detectors focus on an individual student rather than a classroom, and only on a single ethnicity group which could propagate bias when developing pedagogical technologies. In this paper, we leverage two existing ‘Confused’ facial expression datasets (DAiSEE and DevEmo) with an East Asian ‘Confused’ facial expression dataset that we collected. Through model performance and explainableAI, we address potential cultural biases in detecting emotions, particularly in confusion, and identified culturally-specific features that align with prior research. As a proof-of-concept, we deployed this cross-cultural confusion machine learning model in a live semester-long class. This work to integrate cross-cultural facial features highlights the importance of fostering inclusivity in educational technologies.},
booktitle = {Proceedings of the 15th International Learning Analytics and Knowledge Conference},
pages = {473–483},
numpages = {11},
keywords = {Cross-cultural models, Confusion, Affective computing, Retrieval-augmented generation},
location = {
},
series = {LAK '25}
}

@inproceedings{10.1145/3706598.3713254,
author = {Cao, Huajie Jay and Lee, Hee Rin and Peng, Wei},
title = {Empowering Adults with AI Literacy: Using Short Videos to Transform Understanding and Harness Fear for Critical Thinking},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713254},
doi = {10.1145/3706598.3713254},
abstract = {Despite the importance of AI literacy for both children and adults, adults have been understudied. We developed short videos for adults that provided training on the basics of AI understanding, use, and evaluation. In an online experiment, 94 adults aged 30-49 were randomly assigned in a 1:2 ratio to view either short videos on AI history (control group) or AI literacy training videos (treatment group). The results showed that the intervention significantly improved people’s self-efficacy of AI use but not in AI understanding or evaluation. Interestingly, participants’ fears of AI bias, privacy violations, and job replacement increased after the training, although they remained below the midpoints. We argue that the heightened fear in the treatment group reflects a foundation for critical thinking skills, as it moves them closer to a more calibrated, moderate level of fear. Therefore, this study uniquely contributes by utilizing short-form experiential content to both educate and foster a more informed, critical interaction with AI technologies. The implications of designing AI literacy educational materials for adults were discussed.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {203},
numpages = {8},
keywords = {AI literacy, Education, Adult, AI fear, self-efficacy},
location = {
},
series = {CHI '25}
}

@proceedings{10.1145/3678726,
title = {ICEMT '24: Proceedings of the 2024 8th International Conference on Education and Multimedia Technology},
year = {2024},
isbn = {9798400717611},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Tokyo, Japan}
}

@inproceedings{10.1145/3587102.3588822,
author = {Messer, Marcus and Brown, Neil C. C. and K\"{o}lling, Michael and Shi, Miaojing},
title = {Machine Learning-Based Automated Grading and Feedback Tools for Programming: A Meta-Analysis},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588822},
doi = {10.1145/3587102.3588822},
abstract = {Research into automated grading has increased as Computer Science courses grow. Dynamic and static approaches are typically used to implement these graders, the most common implementation being unit testing to grade correctness. This paper expands upon an ongoing systematic literature review to provide an in-depth analysis of how machine learning (ML) has been used to grade and give feedback on programming assignments. We conducted a backward snowball search using the ML papers from an ongoing systematic review and selected 27 papers that met our inclusion criteria. After selecting our papers, we analysed the skills graded, the preprocessing steps, the ML implementation, and the models' evaluations.We find that most the models are implemented using neural network-based approaches, with most implementing some form of recurrent neural network (RNN), including Long Short-Term Memory, and encoder/decoder with attention mechanisms. Some graders implement traditional ML approaches, typically focused on clustering. Most ML-based automated grading, not many use ML to evaluate maintainability, readability, and documentation, but focus on grading correctness, a problem that dynamic and static analysis techniques, such as unit testing, rule-based program repair, and comparison to models or approved solutions, have mostly resolved. However, some ML-based tools, including those for assessing graphical output, have evaluated the correctness of assignments that conventional implementations cannot.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {491–497},
numpages = {7},
keywords = {automated grading, computer science education, machine learning, meta-analysis},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@proceedings{10.1145/3610538,
title = {SA '23: SIGGRAPH Asia 2023 Courses},
year = {2023},
isbn = {9798400703096},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1145/3632620.3671099,
author = {Ko, Shao-Heng and Stephens-Martinez, Kristin},
title = {The Trees in the Forest: Characterizing Computing Students' Individual Help-Seeking Approaches},
year = {2024},
isbn = {9798400704758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632620.3671099},
doi = {10.1145/3632620.3671099},
abstract = {Background and Context. Academic help-seeking is vital to post-secondary computing students’ effective learning. However, most empirical works in this domain study students’ help resource selection and utilization by aggregating the entire student body as a whole. Moreover, existing theoretical frameworks often implicitly assume that whether/how much a student seeks help from a specific resource only depends on context (the type of help needed and the properties of the resources), not the individual student. Objectives. To address the gap, we seek to investigate individual computing students’ help-seeking approaches by analyzing what help-seeking characteristics are individual-driven (and thus stay consistent for the same student across different course contexts) and what are context-driven. Method. We analyzed N = 597 students’ survey responses on their help resource utilization as well as their actual help-seeking records across 6 courses. We examined relations between individual students’ frequency-based help usage metrics, type-of-help requested in office/consulting hours, self-reported order of ideal help resource usage, and their collaboration inclination in small-scale sections. Findings. We found that students’ frequency-based help metrics and their order of ideal help resource usage stays relatively consistent across different course contexts, and thus may be treated as part of students’ individual help-seeking approaches. On the other hand, the type of help students seek in office/consulting hours and how much they collaborate with peers in small sections do not seem to stay consistent across different contexts and thus might be deemed more context-driven than individual-driven. Implications. Our findings reveal that part of students’ help-seeking characteristics is individual-driven. This opens up a possibility for institutions to track students’ help-seeking records in early/introductory courses, so that some preliminary understanding of students can be acquired before they enter downstream courses. Our insights may also help instructors identify which part of students’ help-seeking behavior are more likely to be influenced by their course context and design.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1},
pages = {343–358},
numpages = {16},
keywords = {Computing Education, Help-Seeking},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@inproceedings{10.1145/3649165.3690119,
author = {Kumar, Amruth N.},
title = {Auglets: Intelligent Tutors for Learning Good Coding Practices by Solving Refactoring Problems},
year = {2024},
isbn = {9798400705984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649165.3690119},
doi = {10.1145/3649165.3690119},
abstract = {Code quality is of universal concern among educators. Refactoring code, i.e., revising the structure of a program without changing its behavior is one approach for improving code quality. Numerous software tools have been created to help students refactor the code they write. Only a few software tutors have been reported in literature that help students proactively learn code quality by solving refactoring problems. But they suffer false positive and false negative grading issues because they allow freehand coding. We investigated whether refactoring tutors that do not allow freehand coding could be used to help students learn about non-trivial anti-patterns. We developed and deployed two software tutors for refactoring problems that are based on the principle of "refactoring without rewriting code", and cover a subset of refactoring problems that can be solved using only deletion, duplication, reordering and token-wise editing of lines of code. We investigated whether students needed to learn the anti-patterns covered by the tutors and whether they benefited from using the tutors. In this experience report, we start by describing the tutors - the list of refactoring concepts covered, the user interface, grading, feedback and usage. We report our experience using the tutors over three semesters, which confirmed that both introductory and advanced students needed and benefited from using the tutors despite the limitations of the tutors' coverage. We reflect on what worked and what did not. The tutors currently cover C++, Java and C#. They are available for free for educational use on the web at auglets.org.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1},
pages = {95–101},
numpages = {7},
keywords = {anti-patterns, c++, code quality, java., problem-solving tutor, refactoring},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@inproceedings{10.1145/3641237.3691670,
author = {Eble, Michelle F. and McCullouch, S. B. and Amador, Steven and Blackmon, Codi Renee},
title = {Emerging Ethics: Teaching Communication Design in Professional Writing Courses},
year = {2024},
isbn = {9798400705199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641237.3691670},
doi = {10.1145/3641237.3691670},
abstract = {This experience report discusses the ways graduate student instructors (GSI) have engaged students in upper-level business and scientific writing courses at a large university by incorporating social justice approaches with an emphasis on the ethical uses of emerging technologies. Emerging technologies are often introduced into professional writing courses as powerful tools that can change and influence how common writing tasks and practices are taught. In this report, three doctoral students present specific examples of ethical dilemmas when teaching these courses to provide students with opportunities to incorporate and practice the use of emerging technologies within specific contexts. Each section of this report features one of the GSI's considerations and reflections on how emerging technologies can be incorporated into professional writing courses using various ethical frameworks in socially just ways. The report concludes with some general take-aways from the faculty member's role in privileging the experiences of the GSI's in the use of emerging technologies especially in terms of inclusive practices in the classroom.},
booktitle = {Proceedings of the 42nd ACM International Conference on Design of Communication},
pages = {199–205},
numpages = {7},
keywords = {communication design pedagogy, emerging communication technologies, ethics, inclusive practices, social justice},
location = {Fairfax, VA, USA},
series = {SIGDOC '24}
}

@inproceedings{10.1145/3708394.3708439,
author = {Xie, Yecheng and Lin, Weiyu and Yu, Xinyi},
title = {How Artificial Intelligence has influenced Students' Learning: A Survey Exploring Attitudes towards Tool Usage},
year = {2025},
isbn = {9798400710650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708394.3708439},
doi = {10.1145/3708394.3708439},
abstract = {Amid the convergence of traditional education and the rise of Artificial Intelligence (AI), the education sector faces a significant challenge. To comprehensively assess how AI has influenced students' learning, we apply a targeted questionnaire, gathering information about students' basic information, situation about internet usage, learning needs, and attitudes towards AI tools. Based on 4,606 valid responses, we construct a hierarchical evaluation model, and make correlation analysis accordingly. The result has shown that the influence of AI is general, particularly for students who are in lower-grade or majored in science and engineering fields, and the problem of lack of experience and professionalism is common. Objective conditions and motivation have played a crucial role. To make sensible use of AI in study, the educators may consider giving tips of transparent policy, offering personalized instructions, bettering the access of intelligent tools, and motivating students' self-directed learning.},
booktitle = {Proceeding of the 2024 International Conference on Artificial Intelligence and Future Education},
pages = {260–267},
numpages = {8},
keywords = {Artificial intelligence, comprehensive evaluation, education, students’ attitudes},
location = {
},
series = {AIFE '24}
}

@inproceedings{10.1145/3641237.3691669,
author = {Strubberg, Brandon C and Bennett, Kristin C and Nardone, Carroll Ferguson},
title = {Developing AI Literacy through Discussion and Practice: A Reflection on an AI Seminar},
year = {2024},
isbn = {9798400705199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641237.3691669},
doi = {10.1145/3641237.3691669},
abstract = {This experience report reflects on a longitudinal project aimed at determining ways that generative artificial intelligence (gen AI) can be leveraged as a pedagogical tool to assist in professionalizing students’ rhetorical understanding and use of the tool beyond the academy. Since ChatGPT launched, we have studied students’ engagement with gen AI tools to bridge academic and professional uses, believing that knowing when and how to deploy such tools can facilitate the gen AI literacy students need to have upon entering their professional careers. This report posits that students’ participatory design in the pedagogical structure is vital for creating localized practices that translate to tangible forms of AI literacies.},
booktitle = {Proceedings of the 42nd ACM International Conference on Design of Communication},
pages = {189–193},
numpages = {5},
keywords = {Generative artificial intelligence, digital literacies, technical communication, writing pedagogy},
location = {Fairfax, VA, USA},
series = {SIGDOC '24}
}

@proceedings{10.1145/3658619,
title = {EduCHI '24: Proceedings of the 6th Annual Symposium on HCI Education},
year = {2024},
isbn = {9798400716591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {New York, NY, USA}
}

@inproceedings{10.1145/3657604.3662043,
author = {Adkins, Keith and Joyner, David A.},
title = {Newly Created Assignments and The First Repository Effect on Inter-Semester Plagiarism},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3662043},
doi = {10.1145/3657604.3662043},
abstract = {The Internet---for all of its benefits---makes it easy for students to share assignments. This creates a serious problem for academic institutions. Common mitigation tactics include discouraging students from sharing their work and routinely checking for and removing solutions shared online. While these strategies can be successful in many cases, they are not always sufficient. In our experience, it can be a challenge if either students or hosting sites refuse to remove solutions. Pursuing legal options can be both time consuming and costly. One approach taken to combat this is to routinely create new coding assignments, but this can still require a significant time commitment. It is worth exploring if this effort is worthwhile.In this paper, we present an empirical study based on data that we collected over five semesters while addressing plagiarism within our large online computer science graduate program. We compare plagiarism rates between two courses: one integrating new assignments and the other continuing to reuse older assignments.In this study, we explore the benefits derived from introducing new assignments to counter plagiarism, and how long these benefits last. We then explore the trends that publicly shared solutions have on plagiarism rates, and what those trends tell us about the value of implementing new assignments. Lastly, we explore the effects that the process of detection and intervention have on the frequency of misconduct.We observed that the benefits gained by introducing new assignments faded quickly. Additionally, we observed that proactively seeking the removal of publicly shared solutions may be ineffective unless all solutions are removed. Lastly, we observed that early detection and notification to students results in reduced misconduct over time.Our observations underscore the notion that a single solution posted publicly can swiftly erode the advantages gained from creating new assignments to help reduce plagiarism. This raises questions about whether the advantages of introducing new assignments outweigh benefits gained through reusing and refining assignments over time. More mature and well-developed assignments tend to lend themselves to robust, experience-backed rubrics and dynamic autograders which deliver a pedagogical benefit that may outweigh the integrity benefits of frequently developing new assessments.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {211–220},
numpages = {10},
keywords = {assessment, misconduct, plagiarism detection},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@inproceedings{10.1145/3689535.3689538,
author = {Sentance, Sue and Watson, Steven and Addo, Salomey Afua and Shi, Shengpeng and Waite, Jane and Yu, Bo},
title = {Developing Computing Teacher Guidance on GenAI},
year = {2024},
isbn = {9798400711770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689535.3689538},
doi = {10.1145/3689535.3689538},
abstract = {Generative AI (GenAI) is becoming widely available for use in schools by teachers and students. While many educators appreciate the potential benefits of GenAI for enhancing learning, there are also significant concerns about authorship, authenticity, plagiarism, ethics, biases, and the broader implications of their use in education. For computing teachers in schools, these issues can be even more acute. In this project, we established a working group of practising computing teachers to bring together a range of views and experiences. Initial results of the project led to a booklet for computing teachers on how to use GenAI, illustrating the effectiveness of teacher-researcher partnerships in developing resources for school use. This project will be followed by further work on computing teachers’ actual experience of GenAI in practice.},
booktitle = {Proceedings of the 2024 Conference on United Kingdom &amp; Ireland Computing Education Research},
articleno = {12},
numpages = {1},
keywords = {AI education, K-12 education, generative AI, teachers},
location = {Manchester, United Kingdom},
series = {UKICER '24}
}

@proceedings{10.1145/3702212,
title = {CEP '25: Proceedings of the 9th Conference on Computing Education Practice},
year = {2025},
isbn = {9798400711725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3670653.3677507,
author = {Kubullek, Ann-Kathrin and Kuma\c{c}, Nadire and Dogang\"{u}n, Ayseg\"{u}l},
title = {Understanding the Adoption of ChatGPT in Higher Education: A Comparative Study with Insights from STEM and Business Students},
year = {2024},
isbn = {9798400709982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670653.3677507},
doi = {10.1145/3670653.3677507},
abstract = {Since ChatGPT’s introduction, generative artificial intelligence (AI) has significantly influenced the media, technological innovation, and educational discourse. Its increasing importance, especially in academia, necessitates a detailed examination of the impact of AI on higher education, particularly on how it changes teaching and learning processes. This study therefore looks at the factors affecting students’ attitudes towards AI technologies in the university setting, with a particular focus on the differences between business and STEM programmes. Using a mixed methods approach, the study combines surveys and interviews to collect data on students’ perceptions, attitudes and experiences with generative AI technology in academia. The data collected is analysed both quantitatively and qualitatively to reveal significant trends and insights into the adoption and use of generative AI tools in the university environment. The main objective of the study is to shed light on the determinants that determine the varying degrees of AI adoption in different academic disciplines. The findings have the potential to inform the implementation of educational technology and assist in the development of strategies for the effective integration of generative AI tools to meet the different needs and preferences of students in a range of academic contexts.},
booktitle = {Proceedings of Mensch Und Computer 2024},
pages = {684–689},
numpages = {6},
keywords = {ChatGPT, STEM degree programs, academic disciplines, acceptance of AI, business degree programs, generative AI adoption, higher education, students},
location = {Karlsruhe, Germany},
series = {MuC '24}
}

@article{10.1145/3686260,
author = {Sankaralingam, Karu},
title = {A Whimsical Odyssey Through the Maze of Scholarly Reviews},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {67},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/3686260},
doi = {10.1145/3686260},
abstract = {Navigating the often-bumpy road of academic review.},
journal = {Commun. ACM},
month = oct,
pages = {6–7},
numpages = {2}
}

@proceedings{10.1145/3661904,
title = {ICETT '24: Proceedings of the 2024 10th International Conference on Education and Training Technologies},
year = {2024},
isbn = {9798400717895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Macau, China}
}

@inproceedings{10.1145/3677619.3678117,
author = {Morales-Navarro, Luis and Kafai, Yasmin B},
title = {Unpacking Approaches to Learning and Teaching Machine Learning in K-12 Education: Transparency, Ethics, and Design Activities},
year = {2024},
isbn = {9798400710056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677619.3678117},
doi = {10.1145/3677619.3678117},
abstract = {In this conceptual paper, we review existing literature on artificial intelligence/machine learning (AI/ML) education to identify three approaches to how learning and teaching ML could be conceptualized. One of them, a data-driven approach, emphasizes providing young people with opportunities to create data sets, train, and test models. A second approach, learning algorithm-driven, prioritizes learning about learning algorithms. In addition, we identify efforts within a third approach that integrates the previous two. In our review, we focus on unpacking how the approaches: (1) glassbox and blackbox different aspects of ML, (2) build on learner interests and provide opportunities for designing applications, (3) integrate ethics and justice. In the discussion, we address the challenges and opportunities of current approaches and suggest future directions for the design of tools and learning activities.},
booktitle = {Proceedings of the 19th WiPSCE Conference on Primary and Secondary Computing Education Research},
articleno = {3},
numpages = {10},
keywords = {algorithmic justice, artificial intelligence, computing education, ethics, k-12, machine learning},
location = {Munich, Germany},
series = {WiPSCE '24}
}

@proceedings{10.1145/3641142,
title = {ACSW '24: Proceedings of the 2024 Australasian Computer Science Week},
year = {2024},
isbn = {9798400717307},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1145/3652620.3687774,
author = {Chen, Kua and Chen, Boqi and Yang, Yujing and Mussbacher, Gunter and Varr\'{o}, D\'{a}niel},
title = {Embedding-based Automated Assessment of Domain Models},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687774},
doi = {10.1145/3652620.3687774},
abstract = {Domain modeling is an essential component in many software engineering courses since it serves as a way to represent and understand the concepts and relationships in a problem domain. Course instructors evaluate student-generated diagrams manually, comparing them against a reference solution and providing feedback. However, as enrollment in software engineering courses continues to rise, manual grading of a large number of student submissions becomes an overwhelming and time-intensive task for instructors. Hence, there is a need for automated assessment of domain models which assists course instructors during the grading process. In this paper, we propose a novel text embedding-based approach that automatizes the assessment of domain models expressed in a textual domain-specific language, against reference solutions created by modeling experts. Our algorithm showcases remarkable proficiency in matching model elements across domain models, achieving an F1-score of 0.82 for class matching, 0.75 for attribute matching, and 0.80 for relation matching. Our algorithm also yields grades highly correlated with human grader assessments, with correlations exceeding 0.8 and mean absolute errors below 0.05.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {87–94},
numpages = {8},
keywords = {domain modeling, text embeddings, domain model assessment},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3687311.3687317,
author = {Ji, Zibo and Li, Yanjun and Yang, Ruiting and Wu, Haoning},
title = {Research on the application and practice of curriculum with AI assistance based on students' adaptive learning needs},
year = {2024},
isbn = {9798400709920},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3687311.3687317},
doi = {10.1145/3687311.3687317},
abstract = {In the era of continuous development of education from informationization to digital transformation, in order to improve students' learning autonomy in mechanics courses and to solve the problem of resource richness and personalized demand of mechanics course teaching under the demand of students' self-adaptive learning, through the fusion technology of big language model and knowledge graph, the interaction technology of Solidworks 3D modeling and Realibox rendering, and cloud computing, cloud supervision and other technological tools to assist teaching by providing knowledge systematic model and visualization model to help students effectively complete the learning tasks and cultivate and improve their independent learning ability. Supervision and other technical tools through the provision of knowledge systematic model and visualization model to assist teaching, help students effectively complete the learning task and cultivate and improve independent learning ability.},
booktitle = {Proceedings of the 2024 International Conference on Intelligent Education and Computer Technology},
pages = {30–34},
numpages = {5},
location = {Guilin, China},
series = {IECT '24}
}

@article{10.1145/3617946.3617959,
author = {Krusche, Stephan and Bell, Jonathan and Tenbergen, Bastian},
title = {Software Engineering Education for the Next Generation:SEENG 2023 Workshop Report},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/3617946.3617959},
doi = {10.1145/3617946.3617959},
abstract = {The 5th International Workshop on Software Engineering Education for the Next Generation was held on May 16, 2023 in Melbourne, Australia. The workshop was part of the 45th International Conference on Software Engineering. It specifically supported the general theme of "Educating the Next Generation of Software Engineers". Building on its predecessors, the workshop used a highly interactive format, structured around eight short paper presentations to generate discussion topics, an activity to select the most interesting topics, and structured breakout sessions. This enabled the participants to discuss the most interesting topics in detail. Participants presented the results of the breakout sessions using mind maps.},
journal = {SIGSOFT Softw. Eng. Notes},
month = oct,
pages = {66–69},
numpages = {4}
}

@inproceedings{10.1145/3641555.3705150,
author = {Vandenberg, Jessica and Torbey, Ryan and Zhang, Cecilia Xuning and Mott, Bradford and Bailey, Keisha and Wilson, Joseph},
title = {Conceptualizing the Support and Learning of K-2 Educators around Artificial Intelligence in Language Arts},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705150},
doi = {10.1145/3641555.3705150},
abstract = {In an age where artificial intelligence (AI) plays an increasingly pivotal role in daily life, it is essential to equip our youngest learners with foundational knowledge of AI. The AI by 8 project aims to empower kindergarten through second grade teachers in rural North Carolina by introducing AI concepts through engaging, unplugged activities integrated into English Language Arts (ELA) instruction. This initiative seeks to address the gap in AI education expertise among early childhood educators and seeks to foster a generation of students who are well-prepared to navigate a technology-driven future. We present in this poster the guiding theoretical framework for our work, outlining the objectives of the research-practice partnership, and our initial efforts at recruiting rural K-2 teachers.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1647–1648},
numpages = {2},
keywords = {K-2 teachers, artificial intelligence education, rural populations, unplugged},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3532512.3539664,
author = {Lewis, Clayton},
title = {Automatic Programming and Education},
year = {2022},
isbn = {9781450396561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3532512.3539664},
doi = {10.1145/3532512.3539664},
abstract = {Automatic programming, as supported by recent language-model based AI systems, potentially allows a new approach to making computation a useful tool for learning, a goal of the Boxer project. This paper shows that the Codex system can be used to support some of the explorations in mathematics for which Boxer has been used. Virtually no knowledge of programming is required. Reflecting on the lessons from this exploration may sharpen the goals we bring to educational computing. What knowledge about computing, as distinct from the ability to creatively use computing, should learners gain?},
booktitle = {Companion Proceedings of the 6th International Conference on the Art, Science, and Engineering of Programming},
pages = {70–80},
numpages = {11},
keywords = {Boxer, automatic programming, computational literacy, education},
location = {Porto, Portugal},
series = {Programming '22}
}

@inproceedings{10.1145/3545945.3569793,
author = {Holland-Minkley, Amanda and Barnard, Jakob and Barr, Valerie and Braught, Grant and Davis, Janet and Reed, David and Schmitt, Karl and Tartaro, Andrea and Teresco, James D.},
title = {Computer Science Curriculum Guidelines: A New Liberal Arts Perspective},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569793},
doi = {10.1145/3545945.3569793},
abstract = {ACM/IEEE curriculum guidelines for computer science, such as CS2013 or the forthcoming CS2023, provide well-researched and detailed guidance about the content and skills that make up an undergraduate computer science (CS) program. Liberal arts CS programs often struggle to apply these guidelines within their institutional context and goals. Historically, this has been addressed through the development of model CS curricula tailored for the liberal arts context. We take a different position: that no single model curriculum can apply across the wide range of liberal arts institutions. Instead, we argue that liberal arts CS educators need best practices for using guidelines such as CS2023 to inform curriculum design. These practices must acknowledge the opportunities and priorities of a liberal arts philosophy as well as a program's mission and identity. This paper reviews the context and motivation behind computing in the liberal arts. We also review the history of liberal arts CS educators and ACM/IEEE curriculum guidelines. We present data and trends about liberal arts computing programs, discussing how this informs curriculum design. Finally, we propose a process that guides programs to work with curriculum guidelines through the lens of institutional and program missions and identities, goals, and situational factors.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {617–623},
numpages = {7},
keywords = {cs education, curriculum, liberal arts},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3723010.3723022,
author = {Bugert, Flemming and Nadimpalli, Vamsi Krishna and Bittner, Dominik and Ezer, Timur and Grabinger, Lisa and Maier, Robert and R\"{o}hrl, Simon and Staufer, Susanne and Hauser, Florian and Mottok, J\"{u}rgen},
title = {ML based Evaluation Methodology for Learning Path Recommender Systems},
year = {2025},
isbn = {9798400712821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3723010.3723022},
doi = {10.1145/3723010.3723022},
abstract = {In education, recommender systems can provide students with personalized learning materials based on their preferences. When comparing various recommendation algorithms, the main question is, which algorithm provides the most suitable recommendations for each student. Answering this question requires a quantitative evaluation methodology (i.e. a concrete metric) for ranking the results of (even non-deterministic) recommender systems. While there is already literature on this topic, the uniqueness of our approach lies in the application of machine learning: we deploy a likelihood based analysis via Hidden Markov Models named Aiakos. With this strategy, we aim to provide data-driven insights about accuracy and stability of recommendations towards a more reasonable selection of the appropriate recommender system. The training data for the Hidden Markov Models is collected from 80 students. Data from another 26 students is then used to discuss the behavior of our evaluation procedure considering a single recommendation as well as the results from 100 recommendations. Furthermore, the proposed concept allows to be applied to other domains as well.},
booktitle = {Proceedings of the 6th European Conference on Software Engineering Education},
pages = {40–48},
numpages = {9},
keywords = {hidden markov models, recommender systems, evaluation, maximum likelihood voting},
location = {
},
series = {ECSEE '25}
}

@inproceedings{10.1145/3641554.3701851,
author = {Ko, Shao-Heng and Stephens-Martinez, Kristin and Zahn, Matthew and Velasco, Yesenia and Battestilli, Lina and Heckman, Sarah},
title = {Student Perceptions of the Help Resource Landscape},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701851},
doi = {10.1145/3641554.3701851},
abstract = {Background and Context. Existing works in computing students' help-seeking and resource selection identified an expanding set of important dimensions that students consider when choosing a help resource. However, most works either assume a predefined list of help resources or focus on one specific help resource, while the landscape of help resources evolve at a faster speed.Objectives. We seek to study how students value each dimension in the help landscape in their resource selection and utilization processes, as well as how their identities relate to their perceptions of the landscape.Method. We surveyed N=1,625 students on their perceptions of 8 dimensions across 12 offerings of 7 courses at 2 institutions.Findings. We found a consistent pattern of four distinct dimension tiers ordered from most to least important: (1) timeliness of help, (2) availability and adaptability of the resource, (3) the resource's time/space anchor and the effort to phrase the help need, (4) formality and socialness of the resource. We also found men and first-years rate all dimensions as less important than their classmates.Implications. Our results reveal what the students collectively value most when selecting help resources and thus can inform practitioners seeking to improve their course help ecosystem.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {596–602},
numpages = {7},
keywords = {computing education, help-seeking},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3641554.3701916,
author = {Ebert, Jack and Kramarczuk, Kristina},
title = {Leveraging Undergraduate Perspectives to Redefine AI Literacy},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701916},
doi = {10.1145/3641554.3701916},
abstract = {Artificial intelligence (AI) represents the future of the workforce, but existing curricula inadequately prepare students to comprehend and use these new technologies. Despite the push for educators to teach AI literacy, there is a distinct lack of research exploring student perspectives on the topic. Utilizing an explanatory sequential mixed methods research design, we first administered an AI literacy survey to undergraduate students in a computing major to learn how they think about AI, and then conducted focus group interviews after further refining our research questions. There was a discrepancy between undergraduate competence with AI applications and underlying AI principles, which were conflated on the survey and positively influenced overall knowledge. Participant confidence in AI's capability as a learning tool was infrequently limited by perception of personal ability, but rather by beliefs about limitations in AI tool efficacy. Participants believed that students pursuing any field would benefit from AI literacy and that AI literacy education, if implemented effectively, could mitigate concerns with AI pervasion in the workplace. A combination of surveys and assessments will be beneficial when centering students in AI curricula, the former establishing a student's AI confidence and the latter competence.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {290–296},
numpages = {7},
keywords = {ai literacy, artificial intelligence, generative ai},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3689187.3709612,
author = {Schulte, Carsten and Sentance, Sue and Sparmann, S\"{o}ren and Altin, Rukiye and Friebroon-Yesharim, Mor and Landman, Martina and R\"{u}cker, Michael T. and Satavlekar, Spruha and Siegel, Angela and Tedre, Matti and Tubino, Laura and Vartiainen, Henriikka and Vel\'{A}zquez-Iturbide, J. \'{A}ngel and Waite, Jane and Wu, Zihan},
title = {What We Talk About When We Talk About K-12 Computing Education},
year = {2025},
isbn = {9798400712081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689187.3709612},
doi = {10.1145/3689187.3709612},
abstract = {K-12 computing education research is a rapidly growing field of research, both driven by and driving the implementation of computing as a school and extra-curricular subject globally. In the context of discipline-based education research, it is a new and emerging field, drawing on areas such as mathematics and science education research for inspiration and theoretical bases. The urgency around investigating effective teaching and learning in computing in school alongside broadening participation has led to much of the field being focused on empirical research. Less attention has been paid to the underlying philosophical assumptions informing the discipline, which might include a critical examination of the rationale for K-12 computing education, its goals and perspectives, and associated inherent values and beliefs. In this working group, we conducted an analysis of the implicit and hidden values, perspectives and goals underpinning computing education at school in order to shed light on the question of what we are talking about when we talk about K-12 computing education. To do this we used a multi-faceted approach to identify implicit rationales for K-12 computing education and examine what these might mean for the implemented curriculum. Methods used include both traditional and natural language processing techniques for examining relevant literature, alongside an examination of the theoretical literature relating to education theory. As a result we identified four traditions for K-12 computing education: algorithmic, design-making, scientific and societal. From this we have developed a framework for the exemplification of these traditions, alongside several potential use cases. We suggest that while this work may provoke some discussion and debate, it will help researchers and others to identify and express the rationales they draw on with respect to computing education.},
booktitle = {2024 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {226–257},
numpages = {32},
keywords = {computing education, curriculum, educational traditions, k-12 education, philosophy, rationales},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@book{10.1145/3664191,
author = {Kumar, Amruth N. and Raj, Rajendra K. and Aly, Sherif G. and Anderson, Monica D. and Becker, Brett A. and Blumenthal, Richard L. and Eaton, Eric and Epstein, Susan L. and Goldweber, Michael and Jalote, Pankaj and Lea, Douglas and Oudshoorn, Michael and Pias, Marcelo and Reiser, Susan and Servin, Christian and Simha, Rahul and Winters, Titus and Xiang, Qiao},
title = {Computer Science Curricula 2023},
year = {2024},
isbn = {9798400710339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA}
}

@inproceedings{10.1145/3606094.3606101,
author = {Vargas-Murillo, Alfonso Renato and Pari-Bedoya, Ilda Nadia Monica de la Asuncion and Guevara-Soto, Francisco de Jesus},
title = {The Ethics of AI Assisted Learning: A Systematic Literature Review on the Impacts of ChatGPT Usage in Education},
year = {2023},
isbn = {9798400700422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3606094.3606101},
doi = {10.1145/3606094.3606101},
abstract = {This systematic literature review explores how gamification in legal education might be In recent years, ChatGPT has become a noteworthy subject in the educational field due to the popularity it gained amongst students across different levels of education all over the world, who use this technology to assess their academic homework, transforming ChatGPT in some sort of auxiliary tool that aids them with the completion of certain tasks that would take more time to complete, such as research and data comparison, to name a few examples; but this form of AI assisted learning, as it were, has also become a problematic subject. This artificial intelligence chatbot is, undeniably, a remarkable advancement in AI regarding the improvements it presents compared to other similar technologies, and it clearly paves the way for future applications not only in education, but also at a social level, in a world more driven towards the development and optimization of digital tools with the help of machine learning. Nevertheless, this sort of technology should be question ed when its application permeates deeply in the performance and development of students and their learning process, especially when taking in consideration the level of accessibility that ChatGPT has worldwide. Students should have an ethical standpoint on whether they use ChatGPT to complement their learning process and how much input is this technology having in their academic work, so they learn to use it more effectively and avoid the abuse of ChatGPT usage, in order to seize the benefits that this AI may have on education. This study's objective is to analyze the current literature around the use of ChatGPT in education, for which we conducted a Systematic Literature Review (SLR) across multiple journal databases such as Scopus, ScienceDirect, ProQuest, IEEE Xplore and ACM Digital Library.},
booktitle = {Proceedings of the 2023 8th International Conference on Distance Education and Learning},
pages = {8–13},
numpages = {6},
keywords = {Artificial Intelligence, Assisted Learning, Ethics, Systematic Literature Review},
location = {Beijing, China},
series = {ICDEL '23}
}

@article{10.1145/3711675,
author = {Neville-Neil, George V.},
title = {The Drunken Plagiarists: Working with Co-pilots},
year = {2025},
issue_date = {November/December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {6},
issn = {1542-7730},
url = {https://doi.org/10.1145/3711675},
doi = {10.1145/3711675},
abstract = {Before trying to use these tools, you need to understand what they do, at least on the surface, since even their creators freely admit they do not understand how they work deep down in the bowels of all the statistics and text that have been scraped from the current Internet. The trick of an LLM is to use a little randomness and a lot of text to Gauss the next word in a sentence. Seems kind of trivial, really, and certainly not a measure of intelligence that anyone who understands the term might use. But it's a clever trick and does have some applications.},
journal = {Queue},
month = jan,
pages = {18–22},
numpages = {5}
}

@inproceedings{10.1145/3690712.3690729,
author = {Calderwood, Alexander},
title = {Designing for Posthuman Critical Literacy},
year = {2024},
isbn = {9798400710315},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3690712.3690729},
doi = {10.1145/3690712.3690729},
abstract = {This paper discusses positions taken up by the field of contemporary literacy studies as they relate to digital writing technology. Literacy studies have not, to our knowledge, been sufficiently examined from the world of HCI and writing support. We aim to show that literacy studies as a conceptual framework offers a number of theoretical challenges to prevailing views of writing that underpin writing support technology development. We discuss the notions assemblage and posthuman critical literacy, which may indicate new research directions for writing support technology in theoretical, artistic, and ideological fronts. Finally, we describe a textual instrument that demonstrates these ideas.},
booktitle = {Proceedings of the Third Workshop on Intelligent and Interactive Writing Assistants},
pages = {65–68},
numpages = {4},
keywords = {critical, deleuze, instrument, posthuman, support, textual, writing},
location = {Honolulu, HI, USA},
series = {In2Writing '24}
}

@article{10.1145/3721986,
author = {Mayer, Paul and Baraniuk, Rich},
title = {The Importance of Teaching Logic to Computer Scientists and Electrical Engineers},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
url = {https://doi.org/10.1145/3721986},
doi = {10.1145/3721986},
abstract = {It is argued that logic, and in particular mathematical logic, should play a key role in the undergraduate curriculum for students in the computing fields, which include electrical engineering (EE), computer engineering (CE), and computer science (CS). This is based on (1) the history of the field of computing and its close ties with logic, (2) empirical results showing that students with better logical thinking skills perform better in tasks such as programming and mathematics, and (3) the skills students are expected to have in the job market. Further, the authors believe teaching logic to students explicitly will improve student retention, especially involving underrepresented minorities in STEM1, whose rate of attrition is higher than for non-minority students. Though this work focuses specifically on the computing fields, these results demonstrate the importance of logic education to STEM (science, technology, engineering, and mathematics) as a whole.},
journal = {ACM Trans. Comput. Educ.},
month = may,
articleno = {12},
numpages = {12},
keywords = {logic, engineering education, computer science education, student attrition, retention}
}

@inproceedings{10.1145/3678392.3678410,
author = {Lu, Yao and Sun, Zhong},
title = {AI-Enabled Education: Innovative Design and Practice of Online Training for Rural Teachers},
year = {2024},
isbn = {9798400717123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678392.3678410},
doi = {10.1145/3678392.3678410},
abstract = {At present, the innovation and development of AI-enabled education has become the key to the digital transformation of education in China. In the context of digital transformation of education, rural teachers’ digital literacy needs to be effectively improved to cope with the digital challenges of the intelligent era. This study focuses on the topic of AI technology empowering rural teachers in China to improve their digital literacy. Based on the theory of community of practice, Cloudbridge Academy designed an online course on “AI-enabled teaching” for rural teachers in terms of learning objectives, learning content, interactive strategies, resources and evaluation, and relied on the training program of Cloudbridge Academy to improve the digital literacy of rural teachers. The course was put into practice to build an online community of practice for teachers and explore the practical way of digital transformation of teacher training, with the aim of providing references for high-quality professional development of teachers empowered by AI technology.},
booktitle = {Proceedings of the 2024 10th International Conference on Frontiers of Educational Technologies},
pages = {60–66},
numpages = {7},
keywords = {artificial intelligence educational applications, digital literacy, rural teachers, teacher professional development, teacher training},
location = {Malacca, Malaysia},
series = {ICFET '24}
}

@proceedings{10.1145/3626686,
title = {ICDTE '23: Proceedings of the 7th International Conference on Digital Technology in Education},
year = {2023},
isbn = {9798400708527},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hangzhou, China}
}

@article{10.1145/3576896,
author = {Hope, Tom and Downey, Doug and Weld, Daniel S. and Etzioni, Oren and Horvitz, Eric},
title = {A Computational Inflection for Scientific Discovery},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/3576896},
doi = {10.1145/3576896},
abstract = {Enabling researchers to leverage systems to overcome the limits of human cognitive capacity.},
journal = {Commun. ACM},
month = jul,
pages = {62–73},
numpages = {12}
}

@inproceedings{10.1145/3641554.3701892,
author = {Niu, Jeffrey and Wong, Jessica and Lake, Charlie and Rahardjo, Justin and Zarkoob, Hedayat and Ola, Oluwakemi and Belleville, Patrice and Mochetti, Karina and Allen, Meghan and Moosvi, Firas and Wolfman, Steven},
title = {Expanding the Horizons of Autograding: Innovative Questions at UBC},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701892},
doi = {10.1145/3641554.3701892},
abstract = {The popularity of autograding has grown due to increasing class sizes and the need to reduce grading load while ensuring quality. Autograding has conventionally been used for multiple choice and fill in the blank questions, or to check code correctness. In this work, we discuss the use of autograders at UBC and some non-conventional autograding implementations in our curricula. We reflect upon our autograder use in our courses and discuss the benefits, implications, and considerations of this pedagogical choice.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {868–874},
numpages = {7},
keywords = {automated assessment, computer science education},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3649165.3690117,
author = {Hollands, Fiona M. and DiPaola, Daniella and Breazeal, Cynthia and Ali, Safinah},
title = {AI Mastery May Not Be For Everyone, But AI Literacy Should Be},
year = {2024},
isbn = {9798400705984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649165.3690117},
doi = {10.1145/3649165.3690117},
abstract = {Despite the abundance of advice from policy bodies, professional associations, advocacy groups, and scholars on how K-12 schools should assimilate AI and provide AI education, practical plans are lacking from K-12 education leaders themselves. Education leaders must make strategic decisions about how to prepare teachers and students for an AI-infused future. Simultaneously, educators need immediate support and guidance on how to manage the arrival of tools that render some existing educational practices obsolete and prompt the need to teach new skills and awareness. Near term, it may be unrealistic to expect all students to master the ability to develop AI applications; universal AI literacy is a more feasible goal. We introduce a set of short-format, modular AI literacy courses and report how they were implemented and affected teachers' and students' knowledge and perceptions of AI. Using an online questionnaire, we collected data from 265 individuals worldwide who accessed the courses, including 190 teachers who implemented them with over 11,800 students. We conducted 17 teacher interviews to gather feedback and to better understand how courses were adapted for local contexts. Teachers reported an increase in their own and their students' knowledge of AI concepts; and increased optimism about the potential benefits of AI to society and their ability to influence the future of AI. Key takeaways are that AI literacy instruction should be designed for adaptability to local contexts and cultures and that steps should be taken to institutionalize the integration of AI literacy into the regular school curriculum.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1},
pages = {60–66},
numpages = {7},
keywords = {ai literacy, ai literacy curricula, artificial intelligence, k-12 education},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@inproceedings{10.1145/3649405.3659537,
author = {Aly, Sherif G. and Becker, Brett A. and Kumar, Amruth N. and Raj, Rajendra K.},
title = {Computer Science Curricula 2023 (CS2023): Rising to the Challenges of Change in AI, Security, and Society},
year = {2024},
isbn = {9798400706035},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649405.3659537},
doi = {10.1145/3649405.3659537},
abstract = {Model curricula for baccalaureate computer science (CS) have been published regularly from 1968 through 2013. In early 2021, the ACM, IEEE-Computer Society, and the Association for the Advancement of Artificial Intelligence (AAAI) constituted a task force to revise these curricula, which have now been released as Computer Science 2023 Curricula (CS2023). The CS2023 curricular guidelines inform educators and administrators on the what, why, and how to cover undergraduate CS over the next decade. Like past guidelines, CS2023 provides curricular content - a knowledge model largely backward compatible with CS2013, supplemented by a competency framework influenced by Computing Curricula 2020 (CC2020) - and complementary curricular practices, which include articles by international experts on program design and delivery. Ongoing drafts of CS2023 were disseminated via the CS2023 website, along with regular publications or presentations at various computing education venues.This panel focuses on three among the 17 CS2023 knowledge areas: Society, Ethics, and the Profession (SEP), Artificial Intelligence (AI), and Security (SEC). While the other 14 knowledge areas remain important in CS education, these three have been in the news due to inadequacies in current CS education. The panelists, who served on the CS2023 steering committee, will discuss how CS2023 addresses these challenges. Attendees will appreciate the approach taken by CS2023 toward these three hot-button items of CS education, especially constraints on curriculum design, and how CS2023 may be used to educate the next generation of CS graduates to rise to these three challenges.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 2},
pages = {852–853},
numpages = {2},
keywords = {artificial intelligence education, computer science curricular guidelines, professional ethics education, security education, societal considerations},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3597503.3639192,
author = {Sa\u{g}lam, Timur and Br\"{o}del, Moritz and Schmid, Larissa and Hahner, Sebastian},
title = {Detecting Automatic Software Plagiarism via Token Sequence Normalization},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639192},
doi = {10.1145/3597503.3639192},
abstract = {While software plagiarism detectors have been used for decades, the assumption that evading detection requires programming proficiency is challenged by the emergence of automated plagiarism generators. These generators enable effortless obfuscation attacks, exploiting vulnerabilities in existing detectors by inserting statements to disrupt the matching of related programs. Thus, we present a novel, language-independent defense mechanism that leverages program dependence graphs, rendering such attacks infeasible. We evaluate our approach with multiple real-world datasets and show that it defeats plagiarism generators by offering resilience against automated obfuscation while maintaining a low rate of false positives.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {113},
numpages = {13},
keywords = {software plagiarism detection, plagiarism obfuscation, obfuscation attacks, code normalization, PDG, tokenization},
location = {Lisbon, Portugal},
series = {ICSE '24}
}

@inproceedings{10.1145/3649165.3703624,
author = {Zilles, Craig and Zhao, Chenyan and Chen, Yuxuan and Matthews, Evan Michael and West, Matthew},
title = {A Case for Bayesian Grading},
year = {2024},
isbn = {9798400705984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649165.3703624},
doi = {10.1145/3649165.3703624},
abstract = {Academic integrity continues to be an issue in education. Students' grades are often computed using a collection of evidence that varies in its trustworthiness (e.g., a proctored exam can be trusted more than an out-of-class programming project) due to practical constraints. When a student cheats, their trusted and less trustworthy scores are inconsistent, which presents instructors a choice between rewarding the cheating behavior and the burden of investigating / making cheating allegations.In this position paper, we propose that Bayesian inference might be a useful tool in assigning grades derived from trusted and less trusted evidence. Rather than compute grades by performing arithmetic on both trusted and untrusted assessments, we instead try to infer a latent variable, the student's mastery of the course material, from these observed performances and their potential for cheating. Key to this approach is that grades can be assigned that discount suspicious work without needing to explicitly make a cheating allegation. A logical conclusion of this approach is that the needed amount of trusted assessments for a given student depends on how inconsistent are their trusted and untrusted assessments.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1},
pages = {275–278},
numpages = {4},
keywords = {bayesian inference, cheating, grading, trust},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@inproceedings{10.1145/3678610.3678625,
author = {Chan, Shiau Wei and Norhisham, Nur Intan Shahira and Ismail, Fadillah and Ahmad, Md Fauzi},
title = {Students' Perceptions and Intentions Regarding ChatGPT Usage in Higher Education},
year = {2024},
isbn = {9798400716799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678610.3678625},
doi = {10.1145/3678610.3678625},
abstract = {The adoption of new technologies such as ChatGPT has opened up the potential to elevate education to the next level. However, several issues have arisen with ChatGPT, including concerns related to academic integrity and the challenge of distinguishing between AI-generated and human-generated content. Thus, this study aimed to investigate students' perceptions and intentions regarding the use of ChatGPT in higher education. A total of 320 undergraduate students were selected from the Faculty of Technology Management and Business (FPTP) at Universiti Tun Hussein Onn Malaysia (UTHM). The random sampling method was employed in this study, with a population size of 1976 students. Data were collected through a questionnaire, and statistical analysis was conducted using SPSS software. The findings of this research reveal that students' perception of ChatGPT usage is moderate, while their intention to use ChatGPT is interpreted at a high level. Furthermore, a positive correlation between students' intentions and perceptions toward ChatGPT was discovered. This study is crucial for understanding students' perceptions and intentions to optimize their benefits and comprehend their risks to students, organizations, and ethics.},
booktitle = {Proceedings of the 2024 10th International Conference on E-Society, e-Learning and e-Technologies (ICSLT)},
pages = {49–54},
numpages = {6},
keywords = {ChatGPT, Higher Education, Intention, Perception},
location = {
},
series = {ICSLT '24}
}

@inproceedings{10.1145/3641554.3701900,
author = {Ruth, Barrett and Hott, John R.},
title = {Auto-grading in Computing Education: Perceptions and Use},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701900},
doi = {10.1145/3641554.3701900},
abstract = {Auto-grading technologies have become increasingly prevalent in computing education, driven by the need to handle growing class sizes and provide timely and effective feedback. We conducted a survey of 44 computer science instructors at various institutions in order to gather instructor experience and use of auto-graders, the features instructors value most, and the challenges and limitations faced when using these tools. We specifically asked about factors such as grading strategies and policies, opinions on existing tools, and other automated grading methods they employ. Our results indicated that instructors prefer tools that offer significant customizability and integration capabilities, with functionality and program output-based grading as the most commonly used approaches. They emphasized the need for integrated auto-grading solutions that include robust core features and prioritize extensibility to better align with pedagogical goals and to support instructors in managing the increasing demands of computer science education. Based on these findings, we conclude that existing solutions should be improved to address instructor-reported preferences and diverse educational needs.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1008–1014},
numpages = {7},
keywords = {assessment, autograding, automatic grading, computing education},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3702212.3702225,
author = {Andrei, Oana and Nabi, Syed Waqar and Barr, Matthew and Petrovska, Olga},
title = {Integrating Socially Responsible Computing Competencies in Computer Science and Software Engineering Education},
year = {2025},
isbn = {9798400711725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702212.3702225},
doi = {10.1145/3702212.3702225},
abstract = {We aim to explore and develop strategies for incorporating socially responsible computing into the foundational curriculum of computer science and software engineering programs in higher education in the UK. The objectives of the collaborative session are: reviewing the components of socially responsible computing - ethics, sustainability, societal impact and equity, governance and data protection, security; identifying essential competencies that should be fostered from early on in the program; developing actionable learning outcomes and curriculum changes to embed these principles from the outset.},
booktitle = {Proceedings of the 9th Conference on Computing Education Practice},
pages = {36–37},
numpages = {2},
keywords = {responsible computing, ethics, responsible AI, sustainability, software engineering, computer science education},
location = {
},
series = {CEP '25}
}

@inproceedings{10.1145/3641555.3704717,
author = {Adams, Joel C. and Bailey, Cynthia and Matthews, Suzanne J. and Tymann, Paul},
title = {U.S. Government-Funded Opportunities for CS Educators},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3704717},
doi = {10.1145/3641555.3704717},
abstract = {Have you ever thought about spending an academic leave teaching in a different country or advising United States (U.S.) policy-makers on emerging technologies, or spending time at the National Science Foundation (NSF), or working at one of the U.S. service academies? For computer science academics located in the U.S., the federal government offers a variety of opportunities for those interested in such positions, either as an academic leave or as a career change. This panel session explores several of these opportunities, including: American Association for the Advancement of Science (AAAS) Fellowships, Fulbright U.S. Scholars Awards, Jefferson Science Fellowships, NSF rotator opportunities, and U.S. service academy careers. Panelists will describe their experiences in such positions and answer questions from the audience.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1683–1684},
numpages = {2},
keywords = {AAAS fellowship, Jefferson fellowship, NSF, U.S. government, academic leave, fulbright, sabbatical, service},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3641554.3701830,
author = {W\'{o}jtowicz, Andrzej and Prill, Maciej},
title = {Relational Database Courses with CodeRunner in Moodle: Extending SQL Programming Assignments to Client-Server Database Engines},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701830},
doi = {10.1145/3641554.3701830},
abstract = {Students practice Data Query, Manipulation, and Definition Language statements in a standard introductory relational database course. When teaching large classes, the teacher needs to review many student solutions. Doing it by hand is laborious. However, this task can be quickly and accurately completed using a learning management system (LMS). Moodle is a widely used LMS that can be enhanced with the CodeRunner plugin to facilitate the evaluation of programming tasks. Unfortunately, a basic setup supports only the embedded database engine SQLite, which often is not the preferred choice for a course of this type. We present an open-source implementation that extends CodeRunner with popular client-server database engines, i.e., Microsoft SQL Server, MySQL, and PostgreSQL. Our method checks the correctness of a student's query in terms of output and validates the query by investigating the parse tree derived from the grammar of a given Structured Query Language (SQL) dialect. We present the system's evaluation results during a database course along with the implementation.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1239–1245},
numpages = {7},
keywords = {databases, microsoft sql server, moodle, mysql, postgresql, student assessment},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.1145/3690652,
author = {Yang, Stephanie and Baird, Miles and O’Rourke, Eleanor and Brennan, Karen and Schneider, Bertrand},
title = {Decoding Debugging Instruction: A Systematic Literature Review of Debugging Interventions},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
url = {https://doi.org/10.1145/3690652},
doi = {10.1145/3690652},
abstract = {Students learning computer science frequently struggle with debugging errors in their code. These struggles can have significant downstream effects—negatively influencing how students assess their programming ability and contributing to their decision to drop out of CS courses. However, debugging instruction is often an overlooked topic, and instructors report feeling unaware of effective approaches to teach debugging. Within the literature, research on the topic is sporadic, and though there are rigorous and insightful studies to be found, there is a need to synthesize instructional approaches for debugging. In this article, we review research from 2010 to 2022 on debugging interventions. We summarize the common pedagogical approaches for learning and categorize how these target specific cognitive and non-cognitive debugging skills, such as self-efficacy and emotion regulation. We also present a summary of assessment methods and their outcomes in order to discuss intervention efficacy and directions for further research. Our sample displays a diverse variety of debugging interventions and pedagogical approaches, ranging from games to unplugged activities. An evaluation of article results also presents encouraging findings, revealing several interventions that improved debugging accuracy and learning. Still, we notice gaps in interventions addressing non-cognitive debugging skills and observe limited success in guiding students toward adopting systematic debugging strategies. The review concludes with a discussion of future directions and implications for researchers and instructors in the field.},
journal = {ACM Trans. Comput. Educ.},
month = nov,
articleno = {45},
numpages = {44},
keywords = {debugging, learning Intervention, computer science education}
}

@inproceedings{10.1145/3641555.3705129,
author = {Messer, Marcus and Brown, Neil C. C. and K\"{o}lling, Michael and Shi, Miaojing},
title = {Menagerie: A Dataset of Graded Programming Assignments},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705129},
doi = {10.1145/3641555.3705129},
abstract = {We present Menagerie, an open-ended project-scale second-semester CS1 Java assignment dataset that ran over four academic years (18/19 - 21/22). It comprises 667 submissions, with 273 being subsequently graded post hoc. The assignment was open-ended, with the students being asked to implement a 'predator/prey' simulator and to meet specific criteria, which included adding five species, competing for the same food source and keeping track of the time of day. The submissions were assessed as part of a separate study on the correctness of the solution, how well the code was designed, how readable the code is, and the quality of the documentation.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1547–1548},
numpages = {2},
keywords = {assessment, computer science education, programming assessment},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@proceedings{10.1145/3670013,
title = {IC4E '24: Proceedings of the 2024 15th International Conference on E-Education, E-Business, E-Management and E-Learning},
year = {2024},
isbn = {9798400717062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Fukuoka-shi, Japan}
}

@inproceedings{10.1145/3660650.3660673,
author = {Rajabi, Parsa and Kerslake, Chris},
title = {Can You Spot the AI? Incorporating GenAI into Technical Writing Assignments},
year = {2024},
isbn = {9798400709975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660650.3660673},
doi = {10.1145/3660650.3660673},
abstract = {In an effort to foster critical reflection on the usage of generative AI (genAI) during computer science writing assignments, this three-part assignment challenges students to predict whether their peers can detect which essays are generated using AI. Implemented as part of a third-year professional responsibility and technical writing course for N=200 students during Spring 2024, students individually generated two short persuasive essays, one using genAI and the other without. They then combined the two essays into a single document and submitted it for peer-review. Additionally, they formulated a guess on whether their peers would be able to detect which essay was generated as well as a rationale for their guess. Following the peer-review process, students reflected on their own experience trying to detect which essays were generated as well as the outcome of their guess about their peers abilities as well. Feedback indicates its effectiveness in engaging students in their understanding of the potentials and limitations of genAI. Recommended prerequisites include a clear course AI-usage policy and a brief overview of genAI prompt engineering.},
booktitle = {Proceedings of the 26th Western Canadian Conference on Computing Education},
articleno = {23},
numpages = {2},
keywords = {AI Literacy, AI in Education, AI-usage Policy, ChatGPT, Generative AI, Technical Writing},
location = {Kelowna, BC, Canada},
series = {WCCCE '24}
}

@inproceedings{10.1145/3629296.3629303,
author = {Zhang, Zhuojing and Wasie, Sarrah},
title = {Educational Technology in the Post-Pandemic Era: Current Progress, Potential, and Challenges},
year = {2024},
isbn = {9798400709111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629296.3629303},
doi = {10.1145/3629296.3629303},
abstract = {Information technology has been significantly developed during the past decades. Teachers leverage various technologies to enhance teaching experience and knowledge delivery. Students explore diverse knowledge from numerous learning resources on the Web. Furthermore, the COVID-19 pandemic accelerates processing technology integration in education. During the pandemic, Zoom and Google Classroom were widely used for course content delivery, and they are still presenting their values in the post-pandemic era. Some other applications are also continuously adopted when in-person teaching and learning have been resumed. In this paper, a literature review-based approach is applied to summarize recent achievements in educational technology and present several promising directions for the future of education, including artificial intelligence-powered content generation and metaverse. A roadmap is proposed to illustrate the current and future progress of education technology. Meanwhile, safeguards are also provided to guide schools and parents to be aware of the potential hazards during technology use at the K-12 education level.},
booktitle = {Proceedings of the 15th International Conference on Education Technology and Computers},
pages = {40–46},
numpages = {7},
keywords = {AI-generated content, COVID-19, K-12 education, educational technology},
location = {Barcelona, Spain},
series = {ICETC '23}
}

@proceedings{10.1145/3627050,
title = {IoT '23: Proceedings of the 13th International Conference on the Internet of Things},
year = {2023},
isbn = {9798400708541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Nagoya, Japan}
}

@inproceedings{10.1145/3576882.3617915,
author = {Karvelas, Ioannis and Dillane, Joe and Becker, Brett A.},
title = {Programmers' Views on IDE Compilation Mechanisms},
year = {2023},
isbn = {9798400700484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576882.3617915},
doi = {10.1145/3576882.3617915},
abstract = {In this work we investigate the views of novice programmers on three important IDE mechanisms: compilation, error indication, and error message presentation. We utilize two versions of the BlueJ pedagogical programming environment which encapsulate fundamentally different approaches to these mechanisms. This allows us to examine how effective different means of invoking these mechanisms are for novices. We conducted a survey with 305 programmers with different levels of experience who provided rating scores for the individual mechanisms mentioned. Additionally, participants provided suggestions on how these features should be facilitated to assist novices. The present findings serve as evidence regarding the effectiveness and usability of different mechanisms featured within programming environments. These findings can assist designers of pedagogical programming environments in making evidence-based decisions about their products and facilitate the development of environments that can achieve greater efficacy for novices in their first steps of learning.},
booktitle = {Proceedings of the ACM Conference on Global Computing Education Vol 1},
pages = {98–104},
numpages = {7},
keywords = {BlueJ, CS1, IDE, compilation, compiler error messages, novice programmers, programming, programming behavior, programming environments, programming error messages},
location = {Hyderabad, India},
series = {CompEd 2023}
}

@article{10.5555/3717781.3717788,
author = {Works, Karen E.},
title = {Three Phase - Adversarial Search - Tile Games},
year = {2024},
issue_date = {November 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {5},
issn = {1937-4771},
abstract = {With the advent of chatGPT and Copilot I find that students are not delving deep enough into the implementation of search approaches. To combat this, I decided to implement a three-phase adversarial search project. After lectures on adversarial search approaches and implementation examples, students are given code to a user versus user basic tile game. They are informed of the three phases of the assignment with the goal of encouraging students to understand that they are expected to be able to read and understand an adversarial search logic. In the first phase, all students use the user versus user basic tile game to implement a computer versus user basic tile game app that utilizes an adversarial search. In the second phase, students create and implement their own computer versus user basic tile game app by changing the rules on how the tile game is won and what a valid move is. In the third phase, students are given a timed 10 minute quiz where they are given code for a tile game and the rules for how the game is won and valid moves. The students must identify if the adversarial search is properly implemented and if not then what logic is not correct.},
journal = {J. Comput. Sci. Coll.},
month = nov,
pages = {29–32},
numpages = {4}
}

@inproceedings{10.1145/3568812.3603447,
author = {Wu, Zihan},
title = {Investigating the Effectiveness of Variations of Micro Parsons Problems},
year = {2023},
isbn = {9781450399753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568812.3603447},
doi = {10.1145/3568812.3603447},
abstract = {Parsons problems have been used to provide scaffolding for introductory learners. Instead of asking learners to write from scratch, Parsons problems provide blocks of mixed-up code, and ask learners to rearrange them into the correct order. In traditional Parsons problems, each block would contain one or more lines of code. While traditional Parsons problems have been widely used, there is an untapped potential to adapt them to practice to write a single line of code. My research builds upon the design of traditional Parsons problems and introduces micro Parsons problems – problems that focus on assembling code fragments within a single line. The primary goal of my research is to evaluate the effectiveness of variations of micro Parsons problems to support introductory computer science education. Specifically, I aim to investigate the effects of text entry practice versus: 1) micro Parsons problems for creating SQL and Regular Expressions, 2) personalized micro Parsons problems for just-in-time learning after learners make a mistake, and 3) adaptive micro Parsons problems for fading scaffolding as learners’ skill develops.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2},
pages = {120–122},
numpages = {3},
keywords = {Parsons problems, adaptive learning systems, introductory programming, micro Parsons problems},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3545945.3569723,
author = {Kazemitabaar, Majeed and Chyhir, Viktar and Weintrop, David and Grossman, Tovi},
title = {Scaffolding Progress: How Structured Editors Shape Novice Errors When Transitioning from Blocks to Text},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569723},
doi = {10.1145/3545945.3569723},
abstract = {Transitioning from block-based programming environments to text-based programming environments can be challenging as it requires students to learn new programming language concepts. In this paper, we identify and classify the issues encountered when transitioning from block-based to text-based programming. In particular, we investigate differences that emerge in learners when using a structured editor compared to an unstructured editor. We followed 26 high school students (ages 12-16; M=14 years) as they transitioned from Scratch to Python in three phases: (i) learning Scratch, (ii) transitioning from Scratch to Python using either a structured or unstructured editor, and (iii) evaluating Python coding skills using an unstructured editor. We identify 27 distinct types of issues and show that learners who used a structured editor during the transition phase had 4.6x less syntax issues and 1.9x less data-type issues compared to those who did not. When these learners switched to an unstructured editor for evaluation, they kept a lower rate on data-type issues but faced 4x more syntax errors.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {556–562},
numpages = {7},
keywords = {blocks-to-text, challenges, high school programming, novices, structured editors, thematic analysis, transition},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3702163.3702166,
author = {Asgari, Mohsen and Mannila, Linda and Tsai, Fong-Chun and Str\"{o}mb\"{a}ck, Filip},
title = {Humans or Machines for Teaching: Trust and Preferences among University Students},
year = {2025},
isbn = {9798400717819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702163.3702166},
doi = {10.1145/3702163.3702166},
abstract = {Recent developments in artificial intelligence (AI) have generated discussions around and expectations for its impact in education. In a society where AI plays an increasing role, a basic understanding for the technology and its potential is considered crucial. Still, many educators feel apprehensive towards the use of AI in education, which naturally affects students’ opportunities to learn about and use AI-supported solutions. Users’ preference and trust in relation to these tools also becomes important when integrated in education. Bringing light on students’ perceptions of AI might help educators to better understand the value of such applications. The present study aims to provide insight into how university students perceive AI in general and, more particularly, the idea of having a machine performing tasks that have traditionally been handled by a teacher or a teaching assistant. The results are based on 140 Swedish and Taiwanese university students’ responses to an online questionnaire. Our results indicate that the students have quite positive perceptions of AI: they might still not feel very competent in AI, but show a large interest in the topic and are positive about its consequences for society. Nevertheless they also see potential drawbacks of the technology. As a whole, our findings indicate that Swedish and female students tend to prefer human interactions over AI-supported tools for learning.},
booktitle = {Proceedings of the 2024 16th International Conference on Education Technology and Computers},
pages = {17–25},
numpages = {9},
keywords = {AI in higher education, trust, preference, student perceptions, computer science education},
location = {
},
series = {ICETC '24}
}

@inproceedings{10.1145/3568813.3600124,
author = {Li, Tiffany Wenting and Hsu, Silas and Fowler, Max and Zhang, Zhilin and Zilles, Craig and Karahalios, Karrie},
title = {Am I Wrong, or Is the Autograder Wrong? Effects of AI Grading Mistakes on Learning},
year = {2023},
isbn = {9781450399760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568813.3600124},
doi = {10.1145/3568813.3600124},
abstract = {Errors in AI grading and feedback often have an intractable set of causes and are, by their nature, difficult to completely avoid. Since inaccurate feedback potentially harms learning, there is a need for designs and workflows that mitigate these harms. To better understand the mechanisms by which erroneous AI feedback impacts students’ learning, we conducted surveys and interviews that recorded students’ interactions with a short-answer AI autograder for “Explain in Plain English” code reading problems. Using causal modeling, we inferred the learning impacts of wrong answers marked as right (false positives, FPs) and right answers marked as wrong (false negatives, FNs). We further explored explanations for the learning impacts, including errors influencing participants’ engagement with feedback and assessments of their answers’ correctness, and participants’ prior performance in the class. FPs harmed learning in large part due to participants’ failures to detect the errors. This was due to participants not paying attention to the feedback after being marked as right, and an apparent bias against admitting one’s answer was wrong once marked right. On the other hand, FNs harmed learning only for survey participants, suggesting that interviewees’ greater behavioral and cognitive engagement protected them from learning harms. Based on these findings, we propose ways to help learners detect FPs and encourage deeper reflection on FNs to mitigate the learning harms of AI errors.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1},
pages = {159–176},
numpages = {18},
keywords = {AI error, Bayesian modeling, EiPE, autograder, automated short answer grading, computer science education, explain in plain English, formative feedback, human-AI interaction},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@article{10.1145/3589653,
author = {Mengi, Gopal},
title = {Accessible and Individualized Learning: MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) Cambridge, MA},
year = {2023},
issue_date = {Spring 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {3},
issn = {1528-4972},
url = {https://doi.org/10.1145/3589653},
doi = {10.1145/3589653},
journal = {XRDS},
month = apr,
pages = {58–59},
numpages = {2}
}

@inproceedings{10.1145/3587103.3594137,
author = {Tuson, Ella},
title = {Applications of Programming as Theory Building in Computer Science Education},
year = {2023},
isbn = {9798400701399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587103.3594137},
doi = {10.1145/3587103.3594137},
abstract = {The field of Computer Science has always been one of rapid growth and change. We propose the investigation of Peter Naur's framework of Programming as Theory building as a means to make CS education more resilient to emerging technology and to improve student outcomes by encouraging a focus on internal understanding over the external artifacts of programming.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2},
pages = {621–622},
numpages = {2},
keywords = {CS education, assessment, programming as theory building},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@proceedings{10.5555/3694718,
title = {JCDL '23: Proceedings of the 2023 ACM/IEEE Joint Conference on Digital Libraries},
year = {2024},
isbn = {9798350399318},
publisher = {IEEE Press},
location = {Santa Fe, New Mexico, USA}
}

@inproceedings{10.1145/3563657.3596042,
author = {Goswami, Lahari and Zeinoddin, Pegah Sadat and Estier, Thibault and Cherubini, Mauro},
title = {Supporting Collaboration in Introductory Programming Classes Taught in Hybrid Mode: A Participatory Design Study},
year = {2023},
isbn = {9781450398930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563657.3596042},
doi = {10.1145/3563657.3596042},
abstract = {Hybrid learning modalities, where learners can attend a course in-person or remotely, have gained particular significance in post-pandemic educational settings. In introductory programming courses, novices’ learning behaviour in the collaborative context of classrooms differs in hybrid mode from that of a traditional setting. Reflections from conducting an introductory programming course in hybrid mode led us to recognise the need for re-designing programming tools to support students’ collaborative learning practices. We conducted a participatory design study with nine students, directly engaging them in design to understand their interaction needs in hybrid pedagogical setups to enable effective collaboration during learning. Our findings first highlighted the difficulties that learners face in hybrid modes. The results then revealed learners’ preferences for design functionalities to enable collective notions, communication, autonomy, and regulation. Based on our findings, we discuss design principles and implications to inform the future design of collaborative programming environments for hybrid modes.},
booktitle = {Proceedings of the 2023 ACM Designing Interactive Systems Conference},
pages = {1248–1262},
numpages = {15},
keywords = {collaboration, hybrid classroom, participatory design, programming environment},
location = {Pittsburgh, PA, USA},
series = {DIS '23}
}

@inproceedings{10.1145/3653666.3656107,
author = {Moudgalya, Sukanya Kannan and Swaminathan, Sai},
title = {Toward Data Sovereignty: Justice-oriented and Community-based AI Education},
year = {2024},
isbn = {9798400706264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653666.3656107},
doi = {10.1145/3653666.3656107},
abstract = {Just as food sovereignty is the innate right of all individuals, we argue that data sovereignty should also be treated similarly. We take a critical approach and have leaned on Indigenous scholarship that focuses on rights, control, and power related to data sovereignty. Peoples of the world should have access to, have ownership of, and be decision-making stewards of their own communities' data. We discuss the importance of data sovereignty, implications of a possible 'data apartheid', and ways to possibly achieve data sovereignty in this paper. We present examples of justice-oriented and community-based AI education to serve as starting points.},
booktitle = {Proceedings of the 2024 on RESPECT Annual Conference},
pages = {94–99},
numpages = {6},
keywords = {AI education, community-based, data sovereignty, informal education, justice-oriented},
location = {Atlanta, GA, USA},
series = {RESPECT 2024}
}

@inproceedings{10.1145/3649405.3659536,
author = {\v{R}echt\'{a}\'{c}kov\'{a}, Anna},
title = {Developing Automatic Methods for Teaching Code Quality in Introductory Programming},
year = {2024},
isbn = {9798400706035},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649405.3659536},
doi = {10.1145/3649405.3659536},
abstract = {Teaching code quality through manual code reviews scales poorly. Existing automated tools still miss relevant code quality defects and not all defects they report are relevant; they are also sometimes hard to adopt. The goal of my dissertation will be to identify relevant defects, develop new precise detectors for them and integrate those into an open-source automatic tool. This will improve the quality and availability of automatic code quality feedback.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 2},
pages = {826–827},
numpages = {2},
keywords = {automated feedback, code quality, novice programmers, python, teaching},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@article{10.1145/3638563,
author = {Liu, Yutong and Xiang, Qiao and Chen, Juan and Zhang, Ming and Xu, Jingdong and Luo, Yuan},
title = {Undergraduate Computer Science Education in China},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {2153-2184},
url = {https://doi.org/10.1145/3638563},
doi = {10.1145/3638563},
journal = {ACM Inroads},
month = feb,
pages = {28–36},
numpages = {9}
}

@inproceedings{10.1145/3649217.3653628,
author = {Ehlers, Jens},
title = {Teaching Multiple Data Models and Query Languages},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653628},
doi = {10.1145/3649217.3653628},
abstract = {In this paper we present a method and a tool to integrate multiple data models (relational, document-oriented, graph-based) and the associated query languages into a database course. After introductory lectures, students can work asynchronously and remotely over several weeks of self-study. Supportive practice groups are optional. Students solve queries in SQL, MongoDB Aggregation Pipelines and Cypher, receiving immediate automated feedback on the correctness of their solution. As a gamification element, they can compete with others in terms of a score achieved so far. The selected sample database contains the same data for all used database systems (DBS), but is structured differently due to the heterogeneous models. An essential learning goal for the students is to recognize that there are similar constructs and partly the same keywords in the query languages used for mapping the generic algebraic operations (projection, filter, join, grouping, sorting and other set operations). In a case study, method and tool are applied to two groups of students, with the first group consisting of 117 Computer Science students from a German university and the second group consisting of 119 students from other majors within the same university. For each query, the students have been asked for qualified feedback in the tool - on the difficulty, the comprehensibility, the time required and possibly why they did not solve a query. The employed tool is available as a demo at https://dbql.dev.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {234–240},
numpages = {7},
keywords = {data systems education, database design and models, query languages, student assessment},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3649217.3653642,
author = {Vykopal, Jan and \v{C}eleda, Pavel and \v{S}v\'{a}bensk\'{y}, Valdemar and Hofbauer, Martin and Hor\'{a}k, Martin},
title = {Research and Practice of Delivering Tabletop Exercises},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653642},
doi = {10.1145/3649217.3653642},
abstract = {Tabletop exercises are used to train personnel in the efficient mitigation and resolution of incidents. They are applied in practice to support the preparedness of organizations and to highlight inefficient processes. Since tabletop exercises train competencies required in the workplace, they have been introduced into computing courses at universities as an innovation, especially within cybersecurity curricula. To help computing educators adopt this innovative method, we survey academic publications that deal with tabletop exercises. From 140 papers we identified and examined, we selected 14 papers for a detailed review. The results show that the existing research deals predominantly with exercises that follow a linear format and exercises that do not systematically collect data about trainees' learning. Computing education researchers can investigate novel approaches to instruction and assessment in the context of tabletop exercises to maximize the impact of this teaching method. Due to the relatively low number of published papers, the potential for future research is immense. Our review provides researchers, tool developers, and educators with an orientation in the area, a synthesis of trends, and implications for further work.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {220–226},
numpages = {7},
keywords = {cybersecurity, experiential learning, hands-on training, incident response, systematic literature review, tabletop exercise},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3641237.3691665,
author = {Bryson, Rachel and Cheek, Ryan and Clem, Sam and Stevens, Hannah},
title = {“You want me to teach what?!”: Emerging Technologies, Required Knowledge, and Pedagogical Practice},
year = {2024},
isbn = {9798400705199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641237.3691665},
doi = {10.1145/3641237.3691665},
abstract = {Being assigned to teach unfamiliar content is an experience that many technical communication (TC) instructors can relate to. In this experience report, four TC instructors describe their experiences teaching unfamiliar and emerging technology, providing takeaways for readers about how they might approach similar tasks, particularly when constrained by a lack of time and resources when teaching a class for the first time. The instructors had to adapt their pedagogical approaches to teach a range of technology and software. These approaches include building students’ visual and rhetorical literacy, diving deep into ethics, explicitly teaching students how to learn, and humbly showing vulnerabilities and gaps in knowledge to students. Readers can draw from the instructors’ experiences and lessons learned when faced with teaching unfamiliar and emerging technology.},
booktitle = {Proceedings of the 42nd ACM International Conference on Design of Communication},
pages = {173–177},
numpages = {5},
keywords = {Emerging technology, Pedagogy, Teaching technology, Technical literacy},
location = {Fairfax, VA, USA},
series = {SIGDOC '24}
}

@inproceedings{10.1145/3632620.3671117,
author = {Chatterjee, Amreeta and Choudhuri, Rudrajit and Sarkar, Mrinmoy and Chattopadhyay, Soumiki and Liu, Dylan and Hedaoo, Samarendra and Burnett, Margaret and Sarma, Anita},
title = {Debugging for Inclusivity in Online CS Courseware: Does it Work?},
year = {2024},
isbn = {9798400704758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632620.3671117},
doi = {10.1145/3632620.3671117},
abstract = {Online computer science (CS) courses have broadened access to CS education, yet inclusivity barriers persist for minoritized groups in these courses. One problem that recent research has shown is that often inclusivity biases (“inclusivity bugs”) lurk within the course materials themselves, disproportionately disadvantaging minoritized students. To address this issue, we investigated how a faculty member can use AID—an Automated Inclusivity Detector tool—to remove such inclusivity bugs from a large online CS1 (Intro CS) course and what is the impact of the resulting inclusivity fixes on the students’ experiences. To enable this evaluation, we first needed to (Bugs):&nbsp;investigate inclusivity challenges students face in 5 online CS courses; (Build):&nbsp;build decision rules to capture these challenges in courseware (“inclusivity bugs”) and implement them in the AID tool; (Faculty):&nbsp;investigate how the faculty member followed up on the inclusivity bugs that AID reported; and (Students):&nbsp;investigate how the faculty member’s changes impacted students’ experiences via a before-vs-after qualitative study with CS students. Our results from (Bugs) revealed 39 inclusivity challenges spanning courseware components from the syllabus to assignments. After implementing the rules in the tool (Build), our results from (Faculty) revealed how the faculty member treated AID more as a “peer” than an authority in deciding whether and how to fix the bugs. Finally, the study results with (Students) revealed that students found the after-fix courseware more approachable - feeling less overwhelmed and more in control in contrast to the before-fix version where they constantly felt overwhelmed, often seeking external assistance to understand course content.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1},
pages = {419–433},
numpages = {15},
keywords = {Automated Checker, GenderMag, Inclusivity Bugs, Online CS Education},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@inproceedings{10.1145/3636243.3636264,
author = {Brieven, G\'{e}raldine and Baum, Valentin and Donnet, Benoit},
title = {Tartare: Automatic Generation of C Pointer Statements and Feedback},
year = {2024},
isbn = {9798400716195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636243.3636264},
doi = {10.1145/3636243.3636264},
abstract = {This paper addresses the difficulties students face when learning and practicing pointers (i.e., variables storing the memory address of another variable as its value) in a computer programming class. To improve their understanding and practice, we have developed Tartare, an automatic C pointer statement and feedback generator. By creating statements with automatic feedback, students are given the opportunity to practice at will, each time on a different instance. In addition, if the statement must be done remotely and accounts in the final grade, Tartare discourages academic dishonesty since each student faces their own statement to solve. This paper describes the techniques implemented in Tartare, relying on a pattern template-based approach. The statement variety of Tartare is evaluated. Finally, current limitations and further improvements are discussed. We believe our approach for Tartare can be transposed for automatic exercises generation in various other fields.},
booktitle = {Proceedings of the 26th Australasian Computing Education Conference},
pages = {192–201},
numpages = {10},
keywords = {C programming language, Tartare, pointers, statement generation},
location = {Sydney, NSW, Australia},
series = {ACE '24}
}

@proceedings{10.1145/3576123,
title = {ACE '23: Proceedings of the 25th Australasian Computing Education Conference},
year = {2023},
isbn = {9781450399418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1145/3568813.3600130,
author = {Koutcheme, Charles and Sarsa, Sami and Leinonen, Juho and Haaranen, Lassi and Hellas, Arto},
title = {Evaluating Distance Measures for Program Repair},
year = {2023},
isbn = {9781450399760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568813.3600130},
doi = {10.1145/3568813.3600130},
abstract = {Background and Context: Struggling with programming assignments while learning to program is a common phenomenon in programming courses around the world. Supporting struggling students is a common theme in Computing Education Research (CER), where a wide variety of support methods have been created and evaluated. An important stream of research here focuses on program repair, where methods for automatically fixing erroneous code are used for supporting students as they debug their code. Work in this area has so far assessed the performance of the methods by evaluating the closeness of the proposed fixes to the original erroneous code. The evaluations have mainly relied on the use of edit distance measures such as the sequence edit distance and there is a lack of research on which distance measure is the most appropriate. Objectives: Provide insight into measures for quantifying the distance between erroneous code written by a student and a proposed change. We conduct the evaluation in an introductory programming context, where insight into the distance measures can provide help in choosing a suitable metric that can inform which fixes should be suggested to novices. Method: A team of five experts annotated a subset of the Dublin dataset, creating solutions for over a thousand erroneous programs written by students. We evaluated how the prominent edit distance measures from the CER literature compare against measures used in Natural Language Processing (NLP) tasks for retrieving the experts’ solutions from a pool of proposed solutions. We also evaluated how the expert-generated solutions compare against the solutions proposed by common program repair algorithms. The annotated dataset and the evaluation code are published as part of the work. Findings: Our results highlight that the ROUGE score, classically used for evaluating the performance of machine summarization tasks, performs well as an evaluation and selection metric for program repair. We also highlight the practical utility of NLP metrics, which allow an easier interpretation and comparison of the performance of repair techniques when compared to the classic methods used in the CER literature. Implications: Our study highlights the variety of distance metrics used for comparing source codes. We find issues with the classically used distance measures that can be combated by using NLP metrics. Based on our findings, we recommend including NLP metrics, and in particular, the ROUGE metric, in evaluations when considering new program repair methodologies. We also suggest incorporating NLP metrics into other areas where source codes are compared, including plagiarism detection.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1},
pages = {495–507},
numpages = {13},
keywords = {BLEU, ROUGE, automated program repair, automatic program repair, bug fixing, computing education, dataset, distance measures, distance metrics, educational data mining, feedback, natural language processing, program repair},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@article{10.1145/3732788,
author = {Lottridge, Danielle and Dimalen, Davis and Weber, Gerald},
title = {An Automated Marker for Computer-Human Interaction “MarCHIr”: Assessment of Creative Web Prototypes},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3732788},
doi = {10.1145/3732788},
abstract = {Automated assessment is well-established within computer science courses but largely absent from human-computer interaction courses. Automating the assessment of human-computer interaction (HCI) is challenging because the coursework tends not to be computational but rather highly creative, such as designing and implementing interactive prototypes. We meet this challenge by developing an automarker for HCI “MarCHIr” to assess key aspects of web prototypes: visual design, interactivity, and accessibility. MarCHIr automatically compiles web prototypes and analyses them at the pixel level to assess designs using foundational Gestalt visual principles. While computer science student assessments are often personalised with permutations of numbers, we use random assignment of colours to personalise creative prototype assignments at the scale of hundreds of students. Finally, MarCHIr integrates industry accessibility checks to HCI student assessment. We share two years of case study data across two cohorts of university students and reflect on implications for the use of automated assessment within HCI tertiary education.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Educ.},
month = may
}

@inproceedings{10.1145/3639478.3639804,
author = {Mei\ss{}ner, Niklas},
title = {MEITREX - Gamified and Adaptive Intelligent Tutoring in Software Engineering Education},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639804},
doi = {10.1145/3639478.3639804},
abstract = {Nowadays, learning management systems (LMSs) are established tools in higher education, especially in the domain of software engineering (SE). However, the potential of such educational technologies has not been fully exploited, as student performance in SE education is still strongly dependent on feedback from time-constrained lecturers and tutors. Moreover, current LMSs are not designed for SE courses, as external SE tools are required to fulfill the requirements of lecturers such as programming and UML modeling features. Evolving these LMSs in the direction of intelligent tutoring could assist students in receiving automatic, individual feedback from the LMSs on their learning performance at any time. Also, gamified learning elements can serve to motivate students to engage with SE materials. Therefore, this paper presents an approach combining learning analytics, feedback, and interactive learning such as gamification in one LMS designed for SE education. The system could thus address diverse students with different backgrounds and motivational aspects and provide appropriate individual support to ensure effective SE education.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {198–200},
numpages = {3},
keywords = {software engineering education, student motivation, intelligent tutoring system, learning analytics, gamification, feedback},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@inproceedings{10.1145/3641554.3701832,
author = {Koitz-Hristov, Roxane and Mandl, Franz and Wotawa, Franz},
title = {VisOpt - Visualization of Compiler Optimizations for Computer Science Education},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701832},
doi = {10.1145/3641554.3701832},
abstract = {Visualizations in teaching have become a common practice as they effectively convey theoretical concepts. Compiler construction, a heavily theory-based subject in computer science education, is particularly challenging for students to understand. While many tools simulate a compiler's front end, or analysis phase, applications that focus on the back end, or synthesis phase, are scarce. This paper describes VisOpt, a web-based visualization tool designed for a master's level Compiler Construction course. VisOptfocuses on the synthesis phase, i.e., code optimization and code generation. Its primary objective is to help students comprehend various local compiler optimizations, which can be visualized on the original code, an intermediate representation, or an assembler-like target code. A quasi-experiment with a pre-test-post-test design revealed that students who used VisOpt reported higher self-efficacy compared to those who did not. Although no significant improvement in learning outcomes was observed overall, we propose VisOpt as an engaging pedagogical tool that effectively complements traditional methods for teaching the synthesis phase of compilers.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {603–609},
numpages = {7},
keywords = {compiler optimization, compiler visualization, computer science education, simulation software, visualization},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@proceedings{10.1145/3678392,
title = {ICFET '24: Proceedings of the 2024 10th International Conference on Frontiers of Educational Technologies},
year = {2024},
isbn = {9798400717123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Malacca, Malaysia}
}

@inproceedings{10.1145/3617694.3623223,
author = {Feffer, Michael and Martelaro, Nikolas and Heidari, Hoda},
title = {The AI Incident Database as an Educational Tool to Raise Awareness of AI Harms: A Classroom Exploration of Efficacy, Limitations, &amp; Future Improvements},
year = {2023},
isbn = {9798400703812},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617694.3623223},
doi = {10.1145/3617694.3623223},
abstract = {Prior work has established the importance of integrating AI ethics topics into computer and data sciences curricula. We provide evidence suggesting that one of the critical objectives of AI Ethics education must be to raise awareness of AI harms. While there are various sources to learn about such harms, The AI Incident Database (AIID) is one of the few attempts at offering a relatively comprehensive database indexing prior instances of harms or near harms stemming from the deployment of AI technologies in the real world. This study assesses the effectiveness of AIID as an educational tool to raise awareness regarding the prevalence and severity of AI harms in socially high-stakes domains. We present findings obtained through a classroom study conducted at an R1 institution as part of a course focused on the societal and ethical considerations around AI and ML. Our qualitative findings characterize students’ initial perceptions of core topics in AI ethics and their desire to close the educational gap between their technical skills and their ability to think systematically about ethical and societal aspects of their work. We find that interacting with the database helps students better understand the magnitude and severity of AI harms and instills in them a sense of urgency around (a) designing functional and safe AI and (b) strengthening governance and accountability mechanisms. Finally, we compile students’ feedback about the tool and our class activity into actionable recommendations for the database development team and the broader community to improve awareness of AI harms in AI ethics education.},
booktitle = {Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {3},
numpages = {11},
keywords = {AI harms, AI safety, classroom exploration, educational tool, incident database},
location = {Boston, MA, USA},
series = {EAAMO '23}
}

@inproceedings{10.1145/3626253.3631656,
author = {Grover, Shuchi and Fields, Deborah and Kafai, Yasmin and White, Shana and Strickland, Carla},
title = {Enduring Lessons from 'Computer Science for All' for AI Education in Schools},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3631656},
doi = {10.1145/3626253.3631656},
abstract = {Even as efforts to promote K-12 CS education forge ahead, there is a growing consensus that students must also be taught artificial intelligence (AI) and machine learning (ML) in order to be prepared for the fast-changing world powered by AI/ML. How can ensure that we leverage learnings from two decades of CS education research and practice, and build on successes while mitigating missteps? This panel invites researchers with deep expertise in 'CSForAll' efforts for a timely discussion and sharing of valuable lessons from CS education efforts about pedagogies, attention to equity, and teacher preparation that will also benefit K-12 AI education.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1533–1534},
numpages = {2},
keywords = {artificial intelligence, computational thinking, equity, k-12 ai education, k-12 cs education, machine learning, project-based learning, teacher preparation},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3706598.3713368,
author = {Troiano, Giovanni M and Cassidy, Michael and Morales, Daniel Escobar and Pons, Guillermo and Abdollahi, Amir and Robles, Gregorio and Puttick, Gillian and Harteveld, Casper},
title = {CT4ALL: Towards Putting Teachers in the Loop to Advance Automated Computational Thinking Metric Assessments in Game-Based Learning},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713368},
doi = {10.1145/3706598.3713368},
abstract = {Computational thinking (CT) is essential for the 21st century learner. Yet, assessing CT remains challenging. This is particularly challenging in constructionist learning, where individual idiosyncrasies may clash with one-size-fits-all assessments. Tools like Dr. Scratch offer CT metrics that show promise for effective and scalable CT assessments, particularly in constructionist game-based learning (GBL). Prior work has advanced the design of automated CT metrics but hardly included teachers in the process. We extend Dr. Scratch to improve automated CT assessments for GBL and put teachers in the loop to assess its novel features. Specifically, we interviewed seven middle school teachers employing GBL in STEM curricula and asked them to provide feedback on the newly designed CT metrics. Teachers view the new CT metrics positively, underscoring their potential for adaptive CT assessments despite hindrances. We advance automated CT assessments via teacher evaluation toward design-sensitive CT metrics and CT for all.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {124},
numpages = {23},
keywords = {Computational thinking, automated metrics, computer science education, STEM, assessment},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3701625.3701627,
author = {Cruz, Dhyego Tavares and Almeida, Erlon Pereira and Santos, Jander Pereira and Paix\~{a}o, Felipe de Sant’Anna and de Santana, Enio Garcia and Gomes e Souza, Rodrigo Rocha and Iwamoto, H\'{e}rsio Massanori and Dur\~{a}o, Frederico Ara\'{u}jo and Serafim Prazeres, C\'{a}ssio Vinicius and Machado, Ivan do Carmo and Figueiredo, Gustavo Bittencourt and Maciel Peixoto, Maycon Leone and de Almeida, Eduardo Santana},
title = {Software Development Practices and Tools for University-Industry R&amp;D projects},
year = {2024},
isbn = {9798400717772},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701625.3701627},
doi = {10.1145/3701625.3701627},
abstract = {Research and development (R&amp;D) projects involving universities and industry drive innovation by bringing scientific knowledge closer to practical problems. Nevertheless, the partnership comes with challenges, such as high developer turnover, team members with diverse backgrounds and experience levels, and part-time contributors. In this paper, we report experiences in a large R&amp;D project in the smart home field, in which we proposed software development practices and tools with the goal of promoting short feedback cycles and dissemination of best practices. By surveying the developers of this project, we discovered that the practices and tools were generally well accepted and also determined specific areas that need improvement. The insights collected in this study can be used by other teams conducting R&amp;D projects.},
booktitle = {Proceedings of the XXIII Brazilian Symposium on Software Quality},
pages = {426–437},
numpages = {12},
keywords = {Software Engineering, R&amp;D, Artificial Intelligence, IA},
location = {
},
series = {SBQS '24}
}

@inproceedings{10.1145/3636555.3636937,
author = {Booth, Brandon M. and Jacobs, Jennifer and Bush, Jeffrey B. and Milne, Brent and Fischaber, Tom and DMello, Sidney K.},
title = {Human-tutor Coaching Technology (HTCT): Automated Discourse Analytics in a Coached Tutoring Model},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636937},
doi = {10.1145/3636555.3636937},
abstract = {High-dosage tutoring has become an effective strategy for bolstering K-12 academic performance and combating education declines accelerated by the COVID-19 pandemic. To achieve high-dosage tutoring at scale, tutoring programs often rely on paraprofessional tutors—recruited tutors with college degrees who lack formal training in education—however, these tutors may require consistent and targeted feedback from instructional coaches for improvement. Accordingly, we developed a human-tutor coaching technology (HTCT) system to automatically extract discourse analytics pertaining to accountable talk moves (or academically productive talk) from tutoring sessions and provide feedback visualizations to coaches to aid their coaching sessions with tutors. We deployed HTCT in a user study using a virtual tutoring platform with 11 real coaches, 40 tutors, and their students to investigate coaches’ usage patterns with HTCT, perceptions of its utility, and changes in tutors’ talk. Overall, we found that coaches had positive perceptions of the system. We also observed an increase in accountable talk from tutors whose coaches used HTCT compared to tutors whose coaches did not. We discuss implications for AI-based applications which offer coaches a promising way to provide personalized, automated, and data-driven feedback to scale high-dosage tutoring.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {725–735},
numpages = {11},
keywords = {coached tutoring, discourse analytics, in situ user study, natural language processing},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3626253.3635593,
author = {Butler, Zack and Bez\'{a}kov\'{a}, Ivona and Xu, Shaoxuan and Brilliantova, Angelina},
title = {Analyzing Student and Instructor Comments using NLP},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635593},
doi = {10.1145/3626253.3635593},
abstract = {We report on our experience using common natural language processing (NLP) tools to analyze two vastly different data sets of free-form responses collected during a study of assignments in introductory computing courses. Our first data set consists of typically short comments left by hundreds of students on assignment surveys. Our second data set is comprised of semi-structured individual interviews of eight instructors of up to an hour long each. We collected the data across several years as part of our investigation of the use of pencil puzzles as a context for introductory computer science. In an earlier work, we manually analyzed a fraction of the student comments (all data collected until that point), using grounded theory. The results were illuminating, but the process was very time consuming, consisting of manual assignment of a small number of codes to each comment. In this work, we investigate the usability of common NLP tools to speed up the process for the entire data set of student comments. We also applied these tools to the instructor interviews. The NLP tools do not appear to be effective to create the code base, but, once the code base was determined, they performed the actual coding (assignment of codes to each student comment) promisingly well. For the long-form instructor interviews, the situation was much more challenging, due to the wide-ranging nature of semi-structured interviews, interleaving discussion topics, and elements of natural speech. We report on the lessons learned while automatically analyzing these complex data sets.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1584–1585},
numpages = {2},
keywords = {instructor interviews, manual qualitative analysis, natural language processing, student survey comments},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3653666.3656092,
author = {Levitt, Diane and Ray, Meg},
title = {Ecosystems That Build Equitable, K-5 Sustainable Computer Science Education},
year = {2024},
isbn = {9798400706264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653666.3656092},
doi = {10.1145/3653666.3656092},
abstract = {The rollout of computer science education has been dependent on a patchwork of uncoordinated professional learning experiences. This has left some schools serving students from underrepresented groups without an articulated, rigorous, joyful K-12 CS education. Based on our work with four urban schools serving such students, we propose that an ecosystem of support that prepares every administrator and teacher to include CS in every student's education with a whole school approach and sustained professional learning, is one way to assure an equitable, sustainable CS education. We propose changes in policy to scaffold such an ecosystem.},
booktitle = {Proceedings of the 2024 on RESPECT Annual Conference},
pages = {254–258},
numpages = {5},
keywords = {K-5 computer science education, computational agency, computational thinking, content coaching, equity, justice-centered computing, teacher professional development},
location = {Atlanta, GA, USA},
series = {RESPECT 2024}
}

@inproceedings{10.1145/3649217.3653637,
author = {Kiesler, Natalie and Opel, Simone and Thorbr\"{u}gge, Carsten},
title = {With Great Power Comes Great Responsibility - Integrating Data Ethics into Computing Education},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653637},
doi = {10.1145/3649217.3653637},
abstract = {Most computing students enter the industry once they graduate. As future software engineers, they will be in powerful positions, making decisions that impact their personal lives, others, and society. Thus, preparing graduates for their careers is crucial by addressing ethical considerations, decision problems, and other concepts related to morals, values, and legal aspects (e.g., data protection, privacy, security, etc.) as part of computing curricula. In this paper, we propose the integration of data ethics into computing programs and provide a framework for an ethics module, including relevant competency-based learning objectives. The proposed module is based on a curricular analysis of all 71 German data science degree programs focusing on ethics courses. The course contents and competency goals were analyzed and classified based on their cognitive complexity. As the results proved the lack of competency-based learning outcomes, we designed observable competency goals, meaning knowledge, skills, and dispositions taken in the context of a task. In addition, we provide suggestions for contents, pedagogical instructions, and assessments in such a course. The proposed module serves as a first draft and resource to support other educators aiming to design such a course and who are willing to integrate it into computing curricula.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {471–477},
numpages = {7},
keywords = {competencies, computing, curricula, data science, ethics, higher education, moral, values},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3626252.3630966,
author = {Zuckerman, Austin L. and Juavinett, Ashley L.},
title = {When Coding Meets Biology: The Tension Between Access and Authenticity in a Contextualized Coding Class},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630966},
doi = {10.1145/3626252.3630966},
abstract = {As programming skills become more demanded in fields outside of computer science, we need to consider how we should be teaching these skills to our students. One option is to encourage students to pursue introductory computer science courses; however, these courses are often geared towards computer science (CS) majors and without important discipline-specific context. Other avenues include short coding modules within disciplinary courses or full courses that blend CS with another discipline. Guided by insights from an introductory CS course in the context of biology, we describe a key tension when coding meets biology: while contextualized programming classes are often perceived as more accessible, students may also view them as less authentic. Taken together, these observations point to specific recommendations for educators who choose to integrate coding and biology in this way. Ultimately, we conclude that discipline-specific programming education is essential to improve equity in computing education.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1491–1497},
numpages = {7},
keywords = {computing in biology, contextualized coding, cs1, equity &amp; inclusion},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.5555/3721488.3721635,
author = {Williams, Tom},
title = {Improvising Interaction: Toward Applied Improvisation Driven Social Robotics Theory and Education},
year = {2025},
publisher = {IEEE Press},
abstract = {Theater-based design methods are seeing increased use in social robotics, as embodied roleplay is an ideal method for designing embodied interactions. Yet theater-based design methods are often cast as simply one possible tool; there has been little consideration of the importance of specific improvisational skills for theater-based design; and there has been little consideration of how to train students in theater-based design methods.We argue that improvisation is not just one possible tool of social robot design, but is instead central to social robotics. Leveraging recent theoretical work on Applied Improvisation, we show how improvisational skills represent (1) a set of key capabilities needed for any socially interactive robot, (2) a set of learning objectives for training engineers in social robot design, and (3) a set of methodologies for training those engineers to engage in theater-based design methods.Accordingly, we argue for a reconceptualization of Social Robotics as an Applied Improvisation project; we present, as a speculative pedagogical artifact, a sample syllabus for an envisioned Applied Improvisation driven Social Robotics course that might give students the technical and improvisational skills necessary to be effective robot designers; and we present a case study in which Applied Improvisation methods were simultaneously used (a) by instructors, to rapidly scaffold engineering students' improvisational skills and (b) by those students, to engage in more effective human-robot interaction design.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1140–1148},
numpages = {9},
keywords = {applied improvisation, social robotics education, theater-based design},
location = {Melbourne, Australia},
series = {HRI '25}
}

@inproceedings{10.1145/3677619.3678114,
author = {Marx, Erik and Witt, Clemens and Leonhardt, Thiemo},
title = {Identifying Secondary School Students' Misconceptions about Machine Learning: An Interview Study},
year = {2024},
isbn = {9798400710056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677619.3678114},
doi = {10.1145/3677619.3678114},
abstract = {Since students are familiar with machine learning (ML)-based applications in their everyday lives, they already construct mental models of how these systems work. This can result in misconceptions that influence the learning of correct ML concepts. Therefore, this study investigates the misconceptions students hold about the functionality of ML-based applications. To this end, we conducted semi-structured interviews with five students, focusing on their understanding of facial recognition and ChatGPT. The interviews were analyzed using an inductively developed code system and qualitative content analysis. This process identified six key misconceptions held by students: “Programmed Behavior,” “Exactness,” “Data Storage,” “Continuous Learning,” “User-trained Model,” and “Autonomous Data Acquisition”. These misconceptions include the notion that AI learns continuously during application, or that training data is saved and reused later. This paper presents the identified misconceptions and discusses their implication for the design and evaluation of effective learning activities in the context of ML.},
booktitle = {Proceedings of the 19th WiPSCE Conference on Primary and Secondary Computing Education Research},
articleno = {6},
numpages = {10},
keywords = {artificial intelligence, interview study, machine learning, mental models, misconceptions, qualitative research, students conceptions},
location = {Munich, Germany},
series = {WiPSCE '24}
}

@inproceedings{10.1145/3587102.3588787,
author = {Lehtinen, Teemu and Sepp\"{a}l\"{a}, Otto and Korhonen, Ari},
title = {Automated Questions About Learners' Own Code Help to Detect Fragile Prerequisite Knowledge},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588787},
doi = {10.1145/3587102.3588787},
abstract = {Students are able to produce correctly functioning program code even though they have a fragile understanding of how it actually works. Questions derived automatically from individual exercise submissions (QLC) can probe if and how well the students understand the structure and logic of the code they just created. Prior research studied this approach in the context of the first programming course. We replicate the study on a follow-up programming course for engineering students which contains a recap of general concepts in CS1. The task was the classic rainfall problem which was solved by 90% of the students. The QLCs generated from each passing submission were kept intentionally simple, yet 27% of the students failed in at least one of them. Students who struggled with questions about their own program logic had a lower median for overall course points than students who answered correctly.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {505–511},
numpages = {7},
keywords = {QLC, online education, prerequisite knowledge, program comprehension},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3610969.3611115,
author = {Devlin, Marie},
title = {Subject to Change: A Computing Education Research Journey},
year = {2023},
isbn = {9798400708763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610969.3611115},
doi = {10.1145/3610969.3611115},
abstract = {The Computing discipline evolves constantly and therefore so does the environment where we conduct Computing Education Research (CER). Sometimes the terrain can be quite perilous, but you have to persist because ours is a subject that needs to change, especially pedagogically. In this talk I give a brief overview of my journey to become a researcher in Computing Education and outline some of the work I have done at Newcastle and its impact. I give a brief overview of some of the funded projects I became involved in and how my interest in Computing Education evolved into the Educational Practice In Computing research group at Newcastle (EPiC). I give some tips and advice to people new to Computing Education Research, based on my experience of navigating the landscape over the years (without a map), and then outline my next steps.},
booktitle = {Proceedings of the 2023 Conference on United Kingdom &amp; Ireland Computing Education Research},
articleno = {1},
numpages = {2},
location = {Swansea, Wales Uk},
series = {UKICER '23}
}

@inproceedings{10.1145/3689535.3689563,
author = {Waite, Jane},
title = {Some theories from abroad for AI interaction literacy.},
year = {2024},
isbn = {9798400711770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689535.3689563},
doi = {10.1145/3689535.3689563},
abstract = {I have become rather ‘het up’ about the use of AI applications in teaching and learning. I am worried that the digital divide will widen rather than narrow with the increasing use of this technology. A question that bothers me is, "Why are some of our students better at using the output from AI applications than others?", and what can we do about this? I want to get us all thinking and talking about this issue. In this keynote, I will be rather self-indulgent and share my two favourite theories from ‘abroad’, as they are from general education and sociology. Also, I have presented about these theories ‘abroad’ at various international conferences and can show some related photos at the keynote. I propose that the two theories, the Semantics dimension of LCT and feedback literacy, can help us explore and think more deeply about AI interaction literacy.},
booktitle = {Proceedings of the 2024 Conference on United Kingdom &amp; Ireland Computing Education Research},
articleno = {1},
numpages = {3},
keywords = {AI education, AI interaction literacy, K-12 education},
location = {Manchester, United Kingdom},
series = {UKICER '24}
}

@inproceedings{10.1145/3649217.3653576,
author = {Engineer, Rutwa and Sibia, Naaz and Kaler, Michael and Simion, Bogdan and Zhang, Lisa},
title = {Early Computer Science Students' Perspectives Towards The Importance Of Writing},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653576},
doi = {10.1145/3649217.3653576},
abstract = {Faculty and industry practitioners recognize written communication to be important in computer science, but it can be challenging to convince students of the same. As student perceptions are molded early in a program of study, we focus on early-year CS students to understand their perceptions towards the importance of writing in CS, with the goal of framing discipline-specific writing pedagogy. We qualitatively analyze responses from first and second-year CS students in a survey about the role of writing in their field. The responses reveal that a majority view writing as an indispensable skill. Specifically, students recognize it as a fundamental skill, applicable across diverse contexts, and uniquely relevant in CS compared to other fields. We identified 4 perceptions that they hold which are helpful to their development as writers: that writing is a useful fundamental skill, which is useful for achieving various goals in a variety of contexts, and that writing in CS is different than in other fields. However, 20% of responses include reasons why writing is not important in CS, and we identify 4 perceptions harmful to students' development as writers: that writing skills can be avoided, are defined narrowly, do not need to be developed beyond a baseline, and come at the cost of computing skills. We believe that there is an opportunity to align discipline-specific writing instruction with these useful and harmful perceptions.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {332–338},
numpages = {7},
keywords = {computer science education, curriculum, technical writing skills, wac, wid, written communication, wtl},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@article{10.1145/3665932,
author = {D\"{o}rpinghaus, Jens and Binnewitt, Johanna and Samray, David and Hein, Kristine},
title = {Understanding Informatics in Continuing Vocational Education and Training Data in Germany},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
url = {https://doi.org/10.1145/3665932},
doi = {10.1145/3665932},
abstract = {Objectives. The purpose of this study is to reveal the importance of informatics in continuing vocational education in Germany. The labour market is a field with diverse data structures and multiple applications, for example connecting jobseekers and trainings or jobs. The labour market heavily relies on vocational education and training and advanced vocational qualification to meet challenges, e.g., digitalization. Study Methods. Since continuing vocational education and training (CVET) is a structurally important lever for the digital transformation of work, this article presents a methodological procedure for content analysis that provides information about the significance of computer science in unregulated continuing education offerings and in formal continuing education regulations. Findings. The question of the extent to which continuing education programs include informaticss topics is investigated, assuming that they can be found in continuing education as cross-cutting topics in a wide variety of thematic contexts. Our results indicating the need for training in computing education. At the same time, computing education offers the highest share of unregulated CVET programs. This could reflect the fact that training and further education regulations in Germany are designed open to technology. Conclusions. We present a novel and unique approach to analyze the importance of informatics and digitalization in CVET advertisements and official regulations for the same.},
journal = {ACM Trans. Comput. Educ.},
month = aug,
articleno = {36},
numpages = {22},
keywords = {Labor market research, CVET advertisements, educational data mining}
}

@proceedings{10.1145/3629296,
title = {ICETC '23: Proceedings of the 15th International Conference on Education Technology and Computers},
year = {2023},
isbn = {9798400709111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Barcelona, Spain}
}

@inproceedings{10.1145/3626252.3630761,
author = {Mason, Raina and Simon and Becker, Brett A. and Crick, Tom and Davenport, James H.},
title = {A Global Survey of Introductory Programming Courses},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630761},
doi = {10.1145/3626252.3630761},
abstract = {We present results of an in-depth survey of nearly 100 introductory programming (CS1) instructors in 18 countries spanning six continents. Although CS1 is well studied, relatively few broadly-scoped studies have been conducted, and none prior have exceeded regional scale. In addition, CS1 is a notoriously fickle and often changing course, and many might find it beneficial to know what other instructors are doing across the globe; perhaps more so as we continue to understand the impact of the COVID-19 pandemic on computing education and as the effects of Generative AI take hold. Expanding upon several surveys conducted in Australasia, the UK, and Ireland, this survey facilitates a direct comparison of global trends in CS1. The survey goes beyond environmental factors such as languages used, and examines why CS1 instructors teach what they do, in the ways they do. In total the survey spans 84 institutions and 91 courses in which a total of over 40,000 students are enrolled.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {799–805},
numpages = {7},
keywords = {covid-19, cs 1, cs-1, cs1, global, instructors, introductory programming, novice programmers, programming languages, survey, teaching languages},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@article{10.1145/3688397,
author = {Payton, Ryan L.},
title = {Awareness, Education, and Adoption of Cloud Computing for Academic Research},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {2153-2184},
url = {https://doi.org/10.1145/3688397},
doi = {10.1145/3688397},
journal = {ACM Inroads},
month = nov,
pages = {82–91},
numpages = {10}
}

@inproceedings{10.1145/3626253.3635545,
author = {Skripchuk, James and Bacher, John and Shi, Yang and Tran, Keith and Price, Thomas},
title = {Novices' Perceptions of Web-Search and AI for Programming},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635545},
doi = {10.1145/3626253.3635545},
abstract = {External help resources are frequently used by novice programmers solving classwork in undergraduate computing courses. Traditionally, these tools consisted of web-based resources such as tutorial websites and Q&amp;A forums. With the rise of AI code-generation and explanation tools, understanding how students use external resources and their roles in classroom have become especially relevant. Despite this, little research has directly investigated the extent to which students intent to use these tools and what factors influence their beliefs. It is unknown when students think it is appropriate to use these tools and what features they find valuable. Understanding these beliefs would allow instructors and researchers to better focus their efforts on what aspects of pedagogy and tool usage should be addressed. We administered a pilot vignette-style survey to introductory programming classes at an R1 University (n=45), giving students scenarios of external resource usage while questioning their attitudes, subjective norms, and their perceived behavioral control on using these external resources. We share preliminary findings on free response data, showcasing the variety of beliefs and opinions that novice programming students have on when and how much external resource usage is acceptable in the classroom. Some students felt that AI tools can provide more exact solutions than searching for help online, but also expressed that this exactness could be detrimental to their learning. Others expressed awareness that professionals use these resources, and expressed a desire to learn how to use them in a way to help their educational and career goals.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1818–1819},
numpages = {2},
keywords = {ai coding tools, cs education, help-seeking, student perspectives, web-search},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3641554.3701783,
author = {Qin, Meiying},
title = {Approachable Machine Learning Education: A Spiral Pedagogy Approach with Experiential Learning},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701783},
doi = {10.1145/3641554.3701783},
abstract = {Machine learning (ML) is an important subject for computer science students to learn due to its broad applications. Introductory courses often present techniques in a linear sequence, resulting in a steep learning curve that can overwhelm students and limit the time for experiential learning through course projects. To address this, I restructured the course using a spiral approach, presenting concepts in three iterations. Each iteration delves deeper into the material and introduces complex computational topics progressively. This method includes a built-in repetition mechanism that reinforces learning and enhances understanding. Moreover, this approach allows time for hands-on projects that apply theory to real-world scenarios, helping students better understand the course materials. The spiral approach was implemented in an ML course at a local university, resulting in positive student feedback and improved course retention rates.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {924–930},
numpages = {7},
keywords = {computer science education, experiential learning, machine learning, spiral approach},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.1145/3702231,
author = {ter Beek, Maurice and Broy, Manfred and Dongol, Brijesh},
title = {The Role of Formal Methods in Computer Science Education},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {2153-2184},
url = {https://doi.org/10.1145/3702231},
doi = {10.1145/3702231},
journal = {ACM Inroads},
month = nov,
pages = {58–66},
numpages = {9}
}

@inproceedings{10.1145/3641554.3701854,
author = {Ko, Shao-Heng and Chao, Alex and Pang, Violet},
title = {Satisfactory for All: Supporting Mastery Learning with Human-in-the-loop Assessments in a Discrete Math Course},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701854},
doi = {10.1145/3641554.3701854},
abstract = {This experience report documents an attempt at embracing the "A's for all" and equitable grading frameworks in an introductory, proof writing-based discrete mathematics course for computer science majors (with N=138 students) at a medium-sized research-oriented university in the US. Unlike in introductory programming contexts, there is so far no reliable automated grading system that gives formative and adaptive feedback supporting the scope of a proof-based discrete mathematics course. We therefore faced the unique challenge of being unable to automate all assessments and directly offer all students unlimited attempts toward mastery.To address this issue, we adopted a hybrid approach in designing our formative assessments. Using the Exemplary, Satisfactory, Not Yet, and Unassessable (ESNU) discrete grading model, we required all students to get a Satisfactory or above in every question in every assignment within two rounds of human feedback. Students not meeting the goal after two attempts then consulted with course staff members in one-on-one interactions to get diagnostic feedback at any time at their convenience until the semester ended.We document our course policy design in detail, then present data that summarizes both the grading outcomes and student sentiments. We also discuss the lessons learned from our initiative and the necessary staff-side management practices that support our design. This report outlines an example of adopting the A's for all and equitable grading framework in a course context where not all contents can be made autogradable.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {589–595},
numpages = {7},
keywords = {a's for all, discrete mathematics, equitable grading, mastery learning, specifications grading},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@proceedings{10.1145/3723890,
title = {CNSCT '25: Proceedings of the 2025 4th International Conference on Cryptography, Network Security and Communication Technology},
year = {2025},
isbn = {9798400712623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inbook{10.5555/3715982.C6017983,
title = {Instructional Guidance},
year = {2025},
isbn = {9798400714900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This curriculum project involved a review of the Cybersecurity Curricula 2017 (CSEC 2017) guidelines and recommendation of learning outcomes that support foundational collegiate-level courses and upperlevel high school cybersecurity offerings, comparable to Advanced Placement (AP) content. The primary goal of this document is to provide a foundational cybersecurity education framework that can be implemented by both post-secondary institutions and high schools. These guidelines aim to bridge the gap between secondary and post-secondary education in cybersecurity to support broad transferability across institutions and ensure continuity for students.},
booktitle = {Supplement to CSEC 2017: Foundational Cybersecurity Content and Instructional Guidance for Secondary and Postsecondary Cybersecurity Education}
}

@inproceedings{10.1145/3711403.3711448,
author = {Li, Linze and Wang, Cixiao and Chen, Jiaqi and Sun, Haozhi and Zhang, Hanbo},
title = {Artificial Intelligence Education Policy Analysis From International Perspective},
year = {2025},
isbn = {9798400717468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711403.3711448},
doi = {10.1145/3711403.3711448},
abstract = {With the development of the application of artificial intelligence technology in the field of education, countries and international organizations have introduced relevant policies to address the opportunities and challenges it brings to education. A thorough analysis of these policies on a global scale is important for guiding educational innovation and ensuring the effectiveness of technology applications. This article uses a content analysis method, taking 26 artificial intelligence education policies from major countries and international organizations as samples, and employs a "policy tool—policy content" two-dimensional analysis framework to quantitatively analyze the policy texts to reveal the characteristics of international artificial intelligence education policies. The study finds that the current types of policy tools are mainly supply-oriented and environmental-oriented, focusing more on dimensions such as quality assessment and assurance, resource environment construction, and future talent cultivation in terms of content elements. Supply-side tools provide resource support for the construction of intelligent educational environments and innovative cultivation services for the improvement of teachers' and students' intelligent literacy; environmental tools build institutional standards for the quality assurance of intelligent education and create organizational strategies to achieve inclusive and fair education. Finally, the study proposes suggestions for the formulation and optimization of artificial intelligence education policies from four aspects: strengthening talent cultivation, focusing on independent research and development, deepening international exchanges, and systematic policy design, to promote the high-quality development of teaching and education in the digital age.},
booktitle = {Proceedings of the 2024 7th International Conference on Educational Technology Management},
pages = {264–270},
numpages = {7},
keywords = {Artificial Intelligence Education, Content Analysis Method, Generative Artificial Intelligence, Policy Texts, Policy Tools},
location = {
},
series = {ICETM '24}
}

@inproceedings{10.1145/3568812.3603462,
author = {Ko, Shao-Heng},
title = {Characterizing Computing Students’ Academic Help-seeking Behavior},
year = {2023},
isbn = {9781450399753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568812.3603462},
doi = {10.1145/3568812.3603462},
abstract = {Academic help-seeking is a vital part of students’ self-regulated learning strategies. Computing students’ help-seeking horizon has seen several transformations in the past 15 years such that existing frameworks no longer capture current computing students’ learning environment, motivating a dedicated study on computing students’ academic help-seeking behavior. Building on extant works that focus on a single course or help source, my research investigates computing students’ academic help-seeking behavior across different contexts. By analyzing students’ help-seeking records, my research seeks to understand how and why computing students transition between available help resources while seeking help, as well as how this process changes in different contexts.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2},
pages = {73–75},
numpages = {3},
keywords = {Academic help-seeking},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3641554.3701870,
author = {Saligrama, Aditya and Ho, Cody and Tripp, Benjamin and Abbott, Michael and Kozyrakis, Christos},
title = {Teaching Cloud Infrastructure and Scalable Application Deployment in an Undergraduate Computer Science Program},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701870},
doi = {10.1145/3641554.3701870},
abstract = {Making successful use of cloud computing requires nuanced approaches to both system design and deployment methodology, involving reasoning about the elasticity, cost, and security models of cloud services. Building cloud-native applications without a firm understanding of the fundamentals of cloud engineering can leave students susceptible to cost and security pitfalls. Yet, cloud computing is not commonly taught at the undergraduate level. To address this gap, we designed an undergraduate-level course that frames cloud infrastructure deployment as a software engineering practice. Our course featured a number of hands-on assignments that gave students experience with modern, best-practice concepts and tools including infrastructure-as-code (IaC). We describe the design of the course, our experience teaching its initial offering, and provide our reflections on what worked well and potential areas for improvement. Our course material is available at https://infracourse.cloud.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1015–1021},
numpages = {7},
keywords = {application deployment, computing, computing education, infrastructure-as-code, scalability},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3689493.3689981,
author = {Van Praet, Lucas and Hoobergs, Jesse and Schrijvers, Tom},
title = {ASSIST: Automated Feedback Generation for Syntax and Logical Errors in Programming Exercises},
year = {2024},
isbn = {9798400712166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689493.3689981},
doi = {10.1145/3689493.3689981},
abstract = {Introductory programming courses often rely on numerous exercises to help students practice and reinforce their skills. Commonly used automated tests fall short by merely identifying the issues without offering guidance on how to resolve them and manual reviews are too resource-intensive to use in large classes. To address these challenges, we present ASSIST—a tool designed to provide automated, detailed feedback on how to resolve issues in programming exercise submissions with both syntactic and logical errors. ASSIST combines fault-tolerant parsing with fixes based on the context of error nodes to resolve syntactic errors and give feedback. ASSIST feeds this valid program to the Sketch program synthesis tool to determine the needed changes from a set of potential changes induced by rewrite rules, and generates feedback on logic errors based on the needed changes. This dual approach allows ASSIST to offer actionable feedback on both syntax and logic issues in student submissions. We evaluated ASSIST on submissions from an online platform for secondary education. Our findings reveal that, for submissions with syntax errors, ASSIST delivers feedback on all syntax errors in 71% of cases and extends its feedback to cover logical errors in 34% of these submissions. When evaluating all incorrect submissions, ASSIST is able to give feedback on logical errors in 64% of cases. These results indicate that ASSIST can significantly enhance the feedback process in large-scale programming courses, offering a feasible and efficient alternative to current methods.},
booktitle = {Proceedings of the 2024 ACM SIGPLAN International Symposium on SPLASH-E},
pages = {66–76},
numpages = {11},
keywords = {Automated Feedback, Computer Science Education, Program Repair},
location = {Pasadena, CA, USA},
series = {SPLASH-E '24}
}

@inproceedings{10.1145/3641554.3701875,
author = {Hao, Qiang and Liu, Ruohan},
title = {Towards Integrating Behavior-Driven Development in Mobile Development: An Experience Report},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701875},
doi = {10.1145/3641554.3701875},
abstract = {Testing is an important yet often neglected skill in learning and teaching of computing science at the college level. Prior studies explored integrating test-driven development (TDD) into computer science courses with some degree of success, but also observed issues such as students' lack of appreciation, expressed frustration, and inconsistent adherence to TDD. TDD is a software development methodology that emphasizes writing low-level unit test cases prior to writing the corresponding portion of implementation. Behavior-driven development (BDD) was proposed as an evolution of TDD to emphasize software behavior from users' perspective. BDD has been widely adopted in industry, and holds great potential in addressing the issues in using TDD to improve students' learning of testing. However, BDD was rarely explored in enhancing students' mastery of testing. Informed by the literature, this experience report explored the integration of BDD into a mobile development course. Students' performance, attitude and feedback on BDD was examined, and potential improvement on the integration of BDD was discussed. The results of this report sheds light on how to effectively integrate BDD into computer science courses.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {450–456},
numpages = {7},
keywords = {behavior-driven development, continuous integration, mobile development, project-based learning, software engineering education, test-driven development, testing},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3641554.3701861,
author = {Barkhuff, Grace and Pruitt, Ian and Namani, Vyshnavi and Johnson, William Gregory and Borela, Rodrigo and Zegura, Ellen and Bourgeois, Anu G. and Shapiro, Ben Rydal},
title = {Exploring the Humanistic Role of Computer Science Teaching Assistants across Diverse Institutions},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701861},
doi = {10.1145/3641554.3701861},
abstract = {Recently, there has been a growing interest in the role of teaching assistants (TAs) in computer science (CS). This interest is due to the vital role CS TAs play in supporting student learning and their expanding responsibilities driven by growing enrollments in CS programs worldwide. While much of this research focuses on the technical and pedagogical aspects of CS TAs' duties, researchers recognize the need to further explore the unique value human CS TAs provide, particularly with the rise of AI tools and assistants. In this paper, we use qualitative methods to analyze 109 survey responses collected across two different institutions in the United States as part of a larger design-based research project to make two contributions. First, we illustrate how CS TAs adopt humanistic stances and demonstrate care in their roles, thereby expanding prevailing understandings of CS TAs. Second, we detail similarities and differences across CS TAs' experiences at each institution that underscore the importance of understanding CS TAs as they are situated in different institutional contexts. We conclude by discussing implications of this work for computing instruction and TA training, emphasizing the importance of foregrounding the roles and values brought by TAs.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {67–73},
numpages = {7},
keywords = {cs education, pedagogy, qualitative methods, responsive pedagogy, ta training, tas, teaching assistants},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3641554.3701814,
author = {Kumar, Priya C. and Schulman, Jeffrey Samuel and Albargi, Fatimah and Bhattacharyya, Sree and Dong, Hongyi and Liu, Zehao},
title = {Ungrading as a Pedagogy for Teaching Qualitative Research Methods in Computing},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701814},
doi = {10.1145/3641554.3701814},
abstract = {The process of learning research methods is complex, often propelling students to shift their worldviews as they work to understand concepts and develop skills. We argue that a pedagogy of ungrading, which eschews points-based grading for feedback-based assessment, is well suited to helping students shift their worldviews because it centers engagement over measuring outcomes. We-a faculty instructor and five PhD students-present this experience report detailing the design and implementation of ungrading in a graduate qualitative research methods course in an informatics PhD program. We explain the course design and present self-reflections that illustrate how a pedagogy of ungrading facilitated meaningful learning experiences among students whose research interests span the epistemological and methodological spectrum.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {631–637},
numpages = {7},
keywords = {alternative assessment, alternative grading, doctoral students, graduate programs, qualitative research methods, ungrading},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.1145/3595291,
author = {Andriole, Stephen},
title = {Five Ways Executives Misunderstand Technology},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/3595291},
doi = {10.1145/3595291},
abstract = {How executives might better understand technology.},
journal = {Commun. ACM},
month = nov,
pages = {32–34},
numpages = {3}
}

@inproceedings{10.1145/3632621.3671415,
author = {Landesman, Rotem},
title = {Teens' Ethical Sensemaking About Emerging Technologies},
year = {2024},
isbn = {9798400704765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632621.3671415},
doi = {10.1145/3632621.3671415},
abstract = {Emerging technologies, among them generative AI, are continuously being integrated into the mundane fabric of young people’s lives and routines. Recently, scholars called to expand computing education beyond learning to use and create with technologies to think critically and ethically about their potential impacts as a means to encourage the development of a sense of computational empowerment. My research aims to explore this space and opportunities which encourage ethical thinking with youth - specifically adolescents - on and about generative AI, a recent emerging innovation. This exploration will take inspiration from previous work pointing to the efficacy of practices from the field of Philosophy for Children (P4C) as well as recent work pointing to the potential of eliciting ethical thinking through a critical reflection and making framework, and suggest a novel framework to elicit a sense of computational empowerment as youth grow up in our digital world.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 2},
pages = {557–559},
numpages = {3},
keywords = {computing education, ethics in computing, k-12},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@article{10.1145/3636515,
author = {Messer, Marcus and Brown, Neil C. C. and K\"{o}lling, Michael and Shi, Miaojing},
title = {Automated Grading and Feedback Tools for Programming Education: A Systematic Review},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
url = {https://doi.org/10.1145/3636515},
doi = {10.1145/3636515},
abstract = {We conducted a systematic literature review on automated grading and feedback tools for programming education. We analysed 121 research papers from 2017 to 2021 inclusive and categorised them based on skills assessed, approach, language paradigm, degree of automation, and evaluation techniques. Most papers assess the correctness of assignments in object-oriented languages. Typically, these tools use a dynamic technique, primarily unit testing, to provide grades and feedback to the students or static analysis techniques to compare a submission with a reference solution or with a set of correct student submissions. However, these techniques’ feedback is often limited to whether the unit tests have passed or failed, the expected and actual output, or how they differ from the reference solution. Furthermore, few tools assess the maintainability, readability, or documentation of the source code, with most using static analysis techniques, such as code quality metrics, in conjunction with grading correctness. Additionally, we found that most tools offered fully automated assessment to allow for near-instantaneous feedback and multiple resubmissions, which can increase student satisfaction and provide them with more opportunities to succeed. In terms of techniques used to evaluate the tools’ performance, most papers primarily use student surveys or compare the automatic assessment tools to grades or feedback provided by human graders. However, because the evaluation dataset is frequently unavailable, it is more difficult to reproduce results and compare tools to a collection of common assignments.},
journal = {ACM Trans. Comput. Educ.},
month = feb,
articleno = {10},
numpages = {43},
keywords = {Automated grading, feedback, assessment, computer science education, systematic literature review, automatic assessment tools}
}

@inproceedings{10.1145/3649217.3653552,
author = {Taipalus, Toni and Grahn, Hilkka},
title = {Building Blocks Towards More Effective SQL Error Messages},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653552},
doi = {10.1145/3649217.3653552},
abstract = {Reading and interpreting error messages are significant aspects of a software developer's work. Despite the importance and prevalence of error messages, especially for novices, SQL compiler error messages from various relational database management systems have seen limited development since their inception. This lack of progress may stem from the fact that it is not well-understood what constitutes an effective error message. With data from 568 participants across three student cohorts, we investigate whether novel SQL error message design guidelines can explain success in fixing SQL syntax errors. The results indicate that some of the guidelines indeed serve as building blocks toward more effective SQL error messages for novices. However, error messages that adhered to certain guidelines showed inconclusive or negative results. These findings can be applied to iterate on SQL error messages in SQL learning environments or SQL compilers.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {241–247},
numpages = {7},
keywords = {SQL, compiler, computing education, database, database management system, error, error message, error message design, human-computer interaction, novice, relational database},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@article{10.1145/3538534,
author = {Taylor, Jordan and Adjagbodjou, Adinawa},
title = {DEI in computing: centering the margins},
year = {2022},
issue_date = {Summer 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {1528-4972},
url = {https://doi.org/10.1145/3538534},
doi = {10.1145/3538534},
journal = {XRDS},
month = jul,
pages = {6–7},
numpages = {2}
}

@article{10.1145/3689374,
author = {ter Beek, Maurice H. and Chapman, Rod and Cleaveland, Rance and Garavel, Hubert and Gu, Rong and ter Horst, Ivo and Keiren, Jeroen J. A. and Lecomte, Thierry and Leuschel, Michael and Rozier, Kristin Yvonne and Sampaio, Augusto and Seceleanu, Cristina and Thomas, Martyn and Willemse, Tim A. C. and Zhang, Lijun},
title = {Formal Methods in Industry},
year = {2024},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {0934-5043},
url = {https://doi.org/10.1145/3689374},
doi = {10.1145/3689374},
abstract = {Formal methods encompass a wide choice of techniques and tools for the specification, development, analysis, and verification of software and hardware systems. Formal methods are widely applied in industry, in activities ranging from the elicitation of requirements and the early design phases all the way to the deployment, configuration, and runtime monitoring of actual systems. Formal methods allow one to precisely specify the environment in which a system operates, the requirements and properties that the system should satisfy, the models of the system used during the various design steps, and the code embedded in the final implementation, as well as to express conformance relations between these specifications. We present a broad scope of successful applications of formal methods in industry, not limited to the well-known success stories from the safety-critical domain, like railways and other transportation systems, but also covering other areas such as lithography manufacturing and cloud security in e-commerce, to name but a few. We also report testimonies from a number of representatives from industry who, either directly or indirectly, use or have used formal methods in their industrial project endeavours. These persons are spread geographically, including Europe, Asia, North and South America, and the involved projects witness the large coverage of applications of formal methods, not limited to the safety-critical domain. We thus make a case for the importance of formal methods, and in particular of the capacity to abstract and mathematical reasoning that are taught as part of any formal methods course. These are fundamental Computer Science skills that graduates should profit from when working as computer scientists in industry, as confirmed by industry representatives.},
journal = {Form. Asp. Comput.},
month = dec,
articleno = {7},
numpages = {38},
keywords = {Formal methods, computer science education}
}

@inproceedings{10.1145/3631802.3631827,
author = {Hussain, Syed Sajid and Moalagh, Morteza and Farshchian, Babak A.},
title = {Which Threshold Concepts do Computing Students Encounter while Learning Empirical Research Methods?},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631827},
doi = {10.1145/3631802.3631827},
abstract = {A strong foundation in empirical research methods is essential for computing students due to the societal impacts of digital technologies. However, learning empirical research is challenging because of a lack of a research-based approach and the absence of an established pedagogical culture for teaching empirical research methods. In this paper, we ask the research question: Which Threshold Concepts (TCs) do computing students encounter while learning empirical research methods? First, we conducted a systematic mapping review of the literature to identify the candidate TCs in learning empirical research methods. Next, we evaluated the candidate TCs in an explanatory case study of an introductory course in research methods offered to master’s students at the Department of Computer Science at the Norwegian University of Science and Technology (NTNU). We found that a particularly challenging and overarching TC may be developing and operationalizing a conceptual framework, and many other TCs can be linked to the conceptual framework. We also found that it can be difficult for computing students to grasp the nature of research and how empirical research is done. These findings may help understand student challenges while learning empirical research methods and developing solutions to address these challenges.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {12},
numpages = {12},
keywords = {Computing, Empirical research, Research methods, Threshold concept},
location = {Koli, Finland},
series = {Koli Calling '23}
}

@article{10.1145/3589638,
author = {Adair, Amy and Koh, Joewie J.},
title = {Making Speech Recognition Work for Children: An Interview with Amelia Kelly},
year = {2023},
issue_date = {Spring 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {3},
issn = {1528-4972},
url = {https://doi.org/10.1145/3589638},
doi = {10.1145/3589638},
abstract = {What does it take to build new AI technologies for education? Dr. Amelia Kelly, chief technology officer at SoapBox Labs, shares her experience with us in this interview.},
journal = {XRDS},
month = apr,
pages = {26–29},
numpages = {4}
}

@inproceedings{10.1145/3649217.3653572,
author = {Vigl, Wolfgang and Abramova, Svetlana},
title = {Design and Use of Privacy Capture-the-Flag Challenges in an Introductory Class on Information Privacy and Security},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653572},
doi = {10.1145/3649217.3653572},
abstract = {With the advancement of data-intensive technologies and online tracking opportunities, education on privacy is becoming a bigger priority in cybersecurity curricula. Owing to its multidimensional and context-dependent nature, privacy and its protection goals can be introduced to computer science students as collective efforts of system designers to enable data protection, user's informational control, and privacy-preserving data sharing with third parties. Since engaging course participants into hands-on and gamified exercises is generally known to enhance learning experience and effect, we adopted the teaching practice of using Capture-the-Flag (CTF) security challenges and validated its applicability in the privacy education domain. This work presents a pioneering set of 8 CTF-style tasks intentionally designed for the study and demonstration of handpicked privacy concepts, techniques, and attacks. Drawing upon both quantitative and qualitative feedback collected from 27 course participants, this format of homework exercises is found to increase students' confidence in this knowledge domain and perceived as an enjoyable, motivating, and engaging way of learning about information privacy.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {618–624},
numpages = {7},
keywords = {capture-the-flag, ctf, cybersecurity, education, privacy, student engagement},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3649921.3650003,
author = {Robinson, Raquel Breejon and Alvarez, Alberto},
title = {Toward a Design and Play-Focused Approach to Teaching Technical Game Design},
year = {2024},
isbn = {9798400709555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649921.3650003},
doi = {10.1145/3649921.3650003},
abstract = {As of 2023, education surrounding game design has become a fixture in university education systems around the world. As the teaching of game design is an inherently interdisciplinary subject with many connections to the arts and humanities, there are a diverse range of perspectives as to what the focus of each curriculum should include. In this essay, we argue that game programs with a more technical focus should include both design and play-focused approaches embedded into the pedagogy. We present two case studies drawing from our education journeys studying games as well as our experiences teaching in both game and computer science programs, and discuss the resulting benefits of integrating these concepts into our practice.},
booktitle = {Proceedings of the 19th International Conference on the Foundations of Digital Games},
articleno = {39},
numpages = {7},
keywords = {computer science, design, education, game design, larp, pedagogy, play-centric},
location = {Worcester, MA, USA},
series = {FDG '24}
}

@article{10.1145/3527203,
author = {Yadav, Aman and Heath, Marie and Hu, Anne Drew},
title = {Toward justice in computer science through community, criticality, and citizenship},
year = {2022},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/3527203},
doi = {10.1145/3527203},
abstract = {Proposing a justice-centered CS education.},
journal = {Commun. ACM},
month = apr,
pages = {42–44},
numpages = {3}
}

@inproceedings{10.1145/3626252.3630949,
author = {Moudgalya, Sukanya Kannan and Zeller, Amanda},
title = {The Need for More Justice-Oriented Courses in Undergraduate Computer Science Curricula},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630949},
doi = {10.1145/3626252.3630949},
abstract = {There is a pressing need for practices that work toward broadening Black, Indigenous, and People of Color (BIPOC) participation in computing, harness computing for BIPOC community goals, youth civic participation, activism, wealth generation, self-advocacy, and disruption of harmful biases and racist ideologies in computing. Thus, we first justify the need for justice-oriented courses in undergraduate Computer Science (CS) curricula. We then explore the current status of such courses in undergraduate CS. We do this by conducting a document analysis of the learning objectives, stan- dards, and CS course descriptions of the top 20 ranked universities in the U.S. We found that most courses that speak to the human and societal impacts of computing fell into the category of "Human- Computer Interaction". CS courses that explicitly focused on ethical issues, gender, racial, accessibility, and environmental justice were present at much lower numbers. Less than 50% of the top-ranked universities had such courses. If present, the proportion of such courses within a university's curricula was less than 5%. Most of the top-ranked universities subscribed to ABET's standards, which have a curricular requirement of understanding the local and global impacts of computing. Given our pressing needs, there is a need for more justice-oriented courses in undergraduate CS curricula. We describe possible ways to achieve this goal, such as hiring interdisciplinary CS faculty with African-American, Chican@, Indigenous, or gender studies, instructor professional development programs like AiiCE, and considering deep social impact standards, and better alignment with such standards},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {888–894},
numpages = {7},
keywords = {computer science education, ethical dimensions of computer science, inclusive computing education, undergraduate education},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3626253.3635519,
author = {Denzler, Benjamin and Vahid, Frank and Pang, Ashley},
title = {Style Anomalies Can Suggest Cheating in CS1 Programs},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635519},
doi = {10.1145/3626253.3635519},
abstract = {Student cheating on at-home programming assignments is a well-known problem. A key contributor is externally obtained solutions from websites, contractors, and recently generative AI. In our experience, such externally obtained solutions often use coding styles that depart from a class's style, which we call "style anomalies". Examples of style anomalies include using untaught or advanced constructs like pointers or ternary operators or having different indenting or brace usage from the class style. We developed a tool to automatically count style anomalies in student code submissions. We used this tool to find suspected cheating in student submissions for lab assignments across five terms of CS1. This poster presents our findings: Some student submissions were suspected of cheating due to high style anomaly counts and were not flagged as suspicious by a code similarity checker. With the rise of externally obtained solutions from websites, contractors, and generative AI, style anomalies may become an important complement to similarity checking for detecting cheating.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1624–1625},
numpages = {2},
keywords = {cheating, cs1, plagiarism, program autograders, program style},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@article{10.1145/3584859,
author = {Brown, Neil C. C. and Hermans, Felienne F. J. and Margulieux, Lauren E.},
title = {10 Things Software Developers Should Learn about Learning},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {67},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/3584859},
doi = {10.1145/3584859},
abstract = {Understanding how human memory and learning works, the differences between beginners and experts, and practical steps developers can take to improve their learning, training, and recruitment.},
journal = {Commun. ACM},
month = dec,
pages = {78–87},
numpages = {10}
}

@inproceedings{10.1145/3490149.3503667,
author = {He, Shiqing and Jones, Jasmine},
title = {Collaborative Creative Coding Through Drawing Robots},
year = {2022},
isbn = {9781450391474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490149.3503667},
doi = {10.1145/3490149.3503667},
abstract = {With the development of end-user fabrication technology, plotters have become increasingly accessible and attractive to a broader community. Since their invention, these 3-axis CNC machines have been connecting digital and analog creation. Plotters have great potential as a tool to bridge the digital-physical divide, allowing learners from a variety of backgrounds to gain immediate satisfaction from their programming by translating digital output into physical artifacts. In addition, due to their roots in art, plotters may be more inviting to students from artistic backgrounds new to programming. In this studio, we investigate plotters’ potential in introducing creative programming in collaborative learning environments. By introducing collaborative creative programming through the lens of plotters, we explore solutions to challenges in programming education, collaborative programming, and personal fabrication.},
booktitle = {Proceedings of the Sixteenth International Conference on Tangible, Embedded, and Embodied Interaction},
articleno = {54},
numpages = {4},
keywords = {collaborative coding, creative coding, generative art, plotter},
location = {Daejeon, Republic of Korea},
series = {TEI '22}
}

@inproceedings{10.1145/3626252.3630964,
author = {Bhaskar, Niharika and Lewis, Amari N. and Darabi, Rona and Fang, Joana and Liu, Jingting and Vaccaro, Kristen and Politz, Joe Gibbs and Minnes, Mia},
title = {Welcoming Students to Undergraduate Computer Science Programs: On-ramps, Rest Areas, and Lane Changes},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630964},
doi = {10.1145/3626252.3630964},
abstract = {Studying computer science is a journey: people start at different times, travel at different paces, and pause along the way. In this experience report, we describe a peer-led, year-long program designed to welcome students to Computer Science and Engineering as a discipline, department, and academic program. We detail the logistical, curricular, and personnel structures of this program, highlighting design choices we made to (a) open multiple ways to join the program all year, (b) de-emphasize "getting ahead", (c) prioritize reflection, and (d) connect students to existing resources. Throughout, we emphasize the critical role of peer mentors in leading and shaping this space. We share our own lessons learned, as well as reflections from students and mentors on the value of this learning community outside of formal classroom structures.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {109–115},
numpages = {7},
keywords = {broadening participation, hidden curriculum, peer mentors, undergraduate computing programs},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@proceedings{10.1145/3686424,
title = {EDCS '24: Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Education Digitalization and Computer Science},
year = {2024},
isbn = {9798400710360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Shenzhen, China}
}

@inproceedings{10.1145/3626252.3630889,
author = {Saliba, Liam and Shioji, Elisa and Oliveira, Eduardo and Cohney, Shaanan and Qi, Jianzhong},
title = {Learning with Style: Improving Student Code-Style Through Better Automated Feedback},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630889},
doi = {10.1145/3626252.3630889},
abstract = {ccheck, a lenient automatic grader and C style-checker, to guide students to improve their coding practices. Many computing classes rely heavily on autograders--software that automates grading and alleviates staff workload in classes with large enrollments. At best, autograders offer timely and consistent feedback to students. However, existing autograders primarily judge on functional correctness---they are generally strict and inflexible in marking beginner programming assignments. They tend not to provide feedback on programming style and structure, which instead requires delayed, tedious manual assessment. ccheck, the tool we introduce, aims to address this gap and provide more meaningful, real-time feedback with a pedagogical focus.We deploy ccheck in a class of 440 first-year computer science students. Teaching assistants employ the system for marking assistance, while students use the same system for self-evaluation prior to finalizing their submissions. Feedback was solicited through a survey of 76 students and a focus group of the teaching team. 82% of the students surveyed said that the system helped them learn good coding practices, while 75% emphasized that the feedback received from the system is meaningful and helpful. The teaching team focus group related to how they valued the automation of menial marking tasks, which enabled them to direct their time toward other meaningful feedback. Overall, we find that teaching, learning and student experiences are improved through the deployment of ccheck.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1175–1181},
numpages = {7},
keywords = {automatic feedback system, code-style checker, programming style},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@article{10.1145/3710911,
author = {Andrus, McKane and Ghoshal, Sucheta and Dasgupta, Sayamindu},
title = {From Data Activism to Activism in a Time of Data-Centrism: Affirming Epistemological Heterogeneity in Social Movements},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3710911},
doi = {10.1145/3710911},
abstract = {In this paper, we seek to understand how grassroots activists, operating within the hegemony of data-centrism, are often disempowered by data even as they appropriate it towards their own ends. We posit that the shift towards data-driven governance and organizing, by elevating a particular epistemology, can pave over other ways of knowing that are central to social movement practices. Building on Muravyov's [102] concept of ''epistemological ambiguity,'' we demonstrate how data-focused activism requires complex navigations between data-based epistemologies and the heterogeneous, experiential, and relational epistemologies that characterize social movements. Through three case studies (two drawn from existing literature and the third being an original analysis), we provide an analytical model of how generative epistemological refusals can support more value-aligned navigations of epistemological ambiguity that resist data-centrism. Finally, we suggest how these findings can inform pedagogy, research, and technology design to support communities navigating datafied political arenas.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {CSCW013},
numpages = {32},
keywords = {counter-data, critical data literacy, data activism, data epistemologies, data justice, grassroots social movements, housing justice, refusal}
}

@inproceedings{10.1145/3631802.3631803,
author = {Waite, Jane and Kolaiti, Eirini and Thomas, Meurig and Maton, Karl},
title = {Constructing feedback for computer science MCQ wrong answers using semantic profiling (Discussion Paper)},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631803},
doi = {10.1145/3631802.3631803},
abstract = {Computer science (CS) for K-12 students is a hard subject with abstract concepts to learn. Multiple-choice questions (MCQ) are often used to supplement classroom learning, with feedback from wrong answers playing a role in addressing misunderstandings. Semantics, a dimension of Legitimation Code Theory, a framework for understanding knowledge practices, has been suggested as a useful theory for reviewing and structuring CS learning events. In this discussion paper, we explore the use of ‘semantic profiling’ to improve feedback to wrong answers in MCQ for post-16 students studying SQL and relational databases. We describe the reflexive review process we developed and present the semantic profiles of two case studies of new feedback for wrong answers. New answer feedback to five questions was trialed in a pilot study with five students, and students self-reported the new feedback as useful. For example, students liked the metacognitive aspect of feedback that explained why answers were wrong or right and liked the generalised summaries. From our reflexive experience, we suggest reviewing MCQ questions and using semantic profiling for feedback can help MCQ authors and students develop their feedback literacy, particularly for creating learner ‘feedforward’ (take-away) opportunities. The approach we used has promise that we will build on, and we invite other researchers to explore and evaluate this approach.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {39},
numpages = {9},
keywords = {K-12 education, SQL, computing education, databases, feedback literacy},
location = {Koli, Finland},
series = {Koli Calling '23}
}

@inproceedings{10.1145/3631802.3631826,
author = {Winkelnkemper, Felix and Schulte, Carsten},
title = {Reconstructing the Digital – An Architectural Perspective for Non-Engineers (Discussion Paper)},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631826},
doi = {10.1145/3631802.3631826},
abstract = {Knowing and understanding the world of digital artefacts we are living in is a requirement for everyone today, regardless of their general interest in technology. Computer science education, however, often treats pupils as if they all wanted to become engineers. Educational models of computer science are rather not targeted at understanding the behaviour of the digital world, but at constructing it. Our paper complements such classical approaches with an Ontology of the Digital as an approach which reconstructs digital artefacts and thereby creates a model which helps to understand and explain the technological potentials of digital artefacts without relying on minute details of the engineering discipline of computing.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {21},
numpages = {7},
keywords = {CS for all, digital artefacts, explanation model, ontology, technological knowledge},
location = {Koli, Finland},
series = {Koli Calling '23}
}

@book{10.1145/3715982,
author = {Bishop, Matt and Cerrone, Beth and Dai, Jun and Dark, Melissa and Daugherty, Jenny and Huff, Philip and Tang, Cara and Tucker, Cindy},
title = {Supplement to CSEC 2017: Foundational Cybersecurity Content and Instructional Guidance for Secondary and Postsecondary Cybersecurity Education},
year = {2025},
isbn = {9798400714900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This curriculum project involved a review of the Cybersecurity Curricula 2017 (CSEC 2017) guidelines and recommendation of learning outcomes that support foundational collegiate-level courses and upperlevel high school cybersecurity offerings, comparable to Advanced Placement (AP) content. The primary goal of this document is to provide a foundational cybersecurity education framework that can be implemented by both post-secondary institutions and high schools. These guidelines aim to bridge the gap between secondary and post-secondary education in cybersecurity to support broad transferability across institutions and ensure continuity for students.}
}

@inproceedings{10.1145/3545945.3569791,
author = {Prather, James and Denny, Paul and Becker, Brett A. and Nix, Robert and Reeves, Brent N. and Randrianasolo, Arisoa S. and Powell, Garrett},
title = {First Steps Towards Predicting the Readability of Programming Error Messages},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569791},
doi = {10.1145/3545945.3569791},
abstract = {Reading a programming error message is the first step in understanding what it is trying to tell the programmer about how to fix an error in their code. However, these are often difficult to read, especially for novices which is not surprising given that error messages in many of the most popular languages in which novices learn to code were not written with readability in mind. As a result, novices frequently struggle to understand them. This is a long-standing problem, with researchers highlighting concerns about programming error message readability over the last six decades. Very recent work has put forward evidence of the need for measuring readability in error messages and a framework for doing so. This framework consists of four factors of readability for programming error messages: message length, vocabulary, jargon, and sentence construction. We use this framework to implement an approach to automatically assess the readability of programming error messages. Using established readability factors as predictors in a machine learning model, we train several models using a dataset of C and Java error messages. We examine the performance of these models, and apply the best performing model to a previously published set of messages evaluated for readability by experts, non-experts and students. Our results validate the previously proposed readability factors, and our model classifies messages similarly to human raters. Finally, we discuss future work needed to improve the accuracy of the model.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {549–555},
numpages = {7},
keywords = {compiler errors, error messages, errors, machine learning, pems, programming error messages, readability},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3626252.3630832,
author = {Sakzad, Amin and Paul, David and Sheard, Judithe and Brankovic, Ljiljana and Skerritt, Matthew P. and Li, Nan and Minagar, Sepehr and Simon and Billingsley, William},
title = {Diverging assessments: What, Why, and Experiences},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630832},
doi = {10.1145/3626252.3630832},
abstract = {In this experience paper, we introduce the concept of 'diverging assessments', process-based assessments designed so that they become unique for each student while all students see a common skeleton. We present experiences with diverging assessments in the contexts of computer networks, operating systems, ethical hacking, and software development. All the given examples allow the use of generative-AI-based tools, are authentic, and are designed to generate learning opportunities that foster students' meta-cognition. Finally, we reflect upon these experiences in five different courses across four universities, showing how diverging assessments enhance students' learning while respecting academic integrity.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1161–1167},
numpages = {7},
keywords = {assessment-as-learning, authentic assessment, diverging assessment},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3706598.3713493,
author = {Ye, Jingzhou and Li, Yao and Zou, Wenting and Wang, Xueqiang},
title = {From Awareness to Action: The Effects of Experiential Learning on Educating Users about Dark Patterns},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713493},
doi = {10.1145/3706598.3713493},
abstract = {Dark patterns (DPs) refer to unethical user interface designs that deceive users into making unintended decisions, compromising their privacy, safety, financial security, and more. Prior research has mainly focused on defining and classifying DPs, as well as assessing their impact on users, while legislative and technical efforts to mitigate them remain limited. Consequently, users are still exposed to DP risks, making it urgent to educate them on avoiding these harms. However, there has been little focus on developing educational interventions for DP awareness. This study addresses this gap by introducing DPTrek, an experiential learning (EL) platform that educates users through simulated real-world DP cases. Both qualitative and quantitative evaluations show the effectiveness of DPTrek in helping users identify and manage DPs. The study also offers insights for future DP education and research, highlighting challenges such as user-unfriendly taxonomies and the lack of practical mitigation solutions.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {224},
numpages = {22},
keywords = {dark pattern, experiential learning, security, privacy},
location = {
},
series = {CHI '25}
}

@article{10.1145/3730405,
author = {Garcia, Rita and Craig, Michelle},
title = {20-Years Later: A Replication Study on Teaching CS1 Concepts},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3730405},
doi = {10.1145/3730405},
abstract = {Introduction: Computer Science Education does not have a universally defined set of concepts consistently covered in all introductory courses (CS1). One approach to understanding the concepts covered in CS1 is to ask educators. In 2004, Nell Dale did just this. She also collected their perceptions on challenging topics to teach. Dale mused how the findings of a similar survey conducted in later years would compare with her results.Objectives: We answered Dale’s call to consider changes in teaching CS1 concepts by performing a replication study 20 years later. Our goals were to determine how the teaching of CS1 concepts has changed and to identify concepts educators perceive as challenging to teach.Methods: We created a survey based on Dale’s original study and added concepts from the CS2023 recommended curricula to include CS1 concepts for today’s teaching practice. We used a mixed-methods approach to analyse the 178 responses from CS1 educators.Results: Our survey results show Python is predominately used to teach today’s CS1 courses, with educators continuing to teach basic programming concepts similar to 20 years ago. However, our survey shows recursion continues to be challenging to teach, with most secondary school educators perceiving it does not belong in CS1. Today’s educators also teach less of the CS1 concepts from 20 years ago, such as inheritance and polymorphism, and have a limited focus on ethics and professionalism in their courses. Participants also found good learning behaviours like thinking and planning strategies challenging to teach.Conclusion: We conclude our paper by discussing the challenges of conducting a replication study, which includes reproducing studies with limited or no access to the original instruments. We present future research opportunities raised by the study’s findings, including how to support educators in teaching the challenging concept of good learning behaviours and further refine curricular guidelines to remove ambiguity on concepts covered in CS1 and CS2 courses.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Educ.},
month = apr,
keywords = {CS1, Replication Study, CS1 Concepts, Computing Curricula}
}

@inproceedings{10.1145/3598579.3689378,
author = {Kiesler, Natalie and Impagliazzo, John and Biernacka, Katarzyna and Kapoor, Amanpreet and Kazmi, Zain and Ramagoni, Sujeeth Goud and Sane, Aamod and Tran, Keith and Taneja, Shubbhi and Wu, Zihan},
title = {Where's the Data? Finding and Reusing Datasets in Computing Education},
year = {2024},
isbn = {9798400702228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3598579.3689378},
doi = {10.1145/3598579.3689378},
abstract = {Computing education research (CER) is a rapidly advancing discipline, offering vast potential for data-driven, secondary research or replication studies. Although gathering and analyzing data for research seem straightforward, making research data publicly available to the community remains a challenge. Likewise, finding and reusing high-quality, prominent, and well-documented research data proves to be a daunting task. In this working group paper, the authors present their search for available datasets in the CER context (e.g., in databases and repositories). The available datasets are further analyzed using a newly developed metadata scheme and presented to the community as a resource. The second component of this work is a summary of the community's perspective and concerns on publishing their research data, which has been gathered through a survey among 52 computing education researchers. Based on this status quo, this report presents recommendations for measures and future steps for the community to become more accessible and establish open data practices. We thus emphasize the potential of making research data available to enhance productivity, transparency, and reproducibility in the CER community.},
booktitle = {Working Group Reports on 2023 ACM Conference on Global Computing Education},
pages = {31–60},
numpages = {30},
keywords = {computing education, datasets, educational data mining, open data, open science, programming process data, reusing data, secondary research},
location = {Hyderabad, India},
series = {CompEd 2023}
}

@inproceedings{10.1145/3626252.3630794,
author = {Cheng, Alan Y. and Tanimura, Ellie and Tey, Joseph and Wu, Andrew C. and Brunskill, Emma},
title = {Brief, Just-in-Time Teaching Tips to Support Computer Science Tutors},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630794},
doi = {10.1145/3626252.3630794},
abstract = {As enrollments in computing-related programs continue to rise, computer science departments are increasingly relying on teaching assistants (TAs) to provide additional educational support to students, such as one-on-one tutoring or office hours. Tutoring is more effective with highly trained tutors, but most TAs receive little to no training in pedagogical skills. How might we provide support to TAs working with students one-on-one, especially in online settings? We propose a just-in-time intervention that shows a tutor actionable teaching tips and relevant information right before they begin an online tutoring session with a student. We conducted a crossover experiment (n = 46) where participants engaged in two tutoring roleplays for an introductory computer science programming task and found that participants demonstrated effective instructional strategies for much longer periods of time after receiving the intervention. We discuss the implications of these findings for both educators looking to support tutors and researchers seeking to build technology for tutors.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {200–206},
numpages = {7},
keywords = {online tutoring teacher training, remote tutoring, ta training, tutoring},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3641554.3701806,
author = {Kannam, Suhas and Yang, Yuri and Dharm, Aarya and Lin, Kevin},
title = {Code Interviews: Design and Evaluation of a More Authentic Assessment for Introductory Programming Assignments},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701806},
doi = {10.1145/3641554.3701806},
abstract = {Generative artificial intelligence poses new challenges around assessment, increasingly driving introductory programming educators to employ invigilated exams. But exams do not afford more authentic programming experiences that involve planning, implementing, and debugging programs with computer interaction. In this experience report, we describe code interviews: a more authentic assessment method for take-home programming assignments. Through action research, we experimented with the number and type of questions as well as whether interviews were conducted individually or with groups of students. To scale the program, we converted most of our weekly teaching assistant (TA) sections to conduct code interviews on 5 major weekly take-home programming assignments. By triangulating data from 5 sources, we identified 4 themes. Code interviews (1) pushed students to discuss their work, motivating more nuanced but sometimes repetitive insights; (2) enabled peer learning, reducing stress in some ways but increasing stress in other ways; (3) scaled with TA-led sections, replacing familiar practice with an unfamiliar assessment; (4) focused on student contributions, limiting opportunities for TAs to give guidance and feedback. We reflect on the design of code interviews for student experience, academic integrity, and teacher workload.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {554–560},
numpages = {7},
keywords = {authentic assessment, introductory programming, oral exams},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3587102.3588829,
author = {Russell, Se\'{a}n and Caton, Simon and Becker, Brett A.},
title = {Online Programming Exams - An Experience Report},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588829},
doi = {10.1145/3587102.3588829},
abstract = {When seeking to maximise the authenticity of assessment in programming courses it makes sense to provide students with practical programming problems to solve in an environment that is close to real software development practice, i.e., online, open book, and using their typical development environment. This creates an assessment environment that should afford students sufficient opportunities to evidence what they have learned, but also creates practical challenges in terms of academic integrity, flexibility in the automated grading process, and assumptions surrounding how the student may attempt to solve the problems both in terms of correct and incorrect solutions. In this experience report, we outline two independently observed cohorts of students sitting the same Java programming exam, with different weights, over three years. This is undertaken as a reflective exercise in order to derive a series of recommendations and retrospectively obvious pitfalls to act as guidance for educators considering online programming exams for large (i.e. n &gt; 150) introductory programming courses. After discussing our assessment methodology, we provide 4 high-level observations and centre a set of recommendations around these to aid practitioners in their assessment design.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {436–442},
numpages = {7},
keywords = {authentic assessment, plagiarism, programming, video},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@article{10.5555/3737313.3737323,
author = {Barnard, Jakob and Braught, Grant and Davis, Janet and Holland-Minkley, Amanda and Schmitt, Karl and Tartaro, Andrea},
title = {Reviewing and Revising your Undergraduate CS Major: A Structured Design Process for Creating Distinctive Curricula},
year = {2025},
issue_date = {April 2025},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {8},
issn = {1937-4771},
abstract = {Computer science (CS) programs have a variety of reasons for regularly reviewing and revising the curriculum for their undergraduate major. Some of these stem from the rapid pace of change in the discipline and corresponding changes in industry expectations for CS graduates. This has been most recently seen as departments consider how to adjust to advances in generative AI and respond to new international curricular guidelines in the form of CS2023 [1]. Programs also revise their CS major in response to contextual shifts at their institution, such as changes in the size and makeup of the student body, the resources and staffing of a program, assessment results, or new institutional priorities [6]. A shifting student body may come with changes in prior experience with computing and in the professional goals of the students. For smaller programs, staffing changes often affect the balance of expertise within subareas of CS. New institutional priorities such as enabling more study abroad experiences or embedding internship/service-learning into the curriculum can require majors to adjust to both accommodate and support these priorities.},
journal = {J. Comput. Sci. Coll.},
month = may,
pages = {32–34},
numpages = {3}
}

@inproceedings{10.1145/3689535.3689556,
author = {Gilbert, Cole and McDonald, Brian and Scott, Michael James},
title = {Exploring the Relationship between Debugging Self-Efficacy and CASE Tools for Novice Troubleshooting},
year = {2024},
isbn = {9798400711770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689535.3689556},
doi = {10.1145/3689535.3689556},
abstract = {Novice software developers encounter pitfalls which evoke learning experiences that are more often frustrating than enlightening. Such experiences dampen their debugging self-efficacy, impacting their attainment and retention. Structured debugging using computer-aided software engineering (CASE) tools could help students overcome these obstacles. Unfortunately, students find such tools challenging to use because they tend to cater to the needs of experts rather than novices. This paper examines the relationship between debugging self-efficacy and CASE tools. The study challenged 66 undergraduate computing students to complete a small-scale troubleshooting task in C#, allocating them to one of three groups: those using a simplified tool for novices, others using an off-the-shelf tool, and those using no tool. Analysis shows significant differences between the groups (p = .02, &lt;Formula format="inline"&gt;&lt;TexMath&gt;&lt;?TeX $eta ^2_p=.32$?&gt;&lt;/TexMath&gt;&lt;AltText&gt;Math 1&lt;/AltText&gt;&lt;File name="ukicer2024-21-inline1" type="svg"/&gt;&lt;/Formula&gt;). Using an off-the-shelf tool tool decreases debugging self-efficacy (d = −2.5). There was no change in debugging self-efficacy when using the simplified tool. These findings suggest that educators should exercise caution when using off-the-shelf tools due to their impact on students’ debugging self-efficacy. Simplification appears to mitigate the negative effect but does not seem to offer any improvement.},
booktitle = {Proceedings of the 2024 Conference on United Kingdom &amp; Ireland Computing Education Research},
articleno = {7},
numpages = {7},
keywords = {computer-aided software engineering, debugging, experiment, explicit guidance, novice programmers, self-efficacy},
location = {Manchester, United Kingdom},
series = {UKICER '24}
}

@inproceedings{10.1145/3706598.3713624,
author = {Ali, Murtaza and Dasgupta, Sayamindu},
title = {"Even Though I Went Through Everything, I Didn't Feel Like I Learned a Lot": Insights From Experiences of Non-Computer Science Students Learning to Code},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713624},
doi = {10.1145/3706598.3713624},
abstract = {Programming education is increasingly seen as an important curricular component of non-Computer Science (CS) disciplines at the undergraduate level. While existing research has studied non-CS majors’ experiences in introductory programming courses, there is limited work that explores such experiences across universities and disciplines. To address this gap, we conducted semi-structured interviews with 12 non-CS major programming students across several majors and universities and interpreted the results through reflexive thematic analysis. Our findings suggest that while students are excited about and interested in learning programming, they face barriers that often arise from the design of the courses they take and a lack of targeted resources and tools to support them. Building on our findings, we conclude with a set of recommendations for the design of tools, artifacts, and courses that can support programming education for non-major students.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {49},
numpages = {18},
keywords = {computing education, novice programmers, learning to code, non-Computer Science majors},
location = {
},
series = {CHI '25}
}

@article{10.5555/3729857.3729874,
author = {Bandi, Ajay and Blackford, Benjamin and Fellah, Aziz and Linville, Diana and Meyer, Trevor C. and Voss, Robert J.},
title = {Prompting Collaboration: Development of an Multidisciplinary Applied AI Minor Program},
year = {2025},
issue_date = {April 2025},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {6},
issn = {1937-4771},
abstract = {Artificial Intelligence (AI) has rapidly transformed industries and research, becoming a driving force for technological innovation and development [1]. As AI continues to grow and change, it is reshaping the way we approach problem-solving, decision-making, and creative processes across various sectors. Northwest Missouri State University is developing a new multidisciplinary AI minor open to all undergraduate students on campus. The program is tailored for students from any discipline who want to explore how AI can be utilized and integrated into their fields such as computer science, humanities, business, sciences, healthcare, agriculture, and education, among others. The curriculum integrates topics such as foundational AI concepts, prompt engineering and writing processes, ethical considerations in AI, AI in the workplace, and a capstone project. This program also promotes interdisciplinary collaboration and emphasizes the ethical use of AI.By the end of the program, students will be able to use AI to enhance efficiency and accuracy in tasks, develop and evaluate effective prompts, apply generative AI tools across various input formats, and assess the ethical considerations of AI in real-world applications. The panel members are experts from diverse fields, including management, humanities, technical writing, and computer science. The panel discusses the development of the AI minor curriculum and explores opportunities to extend the AI curriculum by offering AI certificates for undergraduate and graduate online professional students. By attending this panel, the audience will gain valuable insights into developing comprehensive AI programs, fostering cross-disciplinary innovation, and preparing students to use AI ethically and effectively across diverse fields.},
journal = {J. Comput. Sci. Coll.},
month = apr,
pages = {129–132},
numpages = {4}
}

@inproceedings{10.1145/3636555.3636861,
author = {Steinbeck, Hendrik and Elhayany, Mohamed and Meinel, Christoph},
title = {Millions of Views, But Does It Promote Learning? Analyzing Popular SciComm Production Styles Regarding Learning Success, User Behavior and Perception},
year = {2024},
isbn = {9798400716188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636555.3636861},
doi = {10.1145/3636555.3636861},
abstract = {With a rising amount of highly successful educational content on major video platforms, science communication (SciComm) can be considered mainstream. Although the success in terms of social media metrics (e.g. followers and watch time) is undoubtedly given, the learning mechanisms of these production styles is under-researched. Through a between-subject-design of 980 adult learners in a MOOC about data science, this study analyzes how much of a difference four popular SciComm production styles about relational databases make in regard to perceived quality, learning success and technical user behavior. Testing the isolated effect showed no statistical difference in the grand scheme of things. Additionally, a multivariate regression model, estimating the overall course points with robust standard errors showed six significant variables: The time spend with the material and the number of exercise submissions are particular noteworthy. Based on our results, an underlying (video) script is more relevant than the actual production style. Prioritizing the preparation of this material instead following a specific, pre-existing video production style is recommended.},
booktitle = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
pages = {134–144},
numpages = {11},
keywords = {field-experiment, science communication, video production styles, video-based education, youtube},
location = {Kyoto, Japan},
series = {LAK '24}
}

@inproceedings{10.1145/3626203.3670588,
author = {Stevens, Cody and Anderson, Sean and Carlson, Adam},
title = {Integrating High Performance Computing into Higher Education and the Pedagogy of Cluster Computing},
year = {2024},
isbn = {9798400704192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626203.3670588},
doi = {10.1145/3626203.3670588},
abstract = {Despite the exponential growth in demand for advanced computational skills driven by big data, machine learning, and artificial intelligence, higher education institutions still face a significant shortage of dedicated course offerings pertaining to High Performance Computing (HPC). This educational deficiency not only hampers the preparedness of undergraduate students for cutting-edge postgraduate programs but also impairs their readiness to enter a dynamic workforce increasingly reliant on sophisticated computational capabilities. Integrating comprehensive HPC courses at the undergraduate level is critical for equipping students with expertise to effectively utilize modern computing technologies, and also for bridging the growing gap between academic preparation and industry demands. At Wake Forest University (WFU), we, members of the HPC Team, are actively working to address the educational gap in HPC by integrating the WFU HPC Facility[4] into higher-level elective courses across various disciplines. Recognizing the foundational importance of these skills, we have developed an introductory course specifically designed to equip students with the knowledge to excel in advanced courses, in graduate and research programs, and to meet the demands of the modern workforce. By integrating the WFU HPC Facility into our curriculum, the University is committed to pioneering a comprehensive educational pathway that empowers students to leverage the full potential of computing technologies in their future careers. WFU is an R-2 liberal arts institution with around 9,000 students[6] that actively supports undergraduate research through a multitude of departments and programs. Undergraduate research is so paramount to the University mission, that WFU has a dedicated center, the Undergraduate Research and Creative Activities (URECA) Center, just for this purpose. Many students engage in research projects that leverage the resources of the WFU HPC Facility. The facility’s main asset, the Distributed Environment for Academic Computing (DEAC) Cluster, contains approximately 4000 CPU cores and 20TB of RAM, and is a true interdisciplinary tool; in 2023 it was utilized by 15 departments and 500 active users to submit over 1.5 million computational tasks on a vast array of research topics. The interdisciplinary nature of the DEAC Cluster played an instrumental role in developing an introductory course in HPC that caters to students from a diverse number of majors. Having supported a wide range of researchers, we designed the course to bridge concepts and applications from Computer Science, Engineering, Data Science, and the Natural Sciences to their respective academic domains. By enabling students from multiple disciplines to access foundational HPC skills, we foster a versatile educational environment where collaborative and interdisciplinary learning thrives. One way that we ensure our introductory course is accessible to all students is that we do not require any prerequisite classes to enroll in the course. Students are also not expected to have any prior experience in programming. We have chosen Python as the primary programming language for the course, as it is one of the most versatile and widely-used languages in the fields of data science and machine learning, and can easily interface with parallel frameworks such as MPI and OpenMP. Students gain hands-on experience by developing asynchronous workflows, which are then executed on the DEAC Cluster. This practical focus not only demystifies complex computational concepts but also empowers students to apply their learning in real-world scenarios. HPC serves as a cornerstone for two distinct user groups, each integral to its advancement and application. The first encompasses those who enable and optimize HPC systems, including Computer Scientists, Computer Engineers, Systems Administrators, and Cyberinfrastructure Professionals, who enhance computational efficiency and build the underlying hardware infrastructure. The second group comprises scientists and researchers across diverse fields such as Statistics, Chemistry, Biology, Physics, Engineering, and more, who leverage HPC as a powerful tool for simulating complex phenomena, analyzing large datasets, and researching novel problems in their respective domains. While current course offerings at other institutions seem to prioritize the first group and educate students on how to build and enable an HPC cluster, we have chosen to prioritize curriculum for the second group as the skills they gain through our course’s curriculum will help them as they continue their academic career in higher level electives and independent research projects with faculty advisors. We choose to offer our course during the Spring semester in order to prepare students who may want to pursue research during the summer session. The first half of the course serves as foundational cluster training, familiarizing students with essential skills to work within an HPC environment. In this segment, students delve into the Linux command line interface (CLI) using Bashcrawl[3] and explore the intricacies of the Linux filesystem and environment modules. A significant focus is placed on understanding and utilizing job schedulers, such as the Slurm resource manager[2]. Another unique feature of this segment is the guided tour of the Wake Forest datacenter. This tour provides students with a tangible understanding of how the theoretical concepts discussed in class are implemented in a real-world HPC cluster. To further provide a reference to the resources requested for their jobs through Slurm, the tour concludes with students disassembling retired compute nodes to learn about the different components that comprise modern servers. The midterm assessment challenges students to submit multiple jobs, analyzing the effects of varying input sizes and the use of multiple CPU processors on calculation speed. Upon completion of this initial phase, students are fully equipped to engage in research activities under an advisor and effectively utilize an HPC cluster outside the confines of the classroom. Many apply for summer grants through the aforementioned URECA program with a faculty advisor. In the latter half of the course, the curriculum shifts towards more advanced topics, focusing on parallel computing frameworks and technologies. Students are introduced to MPI and OpenMP, which are essential for developing parallel applications that can run efficiently on today’s multi-processor systems. Additionally, the course delves into high-speed interconnects, crucial for optimizing communication between different parts of an HPC cluster. One of our final topics covers GPU computing, with a particular emphasis on using NVIDIA GPUs and the CUDA programming platform, enabling students to harness the power of graphical processing for computational tasks. As an example from our Spring 2024 semester, students built a “chatbot” using Meta’s Llama 2 model[5] on both CPU and GPU using LLaMA C++[1], and compared its performance to ChatGPT while interacting freely with it. Our course is designed to complement other specialized courses found in Computer Science or in Computer Engineering programs, such as Parallel Algorithms, Computer Vision, or Deep Learning. It aims to introduce these critical computational concepts and provide a solid foundation that prepares students for these more advanced electives. By the end of the course, students are not only familiar with the basic principles of HPC but are also primed to tackle more specialized studies and research in their future academic and professional pursuits. It is not uncommon that a course may require students to use a specific programming language or software. While there are tools such as Google Colab and zyBooks that provide a browser-based interface to computing resources, those tools can be very limited in what resources they can provide. A faculty member might then want students to install software locally on their laptop, but this can be challenging when students bring their own device to the classroom as they may be running different operating systems or may have different hardware platforms. This can cause the software to behave differently or it may not even be available on that given platform. Courses with significant computational demands are better served utilizing a unified computing environment, and an HPC facility is ideally equipped to provide a consistent learning environment where each student has access to the same software and computing resources. A primary challenge in integrating HPC resources into coursework is instructing students on the use of schedulers for asynchronous workloads. Our introductory HPC course effectively bridges this gap by providing the necessary training and context, enabling students to engage with advanced topics more efficiently, without the steep initial learning curve typically associated with these environments. Our HPC facility has proven instrumental in enhancing educational experiences across a variety of disciplines, demonstrating significant benefits in classes such as Statistics, Natural Language Processing, Parallel Algorithms, Computer Vision, Physics Laboratory, Cancer Biology, Environmental Physics, Computational Modeling, and more. Moreover, students in fields like Finance and Business and Enterprise Management have also successfully leveraged our HPC resources, and have performed analysis on client data that was protected under a nondisclosure agreement which prevented students from storing the data locally on their laptop or with commercial cloud providers. This integration not only facilitates sophisticated computational tasks, but also allows students and faculty to easily share and store large datasets that the students may need to access without having to consume space on their local device. One of our primary goals is to promote diversity and interdisciplinary collaboration within this course, and this semester attracted a notably diverse group of students, with majors ranging from Biology and Statistics to Applied Mathematics, Economics, and Computer Science. Although the course is currently catalogued under the Computer Science department, we recognize that associating it with any single discipline could potentially limit its appeal and accessibility. The diverse enrollment underscores the interdisciplinary relevance of HPC skills across various fields of study. We are leveraging the current success and broad interest in the course as a foundation to establish a new academic program dedicated to High Performance Computing. This new program will serve as a hub for integrating computational skills across different disciplines, fostering a broader understanding and application of HPC in various scientific and economic sectors. The HPC team’s commitment to High Performance Computing education extends beyond traditional academic structures. While we are not developing a new major, minor, certificate track, or concentration in HPC, our objective is to make HPC education accessible and applicable across various disciplines without the constraints of a single departmental bias. This approach allows students from any field to engage with HPC skills as an integral part of their academic and professional development. To achieve this, we are actively collaborating with multiple academic departments to ensure that our HPC course offerings are recognized as fulfilling degree requirements across a range of programs. One way we collaborate with these departments is by altering activities and projects to use different languages and software, such as R and MATLAB, for the Statistics and Engineering departments, while still maintaining the same learning goals we achieve with Python. This strategy not only enhances the versatility of our course but also promotes a more comprehensive integration of the university’s HPC facilities into the curriculum. By doing so, we allow faculty in different departments to integrate our projects into their courses and utilize our HPC facility, even if it is for only one or two projects throughout the semester. Our efforts are focused on fostering a collaborative academic environment where the HPC facility is not just an isolated resource used for research but a central part of our educational infrastructure. By working across disciplines, we hope to catalyze a deeper engagement with HPC technologies throughout the university, enhancing both teaching and research capacities across departments. In conclusion, the escalating demand for big data, machine learning, and artificial intelligence is not only transforming industries but also reshaping educational requirements. As these fields expand, the need for substantial computational resources becomes increasingly critical. The HPC facility at Wake Forest University is exceptionally well-equipped to meet these demands, by providing a unified computing environment that supports an array of academic endeavors. Our initiative to develop introductory HPC courses is a strategic response to this need, preparing students to proficiently utilize HPC resources in higher-level electives and beyond. These courses are pivotal in bridging the gap between conventional academic programs and the rigorous computational needs of modern disciplines. Looking forward, the necessity for such educational offerings will only intensify as the reliance on advanced computational technologies grows. By anticipating and responding to these educational demands, Wake Forest University’s HPC academic program not only enhances student readiness for future challenges but also positions the university at the forefront of academic innovation in the computational sciences.},
booktitle = {Practice and Experience in Advanced Research Computing 2024: Human Powered Computing},
articleno = {106},
numpages = {3},
location = {Providence, RI, USA},
series = {PEARC '24}
}

@inproceedings{10.1145/3704289.3704302,
author = {Pan, Ruoqi and Chen, Liting and Ma, Shiming},
title = {Exploration of Engineering and Design Fusion in 3E of Digital Media Technology Major Empowered by AIGC},
year = {2025},
isbn = {9798400716980},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704289.3704302},
doi = {10.1145/3704289.3704302},
abstract = {With the rapid advancement of generative artificial intelligence, its demonstrated intelligence, cognition, and other abilities have created new opportunities for the reform of education. This paper examines the core curriculum development of digital media technology, the situation of emerging engineering education(3E), and the application of AIGC in education. It concludes that the main bottlenecks in the fusion of engineering and design of digital media technology are curriculum development, assessment, and duration of practical training. Consequently, the study explores the application and facilitation of AIGC within digital media technology and how AIGC alleviates the bottlenecks of major development. Building on this, the paper presents a case study centered on the core curriculum of digital media technology that found AIGC can significantly improve course teaching efficiency. It aims to provide insights and recommendations for teaching practices in core digital media technology courses.},
booktitle = {Proceedings of the 2024 7th International Conference on Big Data and Education},
pages = {36–41},
numpages = {6},
keywords = {AIGC, Curriculum Construction, Digital Media Technology, Engineering and Design Fusion},
location = {
},
series = {ICBDE '24}
}

@inproceedings{10.1145/3610969.3610973,
author = {Addo, Salomey Afua},
title = {Are You Ready to Teach AI in Schools? Teachers' Perspectives of Teaching AI in K-12 Settings},
year = {2023},
isbn = {9798400708763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610969.3610973},
doi = {10.1145/3610969.3610973},
abstract = {Artificial intelligence (AI) has continually made headlines, even more so with the mass interest in generative AI. The implications of AI on society raises the need for its inclusion in the K-12 computing curriculum. However, little research has been conducted to understand teachers’ preparedness to teach AI concepts in K-12. This exploratory study seeks to understand teachers’ motivation and preparedness to teach AI in schools through the lens of Self Efficacy Theory (SET) and Self Determination Theory (SDT).},
booktitle = {Proceedings of the 2023 Conference on United Kingdom &amp; Ireland Computing Education Research},
articleno = {32},
numpages = {1},
keywords = {Artificial intelligence, K-12 computing education, motivation},
location = {Swansea, Wales Uk},
series = {UKICER '23}
}

@inproceedings{10.1145/3649165.3703622,
author = {Alshaigy, Bedour and Grande, Virginia and Kiesler, Natalie and Settle, Amber},
title = {How Do You Solve A Problem Like Recruitment? On The Hiring and Retention of Computing Academics},
year = {2024},
isbn = {9798400705984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649165.3703622},
doi = {10.1145/3649165.3703622},
abstract = {This paper critically examines persistent inequities in existing computing faculty hiring and retention practices, which gravely impact computing educators from marginalized groups. Throughout these processes, applicants fight against multiple systemic barriers, including but not limited to, biased job ads and discriminatory interview practices. The increasing use of generative AI tools to aid in tasks connected to the hiring process, such as writing recommendation letters, exacerbates these biases. The inequities persist despite global initiatives and legal mandates and serve as a direct contradiction to widespread institutional commitments to diversity and inclusion. By building on literature and the lived experiences of the SIGCSE community represented in a recent Technical Symposium session, we raise concerns about the different stages of this process, highlighting the importance of clear expectations and adequate support. The paper concludes with a call to align hiring practices with inclusive institutional values, requiring the academic community to reflect on and revise hiring policies for a more equitable future. It is of paramount importance to address the role of these practices in the erosion of marginalized communities from the computing education community, a marginalization that occurs in many different contexts and negatively impacts everyone involved.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1},
pages = {263–266},
numpages = {4},
keywords = {CS academics, recruitment, retention},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@inproceedings{10.1145/3701716.3715165,
author = {Rossetto, Luca and Sarasua, Cristina and Inel, Oana and Bermeo, Juan Diego and Bola\~{n}os, Pablo Sebastian and Devimeux, Hugues and Duan, Huiran and Hany, Robin and Kyriakou, Athina and Ramasamy, Dhivyabharathi and Ravikumar, Varun Ghat and Timoleon, Konstantina and Vasu, Rosni and Wang, Ruijie and van der Weijden, Daan and Willi, Jan and Bernstein, Abraham},
title = {Alan's Speakeasy - An Ecosystem for the Evaluation of Conversational Agents},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715165},
doi = {10.1145/3701716.3715165},
abstract = {Conversational artificially intelligent agents have received increasing attention in recent years. For many use cases, evaluating such agents requires a human-in-the-loop approach. In this demo paper, we present Alan's Speakeasy, a Web-based ecosystem for the evaluation of conversational agents.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2895–2898},
numpages = {4},
keywords = {agent evaluation, conversational agents, turing-test},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3689492.3689815,
author = {Wirfs-Brock, Rebecca and Wirfs-Brock, Allen and Wirfs-Brock, Jordan},
title = {Discovering Your Software Umwelt},
year = {2024},
isbn = {9798400712159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689492.3689815},
doi = {10.1145/3689492.3689815},
abstract = {We apply the biological-behavioral concept of an umwelt, which is how an organism perceives and acts within its environment, to the practice of software development. By writing narrative descriptions of our own software umwelts and iteratively discussing and analyzing them, we develop prompts that can elicit reflection on how and why we relate to software in the ways that we do.},
booktitle = {Proceedings of the 2024 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {288–302},
numpages = {15},
keywords = {cultures of programming, reflective practices, software development careers, software development practices, umwelt theory},
location = {Pasadena, CA, USA},
series = {Onward! '24}
}

@inproceedings{10.1145/3637907.3637988,
author = {Guo, Xue and He, Xiangchun and Pei, Zhuoyun},
title = {Data-driven Personalized Learning},
year = {2024},
isbn = {9798400716676},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637907.3637988},
doi = {10.1145/3637907.3637988},
abstract = {With the continuous development of big data, artificial intelligence and other technologies, education is becoming more and more intelligent, personalized and accurate, accelerating the process of education modernization in China. On the basis of analyzing the connotation of data-driven and personalized learning, this paper sorts out the main research aspects of data-driven personalized learning at present, and proposes a data-driven personalized learning mechanism from four aspects of data-driven. Through data collection, data modeling, data analysis and data feedback, data collection of learners is completed, characteristics of learners are analyzed, and digital portraits are formed. chatGPT and other generative artificial intelligence are used to provide accurate personalized services for learners and promote the personalized development of learners. Research shows that data-driven personalized learning is more scientific, precise, intelligent and diversified.},
booktitle = {Proceedings of the 2023 6th International Conference on Educational Technology Management},
pages = {49–54},
numpages = {6},
keywords = {Big data, Data-driven, Personalized learning},
location = {Guangzhou, China},
series = {ICETM '23}
}

@proceedings{10.1145/3599732,
title = {SIGUCCS '24: Proceedings of the 2024 ACM SIGUCCS Annual Conference},
year = {2024},
isbn = {9798400702266},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Chicago, IL, USA}
}

@inproceedings{10.1145/3641555.3705278,
author = {Jayaraman, Sharanya and Kolarkar, Ameya},
title = {Using Peer Tutoring to Bolster Retention Rates and Student Performance in CS1 Courses},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705278},
doi = {10.1145/3641555.3705278},
abstract = {Active Learning approaches have found success in CS1 and CS2 courses, consolidating instructional time on the practical, problem-solving aspects of programming. With the increasing availability of generative Artificial Intelligence Assistants, there is a renewed push to focus on higher-order skills beyond syntax and solving programming problems by matching sample outputs.This poster examines the impact of conceptual explanation-based exercises in introductory programming courses through the implementation of a scaffolded semi-flipped classroom. This method is currently in its third semester as a part of an ongoing, iterative, semi-experimental approach to support student resilience in entrance-level courses. This approach aimed to enhance student engagement, retention, and performance by integrating weekly practice sessions and "group-tutoring" sessions facilitated by peer learning assistants. In these sessions, students were encouraged to articulate their problem-solving strategies and the reasoning behind their solutions, fostering a deeper understanding of programming language paradigms and problem-solving techniques.The findings indicate that this method significantly increased classroom engagement, as students became more active participants in their learning journey. Retention rates improved as students became more confident in understanding and applying programming concepts. Overall, student performance saw a notable rise, with students demonstrating a better grasp of programming paradigms and problem-solving approaches beyond rote memorization and matching sample outputs.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1495–1496},
numpages = {2},
keywords = {active learning, cs1/cs2, peer-based learning, self-assessment},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3660650.3660663,
author = {Lascelles-Palys, Louis and Lawrence, Ramon},
title = {Live Session Gamification using PrairieLearn},
year = {2024},
isbn = {9798400709975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660650.3660663},
doi = {10.1145/3660650.3660663},
abstract = {Encouraging students to complete practice questions is challenging, especially with numerous distractions and the capabilities of generative AI. Although there are a variety of techniques and systems for synchronous question answering, these systems are limited in the types of questions that can be asked. Gamification has been applied to help motivate students to practice by using incentives such as badges, bonus marks, and competitions. This work developed an extension to the PrairieLearn system allowing for synchronous question and answer sessions with scoreboards and badge awards as student incentives. A key feature is the capability for automatic grading and including complex questions not easily done by other systems, while still making it fun for students to complete. Student feedback in an upper-year course was very positive with students reporting that it encouraged them to complete the questions.},
booktitle = {Proceedings of the 26th Western Canadian Conference on Computing Education},
articleno = {17},
numpages = {2},
keywords = {PrairieLearn, automatic grading, competition, databases, gamification, leaderboard, student engagement},
location = {Kelowna, BC, Canada},
series = {WCCCE '24}
}

@article{10.1145/3708526,
author = {Moreira, Ana and Lago, Patricia and Heldal, Rogardt and Betz, Stefanie and Brooks, Ian and Capilla, Rafael and Coroam\u{a}, Vlad Constantin and Duboc, Leticia and Fernandes, Jo\~{a}o Paulo and Leifler, Ola and Nguyen, Ngoc-Thanh and Oyedeji, Shola and Penzenstadler, Birgit and Peters, Anne-Kathrin and Porras, Jari and Venters, Colin C.},
title = {A Roadmap for Integrating Sustainability into Software Engineering Education},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3708526},
doi = {10.1145/3708526},
abstract = {The world faces escalating crises: record-breaking temperatures, widespread fires, severe flooding, increased oceanic microplastics, and unequal resource distribution. Academia introduces courses around sustainability to meet the new demand, but software engineering education lags behind. While software systems contribute to environmental issues through high energy consumption, they also hold the potential for solutions, such as more efficient and equitable resource management. Yet, sustainability remains a low priority for many businesses, including those in the digital sector. Business as usual is no longer viable. A transformational change in software engineering education is urgently needed. We must move beyond traditional curriculum models and fully integrate sustainability into every aspect of software development. By embedding sustainability as a core competency, we can equip future engineers not only to minimise harm but also to innovate solutions that drive positive, sustainable change. Only with such a shift can software engineering education meet the demands of a world in crisis and prepare students to lead the next generation of sustainable technology. This article discusses a set of challenges and proposes a customisable education roadmap for integrating sustainability into the software engineering curricula. These challenges reflect our perspective on key considerations, stemming from regular, intensive discussions in regular workshops among the authors and the community, as well as our extensive research and teaching experience in the field.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {139},
numpages = {27},
keywords = {Software engineering, Sustainability, Computing, Education, Software sustainability, Sustainable software, Sustainable development goals, Software competencies, Sustainability skills}
}

@inproceedings{10.1145/3626253.3633407,
author = {Westerlund, Jill and Czajka, Sandra and Kuemmel, Andrew},
title = {Innovative Strategies for genAI in CS Courses},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3633407},
doi = {10.1145/3626253.3633407},
abstract = {Students are using generative artificial intelligence (genAI), organizations are embracing AI and machine learning, tools are emerging almost daily, and addressing these evolving technologies can be overwhelming. Rather than choosing to ignore genAI, instructors of computer science (CS) can find ways to teach with and guide students in the use of genAI in their courses. Teaching about genAI can be incorporated with instruction about effective and appropriate uses of the ever-growing tools.This special session brings together three experienced CS educators who integrate genAI in their work with high school students, college students, and in-service teachers. The session environment allows for participant involvement in three model activities that showcase genAI tools with learner-focused practices. Participants will be provided supporting teaching resources for each guided activity and encouraged to discuss with peers and presenters.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1875–1876},
numpages = {2},
keywords = {ai, assessment, genai, instruction},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3631802.3631832,
author = {Hou, Xinying and Ericson, Barbara Jane and Wang, Xu},
title = {Understanding the Effects of Using Parsons Problems to Scaffold Code Writing for Students with Varying CS Self-Efficacy Levels},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631832},
doi = {10.1145/3631802.3631832},
abstract = {Introductory programming courses aim to teach students to write code independently. However, transitioning from studying worked examples to generating their own code is often difficult and frustrating for students, especially those with lower CS self-efficacy in general. Therefore, we investigated the impact of using Parsons problems as a code-writing scaffold for students with varying levels of CS self-efficacy. Parsons problems are programming tasks where students arrange mixed-up code blocks in the correct order. We conducted a between-subjects study with undergraduate students (N=89) on a topic where students have limited code-writing expertise. Students were randomly assigned to one of two conditions. Students in one condition practiced writing code without any scaffolding, while students in the other condition were provided with scaffolding in the form of an equivalent Parsons problem. We found that, for students with low CS self-efficacy levels, those who received scaffolding achieved significantly higher practice performance and in-practice problem-solving efficiency compared to those without any scaffolding. Furthermore, when given Parsons problems as scaffolding during practice, students with lower CS self-efficacy were more likely to solve them. In addition, students with higher pre-practice knowledge on the topic were more likely to effectively use the Parsons scaffolding. This study provides evidence for the benefits of using Parsons problems to scaffold students’ write-code activities. It also has implications for optimizing the Parsons scaffolding experience for students, including providing personalized and adaptive Parsons problems based on the student’s current problem-solving status.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {1},
numpages = {12},
keywords = {Code writing, Hint, Introductory Programming, Parsons problems, Scaffolding, Self-Efficacy, Undergraduate CS},
location = {Koli, Finland},
series = {Koli Calling '23}
}

@inproceedings{10.1145/3706598.3713196,
author = {Ko, Amy J and Aldana Lira, Carlos and Amaya, Isabel},
title = {Wordplay: Accessible, Multilingual, Interactive Typography},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713196},
doi = {10.1145/3706598.3713196},
abstract = {Educational programming languages (EPLs) are rarely designed to be both accessible and multilingual. We describe a 30-month community-engaged case study to surface design challenges at this intersection, creating Wordplay, an accessible, multilingual platform for youth to program interactive typography. Wordplay combines functional programming, multilingual text, multimodal editors, time travel debugging, and teacher- and youth-centered community governance. Across five 2-hour focus group sessions, a group of 6 multilingual students and teachers affirmed many of the platform’s design choices, but reinforced that design at the margins was unfinished, including support for limited internet access, decade-old devices, and high turnover of device use by students with different access, language, and attentional needs. The group also highlighted open source platforms like GitHub as unsuitable for engaging youth. These findings suggest that EPLs that are both accessible and language-inclusive are feasible, but that there remain many design tensions between language design, learnability, accessibility, culture, and governance.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {220},
numpages = {20},
keywords = {programming language, computing education},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3643834.3660722,
author = {Bhat, Maalvika and Long, Duri},
title = {Designing Interactive Explainable AI Tools for Algorithmic Literacy and Transparency},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3660722},
doi = {10.1145/3643834.3660722},
abstract = {As artificial intelligence (AI) increasingly permeates everyday life, there is a growing need for public understanding of AI’s underlying principles. Existing educational interventions and explainable AI (XAI) tools cater mainly to children or adult experts. In this paper, we present three interactive web-based tools to foster AI learning among adults without technical backgrounds. Designed according to learning sciences and user-centered design principles, these tools simplify complex AI concepts like edge detection, confidence thresholds, and sensitivity, making AI more understandable for beginners and facilitating reflection on ethical issues. We present results from a mixed-methods evaluation of the tools with 42 participants. Results show heightened familiarity and confidence in AI concepts. Our qualitative analysis additionally reveals common interaction patterns amongst participants. This paper offers both a design contribution to the AI education and XAI communities and emergent interaction patterns to support the design of transparent and learner-centered AI for adult novices.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {939–957},
numpages = {19},
keywords = {AI Literacy, AI Transparency, Cognitive Interaction, Confidence Threshold, Design Theory, Edge Detection, Explainable AI, Interaction Design, Sensitivity, Technology-Mediated Learning, User Research},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@article{10.1145/3717596.3717599,
author = {Smith, Julie M.},
title = {In Memoriam: Brett Becker},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {1},
issn = {0097-8418},
url = {https://doi.org/10.1145/3717596.3717599},
doi = {10.1145/3717596.3717599},
abstract = {The SIGCSE community was saddened by the recent loss of Brett Becker (1976 - 2024). He had been serving as vice-chair of SIGCSE. He was an assistant professor in the School of Computer Science at University College Dublin, founder and chair of the Ireland SIGCSE chapter, and associate editor of ACM TOCE. He was also the inaugural chair of the ACM Global Computing Education conference.},
journal = {SIGCSE Bull.},
month = feb,
pages = {4–6},
numpages = {3}
}

@inproceedings{10.1145/3706599.3706666,
author = {Yin, Peggy and Cai, Alice and Chen, Sofia and Baradari, Aida and Gao, Xinyi},
title = {Conflux Collective: A Student-Driven Model for HCI Education Through the Lens of Art-Tech},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3706666},
doi = {10.1145/3706599.3706666},
abstract = {As emerging technologies disrupt traditional ways of innovating and creating, higher education will require new models of interdisciplinary learning. In this case study, we review the operating model of Conflux Collective, a student-driven organization at Harvard University that facilitates interdisciplinary human-computer interaction (HCI) education through art-tech projects. We then outline our organizational impact, challenges, and strategic recommendations for designing HCI-inspired organizations. Our findings aim to guide educators, student organizers, and institutions interested in developing similar programs that embrace disciplinary diversity.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {664},
numpages = {10},
keywords = {higher education, arts and humanities, creative technology},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3623762.3633498,
author = {Ericson, Barbara J. and Pearce, Janice L. and Rodger, Susan H. and Csizmadia, Andrew and Garcia, Rita and Gutierrez, Francisco J. and Liaskos, Konstantinos and Padiyath, Aadarsh and Scott, Michael James and Smith, David H. and Warriem, Jayakrishnan M. and Zavaleta Bernuy, Angela},
title = {Multi-Institutional Multi-National Studies of Parsons Problems},
year = {2023},
isbn = {9798400704055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623762.3633498},
doi = {10.1145/3623762.3633498},
abstract = {Students are often asked to learn programming by writing code from scratch. However, many novices struggle to write code and get frustrated when their code does not work. Parsons problems can reduce the difficulty of a coding problem by providing mixed-up blocks the learner rearranges into the correct order. These mixed-up blocks can include distractor blocks that are not needed in a correct solution. Distractor blocks can include common errors, which may help students learn to recognize and fix such errors. Evidence suggests students find Parsons problems engaging, useful for learning to program, and typically easier and faster to solve than writing code from scratch, but with equivalent learning gains. Most research on Parsons problems prior to this work has been conducted at a single institution. This work addresses the need for replication across multiple contexts.A 2022 ITiCSE Parsons Problems Working Group conducted an extensive literature review of Parsons problems, designed several experimental studies for Parsons problems in Python, and created 'study-in-a-box' materials to help instructors run the experimental studies, but the 2022 working group had only sufficient time to pilot two of these studies.Our 2023 ITiCSE Parsons Problems Working Group reviewed these studies, revised some of the studies, expanded both the programming and natural languages used in some of the studies, created new studies, conducted think-aloud observations on some of the studies, and ran both revised as well as new experimental studies. The think-aloud observations and experimental studies provide evidence for using Parsons problems to help students learn common algorithms such as swap, and the usefulness of distractors in helping students learn to recognize, fix, and avoid common errors. In addition, our 2023 ITiCSE Parsons Problems Working Group reviewed Parsons problem papers published after the 2022 literature review and provided a literature review of multi-national (MIMN) studies conducted in computer science education to better understand the motivations and challenges in performing such MIMN studies.In summary, this article contributes an analysis of recent Parsons problem research papers, an itemization of considerations for MIMN studies, the results from our MIMN studies of Parsons problems, and a discussion of recent and future directions for MIMN studies of Parsons problems and more generally.},
booktitle = {Proceedings of the 2023 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {57–107},
numpages = {51},
keywords = {code puzzles, multi-institutional multi-national study, multi-institutional study, multi-national study, parson's problems, parson's programming puzzles, parson's puzzles, parsons problems, parsons puzzles},
location = {Turku, Finland},
series = {ITiCSE-WGR '23}
}

@inproceedings{10.1145/3545945.3569792,
author = {Goetze, Trystan S.},
title = {Integrating Ethics into Computer Science Education: Multi-, Inter-, and Transdisciplinary Approaches},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569792},
doi = {10.1145/3545945.3569792},
abstract = {While calls to integrate ethics into computer science education go back decades, recent high-profile ethical failures related to computing technology by large technology companies, governments, and academic institutions have accelerated the adoption of computer ethics education at all levels of instruction. Discussions of how to integrate ethics into existing computer science programmes often focus on the structure of the intervention---embedded modules or dedicated courses, humanists or computer scientists as ethics instructors---or on the specific content to be included---lists of case studies and essential topics to cover. While proponents of computer ethics education often emphasize the importance of closely connecting ethical and technical content in these initiatives, most do not reflect in depth on the variety of ways in which the disciplines can be combined. In this paper, I deploy a framework from cross-disciplinary studies that categorizes academic projects that work across disciplines as multidisciplinary, interdisciplinary, or transdisciplinary, depending on the degree of integration. When applied to computer ethics education, this framework is orthogonal to the structure and content of the initiative, as I illustrate using examples of dedicated ethics courses and embedded modules. It therefore highlights additional features of cross-disciplinary teaching that need to be considered when planning a computer ethics programme. I argue that computer ethics education should aim to be at least interdisciplinary-multidisciplinary initiatives are less aligned with the pedagogical aims of computer ethics-and that computer ethics educators should experiment with fully transdisciplinary education that could transform computer science as a whole for the better.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {645–651},
numpages = {7},
keywords = {cross-disciplinary studies, data justice, embedded ethics, ethics course, ethics education, higher education, interdisciplinary studies, interdisciplinary teaching and learning, responsible computing, transdisciplinary studies},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3632620.3671126,
author = {Hassan, Mohammed and Zeng, Grace and Zilles, Craig},
title = {Evaluating How Novices Utilize Debuggers and Code Execution to Understand Code},
year = {2024},
isbn = {9798400704758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632620.3671126},
doi = {10.1145/3632620.3671126},
abstract = {&nbsp;&nbsp; Background: Previous work has shown that students can understand more complicated pieces of code through the use of common software development tools (code execution, debuggers) than they can without them. Objectives: Given that tools can enable novice programmers to understand more complex code, we believe that students should be explicitly taught to do so, to facilitate their plan acquisition and development as independent programmers. In order to do so, this paper seeks to understand: (1) the relative utility of these tools, (2) the thought process students use to choose a tool, and (3) the degree to which students can choose an appropriate tool to understand a given piece of code. Method: We used a mixed-methods approach. To explore the relative effectiveness of the tools, we used a randomized control trial study (N = 421) to observe student performance with each tool in understanding a range of different code snippets. To explore tool selection, we used a series of think-aloud interviews (N = 18) where students were presented with a range of code snippets to understand and were allowed to choose which tool they wanted to use. Findings: Overall, novices were more often successful comprehending code when provided with access to code execution, perhaps because it was easier to test a larger set of inputs than the debugger. As code complexity increased (as indicated by cyclomatic complexity), students become more successful with the debugger. We found that novices preferred code execution for simpler or familiar code, to quickly verify their understanding and used the debugger on more complex or unfamiliar code or when they were confused about a small subset of the code. High-performing novices were adept at switching between tools, alternating from a detail-oriented to a broader perspective of the code and vice versa, when necessary. Novices who were unsuccessful tended to be overconfident in their incorrect understanding or did not display a willingness to double check their answers using a debugger. Implications: We can likely teach novices to independently understand code they do not recognize by utilizing code execution and debuggers. Instructors should teach students to recognize when code is complex (e.g., large number of nested loops present), and to carefully step through these loops using debuggers. We should additionally teach students to be cautious to double check their understanding of the code and to self-assess whether they are familiar with the code. They can also be encouraged to strategically switch between execution and debuggers to manage cognitive load, thus maximizing their problem-solving capabilities.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1},
pages = {65–83},
numpages = {19},
keywords = {code comprehension, debuggers, execution},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@article{10.1145/3735548.3729174,
author = {Holbeck, Rick},
title = {Beyond Detection: Why faculty should focus on AI literacy, not AI policing},
year = {2025},
issue_date = {05-01-2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2025},
number = {5},
url = {https://doi.org/10.1145/3735548.3729174},
doi = {10.1145/3735548.3729174},
abstract = {As generative artificial intelligence (GenAI) tools become increasingly accessible, higher education institutions must address the challenge of maintaining academic integrity while preparing students for an AI-integrated future. Institutions that rely solely on AI-detection software risk fostering adversarial learning environments due to false positives and a punitive culture. Instead, a comprehensive approach that integrates AI literacy with institutional policy can foster ethical engagement with AI. Drawing on recent literature and best practices, this article examines the limitations of detection tools. Within are proposed actionable strategies to prepare students to become responsible users of AI and enable institutions to strike a balance between innovation and academic integrity.},
journal = {ELearn},
month = jun,
articleno = {3}
}

@article{10.1145/3688084,
author = {Han, Ariel and Han, Shenshen},
title = {AIstory-bot: An AI-Based Digital Story Writing Platform for Children},
year = {2024},
issue_date = {Fall 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/3688084},
doi = {10.1145/3688084},
abstract = {AIstory-bot, a text-based chatbot designed to teach students about artificial intelligence (AI), guides students through creative writing activities that provide an inquiry-based, interactive storytelling experience to enhance young learners' engagement, motivation, and efficacy toward AI.},
journal = {XRDS},
month = oct,
pages = {26–31},
numpages = {6}
}

@article{10.1145/3719017,
author = {Brown, Noelle and Nurollahian, Sara and Wiese, Eliane S.},
title = {Evaluating Micro-Insertion as a Method for Teaching Responsible Computing: Results from a Randomized Controlled Experiment},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {1},
url = {https://doi.org/10.1145/3719017},
doi = {10.1145/3719017},
abstract = {While there have been many calls for teaching ethics and responsible computing, it is unclear how responsible computing instruction and technical learning interact. Some instructors even hesitate to include ethics in their courses, fearing it might distract students from learning technical computing content. An approach called micro-insertion offers a path to teaching responsible computing while teaching technical content. Micro-insertion situates technical problems in real-world contexts, with frequent, minimally invasive discussions of potential harms of a computing professional’s technical choices. We conducted a between-subjects laboratory study where 82 post-secondary student participants were randomly assigned to one of two instructional conditions to learn a computing topic. The technical-only condition received traditional instruction, while the micro-insertion condition learned the same technical material contextualized within a real-world scenario with potential broader ethical or social impacts. Our study design included a pre-test, instruction intervention, immediate post-test, and a 2-week delayed post-test. Each test included both technical and contextualized questions, and the instructional time was the same across both conditions. The study revealed no statistically significant differences in learning outcomes between the two conditions. Participants in both conditions improved in solving problems presented both abstractly and contextualized with a real-world narrative. Thus, we did not find evidence that our responsible computing intervention distracted participants from learning the technical topics. Further, participants in the technical-only condition could correctly respond to contextualized questions, suggesting that students might be able to engage with responsible computing micro-insertions without targeted instruction. To more deeply understand students’ reasoning, we conducted follow-up interviews with 13 of the participants. The interviews revealed that participants in both conditions perceived to have learned about responsible computing from the study. However, the depth and quality of participants’ engagement with real-world issues varied significantly, with many only superficially addressing the context and many relying on their own personal experiences as their source of knowledge. We, therefore, encourage future studies to investigate other approaches to responsible computing education to determine which approach results in more meaningful consideration of social and ethical issues and whether that detracts from or supports technical learning.},
journal = {ACM Trans. Comput. Educ.},
month = apr,
articleno = {9},
numpages = {33},
keywords = {ethics, responsible computing, computing ethics education}
}

@inproceedings{10.1145/3627217.3627236,
author = {Joseph, Amrit M and Sarma, Soumyadeep and Shelly and Kumar, Viraj},
title = {Bug-eecha 2.0: An Educational Game for CS1 Students and Instructors},
year = {2023},
isbn = {9798400708404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627217.3627236},
doi = {10.1145/3627217.3627236},
abstract = {In prior work, we have proposed Bug-eecha: a web-based game to help novice programmers comprehend programming problems and develop thorough test suites for such problems. This paper makes three contributions. First, based on student feedback from initial testing, we release a revised version of the game (Bug-eecha 2.0) with improvements to key interface elements. Second, this version includes an initial set of 22&nbsp;problems and a web-interface for instructors to create additional problems. We believe that these two contributions will provide introductory programming (CS1) instructors with the resources necessary to experiment with our gamified approach to problem comprehension and test suite development. Third, based on the challenges we faced in creating appropriate questions for Bug-eecha, we identify a need for more research in designing questions that promote the ability of students to create thorough test suites.},
booktitle = {Proceedings of the 16th Annual ACM India Compute Conference},
pages = {61–65},
numpages = {5},
keywords = {educational game, problem comprehension, testing},
location = {Hyderabad, India},
series = {COMPUTE '23}
}

@inproceedings{10.1145/3545947.3576354,
author = {Blouin, Sophie and Solomon, Bridget and Crane, Brent and Dempsey, David and Siegel, Angela and Poitras, Eric},
title = {The Role of Sketching in Facilitating Problem Solving in Introductory Programming},
year = {2023},
isbn = {9781450394338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545947.3576354},
doi = {10.1145/3545947.3576354},
abstract = {This study examined the effectiveness of sketching flow diagrams in facilitating CS1 students' problem solving and performance on code writing tasks. Five students received training in design strategies and sketched flow diagrams prior to implementing their solution by writing code. Procedures in solving a sequence of four problems depicted in sketches and captured in keystroke log data were coded and scored for each student. The study results showed that specific and generative depictions of procedures predict the correctness of edits made to solutions.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1411},
numpages = {1},
keywords = {introductory programming, sketching, strategic programming knowledge},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3545947.3573353,
author = {Brusilovsky, Peter and Ericson, Barbara J. and Horstmann, Cay S. and Servin, Christian and Vahid, Frank and Zilles, Craig},
title = {Significant Trends in CS Educational Material: Current and Future},
year = {2023},
isbn = {9781450394338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545947.3573353},
doi = {10.1145/3545947.3573353},
abstract = {To recognize the current and future trends and challenges in computer science education educational materials for the next decade, the authors of this work provide a conversation to voice the computer science community's experience and expertise on these trends. One of the biggest challenges for introductory computing courses in the next few years will be leveraging the new capabilities of Artificial Intelligent systems such as Open AI CodeX and GPT3 that can generate code from a textual description, explain code, and translate code between programming languages. These tools could drastically change how introductory programming is taught by allowing students to focus more on understanding code, modifying code, and testing code than on writing code. Learning content is increasingly shifting from paper textbooks to online learning systems, which include not just traditional text and figures, but increasingly use interactive items to provide students with better explanations and illustrations, extensive practice, and frequent immediate formative feedback, typically at a lower cognitive load than classical programming assignment. We will discuss challenges and opportunities for interoperability with publishing and learning management platforms. Another example is how guided-based instruments, such as peer team learning, open educational resources, or workbooks, are adaptive and hybrid according to students' needs.Feedback and point of view from the CS community will be considered as part of the curricular practices "Future of CS educational materials" document, featured in the new version of the CS2023: ACM/IEEE-CS/AAAI Computer Science Curricula.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1253},
numpages = {1},
keywords = {adaptive, animation, assessment, automation, computer science, educational materials, feedback, homework, learning, sharing, textbook, videos},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3664934.3664955,
author = {Baldoni, Matteo and Baroglio, Cristina and Bucciarelli, Monica and Micalizio, Roberto and Gandolfi, Elena and Iani, Francesco and Marengo, Elisa and Capecchi, Sara},
title = {Thinking Strategies Training to Support the Development of Machine Learning Understanding. A study targeting fifth-grade children},
year = {2024},
isbn = {9798400716409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664934.3664955},
doi = {10.1145/3664934.3664955},
abstract = {Artificial Intelligence applications permeate our lives and are increasingly making the news, surprising society with applications that until a few years ago would have been relegated to the science fiction genre. Thanks to generative artificial intelligence, tools that once could only be used by highly qualified technical personnel are now in the hands of potentially inexperienced users, but unfortunately, the understanding of the layman is very far from the machinery behind the scenes. More than ever, it is necessary to help people develop an awareness that allows them to use these tools in an appropriate way and with the appropriate expectations. We believe this problem should be addressed by exploring ways to train thinking strategies to facilitate understanding of machine learning concepts that can be applied in daily life, not just by developing teaching tools on this or that topic. We describe our current activities with 9-10 years old children attending primary school and the ad hoc unplugged training we have developed to foster an understanding of machine learning mechanisms.},
booktitle = {Proceedings of the 2024 9th International Conference on Information and Education Innovations},
pages = {85–92},
numpages = {8},
keywords = {Artificial Intelligence, Education, Machine Learning, Thinking strategies, Training},
location = {Verbania, Italy},
series = {ICIEI '24}
}

@article{10.1145/3624726,
author = {Spector, Alfred Z.},
title = {Gaining Benefit from Artificial Intelligence and Data Science: A Three-Part Framework},
year = {2024},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {67},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/3624726},
doi = {10.1145/3624726},
abstract = {Why ethics is not enough.},
journal = {Commun. ACM},
month = jan,
pages = {35–38},
numpages = {4}
}

@inproceedings{10.1145/3632621.3671427,
author = {Hebbar, Sannidhi V and Harini, Sasmita and Raman, Arun and Kumar, Viraj},
title = {Refute Questions for Concrete, Cluttered Specifications},
year = {2024},
isbn = {9798400704765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632621.3671427},
doi = {10.1145/3632621.3671427},
abstract = {Learners must be able to critique AI-generated code, which is often plausible but not always appropriate. Refute Questions have been proposed as one way to develop this ability. Each such question has two components: a task specification and a purported solution (typically, a single function) for that task. Our prior work&nbsp;[1, 4, 5] has focused on Refute Questions where purported solutions are inappropriate because they are logically incorrect. Given such a question, learners must demonstrate their understanding of both the buggy function and the specification by providing an input on which the function’s behavior does not satisfy the specification. In this work, we modify our conception of Refute Questions in two ways. First, we critique not only the correctness of the purported function but also aspects of its design. Specifically, we focus on the appropriateness of: These design aspects are based on the first two steps of Felleisen et al.’s 6-step function design recipe&nbsp;[3]. Second, our prior work has only considered abstract task specifications. In our experience, AI-generated code is often well-designed for such specifications (e.g., see Figure&nbsp;1). Thus, we consider task specifications that are concrete (i.e., they include task-specific details) and cluttered (i.e., they contain extraneous details). Abstracting away certain details from the real-world specifications and eliminating clutter are important and often challenging skills for learners&nbsp;[6, 7, 8, 9, 10]. Further, today’s code-generation tools often produce poorly-designed code given concrete, cluttered specifications (e.g., see Figure&nbsp;2), partly because the performance of the underlying AI model drops substantially as the length of the specification grows&nbsp;[2]. We believe that our new style of Refute Questions may help learners develop the ability to critique such code. We illustrate our process for designing such Refute Questions with an example. We start with an abstract task specification such as shown in Figure&nbsp;11. Next, we manually invent concretizations for the abstract terms ‘integers’ and ‘sign’ (respectively: ‘bank accounts’ and their ‘type’ – active, dormant, or overdrawn). Further, we clutter the specification by inventing extraneous facts about the percentage of permissible accounts of each type. For these questions, learners critique the AI-generated code by answering a series of True/False questions related to the design aspects noted previously. Further, if the design is appropriate, learners critique the logical correctness of the code, as with standard Refute Questions. This poster will present findings from an initial study of Refute Questions for Concrete, Cluttered Specifications in the context of an online course on programming with AI code-generation tools2, which runs in June 2024.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 2},
pages = {539–540},
numpages = {2},
keywords = {Code critique, Function design, Refute Questions},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@inproceedings{10.1145/3641237.3691673,
author = {Trim, Michelle and Butler, Erin and Suttcliffe, Christina},
title = {Seeing How the Sausage is Made: Data Storytelling as Means and Method in a Computer Science Writing Course},
year = {2024},
isbn = {9798400705199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641237.3691673},
doi = {10.1145/3641237.3691673},
abstract = {As data corpus-driven tools and technologies increasingly push users to passively search for an answer, rather than search to understand, we believe that technical and computing disciplinary writing courses have a duty to teach the process of responsible data storytelling. While students can grasp that generative AI makes mistakes, hallucinates, and perpetuates bias, they can need help understanding the antecedent causes of those difficulties. All algorithmically driven decision-making or recommending software have in common a large data set that has been labeled, either by users or by the system itself. The origins of that data and the reasonable applications/deductions and conclusions possible for any given dataset have everything to do with why some tools help and some tools perpetuate harms. By starting at the very beginning and asking students to make sense of data, students can more easily see how purpose and audience impact analysis of any given collection of data. Once those opportunities for rhetorical choice making are known, students become ready to understand the connection between data and complex A.I. systems and some of the ways that bias and other kinds of harm can result if designers are not careful. Combining instruction in a technical coding environment with basic data literacy lessons such as ‘the seven data stories,’ [14] we developed and delivered a three-week writing unit designed around responsible data exploration and storytelling. In this experience report, we provide the assignment we used, and the scaffolded activities we employed to bring students through the process, remarking on what worked well and what we want to improve. We provide attendees with a link to an R-based notebook with a walk-through lesson on data exploration commands, and the rubric used to assess students’ texts, notebooks with code and commentary and results, all existing in a referential context. We provide the survey results of students’ perception of learning from this activity. Early findings demonstrate that students internalized lessons about the non-objective nature of data analysis and of specific responsible data storytelling practices required by anyone seeking to ethically represent answers within and limitations of any dataset.},
booktitle = {Proceedings of the 42nd ACM International Conference on Design of Communication},
pages = {217–222},
numpages = {6},
keywords = {Data Visualization, Data storytelling, Pedagogy, Technical communication},
location = {Fairfax, VA, USA},
series = {SIGDOC '24}
}

@inproceedings{10.1145/3708557.3716150,
author = {Bhat, Maalvika},
title = {Designing AI Interfaces for Transparent Decision-Making and Ethical Reflection},
year = {2025},
isbn = {9798400714092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708557.3716150},
doi = {10.1145/3708557.3716150},
abstract = {As artificial intelligence (AI) systems increasingly mediate high-stakes decisions in domains such as healthcare, finance, and education, ensuring transparency and ethical accountability in AI interfaces is critical. However, existing interfaces often obscure algorithmic processes, leading to overtrust, disengagement, or misinterpretation of AI-generated outputs. My research explores how interface design—including presentation modes, interactive explainability tools, and speculative design interventions—can shape user perceptions, foster critical reflection, and enhance AI literacy. By integrating controlled experiments, participatory design, and qualitative analysis, my work aims to develop AI interfaces that not only communicate algorithmic decisions effectively but also encourage users to critically assess the ethical implications of AI technologies. I examine the trade-offs between transparency, usability, and engagement, investigating how interfaces can balance cognitive load while making ethical considerations more salient. Through this doctoral consortium, I seek mentorship and feedback on designing adaptive transparency mechanisms and mitigating overtrust in engaging AI interfaces.},
booktitle = {Companion Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {211–214},
numpages = {4},
keywords = {Explainable AI, AI Literacy, AI Transparency, Technology-Mediated Learning, Cognitive Interaction, Interaction Design, User Research, Trust, Control, Agency, Design Theory},
location = {
},
series = {IUI '25 Companion}
}

@inproceedings{10.1145/3723010.3723025,
author = {Ishmael, Ontiretse and Kiely, Etain and Healy, John},
title = {Soft Skills as Predictors of Success in Software Engineering Through Analysis of Confidence, Adaptability and Time Management},
year = {2025},
isbn = {9798400712821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3723010.3723025},
doi = {10.1145/3723010.3723025},
abstract = {While successful software engineering students must demonstrate robust technical skills, the ability to acquire and master soft skills is just as crucial to their long-term professional success. These abilities are essential as they compliment technical expertise. Currently, research on soft skills primarily focuses on areas such as communication, teamwork and leadership. However, there is limited research exploring how confidence, adaptability and time management correlate with academic performance and contribute to building better prepared students for the job market. Time management, adaptation and building confidence are crucial non-technical abilities that allow software engineering students to succeed in a holistic way in academia and in their careers. In addition, the belief of a student in their own abilities to accomplish tasks and understand the concepts or modules learning outcomes presented to them forms the foundation of their growth. This study investigated whether soft skills such as confidence, adaptation and time management can be a predictor of success for software engineering students. The analysis of student self-reported data identified four principal clusters: high achievers (46.4%), mid-high performers (33.3%), mid-performers (13%) and developing performers (7.2%). Performance indicators suggest high reliability in grade prediction with an R2 value of 0.837 indicating that approximately 83.7% of the variance in student grades can be explained by the model results. Furthermore, an RMSE of 3.63 indicates that predictions deviate by less than 4 points on a 100 point scale. A standard deviation of RMSE (0.98) suggests consistent prediction accuracy across different student data. This study shows that software engineering curricula should also focus on equipping students with not only cutting edge technological tools and technical skills but also the most effective approaches to education by incorporating these soft skills.},
booktitle = {Proceedings of the 6th European Conference on Software Engineering Education},
pages = {200–209},
numpages = {10},
keywords = {soft skills, confidence, students, adaptability, time management},
location = {
},
series = {ECSEE '25}
}

@inproceedings{10.1145/3687311.3687323,
author = {Hu, Bingying and Ni, Qin and Gao, Rong},
title = {Development Strategies and Practical Insights of Teachers' Artificial Intelligence Competence},
year = {2024},
isbn = {9798400709920},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3687311.3687323},
doi = {10.1145/3687311.3687323},
abstract = {Artificial Intelligence (AI) in education not only changes the education model but also transforms the role of teachers from traditional "knowledge transmitters" to "learning guides" and "skills trainers". This transformation underscores the importance of teachers in leveraging AI for educational enhancement and professional growth, highlighting the need for improved AI competencies. We adhere to human-centered principles and follow ethical and responsible AI ethical standards to ensure that technology truly serves education. Through a literature review from an international perspective, this study examines global trends in AI educational practices, comparing policies, curricula, and research across Asia, Europe, and Africa. This comparison facilitates a deep understanding of AI's application and challenges within various cultural and educational settings, offering insights into enhancing teacher's AI competencies. The paper concludes by suggesting a detailed strategy for bolstering teachers' AI skills, employing a stepwise approach to foster professional development and elevate educational quality in the AI era while providing guidance for teachers to use AI critically and transformatively.},
booktitle = {Proceedings of the 2024 International Conference on Intelligent Education and Computer Technology},
pages = {1–0},
location = {Guilin, China},
series = {IECT '24}
}

@inproceedings{10.1145/3722237.3722279,
author = {Tao, Si and Liu, Hua and Wang, Suyuan},
title = {AI Empowers the Construction of Marketing Courses in Application-Oriented Universities——From the Perspective of OBE Teaching Philosophy},
year = {2025},
isbn = {9798400712692},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3722237.3722279},
doi = {10.1145/3722237.3722279},
abstract = {In recent years, the disruptive development of Artificial Intelligence (AI) technology has triggered a series of industrial changes. How to cultivate the digital and intelligent literacy of students in marketing majors to meet the needs of marketing marketing is a huge challenge facing the marketing marketing course. In this context, based on the AI technology environment and from the perspective of OBE education philosophy, starting from the market's demand for graduates majoring in marketing, this article redesigns the teaching objectives, teaching content and methods, and teaching evaluation methods of marketing courses to cultivate modern intelligent marketing talents that meet market demand.},
booktitle = {Proceedings of the 2024 3rd International Conference on Artificial Intelligence and Education},
pages = {239–244},
numpages = {6},
keywords = {AI, Marketing, OBE, Teaching Design},
location = {
},
series = {ICAIE '24}
}

@inproceedings{10.1145/3641235.3664442,
author = {Mitch, Roman},
title = {The "How-To" as a Post-AI Assessment Tool},
year = {2024},
isbn = {9798400705175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641235.3664442},
doi = {10.1145/3641235.3664442},
abstract = {This paper introduces a simple but innovative assignment technique, the "How-To" as a way to simplify the relationship with AI in assessment events. As an assignment, the "How-To" guide merges reflective practice with technical skill development in technical or creative fields, inviting students to author a guide based on insights from prior coursework, personal projects or specific directed challenges. This method emphasizes reciprocal learning by engaging students in a reflective process where they must articulate and analyze their technical learning, thus reinforcing their understanding and evidencing their mastery of the given learning outcome. This technique could be more broadly applicable, building on the large success of online how-to tutorials; but in flipping the emphasis from passive consumption to productive engagement, the discussed assignment encourages a blend of narrative, technical detail, and creative expression, culminating in a resource that benefits both the author, their peers and potentially course development itself. This presentation will explore how the assignment’s framework develops as an extension of the principle of ‘ako’, a Mundefinedori concept emphasizing reciprocal learning. The discussion will focus on its application within the specific educational context of Aotearoa New Zealand and the broader value it offers to creative and technical education.},
booktitle = {ACM SIGGRAPH 2024 Educator's Forum},
articleno = {15},
numpages = {2},
keywords = {Creative Coding, Critical Analysis, Design, Education, Interaction, Multimedia, Undergraduate},
location = {Denver, CO, USA},
series = {SIGGRAPH '24}
}

@inproceedings{10.1145/3591196.3596826,
author = {Tanimoto, Steven L. and Inie, Nanna},
title = {The Creativity Game: A Game for Teaching First Steps of Theoretical Creativity},
year = {2023},
isbn = {9798400701801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591196.3596826},
doi = {10.1145/3591196.3596826},
abstract = {Creativity is often highlighted as one of the most relevant competencies or skills of the 21st century. Teaching about theoretical underpinnings of creativity has therefore become relevant in numerous fields, from computer science to business management. If we consider creativity as a literacy, like writing or programming, it is important that people from different backgrounds can learn about the basic creativity concepts and how these might be manifested in practice. This article presents the design rationale of The Creativity Game, a simple online game intended to teach the player about some of the very basic properties or concepts in creativity theory: exploration, value, novelty, constraints, and transformation. The Creativity Game is a prototype presented here to spark conversation about how we teach creativity theory in a tangible way.},
booktitle = {Proceedings of the 15th Conference on Creativity and Cognition},
pages = {274–281},
numpages = {8},
keywords = {Boden, Czikszentmihalyi, aesthetic measure, creativity, creativity game, creativity theory, educational game, game, graphic design, novelty measure, state-space search, value measure},
location = {Virtual Event, USA},
series = {C&amp;C '23}
}

@inproceedings{10.1145/3587103.3594146,
author = {Oliveira, Eduardo},
title = {Investigating Student Errors in Code Refactoring},
year = {2023},
isbn = {9798400701399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587103.3594146},
doi = {10.1145/3587103.3594146},
abstract = {Learning to develop code of good quality is challenging. One way to improve code quality is through code refactoring. Students make several mistakes when refactoring code. This research project aims to comprehend student errors in code refactoring, as well as to evaluate how the use of automated tools can help students remediate these errors.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2},
pages = {605–606},
numpages = {2},
keywords = {code quality, code refactoring, programming education, refactoring misconceptions, refactoring tools, student refactoring errors},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3633083.3633161,
author = {Todorova, Christina and Sharkov, George and Aldewereld, Huib and Leijnen, Stefan and Dehghani, Alireza and Marrone, Stefano and Sansone, Carlo and Lynch, Maurice and Pugh, John and Singh, Tarry and Mezei, Kitti and Antal, P\'{e}ter and Han\'{a}k, P\'{e}ter and Barducci, Alessandro and Perez-Tellez, Fernando and Gargiulo, Francesco},
title = {The European AI Tango: Balancing Regulation Innovation and Competitiveness},
year = {2023},
isbn = {9798400716461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3633083.3633161},
doi = {10.1145/3633083.3633161},
abstract = {In the past few years, the EU has shown a growing commitment to address the rapid transformations brought about by the latest Artificial Intelligence (AI) developments by increasing efforts in AI regulation. Nevertheless, despite the growing body of technical knowledge and progress, the governance of AI-intensive technologies remains dynamic and challenging. A mounting chorus of experts have been sharing their reservations regarding an overemphasis on regulation in Europe. Among their core arguments is the concern that such an approach might hinder innovation within the AI arena. This concern resonates particularly strongly compared to the United States and Asia, where AI-driven innovation appears to be surging ahead, potentially leaving Europe behind. The current contribution is a position paper emphasising the need for balanced AI governance to foster ethical innovation, reliability, and competitiveness of European technology. This paper only explores recent AI regulations and upcoming European laws relevant to the topic to ensure conciseness while underscoring Europe’s role in the global AI landscape. The authors analyse European governance approaches and their impact, especially on SMEs and startups, offering a comparative view of global regulatory efforts. We address the complexities of creating a comprehensive, human-centred AI master’s programme for higher education and the importance of ethical AI education. Finally, we discuss how Europe can seize opportunities to promote ethical and reliable AI progress through education, fostering a balanced approach to regulation and enhancing young professionals’ understanding of ethical and legal aspects.},
booktitle = {Proceedings of the 2023 Conference on Human Centered Artificial Intelligence: Education and Practice},
pages = {2–8},
numpages = {7},
keywords = {AI Education, Artificial Intelligence, Business, Ethics, Human-Centred, Innovation, Regulation, SMEs},
location = {Dublin, Ireland},
series = {HCAIep '23}
}

@article{10.5555/3729849.3729855,
author = {Pang, Ashley and Areizaga, Lizbeth and Denzler, Benjamin and Salloum, Mariam and Vahid, Frank},
title = {Code Replacement Detection as a Cheating Detection Approach in Programming Classes},
year = {2025},
issue_date = {April 2025},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {9},
issn = {1937-4771},
abstract = {Similarity checking has long been the main approach for detecting cheating in programming classes. While still a key approach, similarity checking has increasing limitations, due to more ways for students to copy code from online solutions, low-cost contractors, and artificial intelligence code writing tools - - in such cases, a student may copy a program that is not similar to a program from any classmate. However, the rise in use of program auto-graders, version control, and other tools that capture a student's program history provides new cheating detection approach opportunities. One approach detects when a student's program history includes a code replacement - - an instance where a student's code at one time is followed by new code that is clearly an entirely different program. We manually examined program histories for 5 labs in our CS1 class, for 50 random students per lab, and found code replacement prevalence of 12%, with half not turning up in the similarity checker. Detecting code replacement, and from-the-start solution copying, may become important complements to similarity checking, to catch and preferably prevent cheating in programming classes.},
journal = {J. Comput. Sci. Coll.},
month = apr,
pages = {53–61},
numpages = {9}
}

@inproceedings{10.1145/3660650.3660665,
author = {Akhmetov, Ildar and Schmidt, Logan W.},
title = {Building Bridges in Computer Networks: A Nifty Assignment for Cross-Language Learning and Code Refactoring},
year = {2024},
isbn = {9798400709975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660650.3660665},
doi = {10.1145/3660650.3660665},
abstract = {This nifty assignment is designed to introduce students to fundamental networking concepts, such as the client—server model, sockets, and network protocols, through hands-on experience with cross-language programming and code refactoring. The assignment targets students without a prior background in computer science. By engaging students with starter code in C, Python, and Java, the assignment facilitates the understanding of protocols across different programming languages and emphasizes the importance of code reusability and refactoring. Students are tasked with extending server functionality to include custom commands and are encouraged to use AI tools for code development. This approach aims to prepare students for the evolving pedagogical landscape where AI-assisted development plays a significant role in software engineering practices.},
booktitle = {Proceedings of the 26th Western Canadian Conference on Computing Education},
articleno = {22},
numpages = {2},
keywords = {assignment, client—server, networks, sockets},
location = {Kelowna, BC, Canada},
series = {WCCCE '24}
}

@article{10.1145/3622841,
author = {Crichton, Will and Gray, Gavin and Krishnamurthi, Shriram},
title = {A Grounded Conceptual Model for Ownership Types in Rust},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3622841},
doi = {10.1145/3622841},
abstract = {Programmers learning Rust struggle to understand ownership types, Rust’s core mechanism for ensuring memory safety without garbage collection. This paper describes our attempt to systematically design a pedagogy for ownership types. First, we studied Rust developers’ misconceptions of ownership to create the Ownership Inventory, a new instrument for measuring a person’s knowledge of ownership. We found that Rust learners could not connect Rust’s static and dynamic semantics, such as determining why an ill-typed program would (or would not) exhibit undefined behavior. Second, we created a conceptual model of Rust’s semantics that explains borrow checking in terms of flow-sensitive permissions on paths into memory. Third, we implemented a Rust compiler plugin that visualizes programs under the model. Fourth, we integrated the permissions model and visualizations into a broader pedagogy of ownership by writing a new ownership chapter for The Rust Programming Language, a popular Rust textbook. Fifth, we evaluated an initial deployment of our pedagogy against the original version, using reader responses to the Ownership Inventory as a point of comparison. Thus far, the new pedagogy has improved learner scores on the Ownership Inventory by an average of 9},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {265},
numpages = {29},
keywords = {Rust, concept inventory, ownership types, program state visualization}
}

@inproceedings{10.1145/3626252.3630930,
author = {Zavaleta Bernuy, Angela and Ye, Runlong and Sibia, Naaz and Nalluri, Rohita and Williams, Joseph Jay and Petersen, Andrew and Smith, Eric and Simion, Bogdan and Liut, Michael},
title = {Student Interaction with Instructor Emails in Introductory and Upper-Year Computing Courses},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630930},
doi = {10.1145/3626252.3630930},
abstract = {In computing courses, instructor involvement and social comfort are vital for resilience and belonging. We examine engagement with instructor emails aimed at strengthening the connection with students. We sent weekly emails from instructors to first- and upper-year computing students. These emails included reminders for the assignments due each week. Half of the students received reminders embedded in an informal message that contained approachable wording and relevant current course events, while the rest received a list of precise deadlines. This text had no emotional engagement from the instructor. We collected and analyzed email access and link click rates, along with student survey responses about email preferences and engagement. We found that first-year students had lower email access and link click rates than upper-year students. While we did not find differences in first-year engagement based on the type of email, upper-year students appeared to be more engaged when receiving the intentionally informal version of the email. Understanding the message preferences of computing students can enhance instructor messaging and improve engagement. Strategies should be explored to boost first-year student engagement, while the higher engagement among upper-year students underscores the importance of instructor support in advanced courses.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1477–1483},
numpages = {7},
keywords = {cs2, email engagement, upper-year students},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3689187.3709613,
author = {Begum, Marjahan and Crossley, Julia and Str\"{o}mb\"{a}ck, Filip and Akrida, Eleni and Alpizar-Chacon, Isaac and Evans, Abigail and Gross, Joshua B. and Haglund, Pontus and Lonati, Violetta and Satyavolu, Chandrika and Thorgeirsson, Sverrir},
title = {A Pedagogical Framework for Developing Abstraction Skills},
year = {2025},
isbn = {9798400712081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689187.3709613},
doi = {10.1145/3689187.3709613},
abstract = {Abstraction is a fundamental yet challenging skill to teach and learn in Computer Science education. Traditional frameworks of abstraction and concept formation often emphasize understanding an abstraction over its application, the latter being critical for practical Computer Science. Additionally, a common issue in education is when students may understand a concept in a classroom or a very specific setting but struggle to apply it outside of that context. In response, we present here a novel pedagogical framework designed to enhance both the development and application of abstraction skills in diverse educational contexts within the field of Computer Science. Our framework synthesizes common themes from existing models while introducing a new dimension focused explicitly on the actionable development of abstraction skills. Educators can adapt the framework to various educational contexts to support development of students' abstraction skills. Our framework was iteratively developed through a combination of theoretical analysis and reflective practice across multiple teaching contexts. We demonstrate the suitability of the framework by applying it to various case studies, demonstrating its broad applicability and practical utility. By offering a flexible yet comprehensive structure, our framework enables educators to effectively organize and deliver educational content, guiding students from abstract theoretical concepts to their practical application in Computer Science.},
booktitle = {2024 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {258–299},
numpages = {42},
keywords = {CS1 to CS3, abstraction, abstraction skills, algorithmic thinking, cognitive models, computational thinking, concurrency, data structures, educational frameworks, game theory, inferences, pedagogy, pointers, recursion},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@article{10.1613/jair.1.13550,
author = {Javed, Rana Tallal and Nasir, Osama and Borit, Melania and Vanh\'{e}e, Lo\"{\i}s and Zea, Elias and Gupta, Shivam and Vinuesa, Ricardo and Qadir, Junaid},
title = {Get out of the BAG! Silos in AI Ethics Education: Unsupervised Topic Modeling Analysis of Global AI Curricula},
year = {2022},
issue_date = {May 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {73},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.13550},
doi = {10.1613/jair.1.13550},
abstract = {The domain of Artificial Intelligence (AI) ethics is not new, with discussions going back at least 40 years. Teaching the principles and requirements of ethical AI to students is considered an essential part of this domain, with an increasing number of technical AI courses taught at several higher-education institutions around the globe including content related to ethics. By using Latent Dirichlet Allocation (LDA), a generative probabilistic topic model, this study uncovers topics in teaching ethics in AI courses and their trends related to where the courses are taught, by whom, and at what level of cognitive complexity and specificity according to Bloom’s taxonomy. In this exploratory study based on unsupervised machine learning, we analyzed a total of 166 courses: 116 from North American universities, 11 from Asia, 36 from Europe, and 10 from other regions. Based on this analysis, we were able to synthesize a model of teaching approaches, which we call BAG (Build, Assess, and Govern), that combines specific cognitive levels, course content topics, and disciplines affiliated with the department(s) in charge of the course. We critically assess the implications of this teaching paradigm and provide suggestions about how to move away from these practices. We challenge teaching practitioners and program coordinators to reflect on their usual procedures so that they may expand their methodology beyond the confines of stereotypical thought and traditional biases regarding what disciplines should teach and how.
This article appears in the AI &amp; Society track.},
journal = {J. Artif. Int. Res.},
month = may,
numpages = {33},
keywords = {philosophical foundations, scientific discovery, data mining, discourse modelling}
}

@article{10.1145/3587264,
author = {Kim, Ji-Hoon and Yoo, Sungyeob and Kim, Joo-Young},
title = {South Korea's Nationwide Effort for AI Semiconductor Industry},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {66},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/3587264},
doi = {10.1145/3587264},
journal = {Commun. ACM},
month = jun,
pages = {46–51},
numpages = {6}
}

@inproceedings{10.1145/3545947.3576347,
author = {Hennen, Alexa and Hahnfeldt, Cameron and Potter, Grace and Li, Mengzhen and Ohmann, Peter},
title = {SynpleTest: Using Program Synthesis as a Teaching Aid},
year = {2023},
isbn = {9781450394338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545947.3576347},
doi = {10.1145/3545947.3576347},
abstract = {This poster presents SynpleTest, a teaching tool created for introductory computer science courses. SynpleTest uses program synthesis to generate code based on test cases given by the user. Students must continue to add test cases until SynpleTest generates the program they desire. By doing this, our program teaches the student the importance of producing thoughtful and diverse test cases to improve fundamental understanding of testing programs. Preliminary experimentation in classrooms is underway and initial results of usability and effectiveness are being analyzed.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1403},
numpages = {1},
keywords = {computer science education, cs1, introductory computer science, program synthesis, testing},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3641554.3701862,
author = {Challen, Geoffrey and Nordick, Ben},
title = {Accelerating Accurate Assignment Authoring Using Solution-Generated Autograders},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701862},
doi = {10.1145/3641554.3701862},
abstract = {Students learning to program benefit from access to large numbers of practice problems. Autograders are commonly used to support programming questions by providing quick feedback on submissions. But authoring accurate autograders remains challenging. Autograders are frequently created by enumerating test cases-a tedious process that can produce inaccurate autograders that fail to correctly classify submissions. When authoring accurate autograders is slow, it is difficult to create large banks of practice problems to support beginning programmers.We present solution-generated autograding: a faster, more accurate, and more enjoyable way to create autograders. Our approach leverages a key difference between software testing and autograding: The question author can provide a solution. By starting with a solution, we can eliminate the need to manually enumerate test cases, validate the autograder's accuracy, and evaluate other aspects of submission code quality beyond behavioral correctness. We describe Questioner, an implementation of solution-generated autograding for Java and Kotlin, and share experiences from four years using Questioner to support a large CS1 course: authoring nearly 800 programming questions used by thousands of students to evaluate millions of submissions.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {227–233},
numpages = {7},
keywords = {autograding, code quality evaluation, problem authoring},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.1145/3618304,
author = {Mengi, Gopal},
title = {Empowering Excellence: The Impact of ACM Student Chapters},
year = {2023},
issue_date = {Fall 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1528-4972},
url = {https://doi.org/10.1145/3618304},
doi = {10.1145/3618304},
journal = {XRDS},
month = oct,
pages = {8–9},
numpages = {2}
}

@inproceedings{10.1145/3478431.3499329,
author = {Teresco, James D. and Tartaro, Andrea and Holland-Minkley, Amanda and Braught, Grant and Barnard, Jakob and Baldwin, Douglas},
title = {CS Curricular Innovations with a Liberal Arts Philosophy},
year = {2022},
isbn = {9781450390705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478431.3499329},
doi = {10.1145/3478431.3499329},
abstract = {A liberal arts context offers unique opportunities for curricular innovation that can inform the implementation of computing curricula more broadly. The SIGCSE Committee on Computing Education in Liberal Arts Colleges has collected 18 model curricula during affiliated events at SIGCSE symposia over the past two years. Here we conduct a distillation of the curricula and discuss themes across the curricula including: flexible pathways through majors, interdisciplinary initiatives, and preparing students for a range of careers and their first job. Our discussion focuses on how liberal arts colleges are empowered to think creatively about computing curricula and how research intensive universities, community colleges, and K-12 can leverage these approaches for their context.},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education - Volume 1},
pages = {537–543},
numpages = {7},
keywords = {cs education, curricula, liberal arts},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}

@article{10.5555/3722479.3722504,
author = {Huang, Ching-yu and Wang, Paolien},
title = {Leveraging ChatGPT for SQL Learning: An Interactive Approach},
year = {2024},
issue_date = {October 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {3},
issn = {1937-4771},
abstract = {In today's data-driven world, SQL proficiency is essential for data analysts, software developers, and database fields. However, traditional SQL learning methods often rely heavily on classroom lectures and textbooks, resulting in repetitive activities where students tackle the same exercises and questions. This approach can be inefficient, hindering student learning as instructors must individually address syntax errors and runtime issues, limiting their ability to assist all students effectively. Additionally, student engagement and comprehension may suffer, particularly for those with weak foundations who heavily depend on instructor guidance.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {58–59},
numpages = {2}
}

@inproceedings{10.1145/3689187.3709611,
author = {Toti, Giulia and Lindner, Peggy and Gao, Alice and Baghban Karimi, Ouldooz and Engineer, Rutwa and Hur, Jinyoung and McNeill, Fiona and Reckinger, Shanon and Robinson, Rebecca and Sollazzo, Anna and Wicentowski, Richard},
title = {Diversity, Equity, and Inclusion in Computing Science: Culture is the Key, Curriculum Contributes},
year = {2025},
isbn = {9798400712081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689187.3709611},
doi = {10.1145/3689187.3709611},
abstract = {Undergraduate computer science programs worldwide struggle to attract and retain underrepresented students for many reasons. Culture, stereotype threats, uneven gender and racial representations, lack of role models, and uncertain career prospects for minority groups are among the many reasons behind this situation. Many computer science programs are trying to change course through strategies to foster equity, diversity, and inclusion (EDI), aimed at improving outreach, recruitment, admissions, and retention of underrepresented students. EDI approaches may also include modifications to the undergraduate computer science curriculum. However, if not properly planned, these modifications risk amplifying existing stereotypes rather than producing positive change [38]. In this study, through an extensive literature review, a rigorous curriculum analysis of 49 computer science programs across the globe, and qualitative and quantitative analysis of surveys and interviews bringing in the voices of 613 students and 30 educators participating from around the world, we explore equity, diversity, and inclusion in the computer science curriculum. We highlight the role of inclusive content and course design, discuss program flexibility, and the impact of inclusive courses and program design in attracting and retaining historically marginalized students. Finally, we provide concrete steps to make computing science undergraduate curricula more appealing to a diverse audience.},
booktitle = {2024 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {175–225},
numpages = {51},
keywords = {computer science, computing science, diversity, edi, equity, inclusion, undergraduate curriculum},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@article{10.1145/3528088,
author = {Maraninchi, Florence},
title = {Let us not put all our eggs in one basket},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/3528088},
doi = {10.1145/3528088},
abstract = {Toward new research directions in computer science.},
journal = {Commun. ACM},
month = aug,
pages = {35–37},
numpages = {3}
}

@inproceedings{10.1145/3701268.3701273,
author = {Conway, Brian and Nolan, Keith and Quille, Keith},
title = {HCAI Block Model: A competence model for Human Centred Artificial Intelligence at K-12},
year = {2024},
isbn = {9798400711596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701268.3701273},
doi = {10.1145/3701268.3701273},
abstract = {Artificial Intelligence (AI) is becoming a common topic within the computing K-12 curricula worldwide. While much of the focus of research is on the use of Generative AI in and for education, AI as a core subject area is still gaining popularity, with much of this research focusing on content and tools that effectively support the teaching of AI. However, as we grow as a field, there is a need currently unmet to provide foundations (in the form of a block model as there exists for programming) to allow researchers to build strong pedagogies and methodologies from, and even a base to design activities and content. Compounding this, as ethics and its relationship to AI in the K-12 classroom grows stronger, there is a further need to provide scaffolding to educators and researchers not only on traditional AI concepts, but also on how they link with ethical knowledge, skills and dispositions. In this paper, the Human Centered Artificial Intelligence (HCAI) Block Model is developed and introduced. This is a competence-based model to guide effective teaching and learning of Human Centered Artificial Intelligence, as well as research in the K-12 space. The HCAI Block model’s foundation is developed/adapted from the programming Block model and has been adapted and developed using two lenses. The first was through the data science lens through interaction with Computational Thinking 2.0 and competency-based learning. The second lens was through a human-centred lens. The outcome was a ground-up K-12 model where traditional and technical AI concepts have been developed from the start, integrating ethical considerations and human-centred approaches.},
booktitle = {Proceedings of the 2024 Conference on Human Centred Artificial Intelligence - Education and Practice},
pages = {22–28},
numpages = {7},
keywords = {Computing Education, Machine Learning, Human-Centered AI, Block Model, Ethics, Computational Thinking 2.0},
location = {
},
series = {HCAIep '24}
}

@inproceedings{10.1145/3593434.3593952,
author = {Manfredi, Gilda and Erra, Ugo and Gilio, Gabriele},
title = {A Mixed Reality Approach for Innovative Pair Programming Education with a Conversational AI Virtual Avatar},
year = {2023},
isbn = {9798400700446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593434.3593952},
doi = {10.1145/3593434.3593952},
abstract = {Pair Programming (PP) is an Agile software development methodology that involves two developers working together on a single computer. However, the physical presence of two developers has become a challenge in recent years due to the pandemic, necessitating remote collaboration methods such as Distributed Pair Programming (DPP). DPP has been found to have similar benefits to in-person PP, but the issue of team compatibility remains unresolved. These are more evident in the educational field of Agile methodologies. To address these challenges, we developed a novel approach by creating a Mixed Reality (MR) application that enables users to learn PP with the assistance of a conversational intelligent virtual avatar. The application uses the HoloLens MR device and a Conversational Agent (CA) extension integrated into Visual Studio Code to provide suggestions for improving the code written by the user. The virtual avatar animates these suggestions, making it appear to speak and interact with the user in real time. This system aims to overcome the limitations of common DPP methods, allowing a single developer to learn and apply the PP methodology even when a human partner is unavailable.},
booktitle = {Proceedings of the 27th International Conference on Evaluation and Assessment in Software Engineering},
pages = {450–454},
numpages = {5},
keywords = {artificial intelligence, conversational agents, extended reality, pair programming},
location = {Oulu, Finland},
series = {EASE '23}
}

@article{10.1145/3595634,
author = {Jacques, Lorraine},
title = {Teaching CS-101 at the Dawn of ChatGPT},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {2153-2184},
url = {https://doi.org/10.1145/3595634},
doi = {10.1145/3595634},
abstract = {Recent news suggests that the advent of AI-generated coding tools signal the end of humans programming. This news should not, however, suggest that students not learn how to program but instead that instructors rethink how they teach programming. Math education has already addressed the challenge of teaching fluency when there is technology for basic tasks by having students use multiple representations, different approaches, and explanations of others' work to emphasize problem-solving, critical thinking, and communication while still teaching basic skills. These approaches can also be applied to computer science education, especially in an introductory course, and with the same benefits.},
journal = {ACM Inroads},
month = may,
pages = {40–46},
numpages = {7}
}

@article{10.5555/3636517.3636529,
author = {Miller, Elena and Shaw, Katy and Dodds, Zachary},
title = {Choosing Our Computing Birthplace: VSCode vs Colab as GenEd IDEs},
year = {2023},
issue_date = {October 2023},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {1},
issn = {1937-4771},
abstract = {First impressions are important. The initial environment in which our computing students express themselves helps shape their foundational understanding of what computing is, what it's for, and who participates. This work distills experiences and insights from offering Comp1 and Comp21 with two different IDEs: Microsoft's VSCode and Google's Colab. We identify and describe several axes along which we compare our students' experience of these two. This effort has changed the way we offer Comp1, a degree requirement of all students at our institution, and Comp2, an optional follow-up course, required by some computationally-themed programs.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {103–111},
numpages = {9}
}

@article{10.1145/3699853.3699857,
author = {Denny, Paul and Hamilton, Margaret and Porter, Leo and Morrison, Briana},
title = {ICER 2024: Call for Participation},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {3},
issn = {0097-8418},
url = {https://doi.org/10.1145/3699853.3699857},
doi = {10.1145/3699853.3699857},
abstract = {Do you know that the 20th ICER Conference will be happening soon? In Australia?The ACM Conference on International Computing Education Research (ICER) will be held at Storey Hall in RMIT University right in the centre of Melbourne.},
journal = {SIGCSE Bull.},
month = oct,
pages = {5–6},
numpages = {2}
}

@inproceedings{10.1145/3649217.3653614,
author = {Pang, Ashley and Vahid, Frank},
title = {Performance Analysis and Interviews of Non-CS-Major Students Sanctioned for Cheating in CS1},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653614},
doi = {10.1145/3649217.3653614},
abstract = {College cheating is common, including in computer science (CS) classes like introductory programming (CS1). Much research surveys college students about cheating, but few survey students actually caught cheating, or analyze their performance. We analyzed performance of 24 students sanctioned for cheating on programs in our CS1 over three terms, out of 300+ students, mostly non-CS science and engineering majors. Sanctioned students participated less in lectures (scoring 77% vs. 91%, p = 0.00002), and were less earnest in their completion of the online book's readings (51% vs. 80%, p = 0.0000001) during weeks 1-5. Those findings suggest early disengagement which would correlate with a tendency to cheat, and might also suggest a lack of learning which might help cause cheating. Sanctioned students scored dramatically lower on the earlier midterm exam (61% vs. 83%, p = 0.0001). After being sanctioned with Fs in the course (typically post-midterm), most agreed to an optional later interview to help the professor learn how to prevent cheating, at which point most were quite forthright. Key themes included an inability or unwillingness to devote the time needed to learn programming, a belief that the required CS1 course was not important for non-CS majors, and a disbelief that cheating students would be caught or punished despite warnings. More studies of such experiences may help instructors reduce cheating; in our case, the findings suggest instructors should emphasize relevance, make detection/punishment efforts clear, and detect early disengagement and potentially intervene.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {374–380},
numpages = {7},
keywords = {CS1, cheating, intervention, plagiarism, programming},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3626252.3630809,
author = {Gulati, Rishi and West, Matthew and Zilles, Craig and Silva, Mariana},
title = {Comparing the Security of Three Proctoring Regimens for Bring-Your-Own-Device Exams},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630809},
doi = {10.1145/3626252.3630809},
abstract = {We compare the exam security of three proctoring regimens of Bring-Your-Own-Device, synchronous, computer-based exams in a computer science class: online un-proctored, online proctored via Zoom, and in-person proctored. We performed two randomized crossover experiments to compare these proctoring regimens. The first study measured the score advantage students receive while taking un-proctored online exams over Zoom-proctored online exams. The second study measured the score advantage of students taking Zoom-proctored online exams over in-person proctored exams. In both studies, students took six 50-minute exams using their own devices, which included two coding questions and 8--10 non-coding questions. We find that students score 2.3% higher on non-coding questions when taking exams in the un-proctored format compared to Zoom proctoring. No statistically significant advantage was found for the coding questions. While most of the non-coding questions had randomization such that students got different versions, for the few questions where all students received the same exact version, the score advantage escalated to 5.2%. From the second study, we find no statistically significant difference between students' performance on Zoom-proctored vs. in-person proctored exams. With this, we recommend educators incorporate some form of proctoring along with question randomization to mitigate cheating concerns in BYOD exams.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {429–435},
numpages = {7},
keywords = {bring-your-own-device, byod, cheating, computer-based testing, exam security, online, proctoring, randomization},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@article{10.1145/3665604,
author = {Darji, Aarti},
title = {Revolutionizing Cancer Research with AI: Health Data Science Lab, UT Arlington},
year = {2024},
issue_date = {Summer 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1528-4972},
url = {https://doi.org/10.1145/3665604},
doi = {10.1145/3665604},
journal = {XRDS},
month = jun,
pages = {64–65},
numpages = {2}
}

@article{10.1145/3699853.3699856,
author = {Pearson, Tamara and Strickland, Carla},
title = {RESPECT 2024: Conference Recap},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {3},
issn = {0097-8418},
url = {https://doi.org/10.1145/3699853.3699856},
doi = {10.1145/3699853.3699856},
abstract = {The Conference on Research in Equity and Sustained Participation in Engineering, Computing, and Technology is the premier venue for research on equity, inclusion, and justice in computing and computing education. RESPECT 2024, the ninth edition of this conference, was held on May 16-17, 2024 at the Georgia Tech Conference Center and Hotel in Atlanta, Georgia.},
journal = {SIGCSE Bull.},
month = oct,
pages = {3–5},
numpages = {3}
}

@article{10.5555/3729857.3729865,
author = {Kulkarni, Sourabh and Attarwala, Abbas and Raigoza, Jaime},
title = {Impact of COVID-19 on Live-Coding in First-Year Computer Science Education: A Literature Review},
year = {2025},
issue_date = {April 2025},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {6},
issn = {1937-4771},
abstract = {The COVID pandemic has significantly influenced educational methodologies, leading to a shift towards more interactive and technology-integrated teaching approaches. Live-coding, which involves real-time coding demonstrations, has gained recognition as a valuable tool in computer science education. This study investigates the impact of the pandemic on the popularity and application of live-coding. By conducting a comprehensive literature review of 22 research papers, this study categorizes the papers based on their publication date relative to the pandemic: pre-COVID (2017 to 2019), during COVID (2020 to 2022), and post-COVID (2023 to 2024). The papers were selected using a specific search query for live-coding in introductory computer science education on Google Scholar. A systematic literature review was performed to determine their sentiment towards live-coding, categorizing the sentiments as positive, neutral, or negative. The sentiment data were then statistically analyzed using Fisher's Exact Test to assess significant differences across the three periods. Results from this manuscript indicate shifts in positive sentiment towards live-coding during and after the pandemic.},
journal = {J. Comput. Sci. Coll.},
month = apr,
pages = {65–74},
numpages = {10}
}

@inproceedings{10.1145/3576882.3617917,
author = {Johnson, Colin G.},
title = {Building Technological Improvisation Skills through Student-devised Coursework Topics},
year = {2023},
isbn = {9798400700484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576882.3617917},
doi = {10.1145/3576882.3617917},
abstract = {The ability of improvise solutions to problems using a variety of technologies is an important, if often tacit, desired outcome from advanced computer science education. This paper will describe experience from three modules at two universities where students design their own assessment topic, based on a set of requirements aligned with the learning outcomes.The paper discusses why these technological improvisation skills are important, and how students can learn to cope with combining a variety variety of techniques, ideas and technologies that don't immediately fit well together. This helps students to build important high-level and meta-cognitive skills, and at a practical level provides students with projects that they can demonstrate to prospective employers after graduation.We discuss how these assessments were presented to students, and how the resulting student activities were aligned with learning objectives. We describe how complex ideas such as added value were explained to students, and how these assignments encouraged students to work on projects of ambition and substance, and to encourage their curiosity.Finally, we discuss some difficulties with this kind of assessment, and how we have endeavoured to tackle these difficulties. These include how such a diversity of assessments were marked fairly and consistently, how students who are struggling with the core module content can engage effectively, and how these kind of assessments can be devised without an excessive burden on teaching staff.},
booktitle = {Proceedings of the ACM Conference on Global Computing Education Vol 1},
pages = {91–97},
numpages = {7},
keywords = {assessment, constructive alignment, learning objectives, student-centred learning},
location = {Hyderabad, India},
series = {CompEd 2023}
}

@article{10.1145/3713068,
author = {Nicolajsen, Sebastian and Brabrand, Claus},
title = {What Is Programming?},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {68},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3713068},
doi = {10.1145/3713068},
abstract = {&nbsp;... and what is programming in the age of artificial intelligence?},
journal = {Commun. ACM},
month = jun,
pages = {28–30},
numpages = {3}
}

@inproceedings{10.1145/3706599.3719876,
author = {Darabipourshiraz, Hasti and Bhat, Maalvika and Long, Duri},
title = {Introducing AI Without Computers: Hands-On Literacy and Ethical Sense-Making for Young Learners},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719876},
doi = {10.1145/3706599.3719876},
abstract = {Middle school students encounter AI daily, yet they often lack the tools to critically examine AI’s influence on their lives. We seek to address this issue by designing activities where learners engage with AI in personally relevant contexts and "unplugged" interactions foster collaborative ethical reasoning. We present novel designs of four unplugged activities that address learning objectives such as recognizing AI, understanding AI’s capabilities, and reasoning about AI ethics. We conducted a secondary analysis of data collected during a five-day summer workshop with middle school students where our ’unplugged’ activities were integrated into an existing curriculum. Teacher feedback suggests that the activities were engaging and fostered conceptual understanding. Student surveys revealed growth in students’ comprehension of foundational AI concepts and ethical reasoning; however, abstract concepts were not as well understood. This study contributes a model for designing interactive AI education interventions that emphasize personal relevance and collaborative ethical reasoning.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {353},
numpages = {11},
keywords = {AI Literacy, Middle School Education, Collaborative Learning, AI Education, Unplugged, AI Ethics},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3686397.3686404,
author = {Welsey Burris, John and Regis, Paulo},
title = {Bridging Theory and Practice: Preparing Future IT Service Leaders through Experiential Learning in Cybersecurity Policy and Governance},
year = {2024},
isbn = {9798400717345},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686397.3686404},
doi = {10.1145/3686397.3686404},
abstract = {This paper proposes implementing online, multi-disciplinary tabletop exercises as an experiential learning approach to better prepare future IT service leaders in cybersecurity policy and governance. The interactive simulations will present scenarios spanning regulatory compliance, incident response, risk mitigation, and strategic planning across various industries. Participants will assume diverse roles (e.g. CISOs, legal counsel, operations managers) and apply critical thinking, problem-solving, and communication skills to address the evolving situations. This innovative pedagogy leverages cloud-based tools to facilitate remote, team-based learning experiences that bridge the gap between theoretical concepts and pragmatic application while fostering crucial virtual coordination abilities. The paper outlines the design principles, learning objectives, and anticipated outcomes of these cyber tabletop exercises, aiming to cultivate a holistic cybersecurity mindset that empowers future leaders to develop robust governance frameworks aligning people, processes, and technologies.},
booktitle = {Proceedings of the 2024 8th International Conference on Information System and Data Mining},
pages = {39–43},
numpages = {5},
keywords = {IT service management, collaborative learning, cybersecurity policy, education, experiential learning, governance, workforce development},
location = {
},
series = {ICISDM '24}
}

@article{10.1145/3723348,
author = {Beck, Micah D.},
title = {Accept the Consequences},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {68},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/3723348},
doi = {10.1145/3723348},
abstract = {Computer science is based on the cynical acceptance of a mismatch between theory and practice, according to Micah D. Beck.},
journal = {Commun. ACM},
month = jun,
pages = {6–7},
numpages = {2}
}

@article{10.1145/3703922.3703932,
author = {Miedema, Daphne and Aivaloglou, Efthimia and Amer-Yahia, Sihem and Fletcher, George and Mior, Michael and Taipalus, Toni},
title = {Report on the Second International Workshop on DataSystems Education (DataEd '23)},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {3},
issn = {0163-5808},
url = {https://doi.org/10.1145/3703922.3703932},
doi = {10.1145/3703922.3703932},
abstract = {This report summarizes the outcomes of the second international workshop on Data Systems Education: Bridging Education Practice with Education Research (DataEd '23). The workshop was held in conjunction with the SIGMOD '23 conference in Seattle, USA on June 23, 2023. The aim of the workshop was to provide a dedicated venue for presenting and and discussing data management systems education experiences and research by bringing together the database and the computing education research communities to share findings, to crosspollinate perspectives and methods, and to shed light on opportunities for mutual progress in data systems education. The program featured two keynote talks, eight research paper presentations, and a discussion session. In this report, we present the workshop's main results, observations, and emerging research directions.},
journal = {SIGMOD Rec.},
month = nov,
pages = {35–39},
numpages = {5}
}

@inproceedings{10.1145/3629296.3629321,
author = {Beltran Virguez, Jesus Enrique and Jimenez Garcia, Diana Geraldine and Parada Fonseca, Sandra Patricia},
title = {Assessment in virtual higher education in the face of artificial intelligence},
year = {2024},
isbn = {9798400709111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629296.3629321},
doi = {10.1145/3629296.3629321},
abstract = {The paper presents a phenomenological perspective of contemporary higher education and the challenge posed by the use of artificial intelligence in assessment processes within virtual programs. This analysis decisively leads to a critical construction of the current stance education has adopted. In this regard, it proposes an alternative approach that highlights the original notions of Paideia and Bildung as elements that artificial intelligence cannot provide to students in their evaluation process. The paper emphasizes the need to reorient the test and activity design process that certifies students in their knowledge. This involves tapping into the structural characteristics of human beings where machines lack inherent qualities. This approach positions artificial intelligence as a support rather than a replacement in the development of various activities.},
booktitle = {Proceedings of the 15th International Conference on Education Technology and Computers},
pages = {162–167},
numpages = {6},
keywords = {Artificial intelligence, assessment, higher education},
location = {Barcelona, Spain},
series = {ICETC '23}
}

@article{10.1145/3732791,
author = {Chen, Chun-Ying},
title = {Effects of Worked Examples with Explanation Types and Learner Motivation on Cognitive Load and Programming Problem-solving Performance},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3732791},
doi = {10.1145/3732791},
abstract = {This study examined the effects of worked examples with different explanation types and novices’ motivation on cognitive load, and how this subsequently influenced their programming problem-solving performance. Given the study’s emphasis on both instructional approaches and learner motivation, the Cognitive Theory of Multimedia Learning served as the theoretical framework, as it integrates instructional design with motivational perspectives. The participants consisted of 75 university students who were non-computer majors and enrolled in their first programming course. A 2 \texttimes{} 2 between-subjects ANOVA design was employed, with two factors: explanation type (worked examples with instructional explanations vs. worked examples with guided questions to prompt self-explanations) and learner motivation level (high-motivated vs. less-motivated). The dependent variables included cognitive load components experienced by learners during learning and learning outcomes measured by retention and transfer performance. The results showed that (a) the worked example effect could reduce extraneous load and manage intrinsic load, thereby enhancing retention performance; (b) the combined effects of worked examples and guided self-explanations could benefit transfer learning, regardless of learners’ motivation; and (c) the role of motivation was evident, as high-motivated learners exhibited better retention and transfer performance by exerting more cognitive effort, regardless of instructional approach. The findings suggest that combining worked examples with guided questions to prompt self-explanations through in-code commenting is an effective and constructive activity, enabling novices to focus on actual learning without expending excessive cognitive effort.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Educ.},
month = may,
keywords = {the cognitive theory of multimedia learning, cognitive load theory, the worked example effect, the self-explanation effect, the ICAP framework, computing education}
}

@inproceedings{10.1145/3706598.3713173,
author = {Cao, Huajie Jay and Choi, Kahyun and Park, Claire and Lee, Hee Rin},
title = {AI Literacy for Underserved Students: Leveraging Cultural Capital from Underserved Communities for AI Education Research},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713173},
doi = {10.1145/3706598.3713173},
abstract = {As Artificial Intelligence (AI) continues to influence various aspects of society, the need for AI literacy education for K-12 students has grown. An increasing number of AI literacy studies aim to enhance students’ competencies in understanding, using, and critically evaluating AI systems. However, despite the vulnerabilities faced by students from underserved communities—due to factors such as socioeconomic status, gender, and race—these students remain underrepresented in existing research. To address this gap, this study focuses on leveraging the cultural capital that students acquire from their communities’ unique history and culture for AI literacy education. Education researchers have demonstrated that identifying and mobilizing cultural capital is an effective strategy for educating these populations. Through collaboration with 26 students from underserved communities—including those who are socioeconomically disadvantaged, female, or people of color—this paper identifies three types of cultural capital relevant to AI literacy education: 1) resistant capital, 2) communal capital, and 3) creative capital. The study also emphasizes that collaborative relationships between researchers and students are crucial for mobilizing cultural capital in AI literacy education research.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1017},
numpages = {15},
keywords = {Culturl Capital; Artificial Intelligence; AI; AI Literacy; Underserved Communities; Racism, Sexism, Critical Padagogy, Education; K-12},
location = {
},
series = {CHI '25}
}

@article{10.1145/3639711,
author = {Hill, Robin K. and Baquero, Carlos},
title = {Pondering the Ugly Underbelly, and Whether Images Are Real},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {67},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/3639711},
doi = {10.1145/3639711},
abstract = {The Communications website, https://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts.twitterFollow us on Twitter at http://twitter.com/blogCACMhttps://cacm.acm.org/blogs/blog-cacmRobin K. Hill teaches how proofs lead to the truth, while Carlos Baquero searches for truth in imagery.},
journal = {Commun. ACM},
month = feb,
pages = {8–10},
numpages = {3}
}

@inproceedings{10.1145/3708036.3708045,
author = {Zhu, Di and Zhang, Shu and Zhao, Weinan and Han, Junwei and Zhang, Dingwen},
title = {Current Status and Future Prospects of Brain-Inspired Intelligence Development: A Review},
year = {2025},
isbn = {9798400709999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708036.3708045},
doi = {10.1145/3708036.3708045},
abstract = {Brain-inspired intelligence is a form of machine intelligence that mimics human brain activity. Achieving machine intelligence similar to human consciousness is the ultimate task of brain-inspired intelligence and the long-term goal of mankind. This paper provides an overview of the current status of the brain-inspired field from perspectives of core technologies, talent distribution, and industrial development. It highlights current issues in brain-inspired domain such as the shortage of professional technical talent, slow industrialization processes, inadequate fundamental theoretical research, and a lack of breakthroughs in key technologies. The paper suggests that we should focus on building a talent cultivation system for brain-inspired intelligence, integrating the concept of brain-inspired technologies into educational plans. Furthermore, it recommends advancing the integration of technology and application and strengthening the strategic planning of industries related to brain-inspired intelligence to play a significant role in the new round of development in human society.},
booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology},
pages = {55–60},
numpages = {6},
keywords = {Brain-computer interface, Brain-inspired intelligence, Planning and layout for industry, The talent cultivation model},
location = {
},
series = {ICCSMT '24}
}

@article{10.1145/3701301,
author = {Walker, Henry M.},
title = {CURRICULAR SYNCOPATIONS: A Modest Individual/Group Exercise on Current Events in Computing},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {2153-2184},
url = {https://doi.org/10.1145/3701301},
doi = {10.1145/3701301},
journal = {ACM Inroads},
month = nov,
pages = {14–19},
numpages = {6}
}

@inproceedings{10.1145/3626253.3635567,
author = {Sthapit, Sandeep and Thomas, Madison and Brock, Janet and Barnes, Tiffany},
title = {Cracking the Cultural Code: Understanding the Cultural Barriers for Asian International CS Students in the US},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635567},
doi = {10.1145/3626253.3635567},
abstract = {In the field of computer science, cultural assumptions are embedded in programming languages and problem prompts. However, for students studying computer science in a Western country not from a Western culture, these assumptions can create learning barriers. This paper investigates the impact of cultural assumptions on international students studying computer science in a Western country; with a focus on understanding the barriers students face, how they overcome these barriers, and how barriers can be avoided. By performing thematic analysis on semi-structured interviews with 12 international graduate students at North Carolina State University, the authors found six main themes: Barriers, Increased Work, Emotions, Educational Environment, Overcoming Barriers, and Solutions to Barriers. Analyzing these themes provided insight into what barriers international students face and how they can be alleviated. The most common suggestions participants gave for alleviating barriers were more discussions around problem statements. By shedding light on this topic, the authors hope to inform computer science educators and researchers on the importance of creating inclusive and culturally relevant learning environments that accommodate the needs of diverse student populations.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1830–1831},
numpages = {2},
keywords = {cultural identity, culturally relevant computing, inclusive computing curricula and pedagogy, international stu- dents},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3578245.3584352,
author = {Bondi, Andr\'{e} Benjamin and Xiao, Lu},
title = {Early Progress on Enhancing Existing Software Engineering Courses to Cultivate Performance Awareness},
year = {2023},
isbn = {9798400700729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578245.3584352},
doi = {10.1145/3578245.3584352},
abstract = {Software engineering and computer science courses are frequently focused on particular areas in a way that neglects such cross-cutting quality attributes as performance, reliability, and security. We will describe the progress we have made in developing enhancements to some of our existing software engineering courses to draw attention and even lay the foundations of an awareness of performance considerations in the software development life cycle. In doing so, we wish to make performance considerations integral to the software engineering mindset while avoiding the need to remove current material from our existing courses. This work is part of an NSF-funded project for undergraduate curriculum development.},
booktitle = {Companion of the 2023 ACM/SPEC International Conference on Performance Engineering},
pages = {345–349},
numpages = {5},
keywords = {performance engineering, software engineering curriculum development},
location = {Coimbra, Portugal},
series = {ICPE '23 Companion}
}

@inproceedings{10.1145/3716640.3716650,
author = {Mannila, Linda and Hallstr\"{o}m, Jonas and Nordl\"{o}f, Charlotta and Heintz, Fredrik and Sperling, Katarina and Stenliden, Linn\'{e}a},
title = {Framing AI Literacy for K-12 Education: Insights from Multi-Perspective and International Stakeholders},
year = {2025},
isbn = {9798400714252},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716640.3716650},
doi = {10.1145/3716640.3716650},
abstract = {National and international policy documents emphasize the need for AI-related competencies “for all”, but there is little clarity on what these competencies should include, and determining what non-experts need to know remains a challenge. AI literacy has become a widely discussed topic in this context, often referring to a set of skills that empower individuals to critically evaluate AI, communicate and collaborate effectively with AI systems, and utilize AI as a tool across diverse contexts, including online environments, homes, schools, and workplaces. However, what AI literacy looks like in practice depends on factors such as age, level of education, and individual background. In this article, we frame AI literacy based on a qualitative analysis of the views of 33 international experts from various disciplines on what AI literacy in K-12 education should encompass. This analysis builds on existing AI literacy frameworks, with a focus on understanding and critically evaluating AI’s role in daily life, recognizing and using AI, and designing AI solutions for everyday problems. The findings show that experts emphasize a wide range of knowledge, skills, and attitudes, highlighting the importance of multiple perspectives when exploring this emerging field.},
booktitle = {Proceedings of the 27th Australasian Computing Education Conference},
pages = {85–94},
numpages = {10},
keywords = {AI literacy, K-12 education},
location = {
},
series = {ACE '25}
}

@inproceedings{10.1145/3587103.3595284,
author = {Zavaleta Bernuy, Angela and Sibia, Naaz and Chen, Pan and Huang, Chloe and Petersen, Andrew and Williams, Joseph Jay and Liut, Michael},
title = {VoiceEx: Voice Submission System for Interventions in Education},
year = {2023},
isbn = {9798400701399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587103.3595284},
doi = {10.1145/3587103.3595284},
abstract = {Generating self-explanations has been identified as a successful strategy in helping learners engage with course content and organize what they learn in a structured format. While typing an explanation may allow more structure and formality, explaining by voice can be more natural and help free cognitive resources to focus on learning goals and understanding concepts. As we investigated the effects and students' perceptions of using voice or text to self-explain new course concepts, we failed to find a tool that would meet our needs. We present our work in designing and developing VoiceEx, a submission courseware that allows text and voice input to collect data in both mediums. VoiceEx was created to support a self-explanations intervention for computer science students; however, given its features and the advantages of being able to collect spoken responses, it can be used in a variety of environments. Future refinement of this tool includes artificial intelligence features to better guide students' submissions.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2},
pages = {585–586},
numpages = {2},
keywords = {educational technology, voice recording, voice submission},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3641554.3701926,
author = {Rayavaram, Pranathi and Ukaegbu, Onyinyechukwu and Abbasalizadeh, Maryam and Vellamchetty, Krishna and Narain, Sashank},
title = {CryptoEL: A Novel Experiential Learning Tool for Enhancing K-12 Cryptography Education},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701926},
doi = {10.1145/3641554.3701926},
abstract = {This paper presents an educational tool designed to enhance cryptography education for K-12 students, utilizing Kolb's Experiential Learning (EL) model and engaging visual components. Our tool incorporates the four stages of EL-Concrete Experience, Reflective Observation, Abstract Conceptualization, and Active Experimentation-to teach key cryptographic concepts, including hashing, symmetric cryptography, and asymmetric cryptography. The learning experience is enriched with real-world simulations, customized AI-based conversation agents, video demonstrations, interactive scenarios, and a simplified Python coding terminal focused on cryptography. Targeted at beginners in cybersecurity, the tool encourages independent learning with minimal instructor involvement. An evaluation with 51 middle and high school students showed positive feedback from 93% of participants, who found the simulations, visualizations, AI reflections, scenarios, and coding capabilities engaging and conducive to learning. Comprehension surveys indicated a high understanding of cryptography concepts: hashing (middle school: 89%, high school: 92%), symmetric cryptography (middle school: 93%, high school: 97%), and asymmetric cryptography (middle school: 91%, high school: 94%)},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {980–986},
numpages = {7},
keywords = {cryptography education, experiential learning, interactive learning components, k-12, visual interfaces},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3632971.3632980,
author = {Abdou, Idrissa and Eude, Thierry},
title = {Open-ended questions automated evaluation: proposal of a new generation},
year = {2024},
isbn = {9798400707704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632971.3632980},
doi = {10.1145/3632971.3632980},
abstract = {Abstract. Exams grading for the knowledge validation to recognise competences is an essential element for any learning process. There are two main modes for their evaluation: subjective and automated. Subjective evaluation is accused of many flaws such as the inconsistency of the human corrector and the time it requires. Automating the assessment of open-ended questions saves a lot of time, provides quick feedback to learners and ensures the consistency expected from the human correctors. However, this is a challenging problem to implement because the computer does not have the same faculties as a human.To address this issue, we conducted a literature review on open-ended questions automated evaluation to implement an automatic exam grading system with similar or even higher accuracy than a human corrector. This study allows us to classify the different approaches in three generations: “bag of words” based approaches, classical semantic similarity-based approaches and machine learning based approaches. The third generation offers the best state-of-the-art results despite criticism of it. These approaches rely on neural networks which need to have a large dataset for effective training.To tackle this handicap, we propose a fourth generation (section 3). This contribution relies on the use of pre-trained models for which a dataset for training is not necessary knowing that they are zero-shot-learners. After implementing our architecture, we conducted our experiments with the three main French-speaking models available on Hugging Face. The best model agrees with the human corrector at 96%.},
booktitle = {Proceedings of the 2023 International Joint Conference on Robotics and Artificial Intelligence},
pages = {143–147},
numpages = {5},
keywords = {Automatic exam rating, Open-ended questions assessment, Pre-trained models, Zero-shot-learning},
location = {Shanghai, China},
series = {JCRAI '23}
}

@article{10.5555/3715638.3715661,
author = {Attarwala, Abbas and Raigoza, Pablo},
title = {Pedagogical Implications of Parser Combinators in Programming Languages Courses: A Comparative Study},
year = {2024},
issue_date = {September 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {4},
issn = {1937-4771},
abstract = {This paper recounts the experience of teaching parser combinators in a programming language course using OCaml at both Boston University and California State University, Chico. The main focus is on how parser combinators are introduced when teaching parsing to students who are new to functional programming. Techniques such as boxes and color coding are employed to simplify the understanding of the concepts. Furthermore, teaching course evaluation data are presented to compare course outcomes, contrasting semesters when parser combinators were not used with those when they were incorporated into the teaching. Reflections and feedback from students provide insight into the effectiveness of these teaching methods. Additionally, a two-tailed Welch t-test is conducted on the teaching course evaluation data to assess the impact of using parser combinators.},
journal = {J. Comput. Sci. Coll.},
month = sep,
pages = {72–82},
numpages = {11}
}

@inproceedings{10.1145/3641555.3705243,
author = {Dougherty, Ryan E.},
title = {Scaffolding Mock Conference Projects in Theory of Computing Courses},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705243},
doi = {10.1145/3641555.3705243},
abstract = {Theory of Computing (ToC) courses have important connections to other CS courses as ToC is a foundation for them. ToC course grading schemes often involve mostly exams, and sometimes a small weight for traditional assignments. Recent work experimented with a "mock conference'' project where students write up solutions to ToC problems as if they were submitting them to a "real'' CS conference. In this poster we give our experiences in scaffolding this existing project idea.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1441–1442},
numpages = {2},
keywords = {automata theory, computer science education, formal languages, theoretical computer science},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3699538.3699560,
author = {S\"{o}lch, Maximilian and Paulsen, Markus and Krusche, Stephan},
title = {Transforming Computer-Based Exams with BYOD: An Empirical Study},
year = {2024},
isbn = {9798400710384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3699538.3699560},
doi = {10.1145/3699538.3699560},
abstract = {The assessment landscape in computing education is at a pivotal juncture, necessitating research-backed innovations to align with evolving pedagogical requirements. Paper-based exams, despite their historical prominence, present substantial limitations in assessing coding skills and knowledge. This paper rigorously interrogates these limitations and advocates for a paradigm shift towards computer-based assessments, with a particular focus on Bring Your Own Device (BYOD) configurations. It shows an implementation of a special exam mode in the Artemis learning platform, specifically designed for BYOD contexts with a process that is scalable to exams with more than thousand students. In a field study employing action research, we used Artemis in three separate large-scale exams, involving 920 students in total, and gathered both quantitative and qualitative data through an online survey. The empirical evaluation of the data reveals a marked preference among students for computer-based assessments in computing education, specifically when utilizing personal setups. Findings indicate a reduced possibility in instances of academic dishonesty as compared to remote exam environments. This research substantiates the potential of computer-based exams in BYOD scenarios to rectify the incongruence between learning activities and assessment methods, thereby making a significant contribution to enhancing learning outcomes in computing education following constructive alignment.},
booktitle = {Proceedings of the 24th Koli Calling International Conference on Computing Education Research},
articleno = {17},
numpages = {11},
keywords = {Computing Education, Computer-Based Assessment, Academic Integrity, Constructive Alignment, Large-Scale Examinations, Educational Technology},
location = {
},
series = {Koli Calling '24}
}

@inproceedings{10.1145/3700297.3700382,
author = {Chen, Zhiqiang and Chen, Yanru},
title = {Advancing AI-Driven BETC Models for Transformative Educational Data Mining and Learning Analytics},
year = {2024},
isbn = {9798400707100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3700297.3700382},
doi = {10.1145/3700297.3700382},
abstract = {This research paper presents an innovative integration of Artificial Intelligence (AI) with the British Technology Education Council (BETC) method, specifically tailored for civil engineering education. The study introduces an AI-driven BETC model that integrates real-world engineering data, industry standards, and virtual construction technologies to significantly enhance learning efficiency and teaching feedback precision. The model was rigorously evaluated through quantitative metrics, demonstrating a substantial increase in pre-class task completion rates from 30% to 87%, accuracy of achievements from 38% to 82%, and student satisfaction from 56% to 97%. These results not only validate the effectiveness of the AI-BETC model but also highlight the transformative potential of AI in educational data mining and learning analytics within the civil engineering sector. The study's findings underscore the importance of adopting advanced technologies in educational practices to improve outcomes and prepare students for the challenges of the future.},
booktitle = {Proceedings of the 2024 International Symposium on Artificial Intelligence for Education},
pages = {495–500},
numpages = {6},
keywords = {AI, BETC, virtual construction workshop},
location = {
},
series = {ISAIE '24}
}

@inproceedings{10.1145/3702163.3702179,
author = {Mohd A'seri, Muhamad Safwan and Mahmud, Malissa Maria and Yaacob, Yazilimiwati and Ahmad, Rozaini and Nagasundram, Usha and Mustamam, Nur Izzati},
title = {Beyond the Textbook: A Study of ChatGPT Patterns of Use Perceptions and Experiences Among Students in Higher Education},
year = {2025},
isbn = {9798400717819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702163.3702179},
doi = {10.1145/3702163.3702179},
abstract = {This study investigates the utilization pattern, perception, and experience of Higher Education Institutes (HEIs) students towards the ChatGPT application in an academic context. It employs a quantitative approach utilizing a questionnaire as the research instrument. The study sample was selected using a simple random sampling method from Sunway University and Sunway College in Kuala Lumpur, Malaysia. The survey participants, enrolled in General Studies Subjects (MPU) during their short semester between September and December 2023, were selected using a simple random sampling method. Out of 150 students who received the survey via Google Forms, 119 provided complete responses suitable for analysis. The research primarily focused on calculating mean scores to assess three key dimensions: its use patterns of ChatGPT, perceptions and experiences among students towards its adoption in educational contexts. A descriptive analysis was conducted to determine student frequency and percentage values for ChatGPT usage. At the same time, mean scores were utilized to evaluate higher education institutes (HEIs) students' perceptions and experiences with the application in an academic context. This descriptive analysis revealed a spectrum of responses that ranged from low to very high levels across these dimensions. The findings of this study offer extensive insight into the current incorporation and perception of ChatGPT within Higher Education Institutions (HEIs), showcasing the diverse range of engagement and acceptance levels among students.},
booktitle = {Proceedings of the 2024 16th International Conference on Education Technology and Computers},
pages = {110–116},
numpages = {7},
keywords = {ChatGPT, Experiences, Higher education institutions, Perceptions, Use patterns},
location = {
},
series = {ICETC '24}
}

@article{10.5555/3715622.3715632,
author = {Beck, Shannon and Goines, Timothy and Biller, Jeffrey},
title = {Lawyer Up! Joint Introductory Computer Science and Law Courses},
year = {2024},
issue_date = {October 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {2},
issn = {1937-4771},
abstract = {Courses typically focus on a single discipline, limiting exposure to cross-disciplinary topics. To address this, we designed an interdisciplinary two-course set within our undergraduate Honors program: an introductory computing course and an introductory law course. This initiative breaks down barriers between computing and law, enabling students to integrate both disciplines and foster a comprehensive understanding of complex issues. The Computer Science (CS) course covers introductory programming and principles, while the Law course surveys American Law and basic legal reasoning. Initially, the topics are independent but they strongly converge in the term's second half. Students explore the intersection of CS and Law through discussions, debates, guest speakers, and a cross-disciplinary project. Conducted for two years at the United States Air Force Academy (USAFA), this report shares our curriculum and experiences, aiming to inspire wider adoption of our inter-departmental model.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {62–72},
numpages = {11}
}

@inproceedings{10.1145/3653666.3656082,
author = {Medina-Kim, Gabriel},
title = {Disidentifying with Broadening Participation},
year = {2024},
isbn = {9798400706264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653666.3656082},
doi = {10.1145/3653666.3656082},
abstract = {Broadening participation sends mixed messages to minoritized students. Promises of economic security, democratized technopolitics, and creative expression are interwoven with the discourses of international economic competitiveness, militarism, and cyberlibertarian entrepreneurship. This perspective paper reads the queer of color theory of Jose Esteban Mu\~{n}oz through the project of broadening participation. Disidentification describes tactical misrecognition by minoritarian subjects, especially by assimilating the dominant cultural images. Instead of arguing for specific kinds of representation, this paper describes paradoxical experiences of minoritization-navigating "who" is a computer scientist-and how they can take us far beyond the expectations of dominant representations.},
booktitle = {Proceedings of the 2024 on RESPECT Annual Conference},
pages = {163–168},
numpages = {6},
keywords = {assimilation, broadening participation, disidentifications, queer of color theory, representations of computer science},
location = {Atlanta, GA, USA},
series = {RESPECT 2024}
}

@inproceedings{10.1145/3585059.3611441,
author = {Servin, Christian and Hawthorne, Elizabeth K. and Postner, Lori and Tang, Cara and Tucker, Cindy S.},
title = {Mathematical Considerations in Two-Year Computing Degrees: The Evolution of Math in Curricular Guidelines},
year = {2023},
isbn = {9798400701306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585059.3611441},
doi = {10.1145/3585059.3611441},
abstract = {Incorporating mathematics in computing has been a subject of ongoing deliberation within computing curriculum recommendations spanning several decades. Depending on the specific computing area or program, there is a variation in the advocated number of math contact hours. For instance, two-year programs like community colleges require many math hours due to curriculum alignment and agreements with four-year colleges. In other cases, the regional workforce and the career opportunities provided by local industry partners influence a degree program’s math courses/topics. The debate surrounding the appropriate number of math contact hours in computing programs has persisted over the years and has yet to be a definitive solution. Curricular guidelines such as ACM/IEEE-CS/AAAI CS 2023 and Information Technology 2017 have proposed math education based on competencies and workforce perspectives. Specifically, the former now recommends a tailored math background based on the knowledge area within computer science. This approach allows for a more flexible structure of knowledge units, encompassing specific topics, learning outcomes, and competencies. This paper explores various approaches and solutions proposed in the first two years of computing education programs to address challenges and gaps in computing education. It also highlights the short-term and long-term challenges computing programs and educators face.},
booktitle = {Proceedings of the 24th Annual Conference on Information Technology Education},
pages = {209–212},
numpages = {4},
keywords = {community college, cs2023, curriculum, mathematics, two-year},
location = {Marietta, GA, USA},
series = {SIGITE '23}
}

@inproceedings{10.1145/3708036.3708125,
author = {Ye, Yuan},
title = {Research on the teaching mode of digital literacy curriculum group under the perspective of digital revolution},
year = {2025},
isbn = {9798400709999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708036.3708125},
doi = {10.1145/3708036.3708125},
abstract = {This paper focuses on the reform and innovation of digital education under the background of a new round of scientific and technological revolution. Based on the analysis of the current situation of digital education, the teaching exploration of digital transformation is carried out around the professional basic curriculum group of design. Through the research of the new generation of digital technology to deeply empower the digital literacy curriculum group education and teaching links, ways and methods, the teaching mode of digital literacy for undergraduate design students is fully presented.},
booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology},
pages = {520–526},
numpages = {7},
keywords = {Curriculum group, Digital literacy, Digital revolution, Teaching mode},
location = {
},
series = {ICCSMT '24}
}

@inproceedings{10.1145/3639474.3640050,
author = {Cunha, Alcino and Macedo, Nuno and Campos, Jos\'{e} Creissac and Margolis, Iara and Sousa, Emanuel},
title = {Assessing the impact of hints in learning formal specification},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640050},
doi = {10.1145/3639474.3640050},
abstract = {Background: Many programming environments include automated feedback in the form of hints to help novices learn autonomously. Some experimental studies investigated the impact of automated hints in the immediate performance and learning retention in that context. Automated feedback is also becoming a popular research topic in the context of formal specification languages, but so far no experimental studies have been conducted to assess its impact while learning such languages. Objective: We aim to investigate the impact of different types of automated hints while learning a formal specification language, not only in terms of immediate performance and learning retention, but also in the emotional response of the students. Method: We conducted a simple one-factor randomised experiment in 2 sessions involving 85 BSc students majoring in CSE. In the 1st session students were divided in 1 control group and 3 experimental groups, each receiving a different type of hint while learning to specify simple requirements with the Alloy formal specification language. To assess the impact of hints on learning retention, in the 2nd session, 1 week later, students had no hints while formalising requirements. Before and after each session the students answered a standard self-reporting emotional survey to assess their emotional response to the experiment. Results: Of the 3 types of hints considered, only those pointing to the precise location of an error had a positive impact on the immediate performance and none had significant impact in learning retention. Hint availability also causes a significant impact on the emotional response, but no significant emotional impact exists once hints are no longer available (i.e. no deprivation effects were detected). Conclusion: Although none of the evaluated hints had an impact on learning retention, learning a formal specification language with an environment that provides hints with precise error locations seems to contribute to a better overall experience without apparent drawbacks. Further studies are needed to investigate if other kind of feedback, namely hints combined with some sort of self-explanation prompts, can have a positive impact in learning retention.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {151–161},
numpages = {11},
keywords = {learning formal specification, automated hints, user study, alloy},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@article{10.1145/3717596.3717600,
author = {Barendsen, Erik and Paterson, Jim and Quille, Keith},
title = {ITiCSE Announcement},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {1},
issn = {0097-8418},
url = {https://doi.org/10.1145/3717596.3717600},
doi = {10.1145/3717596.3717600},
abstract = {The 30th annual ACM Conference on Innovation and Technology in Computer Science Education (ITiCSE) will be held at Radboud University in Nijmegen, Netherlands, from 30 June to 2 July 2025. ITiCSE is a computing education conference held annually, typically in Europe, sponsored by ACM SIGCSE and in collaboration with ACM Europe Council and Informatics Europe.},
journal = {SIGCSE Bull.},
month = feb,
pages = {6–7},
numpages = {2}
}

@inproceedings{10.1145/3641554.3701848,
author = {Ohmann, Peter and Novak, Ed},
title = {A Multi-Institutional Assessment of Oral Exams in Software Courses},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701848},
doi = {10.1145/3641554.3701848},
abstract = {Oral exams are an inviting alternative to traditional paper-and-pencil exams. However, they are largely under-utilized in computer science education. In this report, we describe our design for comprehensive final oral exams in five software engineering class sections, across two different small institutions. We present our exam format and our subjective assessment of the exam format in assessing student knowledge as instructors. We also gather quantitative and qualitative data from student surveys. We surveyed students before and after the oral exam to assess their perceptions of it, including their predicted grade and their subjective opinions and experiences. Our work shows evidence that oral exams are effective and practical mechanisms for software engineering classes of a smaller size (approximately 20 students). Student survey responses indicated favorable feedback for our oral exam format; students viewed oral exams as a good assessment of their knowledge and useful beyond that individual class.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {882–888},
numpages = {7},
keywords = {oral exam, software engineering education, student survey},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.5555/3715622.3715634,
author = {Attarwala, Abbas and Raigoza, Pablo},
title = {Assessing Student Perceptions of Co-Teaching in a 3rd-Year Computer Science Course},
year = {2024},
issue_date = {October 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {2},
issn = {1937-4771},
abstract = {In this research, the effects of various teaching methodologies such as solo teaching, parallel coordinated teaching (PCT), and sequential teaching (SEQT) on student perceptions in a third-year programming language course at Boston University (BU) are studied. PCT and SEQT, as variants of co-teaching, contrast with the independent approach of solo teaching. This research uses student evaluation data to analyze eight distinct evaluative questions, including areas such as fairness in grading, stimulation of student's interest in the course material, and overall instructor ratings. These eight questions are analyzed using student course evaluations across the three aforementioned teaching methodologies to determine if there are statistically significant differences in perceptions. The results show that consistent instructor presence throughout the semester, as seen in solo teaching and PCT scenarios, significantly enhances student perceptions of fairness and overall satisfaction. In contrast, SEQT, which involves instructor changes in the middle of the semester, is associated with less favorable student evaluations. The study highlights the importance of instructor consistency and the potential disruptions caused by changing instructors mid-course.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {83–94},
numpages = {12}
}

@article{10.1145/3679205,
author = {Vahid, Frank},
title = {AI in CS Education: Opportunities, Challenges, and Pitfalls to Avoid},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2153-2184},
url = {https://doi.org/10.1145/3679205},
doi = {10.1145/3679205},
journal = {ACM Inroads},
month = aug,
pages = {52–57},
numpages = {6}
}

@inproceedings{10.1145/3641555.3705223,
author = {Tsang, Jedidiah and Li, Carol and Park, Su Min and Yan, Lisa},
title = {On a Time Crunch: Examining Learning Outcomes Within a One Unit Computing Ethics Course},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705223},
doi = {10.1145/3641555.3705223},
abstract = {This study examines the challenges and opportunities of teaching computing ethics within the context of a large, low-workload, standalone course. CS199 is a one-unit, pass/fail computing ethics course designed to provide students with exposure to a wide array of topics and promote critical peer-based engagement. We leverage submitted work via Question, Quote, Comment, and Replies (QQCRs) and podcasts to facilitate discussions outside the classroom. While QQCRs have shown promise in promoting engagement and exposing students to diverse perspectives, limitations remain in stimulating deeper critiques of the material. We reflect on the effectiveness of asynchronous discussion and its alignment with broader learning goals in computing ethics education.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1639–1640},
numpages = {2},
keywords = {computing ethics, course design, course forums, learning outcomes},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3696230.3696234,
author = {Humphrys, Mark},
title = {Ancient Brain: A JavaScript coding platform for education with 3D graphics, Websockets, AI and support for teachers},
year = {2024},
isbn = {9798400717574},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696230.3696234},
doi = {10.1145/3696230.3696234},
abstract = {This paper introduces a JavaScript coding site called "Ancient Brain” (at ), which is designed for education with support for students and teachers. It has an extensive list of features to support coding almost anything that can be coded in JavaScript, including HTML-based pages, 2D graphics worlds, 3D graphics worlds, Internet-enabled Websockets apps, and AI apps. All are coded and run on the site in the browser with no install. Support for teaching includes "teacher" and "student" accounts where student code is hidden from other students but not from the teacher, who can run and even edit the code written by all their students. This site has been tested with several years of undergraduate and taught postgraduate students, though not written up in a paper until now. At time of writing there are 9,323 JavaScript creations on the site. We survey some of the extraordinary range of programs on the site, written by, at last count,&nbsp;2,150 coders. The next step is to take the site into secondary schools. We will discuss how this could be done, with examples.},
booktitle = {Proceedings of the 2024 8th International Conference on Digital Technology in Education (ICDTE)},
pages = {28–38},
numpages = {11},
keywords = {AI in education, Coding sites, Education, JavaScript, WebGL, Websockets},
location = {
},
series = {ICDTE '24}
}

@article{10.1145/3649509,
author = {Walker, Henry M.},
title = {CLASSROOM VIGNETTES: Experiencing Efficiency of Algorithms and Implementations},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {2153-2184},
url = {https://doi.org/10.1145/3649509},
doi = {10.1145/3649509},
journal = {ACM Inroads},
month = may,
pages = {10–17},
numpages = {8}
}

@article{10.1145/3656021.3656027,
author = {Gulley, Paige},
title = {Creative Coding as Method and Goal in a Gender Diverse First-Year Seminar},
year = {2024},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0095-2737},
url = {https://doi.org/10.1145/3656021.3656027},
doi = {10.1145/3656021.3656027},
abstract = {FIERCE (Fostering Inclusion by Engaging in Real-world Computing Education) is a first-year academic program for gender diverse students entering UMass's computer science or informatics degree program. In response to feedback that students wanted to be exposed to more actual programming prompts in addition to the more theoretical content which was the focus of the previous year's seminar, this year's seminar is focused on Creative Coding.},
journal = {SIGCAS Comput. Soc.},
month = apr,
pages = {16},
numpages = {1}
}

@book{10.1145/3678016,
author = {Computer Science Teachers Association and Institute for Advancing Computing Education and Association for Computing Machinery and Code.org and College Board and CSforALL and Expanding Computing Education Pathways Alliance},
title = {Reimagining CS Pathways: High School and Beyond},
year = {2024},
isbn = {9798400710957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA}
}

@inproceedings{10.1145/3706599.3707654,
author = {Ackerman, Matthew Kenneth and Armour, Veronica and Najeeb, Zainab and Mendoza, Nicole},
title = {Experiential Learning Internship for Teaching Design Research},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3707654},
doi = {10.1145/3706599.3707654},
abstract = {HCI researchers have explored ways to improve HCI and design education. We designed and conducted an experiential learning internship to teach principles of design research to first-year undergraduate students. We describe the framework we used for the design of the internship and the lessons we learned in a self-reflective case study. We provide the following recommendations to other educators who want to incorporate similar programs: 1) prepare for clients having differences in opinion, 2) use multiple educational methods to supplement the experience, 3) allow interns to navigate a project space on their own, but identify areas that may create major delays, and 4) prepare educational material on the latest trends and tools that may be of interest to the interns.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {674},
numpages = {5},
keywords = {Design research, Experiential learning, HCI Education},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3626252.3630961,
author = {H\"{u}sing, Sven and Schulte, Carsten and Sparmann, S\"{o}ren and Bolte, Mario},
title = {Using Worked Examples for Engaging in Epistemic Programming Projects},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630961},
doi = {10.1145/3626252.3630961},
abstract = {Programming nowadays has become an important tool in various scientific fields, not only in terms of software engineering but also for knowledge acquisition. In this regard, we analyze epistemic programming as an interdisciplinary, cognitive perspective on programming, enabling programmers to gain insights into individual interests. As a first step towards implementing epistemic programming in computing education, we assess, whether novice programmers can utilize epistemic programming as a means of gaining insights and exploring their interests with the help of a worked example. Therefore, we report on findings from a study, analyzing eye-tracking data, screen recordings, and transcripts from retrospective think-aloud interviews.The results indicate that the participants were able to engage in a data-driven epistemic programming process and to gain individual insights. They used the given worked example - especially at the beginning of the process - by adopting code and then adapting it according to their interests, following different approaches.The study thus provides some first indications that engaging in epistemic programming with the support of worked examples can serve as a way of individualized knowledge acquisition - also for programming novices - and thus illustrates a new perspective on computing education for all.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {443–449},
numpages = {7},
keywords = {approach for novice programmers, data-driven projects, epistemic programming, eye-tracking, worked examples},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3699538.3699559,
author = {Sollazzo, Anna and Cutts, Quintin},
title = {Towards a Theory of Humanistic Computing and How to Teach It},
year = {2024},
isbn = {9798400710384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3699538.3699559},
doi = {10.1145/3699538.3699559},
abstract = {Early in the field’s development, it was argued that to belong in the Digital Humanities (DH) one must know how to ‘build’—to code. Despite this stated focus, little work has addressed the specific computing needs of DH or the best approaches to teaching computing in that context. The grand challenge of DH is the epistemological dissonance between its constituent domains. Modern computing, anchored as it has become in the knowledge-cultures of science and engineering, is built around assumptions and priorities which often oppose those of humanistic inquiry. Correspondingly, there is an argument that to undertake meaningful humanistic work in a digital space, it is necessary to develop a distinct practice of computing which is aligned with humanistic values. What precisely this paradigm of ‘humanistic computing’ entails, however, is not well defined. The aim of this paper is twofold. Using interviews with DH computing instructors to augment limited existing literature, it first looks to understand the specificity of DH computing education—in terms of both key competencies and areas of interest, and the characteristics and challenges of DH students as a learner group. Second, it aims to establish a working definition of humanistic computing. Taken together, these findings will support the development of a computing curriculum tailored to the Digital Humanities.},
booktitle = {Proceedings of the 24th Koli Calling International Conference on Computing Education Research},
articleno = {16},
numpages = {11},
keywords = {digital humanities, interdisciplinary computing, epistemology},
location = {
},
series = {Koli Calling '24}
}

@inproceedings{10.1145/3679318.3685359,
author = {Grassini, Simone},
title = {A Psychometric Validation of the PAILQ-6: Perceived Artificial Intelligence Literacy Questionnaire},
year = {2024},
isbn = {9798400709661},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3679318.3685359},
doi = {10.1145/3679318.3685359},
abstract = {The present article introduces and implements an initial validation for the Perceived Artificial Intelligence Literacy Questionnaire (PAILQ-6), a brief tool designed to assess individuals' self-perceived AI literacy. Amidst the growing integration of AI in various aspects of life and its ethical implications, understanding AI becomes crucial for effective interaction with AI technologies. The PAILQ-6 emerges in response to the need for an accessible instrument that evaluates general AI literacy without compromising on clarity or depth, suitable for both academic and practical applications. This paper presents the development process of the PAILQ-6, consisting of six items derived from established components of AI literacy, structured as a seven-point Likert scale for easy administration and digital compatibility. The validation study was conducted from data of a gender-balanced sample of 232 UK adults. The article demonstrates the PAILQ-6's reliability and validity through exploratory factor analysis, showing a two-factor structure. The findings reveal the scale's good internal consistency and convergent validity. The study highlights demographic predictors of AI literacy perceptions, indicating a possible gender disparity and the positive influence of higher education on perceived AI competency.},
booktitle = {Proceedings of the 13th Nordic Conference on Human-Computer Interaction},
articleno = {26},
numpages = {10},
keywords = {Artificial Intelligence, Literacy, Psychology, Questionnaire},
location = {Uppsala, Sweden},
series = {NordiCHI '24}
}

@inproceedings{10.1145/3626252.3630890,
author = {Hung, Eping E. and Vanderberg, Maggie and Krause, Gladys and Skuratowicz, Eva},
title = {Making Abstraction Concrete in the Elementary Classroom},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630890},
doi = {10.1145/3626252.3630890},
abstract = {In recent years, several research projects have introduced elementary school teachers to computational thinking as a first step in familiarizing students with computer science concepts at an early age. A consistent challenge reported in these initiatives is teaching abstraction. This position paper offers preliminary recommendations for abstraction pedagogy in elementary education. These suggestions stem from an analysis of unplugged abstraction examples showcased during a summer institute on computational thinking.By examining commonalities among abstraction examples, key parts of the process of abstraction pertinent to elementary classrooms were identified: (1) the abstraction process is typically performed in reverse since students in elementary school are given abstractions to start with; (2) evaluation of concrete details to support an abstraction is part of the filtering step of abstraction; (3) in the absence of evaluation criteria, pattern recognition can be applied to a set of concrete examples to extract characteristics of an abstraction; and (4) abstractions can be supported by not only concrete details but other abstractions which students will need to develop an understanding of before fully comprehending the initial concept.Preliminary recommendations for abstraction instruction include having students evaluate examples; engaging students in pattern recognition to extract characteristics of an abstraction; developing student fluency in describing abstractions, their supporting examples, and characteristics; and assessing students by asking not only for examples of abstractions, but for their characteristics as well.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {540–546},
numpages = {7},
keywords = {abstraction, computational thinking, elementary education, professional development, unplugged computational thinking},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3687311.3687336,
author = {Li, Xi and Sun, Dandan and Qiu, Jin},
title = {AI-supported Teaching in China (2014–2024)},
year = {2024},
isbn = {9798400709920},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3687311.3687336},
doi = {10.1145/3687311.3687336},
abstract = {This article provides a comprehensive overview of AI-supported teaching in China, encompassing a decade (2014–2024)'s worth of the selected research findings from the influential local journals. The researchers performed a bibliometric analysis to uncover the most commonly explored theme (AI-supported teaching) in the field. Next, a thorough examination was conducted on 43 studies with these specific categories and sub-categories. The review concluded by addressing the shortcomings and offering suggestions for the further research. A comprehensive examination of Chinese practitioners and researchers’ endeavors to enhance AI-assisted teaching is expected to lend valuable insights to the global advancement of AI-supported teaching.},
booktitle = {Proceedings of the 2024 International Conference on Intelligent Education and Computer Technology},
pages = {134–139},
numpages = {6},
location = {Guilin, China},
series = {IECT '24}
}

@article{10.1145/3732895.3732900,
author = {Blaser, Brianna},
title = {RESPECT 2025: Designing an Accessible Future for Equitable Computer Science},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {2},
issn = {0097-8418},
url = {https://doi.org/10.1145/3732895.3732900},
doi = {10.1145/3732895.3732900},
abstract = {The ACM SIGCSE Conference on Research on Equity and Sustained Participation in Engineering, Computing, and Technology (RESPECT) 2025 will be held 14–16 July at the New Jersey Institute of Technology in Newark, NJ, USA. The theme for this year is Designing an Accessible Future for Equitable Computer Science. 2025 marks the fiftieth anniversary of the Individuals with Disabilities in Education Act, landmark legislation in the United States developed to ensure students with disabilities receive a free, appropriate public education. When we began planning the conference, we envisioned an opportunity to reflect on disability inclusion.},
journal = {SIGCSE Bull.},
month = apr,
pages = {4},
numpages = {1}
}

@article{10.1145/3717402.3717405,
author = {Leinonen, Juho},
title = {Koli Calling 2024: Call for Participation},
year = {2025},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0097-8418},
url = {https://doi.org/10.1145/3717402.3717405},
doi = {10.1145/3717402.3717405},
abstract = {We warmly invite you to attend the 24th Koli Calling International Conference on Computing Education Research (Koli Calling 2024), taking place from November 14-17, 2024, in the beautiful Koli National Park in Eastern Finland.},
journal = {SIGCSE Bull.},
month = feb,
pages = {5},
numpages = {1}
}

@inproceedings{10.1145/3626252.3630774,
author = {Minagar, Sepehr and Sakzad, Amin and Tack, Guido and Rudolph, Carsten and Sheard, Judithe},
title = {ALAN: Assessment-as-Learning Authentic Tasks for Networking},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630774},
doi = {10.1145/3626252.3630774},
abstract = {In this experience paper, we present ALAN, a framework to automate the generation of authentic assessment tasks in networking courses (NC). Using ALAN, all students in a cohort complete a set of assessment tasks generated from the same skeleton, with each student having their own parameters as input. The way we run ALAN assessments fosters students' self-regulation and peer learning and activates students' engagement in learning through assessment. We present three different ALAN assessments. We finally report on student perceptions and satisfaction and reflect on our experience.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {853–859},
numpages = {7},
keywords = {assessment-as-learning, authentic assessment, automation},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@article{10.1145/3637456,
author = {Mengi, Gopal},
title = {Four Years of Excellence: IIIT Delhi Empowers Future Technologists},
year = {2024},
issue_date = {Winter 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1528-4972},
url = {https://doi.org/10.1145/3637456},
doi = {10.1145/3637456},
journal = {XRDS},
month = feb,
pages = {12–13},
numpages = {2}
}

@proceedings{10.1145/3610540,
title = {SA '23: SIGGRAPH Asia 2023 Educator's Forum},
year = {2023},
isbn = {9798400703119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Sydney, NSW, Australia}
}

@inproceedings{10.1145/3641554.3701852,
author = {Hunter, Jessica and Bai, Elena and Alberini, Giulia and Robinson, Kristy A.},
title = {Needs-Supportive Teaching Interventions in an Intro Computer Science Course: Exploring Impacts on Student Motivation and Achievement},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701852},
doi = {10.1145/3641554.3701852},
abstract = {The instructor of a large, introductory computer science (CS) course at a public Canadian university implemented two interventions designed to support students' academic success and basic psychological needs as posited by self-determination theory (SDT). Interventions involved providing grading scheme choice for all students and sending targeted support emails to students who struggled on early term assessments. In keeping with SDT, we assessed the possible effect of these interventions on students' perceptions of competence (self-efficacy), autonomy, relatedness (via measures of instructor warmth), and final grades, by comparing the intervention cohort with a previous control cohort. Results indicate that all students in the intervention term may have benefited from grading scheme choice, as they earned higher final grades and felt more autonomous than the control group students. Moreover, struggling students who received support emails earned an average final grade 11.3% higher than struggling students in the control term. These students also performed closer to their non-struggling counterparts than those in the control group, reducing the achievement gap between early struggling and non-struggling students by 8.1%. Furthermore, even when controlling for past achievement, perceptions of self-efficacy and autonomy support positively predicted students' final grades across groups, with a small effect size. These results offer theoretical and practical insight into effective, light-touch teaching interventions which CS instructors can implement in large courses.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {534–539},
numpages = {6},
keywords = {academic achievement, grading scheme choice, motivation, targeted support, teaching interventions},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.1145/3732895.3732901,
author = {Morrison, Briana and Montero, Calkin Suero and Porter, Leo and Brown, Neil},
title = {ICER 2025 Call for Participation},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {2},
issn = {0097-8418},
url = {https://doi.org/10.1145/3732895.3732901},
doi = {10.1145/3732895.3732901},
abstract = {We would like to invite you to Charlottesville, Virginia, USA to attend the 21st ICER Conference. The ACM Conference on International Computing Education Research (ICER) will be held 3–6 August 2025 in the Forum Hotel on the beautiful grounds of the University of Virginia. The conference will include an in-person Doctoral Consortium on Sunday, 3 August. Attendees are invited to an opening reception in The Rotunda, the original library designed by Thomas Jefferson (third President of the United States) and centerpiece of the Academical Village on Sunday evening, 5–6:30pm.},
journal = {SIGCSE Bull.},
month = apr,
pages = {4–5},
numpages = {2}
}

@article{10.1145/3677610,
author = {Touretzky, David S. and Chen, Angela and Pawar, Neel},
title = {Neural Networks in Middle School},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2153-2184},
url = {https://doi.org/10.1145/3677610},
doi = {10.1145/3677610},
journal = {ACM Inroads},
month = aug,
pages = {24–28},
numpages = {5}
}

@article{10.1145/3732895.3732899,
author = {Barendsen, Erik and Paterson, Jim and Quille, Keith},
title = {ITiCSE 2025 Call for Participation},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {2},
issn = {0097-8418},
url = {https://doi.org/10.1145/3732895.3732899},
doi = {10.1145/3732895.3732899},
abstract = {You are warmly welcomed to the 30th annual ACM Conference on Innovation and Technology in Computer Science Education (ITiCSE), which will be held at Radboud University in Nijmegen, Netherlands from 30 June to 2 July 2025. ITiCSE is a computing education conference held annually, typically in Europe, sponsored by ACM SIGCSE and in collaboration with ACM Europe Council and Informatics Europe.},
journal = {SIGCSE Bull.},
month = apr,
pages = {3–4},
numpages = {2}
}

@inproceedings{10.1145/3641555.3705152,
author = {Lee, En-Shiun Annie and Woodhead, Sean and Kuber, Karthik and Rohian, Hashmat and Koornneef, Stacey A.},
title = {Crafting for Career Agility: An Outcome-Based Redesign of a Machine Learning Curriculum within a Program Bundle},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705152},
doi = {10.1145/3641555.3705152},
abstract = {In this paper, we focus on the certificate redesign of our machine learning program within the context of a three-program bundle; we used an outcome-based approach with the student's targeted career outcome in mind. The purpose of this curriculum redesign is threefold: (1) To align the course content with the industry's rapidly changing needs and demands of the job market; (2) To identify the proper sequence of laddering structure for students taking multiple programs; and (3) To create a natural and streamline flow of the learning experience (i.e., removing overlapping content). We show that the resulting redesign is a holistic set of certificate programs with industry-relevant content tailored for skills-based experiential learning.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1515–1516},
numpages = {2},
keywords = {ai graduate program, computer science education},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3689187.3709608,
author = {Hooshangi, Sara and Shakil, Asma and Dasgupta, Subhasish and C. Davis, Karen C. and Farghally, Mohammed and Fitzpatrick, KellyAnn and Gutica, Mirela and Hardt, Ryan and Riddle, Steve and Seyam, Mohammed},
title = {Instructors' Perspectives on Capstone Courses in Computing Fields: A Mixed-Methods Study},
year = {2025},
isbn = {9798400712081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689187.3709608},
doi = {10.1145/3689187.3709608},
abstract = {Team-based capstone courses are integral to many undergraduate and postgraduate degree programs in the computing field. They are designed to help students gain hands-on experience and practice professional skills such as communication, teamwork, and self-reflection as they transition into the real world. Prior research on capstone courses has focused primarily on the experiences of students. The perspectives of instructors who teach capstone courses have not been explored comprehensively. However, an instructor's experience, motivation, and expectancy can have a significant impact on the quality of a capstone course. In this working group, we used a mixed methods approach to understand the experiences of capstone instructors. Issues such as class size, industry partnerships, managing student conflicts, and factors influencing instructor motivation were examined using a quantitative survey and semi-structured interviews with capstone teaching staff from multiple institutions across different continents. Our findings show that there are more similarities than differences across various capstone course structures. Similarities include team size, team formation methodologies, duration of the capstone course, and project sourcing. Differences in capstone courses include class sizes and institutional support. Some instructors felt that capstone courses require more time and effort than regular lecture-based courses. These instructors cited that the additional time and effort is related to class size and liaising with external stakeholders, including industry partners. Some instructors felt that their contributions were not recognized enough by the leadership at their institutions. Others acknowledged institutional support and the value that the capstone brought to their department. Overall, we found that capstone instructors were highly intrinsically motivated and enjoyed teaching the capstone course. Most of them agree that the course contributes to their professional development. The majority of the instructors reported positive experiences working with external partners and did not report any issues with Non-Disclosure Agreements (NDAs) or disputes about Intellectual Property (IP). In most institutions, students own the IP of their work, and clients understand that. We use the global perspective that this work has given us to provide guidelines for institutions to better support capstone instructors.},
booktitle = {2024 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {68–94},
numpages = {27},
keywords = {capstone course, experiential learning, faculty motivation, industry collaboration, teamwork},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@article{10.1145/3717838,
author = {Wee, Chyanna and Wang, Lillian Yee Kiaw and Ong, Huey Fang},
title = {TeachVR: An Immersive Virtual Reality Framework for Computational Thinking Based on Student Preferences},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {1},
url = {https://doi.org/10.1145/3717838},
doi = {10.1145/3717838},
abstract = {This study presents the development of a student-centric framework for utilizing virtual reality (VR) technologies in education, specifically focusing on enhancing computational thinking skills. While numerous frameworks exist in this domain, they often lack consideration of student preferences, which are integral for fostering learner autonomy. Our proposed framework, with components developed from the constructivist learning theory, emphasises creating knowledge through interaction with the environment, focusing on autonomy, mastery and purpose as drivers of intrinsic outcomes. Through a survey administered to hundred and fifty-seven participants, we sought to identify student-preferred strategies for learning computational thinking skills via VR interventions. Results highlighted key challenges students face when working on computational tasks are related to algorithmic and abstraction thinking. To ease the aforementioned challenges, our findings suggest a preference among students for situated-based learning approaches within VR environments. Additionally, participants recognized the importance of motivational outcomes in improving autonomy and mastery within VR-based learning tasks. Students also preferred tasks that enhanced self-efficacy, contributing to a greater sense of purpose in their learning endeavours. Overall, this investigation sets a foundation for more student-centric, constructivist and intrinsically-based VR frameworks in education.},
journal = {ACM Trans. Comput. Educ.},
month = apr,
articleno = {8},
numpages = {36},
keywords = {Virtual Reality}
}

@article{10.1145/3717402.3717406,
author = {Stone, Jeffrey A. and Yuen, Timothy},
title = {SIGCSE Technical Symposium 2025: Leading the Transformation},
year = {2025},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0097-8418},
url = {https://doi.org/10.1145/3717402.3717406},
doi = {10.1145/3717402.3717406},
abstract = {The 2025 SIGCSE Technical Symposium will take place in Pittsburgh, Pennsylvania, from February 26 to March 01, 2025. The SIGCSE community will gather at the David L. Lawrence Convention Center on the banks of the Allegheny River. Our theme for this year is "Leading the Transformation," reflecting the role of the computer science education community in adapting educational practice to new technologies and challenges.},
journal = {SIGCSE Bull.},
month = feb,
pages = {5–6},
numpages = {2}
}

@inproceedings{10.1145/3708394.3708447,
author = {Chen, Donghua},
title = {Research on the Innovation of Ideological Education System in Universities under the Perspective of New Quality Productivity},
year = {2025},
isbn = {9798400710650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708394.3708447},
doi = {10.1145/3708394.3708447},
abstract = {This study was conducted to explain the system and mode innovation of ideological education in universities under the perspective of new quality productivity, analyzing the impact on ideological education of universities by sorting out the characteristics of new quality productivity, put forward a new mode of ideological education of universities adapted to the development of new quality productivity, and provide theoretical support and practical guidance to enhance the effectiveness of ideological education of universities.},
booktitle = {Proceeding of the 2024 International Conference on Artificial Intelligence and Future Education},
pages = {309–314},
numpages = {6},
keywords = {ideological education, innovation, new quality productivity},
location = {
},
series = {AIFE '24}
}

@inproceedings{10.1145/3641555.3705131,
author = {Joshi, Deepti and Joswick, Candace and Albert, Jennifer and Jocius, Robin and Blanton, Melanie and Petrulis, Robert and Dawson, Trent},
title = {Assessing Elementary Teachers' Knowledge of Integrated Computational Thinking},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705131},
doi = {10.1145/3641555.3705131},
abstract = {During the UnboxingCT project summer professional development, the Integrated CT Assessment was piloted with 72 elementary teachers. The assessment is based on computational thinking integration literature and asks teachers to identify different computational thinking concepts in content area scenarios. The assessment allowed us to identify which computational thinking concepts teachers were most familiar with prior to the professional development and assess changes in their understanding following the professional development. Our next step will be validation of the assessment with a larger group of teachers.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1501–1502},
numpages = {2},
keywords = {assessment, computational thinking, integrated assessment, teacher assessment, teacher professional development},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3631700.3664917,
author = {Kasinidou, Maria},
title = {Development of Personalised Educational Tools for AI Literacy Using Participatory Design},
year = {2024},
isbn = {9798400704666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631700.3664917},
doi = {10.1145/3631700.3664917},
abstract = {The flourishing presence of Artificial Intelligence (AI) in daily life emphasizes the necessity for education about AI. It is crucial to create tailored educational materials and tools to meet the unique educational needs of diverse groups. This ongoing project aims to develop personalised educational tools for AI literacy for different groups of society (i.e., children, teachers, and adults). The perception and knowledge of AI by these groups will be explored. Participatory design workshops with teachers and adults will assist in the development of personalised educational tools for these groups tailored to the specific needs of each group and their perceptions of AI. Finally, this project aims to identify the best practices for developing personalised educational tools for AI literacy for each group.},
booktitle = {Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {30–34},
numpages = {5},
keywords = {AI education, AI literacy, participatory design, personalisation},
location = {Cagliari, Italy},
series = {UMAP Adjunct '24}
}

@inproceedings{10.1145/3637907.3637966,
author = {Lin, Xiuyu and Chen, Zhirong},
title = {Knowledge Transfer Happen in Virtual Simulation Experiment Teaching: A Factor Study Based on Educational Experiment},
year = {2024},
isbn = {9798400716676},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637907.3637966},
doi = {10.1145/3637907.3637966},
abstract = {The factors influencing the effect of virtual simulation experimental teaching on students' knowledge transfer are multidimensional, and the effect of virtual simulation experimental teaching on students' knowledge transfer varies depending on different factors. In order to study the influencing factors of virtual simulation experiment teaching on students' knowledge transfer effect, this article establishes a research model based on knowledge transfer and immersive learning theory. 52 learners who participated in virtual simulation experimental teaching were used as the research objects to analyze the factors influencing the effect of virtual simulation experimental teaching on students' knowledge transfer through structural equation modeling. Results indicated that immersion as a technical characteristic of virtual simulation positively influences students' presence and intrinsic motivation, and presence positively affects knowledge transfer; virtual simulation experimental learning supports positively influence students' knowledge structure and self-regulation, and self-regulation positively affects knowledge transfer effects. However, this article also found unexpected results that intrinsic motivation and knowledge structure did not have a positive effect on knowledge transfer. The findings can provide useful references for future educators to design virtual simulation experimental teaching, thus promoting the effect of knowledge transfer.CCS CONCEPTS • Applied computing∼Computer-assisted instruction},
booktitle = {Proceedings of the 2023 6th International Conference on Educational Technology Management},
pages = {137–147},
numpages = {11},
keywords = {influencing factors, knowledge transform, virtual reality, virtual simulation experiment},
location = {Guangzhou, China},
series = {ICETM '23}
}

@inproceedings{10.1145/3632621.3671433,
author = {Yang, Ellia and Ogan, Amy and Hammer, Jessica and Solyst, Jaemarie},
title = {Designing an AI Literacy Transformational Game for Families},
year = {2024},
isbn = {9798400704765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632621.3671433},
doi = {10.1145/3632621.3671433},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 2},
pages = {548},
numpages = {1},
keywords = {AI literacy, design, families, transformational games, youth},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@inproceedings{10.1145/3641555.3705261,
author = {Barkhuff, Grace and Pruitt, Ian},
title = {Teaching Assistants' Experiences of and Opinions on CS Ethics},
year = {2025},
isbn = {9798400705328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641555.3705261},
doi = {10.1145/3641555.3705261},
abstract = {Computer Science (CS) ethics coursework is required at many institutions for undergraduate CS majors. However, little is known about how students perceive the addition of this topic into the CS curriculum which is otherwise largely technical in nature. To understand perceptions of CS ethics as a course, we included questions about CS ethics instruction on a survey of 88 graduate and undergraduate teaching assistants (TAs) at two institutions in the United States. We found that learning CS ethics was not a universal experience for all TAs, but that a majority of TAs felt that CS ethics is a vital curricular topic. We recommend further research with a wider study population to better understand the perceptions of students of CS ethics coursework.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1377–1378},
numpages = {2},
keywords = {cs ethics education, ethics, ethics education, tas, teaching assistants},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3632621.3671430,
author = {Solyst, Jaemarie and Amspoker, Emily and Yang, Ellia and Luo, Yi and Hammer, Jessica and Ogan, Amy},
title = {Scaffolding Critical Thinking about Stakeholders' Power in Socio-Technical AI Literacy},
year = {2024},
isbn = {9798400704765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632621.3671430},
doi = {10.1145/3632621.3671430},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 2},
pages = {544–545},
numpages = {2},
keywords = {AI literacy, computer science education, learning activity, middle school, responsible AI, youth},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@inproceedings{10.1145/3641554.3701966,
author = {Solyst, Jaemarie and Amspoker, Emily and Yang, Ellia and Eslami, Motahhare and Hammer, Jessica and Ogan, Amy},
title = {RAD: A Framework to Support Youth in Critiquing AI},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701966},
doi = {10.1145/3641554.3701966},
abstract = {Artificial intelligence (AI) is ubiquitous in K-12 youths' everyday lives. However, it has become increasingly well-documented that AI can cause harm by reflecting and amplifying societal biases. While many youth are not currently empowered to engage in broader responsible AI discourse and processes, there is great potential. Foundational to engaging in critical conversations is ability to critique AI. We present the RAD framework, designed to scaffold critique of AI in three steps: Recognize (harms of AI), Analyze (societal aspects of AI harms), and Deliberate (what more responsible AI could be). We ran a workshop study with racially diverse middle school girls (N = 21) to investigate its effectiveness. We found that through being scaffolded with the framework, the youth could articulate biases that they saw in an AI scenario and consider how biases may impact different stakeholders. They then could contemplate how different stakeholders had varying amounts of power in the AI scenario and what that meant in terms of creating more responsible AI systems and processes. After participating in the study, the youth felt more strongly about voicing their opinions about AI with others. The RAD framework and activities work toward emboldening youths' engagement in critical discourse about AI.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1071–1077},
numpages = {7},
keywords = {ai literacy, algorithmic justice, k-12, socio-technical, youth},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3726010.3726049,
author = {Wang, Yadong and Wang, Yihan and Wang, Yan},
title = {The Innovative Teaching Practice of Cultural and Creative Product Design in Classroom Supported by AIGC - Taking Folk Art as An Example},
year = {2025},
isbn = {9798400712845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726010.3726049},
doi = {10.1145/3726010.3726049},
abstract = {This study aims at the shortcomings of the traditional teaching mode of cultural and creative product design in cultivating students' new design abilities such as interdisciplinary research, cross-cultural communication, and cross-domain collaboration. Using folk art as the design material, task-oriented cultural and creative product design teaching practice is carried out. In the process of practice, the technology of artificial intelligence generated content (AIGC) is effectively integrated into the main links of design teaching. Based on the actual teaching effect and experimental data, an innovative teaching mode based on AIGC is established. This initiative aims to expand the application scope and form of artificial intelligence in classroom teaching and provide reference strategies for cultivating students' new design abilities such as interdisciplinary research, cross-cultural communication, and cross-domain collaboration.},
booktitle = {Proceedings of the 2024 International Conference on Artificial Intelligence, Digital Media Technology and Interaction Design},
pages = {251–261},
numpages = {11},
keywords = {Artificial intelligence generation technology, Cultural and creative product design, Folk art, Teaching mode},
location = {
},
series = {ICADI '24}
}

@article{10.1145/3701223,
author = {Shein, Esther},
title = {The Evolution of Computer Science at the University Level},
year = {2025},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {68},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/3701223},
doi = {10.1145/3701223},
abstract = {Combining computer science with arts disciplines may be the wave of the future for education.},
journal = {Commun. ACM},
month = jan,
pages = {14–16},
numpages = {3}
}

@inproceedings{10.1145/3478431.3499304,
author = {Adams, Joel C. and Allen, Bryce D. and Fowler, Bryan C. and Wissink, Mark C. and Wright, Joshua J.},
title = {The Sounds of Sorting Algorithms: Sonification as a Pedagogical Tool},
year = {2022},
isbn = {9781450390705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478431.3499304},
doi = {10.1145/3478431.3499304},
abstract = {Much work already exists on algorithm visualization-the graphical representation of an algorithm's behavior-and its benefits for student learning. Visualization, however, offers limited benefit for students with visual impairments. This paper explores algorithm sonification-the representation of an algorithm's behavior using sound. To simplify the creation of sonifications for modern algorithms, this paper presents a new Thread Safe Audio Library (TSAL). To illustrate how to create sonifications, the authors have added TSAL calls to four common sorting algorithm implementations, so that as the program accesses a value being sorted, the program plays a tone whose pitch is scaled to that value's magnitude. In the resulting sonifications, one can (in real time) hear the behavioral differences of the different sorting algorithms as they run, and directly experience how fast (or slow) the algorithms sort the same sequence, compared to one another. This paper presents experimental evidence that the sonifications improve students' long-term recall of the four sorting algorithms' relative speeds. The paper also discusses other potential uses of sonification.},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education - Volume 1},
pages = {189–195},
numpages = {7},
keywords = {accessibility, algorithm, audio, graphics, hearing, media, sonification, sorting, sound, visualization},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}

@article{10.1145/3676561,
author = {Goldweber, Mikey},
title = {CURRICULUM MATTERS},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2153-2184},
url = {https://doi.org/10.1145/3676561},
doi = {10.1145/3676561},
abstract = {How to Read the CS2023 Report},
journal = {ACM Inroads},
month = aug,
pages = {18–19},
numpages = {2}
}

@article{10.1145/3650115,
author = {DeLiema, David and Bye, Jeffrey K. and Marupudi, Vijay},
title = {Debugging Pathways: Open-Ended Discrepancy Noticing, Causal Reasoning, and Intervening},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
url = {https://doi.org/10.1145/3650115},
doi = {10.1145/3650115},
abstract = {Learning to respond to a computer program that is not working as intended is often characterized as finding a singular bug causing a singular problem. This framing underemphasizes the wide range of ways that students and teachers could notice discrepancies from their intention, propose causes of those discrepancies, and implement interventions. Weaving together a synthesis of the existing research literature with new multimodal interaction analyses of teacher–student conversations during coding, we propose a framework for debugging that foregrounds this open-endedness. We use the framework to structure an analysis of three naturalistic debugging situations (with US 5th–10th graders) that range from solo debugging to collaborative discourse. We argue that a broken computer program is a polysemous object through which teachers and students actively and publicly notice, reason about, and negotiate different debugging pathways. We document students and teachers improvisationally altering a debugging pathway, justifying a particular pathway, and outwardly discussing competing pathways. This paper provides a framework for structuring debugging pedagogy to be less about scaffolding a student toward a specific pathway to a fix and more about exploring multiple possible pathways and judging the (learning) value of various routes.},
journal = {ACM Trans. Comput. Educ.},
month = may,
articleno = {28},
numpages = {34},
keywords = {Debugging, CS education, causal reasoning, interaction analysis}
}

@article{10.1145/3644477,
author = {Kumar, Amruth N. and Raj, Rajendra K.},
title = {Toward a Globalized Understanding of Computer Science Education},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {2153-2184},
url = {https://doi.org/10.1145/3644477},
doi = {10.1145/3644477},
journal = {ACM Inroads},
month = feb,
pages = {5},
numpages = {1}
}

@inproceedings{10.1145/3623762.3633494,
author = {Cutts, Quintin and Kallia, Maria and Anderson, Ruth and Crick, Tom and Devlin, Marie and Farghally, Mohammed and Mirolo, Claudio and Runde, Ragnhild Kobro and Sepp\"{a}l\"{a}, Otto and Urquiza-Fuentes, Jaime and Vahrenhold, Jan},
title = {Arguments for and Approaches to Computing Education in Undergraduate Computer Science Programmes},
year = {2023},
isbn = {9798400704055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623762.3633494},
doi = {10.1145/3623762.3633494},
abstract = {Computing education (CE), the scientific foundation of the teaching and learning of subject matter specific to computing, has matured into a field with its own research journals and conferences as well as graduate programmes. Yet, and unlike other mature subfields of computer science (CS), it is rarely taught as part of undergraduate CS programmes. In this report, we present a gap analysis resulting from semi-structured interviews with various types of stakeholders and derive a set of arguments for teaching CE courses in undergraduate CS programmes. This analysis and the arguments highlight a number of opportunities for the discipline of CS at large, in academia, in industry, and in school education, that would be opened up with undergraduate CE courses, as well as potential barriers to implementation that will need to be overcome. We also report on the results of a Delphi process performed to elicit topics for such a course with various audiences in mind. The Delphi process yielded 19 high-level categories that encompass the subject matter CE courses should incorporate, tailored to the specific needs of their intended student audiences. This outcome underscores the extensive range of content that can be integrated into a comprehensive CE programme. Based on these two stakeholder interactions as well as a systematic literature review aiming to explore the current practices in teaching CE to undergraduate students, we develop two prototypical outlines of such a course, keeping in mind that departments may have different preferences and affordances resulting in different kinds of CE offerings. Overall, input from external stakeholders underscores the clear significance of undergraduate CE courses. We anticipate leveraging this valuable feedback to actively promote these courses on a broader scale.},
booktitle = {Proceedings of the 2023 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {160–195},
numpages = {36},
keywords = {argument, computing education, curriculum outline, undergraduate},
location = {Turku, Finland},
series = {ITiCSE-WGR '23}
}

@inproceedings{10.5555/3716662.3716772,
author = {Rothschild, Annabel and Wang, Ding and Vilvanathan, Niveditha Jayakumar and Wilcox, Lauren and DiSalvo, Carl and DiSalvo, Betsy},
title = {The Problems with Proxies: Making Data Work Visible through Requester Practices},
year = {2025},
publisher = {AAAI Press},
abstract = {Fairness in AI and ML systems is increasingly linked to the proper treatment and recognition of data workers involved in training dataset development. Yet, those who collect and annotate the data, and thus have the most intimate knowledge of its development, are often excluded from critical discussions. This exclusion prevents data annotators, who are domain experts, from contributing effectively to dataset contextualization. Our investigation into the hiring and engagement practices of 52 data work requesters on platforms like Amazon Mechanical Turk reveals a gap: requesters frequently hold naive or unchallenged notions of worker identities and capabilities and rely on ad-hoc qualification tasks that fail to respect the workers' expertise. These practices not only undermine the quality of data but also the ethical standards of AI development. To rectify these issues, we advocate for policy changes to enhance how data annotation tasks are designed and managed and to ensure data workers are treated with the respect they deserve.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {1255–1268},
numpages = {14},
location = {San Jose, California, USA},
series = {AIES '24}
}

@article{10.1145/3654789,
author = {Borchardt, Mara and Roggi, In\'{e}s and Schapachnik, Fernando},
title = {Keys to a Comprehensive Computer Science at School Policy in Argentina},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {67},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/3654789},
doi = {10.1145/3654789},
journal = {Commun. ACM},
month = aug,
pages = {83–85},
numpages = {3}
}

@inproceedings{10.1145/3677619.3678716,
author = {Mannila, Linda},
title = {Co-Designing AI literacy for K-12 Education},
year = {2024},
isbn = {9798400710056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677619.3678716},
doi = {10.1145/3677619.3678716},
abstract = {Today’s children are the first generation to grow up with AI as a natural part of their daily lives; to benefit from the opportunities AI brings, but also to face AI-related risks such as privacy invasion, exposure to biased or harmful content, manipulation through personalized advertisements, and potential dependency on AI for decision-making. Consequently, AI literacy is becoming an essential component of modern education. AI, however, is both an unfamiliar topic and a moving target due to fast development. Determining what non-experts need to know about AI is therefore not straightforward, and the answer varies based on the educational context and level. This keynote begins by exploring the challenges of introducing AI literacy in K-12 education, drawing on insights from previous computing-related K-12 curricular reforms. It then presents a four-stage process of co-creating AI lesson plans with students, teachers, and teacher trainers in Finland. Finally, we discuss the lessons learned and future ideas for AI in K-12 education.},
booktitle = {Proceedings of the 19th WiPSCE Conference on Primary and Secondary Computing Education Research},
articleno = {2},
numpages = {3},
keywords = {AI literacy, Co-Design, K-12 education},
location = {Munich, Germany},
series = {WiPSCE '24}
}

@inproceedings{10.1145/3649405.3659524,
author = {Burridge, Joshua},
title = {Individualising Assessments at Scale},
year = {2024},
isbn = {9798400706035},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649405.3659524},
doi = {10.1145/3649405.3659524},
abstract = {This paper presents a novel 'Socketed' approach to assessment design in computing education, aiming to reconcile the need for individual student assessment with the benefits of collaborative learning. By providing a skeleton project to be combined with individualised components, students experienced a tailored yet unified assessment structure. Initial feedback shows improved comprehension and peer collaboration, and suggests mitigation of pressures towards academic dishonesty. Motivating context criteria and design considerations are provided to assist educators in implementing this in their own teaching.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 2},
pages = {791–792},
numpages = {2},
keywords = {individualized assessment, personalization, socketed specifications},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3649217.3653578,
author = {Prickett, Tom and Crick, Tom and Davenport, James H. and Bowers, David S. and Hayes, Alan and Irons, Alastair},
title = {Embedding Technical, Personal and Professional Competencies in Computing Degree Programmes},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653578},
doi = {10.1145/3649217.3653578},
abstract = {Many factors influence computing graduate employment prospects, including human capital, social capital, individual attributes, individual career-building behaviours, perceived employability, and labour market factors. Whilst most computing graduates go on to be beneficially employed, a small minority remain under-employed or unemployed. Computing curricular recommendations increasingly advocate a competency-based approach to bolster graduates' perceived employability. Hence, the discipline is evolving to incorporate competency-based approaches. However, competency-based can mean any of three different types of competency: technical, personal and professional. Technical Competency is the ability to apply acquired content knowledge and skills to develop solutions to unseen problems. Personal Competency is the personal behaviours and interpersonal skills required for success in the modern workplace. Professional Competency is Technical and Personal combined and applied in a real-world context.This position paper provides illustrative examples of how to embed all three kinds of Competency. Based on examples from representative undergraduate computing programmes at UK universities, it provides examples of embedding each kind of competency: Technical Competency (teaching programming through craft computing and approaches for developing cybersecurity competency), Personal Competency (teaching teamwork through project-based learning and creativity via problem-based learning), and Professional Competency (developing work-ready competency using industrial placements, and co-design/co-delivery with industry via degree apprenticeships), providing a valuable foundation and framing for portability and extension in other institutions and jurisdictions. Furthermore, these distinctive types of competency form a helpful taxonomy when considering how to embed competency in computing courses and are candidates for inclusion within future computing curricula guidelines.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {346–352},
numpages = {7},
keywords = {computing competencies, curriculum design, personal competencies, professional competencies},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3631802.3631829,
author = {Ten\'{o}rio, Kamilla and Romeike, Ralf},
title = {AI Competencies for non-computer science students in undergraduate education: Towards a competency framework},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631829},
doi = {10.1145/3631802.3631829},
abstract = {Artificial Intelligence (AI) has been increasingly applied in various societal areas such as medicine, education, and science. For example, through the generation of more accurate medical diagnoses to support patients’ treatment, more content personalization to provide adaptive learning for students and more accurate predictions for future climate changes. Consequently, there is an increasing demand for professionals from different fields with AI competencies. These future professionals need preparation during their undergraduate education to deal with the remarkable AI breakthroughs in their domains and to understand, use, and help with the responsible development of these technologies. However, to address AI to non-computer science students in undergraduate education, it is necessary to thoroughly investigate the core AI competencies essential to these students acquire in order to prepare them effectively. Based on this, the objective of the research is to develop a framework with core AI competencies that can be adopted in future work to inform AI education for this target audience. Therefore, towards the AI competency framework for non-computer science students in undergraduate education, as an initial part of the process, we conducted semi-structured interviews with professionals working in the intersection of AI and other domains. The objective of the interviews was to qualitatively investigate the AI competencies considered suitable for incorporation into the undergraduate education curricula of non-computer science students from these professionals’ points of view. In this work, we present the results of these interviews and the list of core AI competencies for non-computer science students in undergraduate education according to these professionals. In summary, this list encompasses different perspectives, varying from basic AI competencies related to AI definition, history, and capabilities to more complex theoretical knowledge and practical skills regarding data and machine learning. The list also includes responsible AI competencies, covering AI’s social, ethical, and legal aspects.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {6},
numpages = {12},
keywords = {AI education, competency-based education, interviews, undergraduate education},
location = {Koli, Finland},
series = {Koli Calling '23}
}

@article{10.1145/3589252,
author = {Adair, Amy},
title = {Teaching and Learning with AI: How Artificial Intelligence is Transforming the Future of Education},
year = {2023},
issue_date = {Spring 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {3},
issn = {1528-4972},
url = {https://doi.org/10.1145/3589252},
doi = {10.1145/3589252},
journal = {XRDS},
month = apr,
pages = {7–9},
numpages = {3}
}

@inproceedings{10.1145/3649217.3653539,
author = {Brooks, Alexi},
title = {Agile Ethics: A Low Stakes, Skills-based Framework for Teaching CS Ethics},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653539},
doi = {10.1145/3649217.3653539},
abstract = {Computer Science educators widely agree that ethics is a vital and underdeveloped part of the CS curriculum. Attempts to increase ethics content within undergraduate CS programs have faced challenges integrating material into the current coursework. I present an ethical framework applicable to the core Computer Science activities of programming and software development, with potential for extension into other CS subfields. The application of this framework to a specific educational intervention is reserved for future work. In this paper, I focus on the framework itself and its theoretical justification. By shifting emphasis from hard ethical quandaries and advanced CS products to mundane challenges faced by a front line software developer, this framework may allow instructors to more easily and effectively integrate ethics material into introductory CS coursework. While instructors may continue to apply prepared scenarios, the framework de-emphasizes those in favor of scaffolding student coding practices that maximize the frequency of practice with ethical skills.The Agile Ethics framework provides a structure which students and educators can use to think about (1) when during a project ethical reasoning is needed, (2) what questions need to be asked at that time, and (3) how to apply Computer Science skills and knowledge to answer each question. Frequent, low intensity instances of ethical reasoning under this framework reinforce the integration of ethics as a habitual part of the software development process.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {492–498},
numpages = {7},
keywords = {computing education, cs1, ethics},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3649405.3659476,
author = {Zabner, David},
title = {A Learning Theory of Programming Language Acquisition},
year = {2024},
isbn = {9798400706035},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649405.3659476},
doi = {10.1145/3649405.3659476},
abstract = {CS education lacks sufficiently robust and specific theories of learning for programming language acquisition (PLA) to guide pedagogical design, assessment of students' learning, and research programs. In designing theories of learning for PLA, it is prudent to draw on research from second language acquisition (SLA). I present a theory of learning for PLA and explore its implications.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 2},
pages = {838–839},
numpages = {2},
keywords = {computing education, cs1, pla, programming language acquisition},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@article{10.1145/3637464,
author = {Smith, Alaina and Gilbert, Juan},
title = {Computing for Social Good: University of Florida},
year = {2024},
issue_date = {Winter 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1528-4972},
url = {https://doi.org/10.1145/3637464},
doi = {10.1145/3637464},
journal = {XRDS},
month = feb,
pages = {38–39},
numpages = {2}
}

@inproceedings{10.1145/3649165.3703623,
author = {Ravi, Prerna and Parks, Robert and Masla, John and Abelson, Hal and Breazeal, Cynthia},
title = {"Data comes from the real world": A Constructionist Approach to Mainstreaming K12 Data Science Education},
year = {2024},
isbn = {9798400705984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649165.3703623},
doi = {10.1145/3649165.3703623},
abstract = {Data science is emerging as a crucial 21st-century competence, influencing professional practices from citing evidence when advocating for social change to developing artificial intelligence (AI) models. For middle and high school students, data science can put formerly decontextualized subjects into real-world scenarios. Many existing curricula, however, lack authenticity and personal relevance for students. A critique of data science courseware cites the lack of "author proximity," in which students do not contribute to the data's production or see their personal experiences reflected in the data. This paper introduces a novel data science curriculum to scaffold middle and high school students in undertaking real-world data science practices. Through project-based learning modules, the curriculum engages students in investigating solutions to community-based problems through visualization and analysis of live sensor data and public data sets. Materials include formative assessments to help educators (especially those from non-math and computing backgrounds) measure their students' abilities to identify statistical patterns, critically evaluate data biases, and make predictions. As we pilot and co-design with teachers, we will look closely at whether the curriculum's resources can successfully support non-technical practitioners engaging in an integrated curriculum.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1},
pages = {271–274},
numpages = {4},
keywords = {computational action, k12 data science, participatory data collection, project-based learning, sensors},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@proceedings{10.1145/3563357,
title = {BuildSys '22: Proceedings of the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
year = {2022},
isbn = {9781450398909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Over the past thirteen years, BuildSys has been an interdisciplinary conference that brings together various stakeholders, including researchers, practitioners, and policymakers from different disciplines, including civil engineering, mechanical engineering, environmental science, electrical and computer engineering, computer science, system management and control, and many others. This year is no exception, with papers and attendees from all these disciplines and regions worldwide. The conference's focus extends beyond building systems to the built environment more generally.},
location = {Boston, Massachusetts}
}

@inproceedings{10.1145/3626253.3635554,
author = {Smith, Julie M. and Twarek, Bryan and McGill, Monica M.},
title = {Reimagining CS Courses for High School Students},
year = {2024},
isbn = {9798400704246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626253.3635554},
doi = {10.1145/3626253.3635554},
abstract = {Traditionally, computer science (CS) in the United States has been an elective subject at the high school level. In recent years, however, some school systems have created a CS graduation requirement. Designing a required CS course that meets the needs of anticipated future advancements in the field necessitates exploring the research question, What computing content do high school teachers, college instructors, and computing industry professionals prioritize in a required computer science course for high school students? To better understand what these different groups perceive to be the essential content of a foundational high school CS course, we conducted a series of focus groups. These focus groups explored participants' (n = 21) thinking about what content would be most important to prioritize in a required high school CS course. Transcripts of the focus groups were abductively coded and then analyzed to determine what CS content priorities were identified and what disagreements about priorities exist. We found that participants (1) emphasized CS knowledge and skills, with minimal reference to dispositions, (2) prioritized content similar to that found in current CS standards, (3) developed broad, high-level descriptions of content, (4) identified contextually relevant factors, (5) foregrounded AI both a tool and as a subdomain of CS, and (6) emphasized computational thinking. These findings can inform further research on the design and implementation of a required high school CS course designed to meet the needs of the future as well as to support revisions of CS standards for high school students.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1822–1823},
numpages = {2},
keywords = {curriculum, focus groups, high school, standards},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3628516.3655752,
author = {Morales-Navarro, Luis and Kafai, Yasmin and Konda, Vedya and Metaxa, Dana\"{e}},
title = {Youth as Peer Auditors: Engaging Teenagers with Algorithm Auditing of Machine Learning Applications},
year = {2024},
isbn = {9798400704420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628516.3655752},
doi = {10.1145/3628516.3655752},
abstract = {As artificial intelligence/machine learning (AI/ML) applications become more pervasive in youth lives, supporting them to interact, design, and evaluate applications is crucial. This paper positions youth as auditors of their peers’ ML-powered applications to better understand algorithmic systems’ opaque inner workings and external impacts. In a two-week workshop, 13 youth (ages 14-15) designed and audited ML-powered applications. We analyzed pre/post clinical interviews in which youth were presented with auditing tasks. The analyses show that after the workshop all youth identified algorithmic biases and inferred dataset and model design issues. Youth also discussed algorithmic justice issues and ML model improvements. Furthermore, youth reflected that auditing provided them new perspectives on model functionality and ideas to improve their own models. This work contributes (1) a conceptualization of algorithm auditing for youth; and (2) empirical evidence of the potential benefits of auditing. We discuss potential uses of algorithm auditing in learning and child-computer interaction research.},
booktitle = {Proceedings of the 23rd Annual ACM Interaction Design and Children Conference},
pages = {560–573},
numpages = {14},
keywords = {algorithm auditing, algorithmic justice, artificial intelligence, child-computer interaction, machine learning, youth},
location = {Delft, Netherlands},
series = {IDC '24}
}

@inproceedings{10.1145/3626252.3630903,
author = {Smith, Gillian},
title = {Pairing Ungrading with Project-Based Learning in CS1 for Inherently Flexible Course Design},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630903},
doi = {10.1145/3626252.3630903},
abstract = {This experience report details the pedagogical approach and curriculum for an introductory programming course for non-majors that combines creative coding, ungrading, and project-based learning, with typical enrollment between 120-140 students. Through a series of skills labs, a term-long group project, and regular self-evaluation milestones, students both build their confidence and motivation to learn programming, as well as typical introductory programming skills. Key to the course's success is the integration of project-based learning with a self-evaluation approach to ungrading. In this paper, I present the design of the course, the underlying pedagogical approach leading to course design decisions, and offer resources for adopting this approach in similar CS1 courses. The paper closes with discussion reflecting on the experiences observed throughout teaching this course, and suggests that the approach of blending ungrading with project-based learning shows promise as an inherently flexible course design that supports student wellbeing, confidence, and motivation.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1265–1271},
numpages = {7},
keywords = {creative coding, experience report, inclusive pedagogy, project-based learning, ungrading},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3617650.3624928,
author = {Raj, Rajendra K. and Becker, Brett A. and Goldweber, Michael and Jalote, Pankaj},
title = {Perspectives on Computer Science Curricula 2023 (CS2023)},
year = {2023},
isbn = {9798400703744},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617650.3624928},
doi = {10.1145/3617650.3624928},
abstract = {This panel examines Computer Science Curricula 2023 (CS2023) from different perspectives. All panelists serve on the CS2023 steering committee and have an intimate understanding of CS2023. The moderator will lay out its overall vision and structure while panelists will emphasize three major perspectives of CS education: software development fundamentals; systems development; and the increased role of societal, ethical, and professional aspects crucial to a modern CS graduate. Strong interdependencies exist between these perspectives, along with tensions arising from how much can be squeezed into a tight undergraduate CS curriculum. Attendees will take home an understanding of the approach taken by the CS2023 task force, the constraints on curriculum design, and how best to use the CS2023 guidelines to educate the next generation of CS graduates.},
booktitle = {Proceedings of the ACM Conference on Global Computing Education Vol 2},
pages = {187–188},
numpages = {2},
keywords = {computer science curricular guidelines, model curricula},
location = {Hyderabad, India},
series = {CompEd 2023}
}

@inproceedings{10.1145/3700297.3700406,
author = {Zou, Linying and Chen, Daijiang},
title = {Construction of deep learning model based on digital learning and innovation literacy -Take Python programming teaching as an example},
year = {2024},
isbn = {9798400707100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3700297.3700406},
doi = {10.1145/3700297.3700406},
abstract = {Digital learning and innovation is one of the core competencies in information technology education for primary and secondary schools, which is closely related to the current advocacy of deep learning by many educational practitioners. This study aims to guide learners to develop their digital learning and innovation competencies in the context of deep learning. Based on a thorough analysis of the relationship between deep learning and digital learning and innovation, a deep learning model for digital learning and innovation is constructed. A specific Python programming class is used as an example to illustrate how digital learning and innovation competencies can be stimulated and cultivated in the context of deep learning using intelligent educational platforms.},
booktitle = {Proceedings of the 2024 International Symposium on Artificial Intelligence for Education},
pages = {634–638},
numpages = {5},
keywords = {Deep Learning, Digital Learning and Innovation, Smart Education Platform},
location = {
},
series = {ISAIE '24}
}

@inproceedings{10.1145/3677525.3678677,
author = {Kasinidou, Maria and Kleanthous, Styliani and Otterbacher, Jahna},
title = {"Artificial intelligence is a very broad term": How educators perceive Artificial Intelligence?},
year = {2024},
isbn = {9798400710940},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677525.3678677},
doi = {10.1145/3677525.3678677},
abstract = {Artificial intelligence (AI) has seamlessly integrated into our daily routines. Recognizing the significance of AI, educators are expected not only to teach about AI but also to utilize AI tools in their teaching, ensuring that their students are equipped with the necessary knowledge to navigate the AI-driven society. Thus, it is important to understand how educators perceive AI. In this study, we explore educators’ perceptions of AI, as well as whether they teach AI, the challenges they face in teaching AI, potential support for teaching AI in the future, and possible differences between Cypriot and international educators. Two online surveys, involving 197 educators, followed by interviews with 10 Cypriot educators were conducted to examine how they perceive AI. The findings revealed a shared conceptualization of AI as “human-like”, intelligent machines, capable of learning and solving problems using data. Notable differences in the definitions of AI emerged between Cypriot educators and their international counterparts. Furthermore, the findings showed that even though educators perceive teaching AI as important, they lack formal training and specialized educational tools on AI, underscoring the urgent need for targeted interventions and tailored resources.},
booktitle = {Proceedings of the 2024 International Conference on Information Technology for Social Good},
pages = {315–323},
numpages = {9},
keywords = {AI Education, Artificial Intelligence, educators, perception of AI},
location = {Bremen, Germany},
series = {GoodIT '24}
}

@inproceedings{10.1145/3630106.3658988,
author = {Madaio, Michael and Kapania, Shivani and Qadri, Rida and Wang, Ding and Zaldivar, Andrew and Denton, Remi and Wilcox, Lauren},
title = {Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3658988},
doi = {10.1145/3630106.3658988},
abstract = {Prior work has developed responsible AI (RAI) toolkits and studied how AI practitioners use such resources when practicing RAI. However, AI practitioners may not have the relevant skills or knowledge to effectively use RAI resources—particularly as pre-trained AI models have enabled more people to develop AI-based applications. In this paper, we explore current practices and aspirations for learning about RAI on-the-job, by interviewing 16 AI practitioners and 24 RAI educators across 16 organizations. We identify AI practitioners’ learning pathways for RAI, including information foraging and interpersonal learning; the orientations of RAI learning resources towards computational and procedural approaches to RAI; and aspirations for RAI learning, including desires for more sociotechnical approaches to understand potential harms of AI systems—aspirations that can be in tension with organizational priorities. We contribute empirical evidence of what and how AI practitioners are learning about RAI, and we suggest opportunities for the field to better support sociotechnical approaches to learning about RAI on-the-job.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1544–1558},
numpages = {15},
keywords = {Responsible AI, on-the-job learning, sociotechnical AI, training},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@inproceedings{10.1145/3706599.3719993,
author = {Su, Xiaotian and Wang, April Yi},
title = {The Stress of Improvisation: Instructors' Perspectives on Live Coding in Programming Classes},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719993},
doi = {10.1145/3706599.3719993},
abstract = {Live coding is a pedagogical technique in which an instructor writes and executes code in front of students to impart skills like incremental development and debugging. Although live coding offers many benefits, instructors face many challenges in the classroom, like cognitive challenges and psychological stress, most of which have yet to be formally studied. To understand the obstacles faced by instructors in CS classes, we conducted (1) a formative interview with five teaching assistants in exercise sessions and (2) a contextual inquiry study with four lecturers for large-scale classes. We found that the improvisational and unpredictable nature of live coding makes it difficult for instructors to manage their time and keep students engaged, resulting in more mental stress than presenting static slides. We discussed opportunities for augmenting existing IDEs and presentation setups to help enhance live coding experience.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {525},
numpages = {6},
keywords = {live coding, programming education at scale},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3689187.3709610,
author = {Garcia, Rita and Csizmadia, Andrew and Pearce, Janice L. and Alshaigy, Bedour and Glebova, Olga and Harrington, Brian and Liaskos, Konstantinos and Lunn, Stephanie J. and Mackellar, Bonnie and Nasir, Usman and Pettit, Raymond and Schulz, Sandra and Stewart, Craig and Zavaleta Bernuy, Angela},
title = {An International Examination of Non-Technical Skills and Professional Dispositions in Computing -- Identifying the Present Day Academia-Industry Gap},
year = {2025},
isbn = {9798400712081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689187.3709610},
doi = {10.1145/3689187.3709610},
abstract = {Computing graduates are frequently reported by members of industry to lack in professional dispositions and/or non-technical skills (often referred to as "soft skills"). In this work, we conduct a gap analysis of the alignment between academic preparation and industry expectations through a three-pronged study. First, a literature review explored the academic perspective of how fostering professional dispositions and non-technical skills occurs in tertiary computing education. Second, a literature review identifying industry's expectations of those dispositions and skills for entry-level computing professionals. Finally, a mixed-methods approach, combining a survey and structured interviews of computing industry professionals to identify their opinions on the relative importance of those skills and dispositions. In each of these prongs, we additionally consider whether and how Diversity, Equity, Inclusion, and Accessibility (DEIA) may have been approached and/or incorporated.Our work uncovers a number of gaps. Several skills and dispositions, such as leadership, ethics, and inventiveness, are over-represented in the academic literature compared to industry's expectations, while others such as lifelong learning and professionalism are under-emphasised. Furthermore, some terms such as 'ethics' and 'professionalism' are defined differently by various stakeholder groups, leading to a gap between academic training and industry expectations. Finally, several skills and dispositions, such as collaboration, teamwork, communication, and leadership show evidence of exposure in academia, but require more scaffolded instruction to meet industry expectations. We also found a dearth of coverage in the literature and a lack of focus in industry for DEIA considerations.},
booktitle = {2024 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {124–174},
numpages = {51},
keywords = {DEIA, E&amp;I, EDI, accessibility, diversity, equity, inclusion, industry expectations, non-technical skills, professional dispositions, professionalism},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3641554.3701938,
author = {Harrington, Brian and Kulkarni, Aditya and Nalluri, Rohita and Vadarevu, Anagha and Zavaleta Bernuy, Angela},
title = {Literature Mapping: A Scaffolded, Scalable, Low-Overhead Undergraduate Research Experience},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701938},
doi = {10.1145/3641554.3701938},
abstract = {There is a wealth of evidence that involving undergraduate students in research has positive impacts in a variety of areas, from representation and retention to outcomes and self-efficacy. However, developing and growing an undergraduate research program can be daunting, especially for institutions that do not have a large existing research enterprise. In this work, we detail a program that revolves around student-developed literature maps to help students gain the ability to read and assess research papers in a way that is accessible, robust, and requires relatively little faculty overhead. We further detail how this program has been run through 4 iterations, with a total of 47 students producing 5 posters or short papers, and 3 full papers. In this work, we provide our experiences using literature mapping projects to boot-strap an undergraduate research program and provide quantitative and qualitative analysis of the students who have participated. All of the materials, including sample spreadsheets, and scripts to generate LaTeX tables and figures are included for anyone wishing to undertake a literature mapping project of their own.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {464–470},
numpages = {7},
keywords = {literature mapping, literature review, undergraduate research},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.1145/3721295,
author = {Cachero, Cristina and Tom\'{a}s, David and Pujol, Francisco A.},
title = {Gender Bias in Self-Perception of AI Knowledge, Impact, and Support among Higher Education Students: An Observational Study},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
url = {https://doi.org/10.1145/3721295},
doi = {10.1145/3721295},
abstract = {Objectives. This study investigates gender biases in AI perceptions among university students. It focuses on assessing self-perceptions regarding knowledge, impact, and support, with a specific emphasis on identifying any significant gender differences. The main hypotheses are focused on the existence of gender disparities in AI awareness, perceptions, and attitudes among higher education students.Participants. The study involves 380 participants, enrolled in undergraduate courses across various academic disciplines. Participants are university students with diverse backgrounds in terms of age, academic majors, and prior exposure to AI technologies.Study Methods. This research employs an observational study design. The sample size includes 380 participants. The study utilizes a structured questionnaire as the primary instrument for data collection. Outcome measures focus on variables such as perceived knowledge of AI, perceived impact of AI, and levels of support or apprehension towards AI technologies.Findings. The findings reveal significant gender differences, with females exhibiting lower levels than their male counterparts in the level of perceived knowledge about AI ( (text{p} !lt! 0.005) ), exposure awareness (p = 0.001), perceived ability to apply AI (p = 0.004), sensitivity towards AI use of private data (p = 0.004), positive impact on society (p = 0.002), support for AI development ( (text{p}!lt!0.005) ), and positive expectations towards AI ( (text{p}!lt!0.005) ). Statistical analysis, including nonparametric tests, was used to validate these observations.Conclusions. There are notable gender biases in the knowledge and perception of AI among university students. These biases have implications for the future development and adoption of AI technologies, suggesting a need for more gender-inclusive educational strategies in AI. The findings underscore the importance of addressing gender disparities in AI education to ensure equitable access and understanding of these technologies. It is important to integrate gender perspectives in AI curriculum and policy-making to mitigate potential biases and enhance inclusivity in the field of AI.},
journal = {ACM Trans. Comput. Educ.},
month = may,
articleno = {15},
numpages = {26},
keywords = {Artificial intelligence, gender bias, social implications, observational study, university students}
}

@article{10.5555/3665609.3665614,
author = {Rosasco, Nicholas S. and Paxson, Andrew and Hawk, Ethan},
title = {A Pleasant Surprise: A Classic Assignment and an Offbeat Assessment for PDC, HPC, and Related KU Coverage},
year = {2024},
issue_date = {April 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {8},
issn = {1937-4771},
abstract = {This paper presents an assignment that when paired with a discussion or other follow-up can be used to increase and diversify computer science knowledge unit coverage. The knowledge units are taken from the CS2013 and related documents from ACM/IEEE. This guidance to the teaching community notes the criticality of educating students on parallel, distributed, and high performance computing. Information on available, modular resources for addressing these areas in the classroom is presented. Additionally, integrating undergraduate writing expectations into a topics course is discussed.},
journal = {J. Comput. Sci. Coll.},
month = apr,
pages = {70–82},
numpages = {13}
}

@inproceedings{10.1145/3478431.3499302,
author = {Weintrop, David},
title = {iSchools as Venues for Expanding the K-12 Computer Science Teacher Pipeline},
year = {2022},
isbn = {9781450390705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478431.3499302},
doi = {10.1145/3478431.3499302},
abstract = {There is a national shortage of certified computer science (CS) teachers. While significant effort and resources have been put towards addressing this shortage, the number of newly certified CS teachers entering the profession each year remains low. In this position paper, we argue that Colleges of Information Studies (iSchools) can serve as fertile grounds for recruiting future K-12 CS teachers. The motivation for looking to iSchools as venues for expanding the K-12 CS teacher pipeline is threefold: (1) the content taught in undergraduate iSchool programs closely aligns with K-12 CS content, (2) undergraduate programs in iSchools are experiencing rapid growth in enrollment and have a diverse student body, and (3) iSchools focus on social aspects of computing and thus attract students well-suited for K-12 teaching as a profession. To develop these arguments, we review the content of top iSchool undergraduate programs, showing the alignment between iSchool undergraduate coursework and K-12 CS. Further, using the University of Maryland's Bachelor of Science in Information Sciences as a case study, we show how iSchools can serve as contexts for preparing future K-12 CS teachers. Collectively, this work seeks to expand how we think about recruiting future CS teachers, explicating arguing for iSchools as venues for expanding the K-12 CS teacher pipeline.},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education - Volume 1},
pages = {397–403},
numpages = {7},
keywords = {ischools, k-12 computer science, teacher preparation},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}

@inproceedings{10.1145/3657604.3664683,
author = {Yan, Zhenting and Zhang, Rui},
title = {Enhancing Knowledge Tracing Efficacy with Expert-defined Graphs: A Case Study in Introductory Physics Classes},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3664683},
doi = {10.1145/3657604.3664683},
abstract = {Knowledge Tracing (KT) is essential in online education for tracking student progress and forecasting future performance. Despite the effectiveness of existing KT models, enhancing their educational interpretability and reliability remains crucial for both academic and practical applications. This study introduces an improved Graph-based Knowledge Tracing (GKT) model, enriched with domain expertise, instructional insights, and contextual features, to overcome current limitations. Our enhanced GKT model employs an expert-defined graph structure for more accurate domain knowledge representation. It integrates critical contextual features, like question difficulty and prompt usage, into the response matrix for a comprehensive context. Additionally, the model leverages second-order neighborhood features to more effectively capture complex interrelations between knowledge concepts. Validation using an Introductory Physics assignment dataset demonstrated that our updated GKT model surpasses its predecessor in both Area Under the Curve (AUC) and accuracy (ACC) metrics. These improvements are instrumental in refining knowledge graphs and developing personalized teaching strategies, thereby facilitating more effective and personalized educational experiences.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {433–437},
numpages = {5},
keywords = {adaptive learning, educational interpretability, explicit graph structure, graph-based knowledge tracing, modified GNN},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@inproceedings{10.1145/3702386.3702393,
author = {Cai, Fei and Chen, Wanyu and Zhang, Yijia},
title = {Exploration of the new teaching and learning mode enabled by Artificial Intelligence},
year = {2025},
isbn = {9798400710131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3702386.3702393},
doi = {10.1145/3702386.3702393},
abstract = {In recent years, with the development of science and technology, the application of artificial intelligence technology in the field of education begin to increase. Many well-known universities in China have stepped up their pace and actively explored the deep integration of "artificial intelligence + education". A series of innovative practices, such as intelligent teaching system, intelligent classroom, virtual teaching assistant and personalized learning platform, all show that the education industry is undergoing an unprecedented intelligent transformation. But at the same time, the application of artificial intelligence in the teaching process in colleges and universities is not mature, and there are still many problems. In this context, it has become very urgent to explore how to efficiently use artificial intelligence technology to contribute to the higher education in China. This paper introduces the development process of AI and its application in colleges and universities, then analyzes the obstacles of AI when applied in higher education, and finally proposes specific application strategies for artificial intelligence to empower new teaching and learning models in universities. It is hoped that the research of this paper can improve the application of artificial intelligence in the teaching of universities in China.},
booktitle = {Proceedings of the 2024 International Conference on Artificial Intelligence and Teacher Education},
pages = {9–14},
numpages = {6},
keywords = {Artificial Intelligence, Intelligent Assisted Teaching, New Teaching and Learning Mode, Personalized Learning},
location = {
},
series = {ICAITE '24}
}

@inproceedings{10.1145/3722237.3722282,
author = {Xu, Qingzheng and Zhao, Weihu and Wu, Wenhui and Wang, Na},
title = {Overall Design of the Course “Foundations of Artificial Intelligence” in Higher Education Institutions},
year = {2025},
isbn = {9798400712692},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3722237.3722282},
doi = {10.1145/3722237.3722282},
abstract = {“Foundations of Artificial Intelligence” is a foundational elective course in many higher education institutions, aimed at helping students develop intelligent thinking methods and effectively apply artificial intelligence (AI) techniques to solve complex real-world problems. Tailored to the characteristics of a particular class and following the principles and patterns of undergraduate education, this course's teaching elements are scientifically designed. Adhering to the principles of fostering virtues and talents and training students for military purposes, we implement the following two teaching philosophies: student-centered, and combine theory with practice. Focusing on the basic concepts, principles, and methods of AI, the course content is organized according to the “three horizontal and three vertical” structure, forming a system aligned with the development of the discipline. The primary teaching method is offline theoretical lectures, supplemented by online resources. Finally, the course assessment combines formative evaluations and summative evaluations.},
booktitle = {Proceedings of the 2024 3rd International Conference on Artificial Intelligence and Education},
pages = {256–261},
numpages = {6},
keywords = {course assessment, course content, course design, foundations of artificial intelligence, teaching methods, teaching philosophy},
location = {
},
series = {ICAIE '24}
}

@inproceedings{10.1145/3711403.3711424,
author = {Hu, Xuting and Xu, Yuanyuan and Liao, Han and Ma, Jiqiao},
title = {A Systematic Literature Review Study of Digital Transformation in Higher Education: Technologies, Challenges and Trends},
year = {2025},
isbn = {9798400717468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711403.3711424},
doi = {10.1145/3711403.3711424},
abstract = {The digital transformation of higher education is occurring rapidly around the globe, leaving an indelible mark on improving the quality of education, expanding the accessibility of educational resources, and realizing innovation and reform in teaching and research. The extant research on digital transformation in higher education is relatively limited in scope and has certain inherent limitations. Therefore, this paper employs the method of systematic literature review (SLR) to provide a comprehensive analysis of the digital transformation of higher education from three perspectives: digital technology, existing challenges, and future trends. This analysis aims to identify potential correlations between these three dimensions and offer valuable insights for those engaged in the education industry and researchers. Furthermore, this study seeks to deepen our understanding of digital transformation and to provide guidance for future research directions.},
booktitle = {Proceedings of the 2024 7th International Conference on Educational Technology Management},
pages = {146–153},
numpages = {8},
keywords = {Challenges, Digital Technology, Digital Transformation, Future Trend, Higher Education},
location = {
},
series = {ICETM '24}
}

@inproceedings{10.1145/3670013.3670044,
author = {Tien, Yun-Tzu and Chen, Rain},
title = {Challenges of Artificial Intelligence in Design Education},
year = {2024},
isbn = {9798400717062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670013.3670044},
doi = {10.1145/3670013.3670044},
abstract = {In recent years, the rapid development and the various applications of artificial intelligence technology has been gradually replacing many labor-intensive and creative tasks. Traditional design education in schools invests considerable time in cultivating students' basic skills in using techniques, tools, materials, etc., but overlooks the wave which has been brought by artificial intelligence. Artificial intelligence has surpassed human performance in specific fields and is poised to replace the work of many laborers. As artificial intelligence technology advances, its applications will permeate more areas, including thinking and creative abilities that humans have taken pride in. Design education in universities should re-examine the design values of human designers and artificial intelligence respectively. This redefinition of values implies that the abilities required of designers need to be reframed. Design education in universities should also explore how to integrate artificial intelligence-related technologies into current design curricula. New design courses must manifest the value of human designers. Furthermore, design programs offered by universities should begin planning training courses on collaborative design with artificial intelligence and establish relevant mechanisms for making artificial intelligence an effective tool to assist designers. In addition to redefining the value of designers and planning artificial intelligence-assisted design courses, design departments should monitor industry trends and gather year-by-year feedback to provide students with up-to-date career development advice. We should recognize the crisis that artificial intelligence presents to current design education and address the impact of this wave on design students. Artificial intelligence is progressing toward an irreversible trend, and the growth and transformation of designers will become a new challenge for future design education.},
booktitle = {Proceedings of the 2024 15th International Conference on E-Education, E-Business, E-Management and E-Learning},
pages = {123–126},
numpages = {4},
keywords = {Artificial Intelligence, Career Development, Collaborative Design, Design Education, Design Value},
location = {Fukuoka-shi, Japan},
series = {IC4E '24}
}

@article{10.1145/3727986,
author = {Jia, Kaiyue and Leung, Teresa H. M. and Cheung, Ngai Yan Irene and Li, Yixun and Yu, Junnan},
title = {Developing a Holistic AI Literacy Framework for Children},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3727986},
doi = {10.1145/3727986},
abstract = {The increasing prevalence of artificial intelligence (AI) in everyday life has intensified the emphasis on teaching AI literacy to children. However, there is no consensus on the specific knowledge and skills that constitute children’s AI literacy, resulting in varied AI learning materials for young people. We systematically searched for educational practices for children’s AI learning in both formal and informal settings and examined the AI learning content taught to children. Our findings led to the development of a holistic AI literacy framework for children, which contains three high-level dimensions and eight content areas of AI literacy: AI awareness (AI definition, AI application, and AI history), AI mechanics (AI input, learning procedure, and AI output), and AI impacts (AI implication and responsible practice). Theoretically, we contribute a research-based, comprehensive, and current framework for children’s AI literacy, advancing its conceptualization in early life stages. Practically, our framework can guide researchers and practitioners in promoting AI education for the next generation.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Educ.},
month = apr,
keywords = {AI literacy, AI education, Children, Framework development}
}

@inproceedings{10.1145/3613904.3642017,
author = {Chen, Si and Waller, James and Seita, Matthew and Vogler, Christian and Kushalnagar, Raja and Wang, Qi},
title = {Towards Co-Creating Access and Inclusion: A Group Autoethnography on a Hearing Individual's Journey Towards Effective Communication in Mixed-Hearing Ability Higher Education Settings},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642017},
doi = {10.1145/3613904.3642017},
abstract = {We present a group autoethnography detailing a hearing student’s journey in adopting communication technologies at a mixed-hearing ability summer research camp. Our study focuses on how this student, a research assistant with emerging American Sign Language (ASL) skills, (in)effectively communicates with deaf and hard-of-hearing (DHH) peers and faculty during the ten-week program. The DHH members also reflected on their communication with the hearing student. We depict scenarios and analyze the (in)effectiveness of how emerging technologies like live automatic speech recognition (ASR) and typing are utilized to facilitate communication. We outline communication strategies to engage everyone with diverse signing skills in conversations - directing visual attention, pause-for-attention-and-proceed, and back-channeling via expressive body. These strategies promote inclusive collaboration and leverage technology advancements. Furthermore, we delve into the factors that have motivated individuals to embrace more inclusive communication practices and provide design implications for accessible communication technologies within the mixed-hearing ability context.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {55},
numpages = {14},
keywords = {American Sign Language, DHH, Higher Education, Mixed-Ability},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3615430.3615433,
author = {Kourchev, Alex and Donaldson, Jonan P. and Woodward, Jay},
title = {Constructing Creativity: Exploring Conceptualizations and Their Implications for Constructionist Learning Environments},
year = {2024},
isbn = {9798400708961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615430.3615433},
doi = {10.1145/3615430.3615433},
abstract = {This study investigates students' conceptualizations of creativity within a constructionist learning environment and the potential implications for designing engaging learning experiences. We analyzed group project papers from three iterations of an undergraduate course on creativity and identified two distinct conceptualizations of creativity: Extraversion-Freedom and Playful-Adaptation. Each conceptualization is associated with different emergent practices. By understanding and incorporating these conceptualizations into learning environment design, educators can enhance student creativity, engagement, and learning outcomes. Additionally, our findings provide insights into developing nuanced learning experience design tools that align with various perspectives on creativity, fostering a deeper appreciation and understanding of the creative process in constructionist learning settings.},
booktitle = {Proceedings of FabLearn / Constructionism 2023: Full and Short Research Papers},
articleno = {13},
numpages = {3},
keywords = {Constructionist creativity, conceptualizations of creativity, creativity landscape, learning experience design},
location = {New York City, NY, USA},
series = {FLC '23}
}

@inproceedings{10.1145/3686081.3686129,
author = {Li, Ying and Liu, Jiaqi and Ji, Wei and Li, Wenqing},
title = {Research on the Ways to Improve College Teachers' Information Literacy in the Mobile Internet Era},
year = {2024},
isbn = {9798400718151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686081.3686129},
doi = {10.1145/3686081.3686129},
abstract = {In the context of the mobile Internet era, the information literacy of college teachers has continuously become the key to educational quality and innovative teaching. This article deeply explores the necessity and implementation paths for improving information literacy among college teachers in the mobile Internet environment. Through literature review and empirical research, this paper analyzes the current status of information technology application by college teachers and their technical needs in the teaching process. A comprehensive technical training framework is proposed, including the design and application of mobile learning platforms, interactive and cooperative learning tools, as well as the evaluation mechanisms and feedback systems necessary to ensure the continuous improvement of information literacy. The conclusion of this study highlights the role of information technology in improving teachers' teaching capabilities and student learning effects, while pointing out the limitations of the study and providing inspiration for future research directions. These findings provide an empirical basis for university administrators and education policymakers when formulating relevant training plans, and point out the direction for the professional development of university teachers in the mobile Internet era.},
booktitle = {Proceedings of the International Conference on Decision Science &amp; Management},
pages = {282–288},
numpages = {7},
keywords = {college teachers, educational technology, information literacy, mobile Internet},
location = {
},
series = {ICDSM '24}
}

@inproceedings{10.1145/3687311.3687390,
author = {Jiao, Ge and Jiang, Yingjie and Zeng, Lingcheng and Zeng, Lei and Zheng, Guangyong and Li, Kangman},
title = {Research on the cultivation system of information technology teachers' intelligent literacy based on AI-TPACK},
year = {2024},
isbn = {9798400709920},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3687311.3687390},
doi = {10.1145/3687311.3687390},
abstract = {With the rapid advancement of AI technology in the field of basic education, the concept, form, method, mode, and environment of education and teaching are undergoing significant changes. As a result, information technology teachers are confronted with the challenge of restructuring their subject knowledge and developing intelligent literacy. In the context of applying AI to basic education, intelligent literacy has become an essential professional skill for IT teachers. This paper proposes a framework for cultivating intelligent literacy among information technology teachers based on Artificial Intelligence Technological Pedagogical Content Knowledge (AI-TPACK) theory. The study aims to elucidate the meaning of AI-TPACK for information technology teachers, establish AI-TPACK models and practice paths for them, propose strategies for integrating subject teaching knowledge with artificial intelligence technology, and enhance cultivation and development plans for their intelligent literacy. The goal is to enhance the teaching abilities of information technology teachers so that they can acquire comprehensive artificial intelligence literacy in their work and better adapt to the new requirements and challenges presented by the era of AI.},
booktitle = {Proceedings of the 2024 International Conference on Intelligent Education and Computer Technology},
pages = {436–441},
numpages = {6},
location = {Guilin, China},
series = {IECT '24}
}

@article{10.5555/3717781.3717798,
author = {Dogan, Gulustan and Sahin, Elif and Wilkinson, Catherine Fay and Moody, Amelia K. and Song, Yang},
title = {BlueAI: Designing Artificial Intelligence for Environment Science and Climate Change Learning Experiences for K12 Students},
year = {2024},
issue_date = {November 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {40},
number = {5},
issn = {1937-4771},
abstract = {The subject of teaching artificial intelligence (AI) in K-12 settings is rapidly expanding and will significantly affect computer education. While AI is currently a required part of computing curricula at universities, there are unique challenges in incorporating AI into K-12 education. The goal of BlueAI is to prepare K-12 educators to use game-based lessons to teach computational thinking, AI, and computer science skills that will interest children while incorporating important environmental and marine science subjects. We conducted assessments where we taught lessons at two different schools, and presented our findings.},
journal = {J. Comput. Sci. Coll.},
month = nov,
pages = {127–137},
numpages = {11}
}

@inproceedings{10.1145/3653666.3656081,
author = {Quiterio, Ashley and Smith, Michael},
title = {Data Informed Learning for Equity: Empowering Volunteer Coaches in Youth Basketball Leagues},
year = {2024},
isbn = {9798400706264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653666.3656081},
doi = {10.1145/3653666.3656081},
abstract = {We rely on Data Informed Learning (DIL), a framework that combines literacy theories with an emphasis on data from disciplinary contexts, to explore design implementations toward enhancing data literacy practices among volunteer coaches in a youth basketball league. Based on initial survey data, we discuss findings that illustrate the coaches' relationships with data through their values, current practices, and wants, and how their learning goals align with what DIL can offer. This work-in-progress offers potential implications for broadening access to learning with data for underrepresented communities in novel informal ways and extending the Data Informed Learning framework beyond higher education.},
booktitle = {Proceedings of the 2024 on RESPECT Annual Conference},
pages = {139–142},
numpages = {4},
keywords = {coaching, data informed learning, hobbies, sports},
location = {Atlanta, GA, USA},
series = {RESPECT 2024}
}

@inproceedings{10.1145/3641554.3701903,
author = {Henrique, Brendan},
title = {Exploring Critical CS Teacher Education Program Design Through a Science and Technology Studies Approach},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701903},
doi = {10.1145/3641554.3701903},
abstract = {Recent calls to reimagine K-12 computer science teacher education have proposed critical visions of pre-service teacher education built on equity, justice, and ethics. Building on these calls, this position paper demonstrates how critical perspectives can be incorporated into K-12 CS teacher education programs by integrating texts from Science and Technology studies. Central to this approach is allowing teachers to gain proficiency in CS content while navigating the tensions of the transformative and oppressive nature of technology as they imagine their future pedagogy. By integrating the often separated strands of pedagogy, content, and justice, pre-service teachers were empowered to reimagine what computer science would look like for their students. This paper concludes by discussing how a Science and Technology Studies approach could be used for entire course sequences in CS teacher preparation programs and preliminary feedback from teachers in the first iteration of this curriculum.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {478–484},
numpages = {7},
keywords = {pre-service, science and technology studies, teacher education},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3675888.3676118,
author = {Ullas, Gautham S and S, Krishna},
title = {XAI aided Comparative Analysis of X-ray and CT Scans},
year = {2024},
isbn = {9798400709722},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675888.3676118},
doi = {10.1145/3675888.3676118},
abstract = {Early diagnosis and treatment of pneumonia are critical, and they have a substantial influence on patient outcomes. We provide a thorough comparison of pneumonia diagnosis models using both conventional X-ray imaging and modern CT scans in this research study. In this work, we examine how well deep learning (DL) models do in accurately identifying pneumonia using both imaging modalities. Furthermore, we investigate the application of Explainable Artificial Intelligence (XAI) methodologies to better understand these models’ decision-making procedures and establish confidence in their projections. The research utilizes a mix of dataset that includes CT and X-ray pictures of pneumonia patients that were obtained from several web sources. To determine how well deep learning models for each imaging modality identify pneumonia, we train and test them. We employ XAI approaches to clarify decision-making processes within the models, offering insights into the characteristics that influence the classification results. Our results provide insight into the relative effectiveness of pneumonia detection models in CT and X-ray scans through thorough testing and assessment. Additionally, the combination of XAI and Pneumonia detection model approach shows encouraging progress in improving the robustness and interpretability of models, opening the door for more dependable and beneficial diagnostic systems in the field of medical imaging.},
booktitle = {Proceedings of the 2024 Sixteenth International Conference on Contemporary Computing},
pages = {596–600},
numpages = {5},
keywords = {Deep learning, Disease detection, Medical imaging, XAI},
location = {Noida, India},
series = {IC3-2024}
}

@inproceedings{10.1145/3563767.3568127,
author = {Nose, Junya and Cong, Youyou and Masuhara, Hidehiko},
title = {Mio: A Block-Based Environment for Program Design},
year = {2022},
isbn = {9781450399005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563767.3568127},
doi = {10.1145/3563767.3568127},
abstract = {Program design should be taught with a comprehensible guideline and  
appropriate tool support. 
While Felleisen et al.'s program design recipe serves as a good guideline  
for novice learners, no existing tool provides sufficient support for  
step-by-step design. 
We propose Mio, an environment for designing programs based on the design 
recipe. 
In Mio, the programmer uses blocks to express design artifacts, such as  
examples of input and output data. 
The system checks the consistency of the design, gives feedback to the  
programmer, and produces a half-completed program for use in steps after  
designing. 
A preliminary experiment in the classroom showed its ability to make  
program design easier for novices, and to encourage programmers to follow  
the design recipe. 
In this paper, we demonstrate the core features of Mio, report the results  
of the experiment, and discuss our plans for extensions.},
booktitle = {Proceedings of the 2022 ACM SIGPLAN International Symposium on SPLASH-E},
pages = {62–69},
numpages = {8},
keywords = {block-based programming, pedagogic programming environment, program design recipe},
location = {Auckland, New Zealand},
series = {SPLASH-E 2022}
}

@inproceedings{10.1145/3593743.3593781,
author = {Ylipulli, Johanna and H\"{a}m\"{a}l\"{a}inen, Jo\~{a}o},
title = {Towards Practice-oriented Framework for Digital Inequality in Smart Cities},
year = {2023},
isbn = {9798400707582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593743.3593781},
doi = {10.1145/3593743.3593781},
abstract = {Digital equality, entailing access to the Internet and digital devices as well as a set of diverse digital skills, has been recognized as a crucial factor in the creation of just and sustainable smart cities. However, there remains gaps between smart city research and studies focusing on socio-digital inequalities and digital literacy. The aim of this article is two-fold: first, it explores the focal intersections of smart city development, socio-digital inequalities and digital literacy based on literature. Secondly, through an interdisciplinary exploration it aims to demonstrate that there is a need to align the current understanding of digital inequality with the conceptualizations of ‘smart citizenship’ which underline competencies related to critical thinking and resistance. For this purpose, we formulate a tentative, practice-oriented framework for understanding digital inequality in the context of smart cities. The framework is based on previous prominent definitions of digital inequality and digital literacy, but it intends to open new horizons for solutions and services that would increase peoples’ awareness on societal dimensions of digital technologies, and eventually, their digital agency. For the most part, the paper is theoretical, but it also draws examples from the research fieldwork conducted in Barcelona, Spain. Barcelona has been highlighted as one of the rare examples of a city that intends to center people in its smart city policies: We explore how Barcelona approaches the people-centric smart city in practice and introduce Can\`{o}drom, a center for city inhabitants promoting open technologies, participatory democracy, and digital culture.},
booktitle = {Proceedings of the 11th International Conference on Communities and Technologies},
pages = {180–190},
numpages = {11},
keywords = {Digital divide, Digital inequality, Digital literacy, Interdisciplinarity, Smart city, Socio-Digital inequalities, Transdisciplinarity},
location = {Lahti, Finland},
series = {C&amp;T '23}
}

@inproceedings{10.1145/3534678.3539276,
author = {Peng, Shuai and Fu, Di and Cao, Yong and Liang, Yijun and Xu, Gu and Gao, Liangcai and Tang, Zhi},
title = {Compute Like Humans: Interpretable Step-by-step Symbolic Computation with Deep Neural Network},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539276},
doi = {10.1145/3534678.3539276},
abstract = {Neural network capability in symbolic computation has emerged in much recent work. However, symbolic computation is always treated as an end-to-end blackbox prediction task, where human-like symbolic deductive logic is missing. In this paper, we argue that any complex symbolic computation can be broken down to a sequence of finite Fundamental Computation Transformations (FCT), which are grounded as certain mathematical expression computation transformations. The entire computation sequence represents a full human understandable symbolic deduction process. Instead of studying on different end-to-end neural network applications, this paper focuses on approximating FCT which further build up symbolic deductive logic. To better mimic symbolic computations with math expression transformations, we propose a novel tree representation learning architecture GATE (Graph Aggregation Transformer Encoder) for math expressions. We generate a large-scale math expression transformation dataset for training purpose and collect a real-world dataset for validation. Experiments demonstrate the feasibility of producing step-by-step human-like symbolic deduction sequences with the proposed approach, which outperforms other neural network approaches and heuristic approaches.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1348–1357},
numpages = {10},
keywords = {deep neural networks, math formulae, symbolic mathematics, tree representation},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3605468.3609784,
author = {Goode, Joanna},
title = {Putting the Pro in Professional Learning for Computing Teachers: Empowering Educators to Design for Equity},
year = {2023},
isbn = {9798400708510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605468.3609784},
doi = {10.1145/3605468.3609784},
abstract = {This keynote lecture will present research on the influential role of teachers in designing, facilitating, and advocating for equitable learning experiences in computing classrooms. The talk will consider how teachers gain knowledge, skills, and a sense of agency to address inequities in computing learning experiences. Specifically, the discussion will illustrate three distinct ways that teacher leaders can serve as powerful agents of change: as effective facilitators for other teachers’ learning experiences about inclusive computing, as curriculum co-designers of justice-oriented computing curriculum, and as advocates for equitable school policies.},
booktitle = {Proceedings of the 18th WiPSCE Conference on Primary and Secondary Computing Education Research},
articleno = {2},
numpages = {3},
keywords = {Exploring Computer Science, Social justice education, Teacher agency, Teacher leadership},
location = {Cambridge, United Kingdom},
series = {WiPSCE '23}
}

@inproceedings{10.1145/3641554.3701896,
author = {Guzdial, Mark and Nelson-Fromm, Tamara},
title = {Designing Courses for Liberal Arts and Sciences Students Contextualized around Creative Expression and Social Justice},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701896},
doi = {10.1145/3641554.3701896},
abstract = {The goal of teaching everyone computing (explicitly including programming) predates the definition of the computer science (CS) major and even the prospect of a software development career. At the University of Michigan, we are creating courses for non-CS majors which are grounded in the computational practices of liberal arts and sciences faculty. These courses have no connection to the CS major curriculum or software development jobs. We focus here on two of the themes that those faculty valued (Computing for Expression and Computing for Justice) and the introductory courses that we designed around each theme. The courses emphasize gaining broad perspectives of computing, which serve the study of multiple disciplines. Student activities include readings, writing essays, classroom discussion, and open-ended programming homework assignments. This experience report describes our design process, the Creative Expression and Social Justice courses, and an initial evaluation of our design. Most of the programming assignments were written in the block-based programming language Snap!, with some in-class exercises using teaspoon languages. Several units ended with an ebook assignment to connect the Snap! programming to equivalent programs in Python, Processing, and SQL. Interview and survey findings suggest that students found this sequence and the courses useful, despite not counting toward a CS major or focusing on early software development skills. Students described usefulness in terms of developing general computing knowledge, preparation for a range of future careers, and introducing them to other course choices.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {423–429},
numpages = {7},
keywords = {computational literacy, computational science, computational thinking, critical computing, cs for all, digital humanities, liberal arts and sciences},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.1145/3701622,
author = {Green, Karen},
title = {In an Interdisciplinary World, Computer Science Education Must Adapt},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {2153-2184},
url = {https://doi.org/10.1145/3701622},
doi = {10.1145/3701622},
journal = {ACM Inroads},
month = nov,
pages = {67–73},
numpages = {7}
}

@inproceedings{10.1145/3687311.3687339,
author = {Yang, Xin and Zhao, Fengjuan},
title = {Integrating AI with Pedagogies: Drama, Multimodal and the Production-oriented Approach- a Study Based on the 6th SFLEP Intercultural Competence Contest},
year = {2024},
isbn = {9798400709920},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3687311.3687339},
doi = {10.1145/3687311.3687339},
abstract = {Cultural studies have garnered significant attention in China for many decades, and the cultivation of intercultural competence within the academic sphere has been meticulously developed, particularly within the university context. Although intercultural competence is inherently multidisciplinary, its cultivation is predominantly integrated into the pedagogy of foreign language instruction. The discourse surrounding intercultural communication is mainly led by educators and scholars in the field of foreign language studies. The SFLEP Intercultural Competence Contest comprises three pivotal tasks: the development of intercultural case studies, scenario analysis, and the narration of Chinese stories. The rapid development of AI has opened new avenues in the field of education, particularly in language learning. The integration of AI with traditional pedagogies like drama, multimodal learning, and the production-oriented approach has been observed to enrich the learning experience and improve intercultural competence. A thorough examination of the 6th iteration of the contest provides the foundation for this exploration. The study not only highlights the potential of AI integrated approaches but also underscores their significant relevance in enhancing scaffolding techniques in foreign language education.},
booktitle = {Proceedings of the 2024 International Conference on Intelligent Education and Computer Technology},
pages = {151–157},
numpages = {7},
location = {Guilin, China},
series = {IECT '24}
}

@article{10.1145/3623272,
author = {Wong, Gary K. W. and Huen, John H. M.},
title = {Can Blockchain Technology Bring any Value to Education?},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
issn = {2153-2184},
url = {https://doi.org/10.1145/3623272},
doi = {10.1145/3623272},
journal = {ACM Inroads},
month = nov,
pages = {73–77},
numpages = {5}
}

@inproceedings{10.1145/3641554.3701887,
author = {George, Kari L.},
title = {Supporting Inclusive Computing: A Graduate Course to Prepare Future Faculty},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701887},
doi = {10.1145/3641554.3701887},
abstract = {Creating inclusive learning environments is important for retaining students, particularly those from historically marginalized groups in computing. However, both faculty and graduate students -- who are responsible for teaching and mentoring activities - often receive little, if any, training on creating inclusive learning environments. Using a higher education approach, I developed and taught a semester-long course to prepare future faculty members to create inclusive computing learning environments. This experience report describes the process for creating the course including topics, readings, assignments, and approaches for facilitation and learning. I also provide analysis, reflection, and insights from students' surveys and their written reflections throughout the course. The primary contribution of this experience report is to document the learning gains in graduate students' self-efficacy, awareness, and preparation for faculty roles, and to share insights on the implementation of the course to serve as a guide for other departments in the development and execution of similar training approaches.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {374–380},
numpages = {7},
keywords = {broadening participation in computing, faculty development, graduate education},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.5555/3636517.3636520,
author = {Khadivi, Pejman},
title = {AI and Society: Teaching AI to Non-STEM Students},
year = {2023},
issue_date = {October 2023},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {1},
issn = {1937-4771},
abstract = {Artificial Intelligence (AI) is a booming technology with a broad range of applications in almost any aspect of life, technology, science, and society. However, despite its popularity, AI education to non-STEM students is not addressed properly. In this paper, we introduce a new AI course, named AI and Society, that has been designed for non-STEM students with very limited knowledge about math and computer science. In this course, the ultimate goal is to review fundamental concepts of AI and study the application of AI in major global issues such as healthcare, sustainability, transportation, and digital security. Furthermore, the course covers ethical issues in AI, and discuss different ethical aspects of AI systems such as algorithm and technology bias, accountability, and safety.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {18–27},
numpages = {10}
}

@inproceedings{10.1145/3641554.3701868,
author = {Szabo, Claudia and Parker, Miranda C. and Friend, Michelle and Jeuring, Johan and Kohn, Tobias and Malmi, Lauri and Sheard, Judithe},
title = {Models of Mastery Learning for Computing Education},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701868},
doi = {10.1145/3641554.3701868},
abstract = {The application of mastery learning, where students progress through their learning in a self-paced manner until they have mastered specific concepts, is considered appealing for teaching introductory programming courses. Despite its growing popularity in computing and its extensive use in other disciplines, there is no overview of the design of courses that use mastery learning. In this position paper, we present an overview of five mastery learning models and discuss examples of how these can be applied in practice, both in foundational programming as well as more advanced courses. Our analysis focuses on the student progression through the course, the assessment structure, and the support for self-paced learning, including for struggling students. This work provides a greater understanding of mastery learning and its application in a computing education context.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1092–1098},
numpages = {7},
keywords = {competency-based learning, mastery learning, models of instruction},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@inproceedings{10.1145/3632620.3671113,
author = {Tran, Keith and Bacher, John and Shi, Yang and Skripchuk, James and Price, Thomas},
title = {Overcoming Barriers in Scaling Computing Education Research Programming Tools: A Developer's Perspective},
year = {2024},
isbn = {9798400704758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632620.3671113},
doi = {10.1145/3632620.3671113},
abstract = {Background and Context. Research software in the Computing Education Research (CER) domain frequently encounters issues with scalability and sustained adoption, which limits its educational impact. Despite the development of numerous CER programming (CER-P) tools designed to enhance learning and instruction, many fail to see widespread use or remain relevant over time. Previous research has primarily examined the challenges educators face in adopting and reusing CER tools, with few focusing on understanding the barriers to scaling and adoption practices from the tool developers’ perspective. Objectives. To address this, we conducted semi-structured interviews with 16 tool developers within the computing education community, focusing on the challenges they encounter and the practices they employ in scaling their CER-P tools. Method. Our study employs thematic analysis of the semi-structured interviews conducted with developers of CER-P tools. Findings. Our analysis revealed several barriers to scaling highlighted by participants, including funding issues, maintenance burdens, and the challenge of ensuring tool interoperability for a broader user base. Despite these challenges, developers shared various practices and strategies that facilitated some degree of success in scaling their tools. These strategies include the development of teaching materials and units of curriculum, active marketing within the academic community, and the adoption of flexible design principles to facilitate easier adaptation and use by educators and students. Implications. Our findings lay the foundation for further discussion on potential community action initiatives, such as the repository of CS tools and the community of tool developers, to allow educators to discover and integrate tools more easily in their classrooms and support tool developers by exchanging design practices to build high-quality education tools. Furthermore, our study suggests the potential benefits of exploring alternative funding models.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1},
pages = {312–325},
numpages = {14},
keywords = {computing education tools, scaling research tools, tool developer},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}

@inproceedings{10.1145/3628516.3659395,
author = {Greenwald, Eric and Krakowski, Ari and Hurt, Timothy and Grindstaff, Kelly and Wang, Ning},
title = {It's like I'm the AI: Youth Sensemaking About AI through Metacognitive Embodiment},
year = {2024},
isbn = {9798400704420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628516.3659395},
doi = {10.1145/3628516.3659395},
abstract = {The increasing presence and importance of Artificial Intelligence (AI) in our society has led to calls for its inclusion at all levels of education. However, the field is only beginning to understand what how AI learning experiences may be designed to be effective and developmentally appropriate, especially for young children. One challenge children encounter is in conceptualizing the “intelligence” of AI while they are still developing a metacognitive model of their own human intelligence. To investigate potential ways to address this, we developed a strategy, metacognitive embodiment, through which children are supported to (a) elicit a mental model of their own intelligent performance on a task and (b) compare that elicited model to an AI designed to accomplish the same task. From this study we found evidence suggesting that engaging children in metacognitive tasks in coordination with AI learning experiences (where the AI performs an analogous task) better positioned them for sensemaking about the AI’s intelligence.},
booktitle = {Proceedings of the 23rd Annual ACM Interaction Design and Children Conference},
pages = {789–793},
numpages = {5},
keywords = {AI literacy, K-5 education, embodied interaction, informal learning},
location = {Delft, Netherlands},
series = {IDC '24}
}

@article{10.1145/3680288,
author = {Martins, Ramon Mayor and Von Wangenheim, Christiane G. and Rauber, Marcelo F. and Borgatto, Adriano F. and Hauck, Jean C. R.},
title = {Exploring the Relationship between Learning of Machine Learning Concepts and Socioeconomic Status Background among Middle and High School Students: A Comparative Analysis},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
url = {https://doi.org/10.1145/3680288},
doi = {10.1145/3680288},
abstract = {As Machine Learning (ML) becomes increasingly integrated into our daily lives, it is essential to teach ML to young people from an early age including also students from a low socioeconomic status (SES) background. Yet, despite emerging initiatives for ML instruction in K-12, there is limited information available on the learning of students from a low SES background. To address this gap, our study conducted an analysis of comparing the students’ performance assessment scores of ML concepts as a result of the ML4ALL! course among 266 middle and high school students from different socioeconomic backgrounds. The results demonstrated an understanding of ML concepts among students from all SES backgrounds. Although some differences were observed regarding specific parts of the ML development process, these were not substantial enough to identify SES as a determining factor affecting the performance assessment score. Also, when considering the background together with other demographic factors such as sex assigned at birth or educational stage, no significant difference of the students’ performance assessment scores was observed. These findings provide a first indication that a low SES background must not be a barrier to ML competencies and that effective and inclusive ML teaching strategies can ensure equitable access to ML education across diverse socioeconomic backgrounds.},
journal = {ACM Trans. Comput. Educ.},
month = sep,
articleno = {41},
numpages = {31},
keywords = {Learning, machine learning, socioeconomic status, middle school, high school}
}

@inproceedings{10.1145/3723010.3723033,
author = {B\"{o}ttcher, Axel and Thurner, Veronika},
title = {Identifying Competence Gaps and Student Struggle by Monitoring Testing Performance},
year = {2025},
isbn = {9798400712821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3723010.3723033},
doi = {10.1145/3723010.3723033},
abstract = {Creating sufficient motivation for unit testing is often a challenge in software engineering education. As well, it is often difficult for students to decide whether they have written sufficient tests. Classical coverage measures require thorough explanation and thus introduce great complexity, which risks to overwhelm programming novices. Instead, we started using mutation testing early on in programming courses, as mutations are an easy concept to understand. In addition, integrating mutation testing into projects is nowadays straightforward. Furthermore, hunting mutations can create a sense of gaming challenge, and thus, fun for novice programmers.In this paper, we describe experiences with mutation testing in the first and second semester of an introductory course on software development in a Bachelor’s degree program on Computer Science. In order to critically evaluate our approach, we analyze in detail the meta data on testing activities generated in the students’ repositories. Specifically, we analyze their work performance and identify typical performance patterns in terms of programming activities over time, as well as in terms of improvement of code quality during this process. Furthermore, we correlate these performance data to the exam performance.Finally, we classify leftover mutants in terms of skill levels according to Bloom’s revised taxonomy of learning objectives. This provides an insight into students’ abilities, and identifies where we need to adapt our teaching and exercises in the next iteration of teaching these courses, to better support our students along their learning path.},
booktitle = {Proceedings of the 6th European Conference on Software Engineering Education},
pages = {221–230},
numpages = {10},
keywords = {CS education, struggle detection, software testing, mutation testing, programming education},
location = {
},
series = {ECSEE '25}
}

@inproceedings{10.1145/3641554.3701898,
author = {Moudgalya, Sukanya Kannan and Palileo, Carmen and Patil, Srinayana and Linder, Rhema and Swaminathan, Sai},
title = {Rooted in the Collective: A Culturally Situated Artificial Intelligence (AI) Education Workshop For Urban Farmers},
year = {2025},
isbn = {9798400705311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641554.3701898},
doi = {10.1145/3641554.3701898},
abstract = {This paper describes our experiences related to a culturally situated Artificial Intelligence (AI) education workshop for urban farmers. The farmers explored the concept of AI and its implications in the context of their own farm. They engaged in hands-on activities, including using traditional sensemaking practices in conjunction with sensor technology to collect contextual data. They then used a tangible educational tool: a corkboard with push pins and images to build tactile AI models based on their farm data. Throughout this process, they discussed their hopes and desires regarding AI in farming, and their concerns about AI technologies. The perspectives of the urban farmers reveal their preference for AI systems that are contextual, integrate community values, Indigenous knowledge, and environmental concerns, and are rooted in community ownership of data. Our report provides a starting point for conducting future workshops that involve 'critical participatory design' of AI technologies to promote AI literacies rooted in the community.},
booktitle = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1},
pages = {791–797},
numpages = {7},
keywords = {community-centered ai, ethical ai, informal ai education},
location = {Pittsburgh, PA, USA},
series = {SIGCSETS 2025}
}

@article{10.5555/3637068.3637079,
author = {von Briesen, Elizabeth},
title = {Do We Need to Write? Researching Perceptions of Disciplinary Writing Importance and Skills in an Advanced Computer Science Course},
year = {2023},
issue_date = {November 2023},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {5},
issn = {1937-4771},
abstract = {Our research explores perceptions related to writing in the computer science discipline. It is a common misconception that this skill is not important in the field, and we are motivated to dispel that notion and assist students in gaining experience and confidence in their disciplinary writing skills. To that end, we surveyed undergraduate students at the start and end of term in our Artificial Intelligence course, an advanced computer science elective. Students wrote two blogs-like items, one each for audiences with and without technical knowledge of the field, and also produced technical documentation related to two programming assignments. We found that on average, students agreed that writing in the discipline is important, and that they have some confidence in their writing abilities across audiences. While we did not find a statistically significant difference between perceptions at the start and end of the term, our overall results and open-ended feedback indicate that students find writing in the field to be important, and that there is strong interest in further curricular enhancements in this area.},
journal = {J. Comput. Sci. Coll.},
month = nov,
pages = {119–128},
numpages = {10}
}

@inproceedings{10.1145/3613905.3650973,
author = {Cowit, Noah and Yu, Junnan},
title = {Supporting Physically Active CS-Ed for Children: Exploring the Design of Physical Play Friendly Coding Blocks},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650973},
doi = {10.1145/3613905.3650973},
abstract = {In this work, we share our design exploration of coding blocks to incorporate physical play into programming kits for children's computing education. First, we cover the tradition of experiential learning in computing education, with descriptions of where physical play fits into that practice. Next, we describe children's programming workshops to explore how physical play could be incorporated into coding kits. From these workshops, we recommend a set of coding blocks for physical play divided into four categories: (1) motion sensing, (2) sound sensing, (3) proximity sensing, and (4) gameplay information. These coding blocks for the first time systematically present physical play friendly programming commands, supplementing previous works on developing coding tools to combine physical play and coding for children. Finally, we describe our implementation of these coding blocks on the micro:bit—a low-cost widely distributed computer science educational kit—and describe the results of a functionality test with nine graduate design students.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {308},
numpages = {7},
keywords = {Children, Coding blocks, Computational learning, Physical play},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{10.1145/3649165.3690130,
author = {Zahn, Matthew and Heckman, Sarah and Battestilli, Lina},
title = {Investigating Students' Perspectives on the Value of Help-Seeking Resources in CS Education},
year = {2024},
isbn = {9798400705984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649165.3690130},
doi = {10.1145/3649165.3690130},
abstract = {The accessibility and effectiveness of help-seeking resources plays a pivotal role in contributing to the success of students in Computer Science courses. However, students do not always choose to utilize these resources, and when they do, their experiences can vary. While some students commend help-seeking resources for effectively providing clarification on assignment instructions, debugging code, and addressing questions about course concepts, others share instances where their problems were not resolved, or, in some cases, they did not receive any meaningful guidance from these resources. In this study, we examine the experiences of students enrolled in a CS2 course, all of whom had access to the course's help-seeking resources. These experiences were gathered through qualitative interviews at three time points within a semester. Our findings, derived from emergent coding, reveal thematic patterns in student encounters with help-seeking resources and contribute to a broader theme regarding help-seeking resource utilization at different phases of the semester. The findings of this investigation contribute to the wider conversation on student success and help-seeking resource utilization in Computer Science education.},
booktitle = {Proceedings of the 2024 on ACM Virtual Global Computing Education Conference V. 1},
pages = {256–262},
numpages = {7},
keywords = {academic help, cs2 course, help resources, office hours, online forums},
location = {Virtual Event, NC, USA},
series = {SIGCSE Virtual 2024}
}

@article{10.5555/3665609.3665611,
author = {Earth, Steve and Johnson, Jeremy and Char, Bruce},
title = {Programming Skills as a Gateway to Proof Writing Proficiency},
year = {2024},
issue_date = {April 2024},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {39},
number = {8},
issn = {1937-4771},
abstract = {The intersection of programming and proof writing skills in computer science education is a relatively unexplored area. This paper presents the beginning of a longitudinal study that explores this intersection by analyzing student programmers' solutions to logic puzzles at the beginning and end of an intermediate computer science (CS) course. These puzzles, requiring skills akin to proof writing but without the need for advanced mathematical knowledge, serve as a tool to evaluate the development of proof writing skills. We examine the correlation between students' puzzle-solving capabilities and their academic performance in the course, controlling for other variables such as prior mathematics courses and GPA. This study aims to bridge the gap in understanding how programming education contributes to the development of proof writing abilities.},
journal = {J. Comput. Sci. Coll.},
month = apr,
pages = {27–38},
numpages = {12}
}

@inproceedings{10.1145/3664934.3664949,
author = {Chu, Haifeng and Liu, Yangzi},
title = {Research on Reforming the Training Model of Master's Talents in Design in Ethnic Regions of Universities Based on Artificial Intelligence},
year = {2024},
isbn = {9798400716409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664934.3664949},
doi = {10.1145/3664934.3664949},
abstract = {Anchored in the nourishment of innovative metamorphic competencies, predicated upon the pillars of 'Ethnic Culture,' this inquiry ventures forth to unveil pedagogic renovation stratagems for postgraduate design curricula at scholastic institutions situated within ethnic territories, concentrating particularly on the fortification afforded by artificial intelligence. Harnessing the wealth of ethnic cultural studies, this examination entails a pronounced commitment to the synergy of cross-disciplinary amalgamation, under the auspices of artificial intelligence. This initiative contemplates the utilization of AI to orchestrate convergent teams of graduate mentors, the establishment of cooperative educational consortia bridging the scholastic world with industry via AI, the formulation of idiosyncratic pedagogic structures predicated on artificial intelligence, the deployment of AI to enhance capacities in 'Innovative Design' and 'Practical Transformation' within the ambiance of ethnic culture, and the institution of an AI-driven bespoke teaching quality assurance protocol. Through this septenary spectrum of pedagogic reformation exploration, the study aspires to efficaciously catalyze the academic evolution of design disciplines.},
booktitle = {Proceedings of the 2024 9th International Conference on Information and Education Innovations},
pages = {57–62},
numpages = {6},
keywords = {Artificial Intelligence, Ethnic Culture, Master's Program in Design, Training Models},
location = {Verbania, Italy},
series = {ICIEI '24}
}

@inproceedings{10.1145/3615335.3623037,
author = {Trim, Michelle and Gulley, Paige},
title = {Imagining, Generating, and Creating Communication as Feminist Pedagogical Method for Teaching Computing Ethics},
year = {2023},
isbn = {9798400703362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615335.3623037},
doi = {10.1145/3615335.3623037},
abstract = {Our Masters-level computer science ethics course is often students' first, and sometimes only, exposure to ethical education situated within a sociocultural theoretical and feminist pedagogical framework. The Ethical Code assignment for this class typifies a method for teaching 'ethics' in this context, and for including aspects of DEI (Diversity, Equity and Inclusion) training that is largely missing from most students' computer science educational experiences. This experience report details lessons learned across two iterations of this ethical code assignment, from two different course sections. This report also provides critical reflections on the effectiveness of using informal electronic and more formal technical writing assignments to scaffold students’ understanding of power, racism, technostructural-based oppression, decolonial problem solving, and accessibility in the context of computing research.},
booktitle = {Proceedings of the 41st ACM International Conference on Design of Communication},
pages = {206–209},
numpages = {4},
keywords = {Ethics education, Feminist pedagogy in STEM, Technical communication},
location = {Orlando, FL, USA},
series = {SIGDOC '23}
}

@inproceedings{10.1145/3613904.3642539,
author = {Bilstrup, Karl-Emil Kj\ae{}r and Kaspersen, Magnus H\o{}holt and Bouvin, Niels Olof and Petersen, Marianne Graves},
title = {ml-machine.org: Infrastructuring a Research Product to Disseminate AI Literacy in Education},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642539},
doi = {10.1145/3613904.3642539},
abstract = {ml-machine.org is a web- and micro:bit-based educational tool for building machine learning models designed to enable more widespread teaching of AI literacy in secondary education. It has been designed as a research product in collaboration with partners from the educational sector, including the Danish Broadcasting Corporation and the Micro:bit Educational Foundation. ml-machine.org currently has more than 5000 unique users and is used in schools and teacher training. It is publicly available and promoted on the broadcasting corporation’s platforms. We describe the two-year process of developing and disseminating ml-machine.org. Based on interviews with partners and educators, we report on how ml-machine.org supports inquiry into the adoption and appropriation of such educational tools. We also provide insights on working with formal education infrastructures in order to scale and integrate a research product into teacher practices. Based on these experiences, we propose infrastructure as a novel quality of research products.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {272},
numpages = {16},
keywords = {AI Literacy, Participatory Infrastructuring, Research Product},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3613905.3637106,
author = {Cai, Alice and Baradari, Aida and Baradari, Dunya and Chiaravalloti, Treyden and Paschall, Courtnie},
title = {Augmentation Residency: An Immersive Live-Work HCI R&amp;D Model},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3637106},
doi = {10.1145/3613905.3637106},
abstract = {The growing affordability of emerging consumer devices and accessibility of software-hardware prototyping represents an opportunity for a new era of personal fabrication of augmentation technologies. In this case study, we review the inaugural Augmentation Residency, a live-in creative technology residency that implemented a new approach to the collaborative development of human-computer interaction (HCI) systems. Over the course of ten weeks, eight students and young professionals from a wide range of disciplines lived and worked together to build and self-experiment with technologies that integrate extended reality (XR), wearables, brain-computer interfaces (BCI), and artificial intelligence (AI). Made possible by a residential community and interdisciplinary teams, this high-intensity model for HCI research and development (R&amp;D) was structured to encourage experimentation with atypical human-centered design methods and artistic practices. We present our program design, organization, and management, and discuss challenges and suggestions for improvements, to inspire future residencies as vehicles for intensive HCI innovation.},
booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {504},
numpages = {9},
location = {Honolulu, HI, USA},
series = {CHI EA '24}
}

@inproceedings{10.1145/3715340.3715441,
author = {Kuiter, Elias and Th\"{u}m, Thomas and Kehrer, Timo},
title = {Teach Variability! A Modern University Course on Software Product Lines},
year = {2025},
isbn = {9798400714412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715340.3715441},
doi = {10.1145/3715340.3715441},
abstract = {Teaching software product lines to university students is key in disseminating knowledge about software variability. In particular, education is needed to train new researchers and practitioners and, thus, sustain further research on software product lines. However, preparing appropriate teaching material is difficult and time-consuming, even when relying on existing literature. Thus, clone-and-own is a common practice among educators, with all its associated issues. Moreover, there is a lack of full-semester, open courses on software product lines. In this paper, we report on our experience of architecting and designing such a course from scratch, avoiding clone-and-own entirely. In addition, we perform a literature review of influential books on software product lines and which topics they cover. We position our course in terms of these topics, discuss how it compares to existing courses, and justify relevant design decisions. With our course, we aim to strengthen the positive interactions between research, industry, and education. So far, our course has already been held seven times across five universities. A preliminary evaluation of our course indicates that our course is mostly well-received by students.},
booktitle = {Proceedings of the 19th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {1–10},
numpages = {10},
keywords = {open educational resources, software product lines},
location = {
},
series = {VaMoS '25}
}

@inproceedings{10.1145/3626252.3630867,
author = {Wang, Kevin and Lawrence, Ramon},
title = {HelpMe: Student Help Seeking using Office Hours and Email},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630867},
doi = {10.1145/3626252.3630867},
abstract = {Office hours and help sessions provide students with the important opportunity to obtain feedback and guidance while connecting with instructors and peers. However, these out of class help sessions are often underutilized due to problems such as inconvenient times and locations, long wait times, and student misconceptions on their purpose and value. Managing office hours for large classes is difficult for instructors and may result in poor student participation. It is crucial to adopt approaches to manage help sessions more effectively and encourage student attendance. This research examines current problems with help sessions and implements an office hours management system called HelpMe. Interactions in office hours and emails are analyzed to determine the types of questions asked. Student surveys demonstrate a significant change in student perception of office hours, increased engagement, and valuable data on effective practices for deploying office hours.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1388–1394},
numpages = {7},
keywords = {email, help seeking, office hours, student engagement},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3649217.3653583,
author = {Wu, Zihan and Smith, David H.},
title = {Evaluating Micro Parsons Problems as Exam Questions},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653583},
doi = {10.1145/3649217.3653583},
abstract = {Parsons problems are a type of programming activity that present learners with blocks of existing code and requiring them to arrange those blocks to form a program rather than write the code from scratch. Micro Parsons problems extend this concept by having students assemble segments of code to form a single line of code rather than an entire program. Recent investigations into micro Parsons problems have primarily focused on supporting learners leaving open the question of micro Parsons efficacy as an exam item and how students perceive it when preparing for exams.To fill this gap, we included a variety of micro Parsons problems on four exams in an introductory programming course taught in Python. We use Item Response Theory to investigate the difficulty of the micro Parsons problems as well as the ability of the questions to differentiate between high and low ability students. We then compare these results to results for related questions where students are asked to write a single line of code from scratch. Finally, we conduct a thematic analysis of the survey responses to investigate how students' perceptions of micro Parsons both when practicing for exams and as they appear on exams.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {674–680},
numpages = {7},
keywords = {assessment, cs1, micro parsons problems, parsons problems},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3653666.3656066,
author = {Gelder, William and Baker-Ramos, Rachel and Cho, Ayoung and Kolakaluri, Jahnavi and Uchidiuno, Judith and Hester, Josiah},
title = {"Those don't work for us": An Assets-Based Approach to Incorporating Emerging Technologies in Viable Hawaiian Teacher Support Tools for Culturally Relevant CS Education},
year = {2024},
isbn = {9798400706264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653666.3656066},
doi = {10.1145/3653666.3656066},
abstract = {Hawaiian bilingual language immersion (Kaiapuni) schools infuse curricula with place-based education to increase student connection to culture. However, stand-in teachers often lack the background and tools needed to support immersion learning, resulting in discontinuity for students in their culturally relevant education. This experience report describes a partnership between the Ka Moamoa Lab at the Georgia Institute of Technology and Ke Kula Kaiapuni 'O Pu'ohala School to design a teacher-substitute support platform via a hybrid of assets-based design methodology and emerging technology capabilities. We share insights offered by teachers and design requirements for such a platform. We also reflect on how HCI methodologies should adapt to center and respect Native Hawaiian perspectives.},
booktitle = {Proceedings of the 2024 on RESPECT Annual Conference},
pages = {224–230},
numpages = {7},
keywords = {EdTech, Hawai'i, culturally-relevant CS, indigenous knowledge, place-based computing},
location = {Atlanta, GA, USA},
series = {RESPECT 2024}
}

@inproceedings{10.1145/3626252.3630899,
author = {Morales-Navarro, Luis and Shah, Meghan and Kafai, Yasmin B.},
title = {Not Just Training, Also Testing: High School Youths' Perspective-Taking through Peer Testing Machine Learning-Powered Applications},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630899},
doi = {10.1145/3626252.3630899},
abstract = {Most attention in K-12 artificial intelligence and machine learning (AI/ML) education has been given to having youths train models, with much less attention to the equally important testing of models when creating machine learning applications. Testing ML applications allows for the evaluation of models against predictions and can help creators of applications identify and address failure and edge cases that could negatively impact user experiences. We investigate how testing each other's projects supported youths to take perspective about functionality, performance, and potential issues in their own projects. We analyzed testing worksheets, audio and video recordings collected during a two week workshop in which 11 high school youths created physical computing projects that included (audio, pose, and image) ML classifiers. We found that through peer-testing youths reflected on the size of their training datasets, the diversity of their training data, the design of their classes and the contexts in which they produced training data. We discuss future directions for research on peer-testing in AI/ML education and current limitations for these kinds of activities.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {881–887},
numpages = {7},
keywords = {artificial intelligence, computing education, k-12, machine learning},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3649217.3653563,
author = {\v{S}v\'{a}bensk\'{y}, Valdemar and Pankiewicz, Maciej and Zhang, Jiayi and Cloude, Elizabeth B. and Baker, Ryan S. and Fouh, Eric},
title = {Comparison of Three Programming Error Measures for Explaining Variability in CS1 Grades},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653563},
doi = {10.1145/3649217.3653563},
abstract = {Programming courses can be challenging for first year university students, especially for those without prior coding experience. Students initially struggle with code syntax, but as more advanced topics are introduced across a semester, the difficulty in learning to program shifts to learning computational thinking (e.g., debugging strategies). This study examined the relationships between students' rate of programming errors and their grades on two exams. Using an online integrated development environment, data were collected from 280 students in a Java programming course. The course had two parts. The first focused on introductory procedural programming and culminated with exam 1, while the second part covered more complex topics and object-oriented programming and ended with exam 2. To measure students' programming abilities, 51095 code snapshots were collected from students while they completed assignments that were autograded based on unit tests. Compiler and runtime errors were extracted from the snapshots, and three measures - Error Count, Error Quotient and Repeated Error Density - were explored to identify the best measure explaining variability in exam grades. Models utilizing Error Quotient outperformed the models using the other two measures, in terms of the explained variability in grades and Bayesian Information Criterion. Compiler errors were significant predictors of exam 1 grades but not exam 2 grades; only runtime errors significantly predicted exam 2 grades. The findings indicate that leveraging Error Quotient with multiple error types (compiler and runtime) may be a better measure of students' introductory programming abilities, though still not explaining most of the observed variability.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {87–93},
numpages = {7},
keywords = {computer science education, introduction to programming, introductory programming, novice programming, programming education},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3626252.3630754,
author = {Batra, Anna and Zhou, Iris and Choi, Suh Young and Gao, Chongjiu and Xiao, Yanbing and Fereidooni, Sonia and Lin, Kevin},
title = {"It Can Relate to Real Lives": Attitudes and Expectations in Justice-centered Data Structures &amp; Algorithms for Non-Majors},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630754},
doi = {10.1145/3626252.3630754},
abstract = {Prior work has argued for a more justice-centered approach to postsecondary computing education by emphasizing ethics, identity, and political vision. In this experience report, we examine how postsecondary students of diverse gender and racial identities experience a justice-centered Data Structures and Algorithms designed for undergraduate non-computer science majors. Through a quantitative and qualitative analysis of two quarters of student survey data collected at the start and end of each quarter, we report on student attitudes and expectations.Across the class, we found a significant increase in the following attitudes: computing confidence and sense of belonging. While women, non-binary, and other students not identifying as men (WNB+) also increased in these areas, they still reported significantly lower confidence and sense of belonging than men at the end of the quarter. Black, Latinx, Middle Eastern and North African, Native American, and Pacific Islander (BLMNPI) students had no significant differences compared to white and Asian students.We also analyzed end-of-quarter student self-reflections on their fulfillment of expectations prior to taking the course. While the majority of students reported a positive overall sentiment about the course and many students specifically appreciated the justice-centered approach, some desired more practice with program implementation and interview preparation. We discuss implications for practice and articulate a political vision for holding both appreciation for computing ethics and a desire for professional preparation together through iterative design.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {88–94},
numpages = {7},
keywords = {attitudes, computing education, confidence, data structures, diversity, iterative design, non-majors, sense of belonging, social justice},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3658619.3658623,
author = {Delcourt, Catherine and Venkatagiri, Sukrit and Chandrasekharan, Eshwar},
title = {What's in a Social Computing Course: Analyzing Computer and Information Science Syllabi},
year = {2024},
isbn = {9798400716591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658619.3658623},
doi = {10.1145/3658619.3658623},
abstract = {Social computing systems—such as social media and e-commerce platforms as well as search engines and collaboration software— not only drive vast economic value and societal impact, but are also becoming prominent topics in policy discourse. Although social technology companies heavily recruit students from Computer and Information Science (CS and IS) programs, and social computing is a well-established scholarly field within human-computer interaction (HCI) focused on the social interactions between people mediated through computational systems, little is known about social computing education. Consequently, in this paper we analyzed 25 undergraduate and graduate level courses titled “social computing.” First, as a fast-paced discipline that follows developments in computing as well as related societal implications, we highlight foundational and emergent topics. Second, we map these topics onto the life cycle of social computing systems to highlight gaps in coverage. Third, we map social computing topics to the 2023 ACM CS Curricula Body of Knowledge to provide a framework for introducing social computing concepts into CS and IS curricula. We find that social computing courses require diverse skill sets both within HCI and CS, as well as inter-disciplinary concepts from Sociology, Economics, among others. We conclude with guidelines for designing new social computing courses and discuss ways to critically examine the role of—and the power held by—system builders.},
booktitle = {Proceedings of the 6th Annual Symposium on HCI Education},
articleno = {2},
numpages = {8},
keywords = {CSEd, HCI education, computer science education, curriculum, social computing, syllabi},
location = {New York, NY, USA},
series = {EduCHI '24}
}

@inproceedings{10.1145/3626252.3630911,
author = {OConnor, TJ and Schmith, Alex and Stricklan, Chris and Carvalho, Marco and Sudhakaran, Sneha},
title = {PWN Lessons Made Easy with Docker: Toward an Undergraduate Vulnerability Research Cybersecurity Class},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630911},
doi = {10.1145/3626252.3630911},
abstract = {Developing expertise in vulnerability research is critical to closing the cybersecurity workforce shortage. However, very few institutions have adopted vulnerability research into their cybersecurity curriculum, and fewer have examined how to teach this skill to students. The recent emergence of lightweight, container-based virtualization presents a unique opportunity to address this challenge by offering reproducible environments that ease course facilitation. This paper presents an undergraduate vulnerability course design. Our approach leverages a hands-on methodology that challenges students to develop complex binary exploits over our lectures, labs, and exams. We share our detailed design, labs, experiences, lessons learned, and a lightweight virtual environment for this course for others to build on our initial success.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {986–992},
numpages = {7},
keywords = {cybersecurity education, virtualization, vulnerability research},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3626252.3630827,
author = {Rich, Kathryn M. and Spang, Marissa and Bowdon, Jill and Wilson, Joseph P. and Cunningham, Heather and Perkins, McKay},
title = {Developing Culturally Sustaining Elementary Computer Science Education with Indigenous Communities: Lessons Learned through a Research-Practice Partnership},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630827},
doi = {10.1145/3626252.3630827},
abstract = {The Wind River Elementary Computer Science (WRECS) Collaborative is a research-practice partnership (RPP) among three school districts serving Eastern Shoshone and Northern Arapaho communities on the Wind River Reservation, the Wyoming Department of Education (WDE), BootUp Professional Development (BootUp PD), and the American Institutes for Research (AIR). The purpose of the WRECS Collaborative is to develop culturally sustaining elementary computer science (CS) education through integration of CS and Indigenous studies. The Collaborative engaged three cohorts of elementary educators in cycles of professional development, classroom implementation, and group reflection over the 2020-21, 2021-22, and 2022-23 school years. In this experience report, we share a set of reflections and lessons learned as the RPP developed relationships and worked through intersecting priorities, instructional goals, and ways of knowing and learning present within the RPP.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1126–1132},
numpages = {7},
keywords = {culturally sustaining computer science, elementary computer science, indigenous communities, integration, professional development, research practice partnership},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3639474.3640083,
author = {Parthasarathy, P. D. and Joshi, Swaroop},
title = {Teaching Digital Accessibility to Industry Professionals using the Community of Practice framework: An Experience Report},
year = {2024},
isbn = {9798400704987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639474.3640083},
doi = {10.1145/3639474.3640083},
abstract = {Despite recent initiatives aimed at improving accessibility, the field of digital accessibility remains markedly behind contemporary advancements in the software industry, as many real-world software and web applications continue to fall short of accessibility requirements. A persisting skills deficit within the existing technology workforce has been an enduring impediment, hindering organizations from delivering truly accessible software products. This, in turn, elevates the risk of isolating and excluding a substantial portion of potential users. In this paper, we report lessons learned from a training program for teaching digital accessibility using the Communities of Practice (CoP) framework to industry professionals. We recruited 66 participants from a large multinational software company and assigned them to two groups: one participating in a CoP and the other using self-paced learning. We report experiences from designing the training program, conducting the actual training, and assessing the efficiency of the two approaches. Based on these findings, we provide recommendations for practitioners in Learning and Development teams and educators in designing accessibility courses for industry professionals.},
booktitle = {Proceedings of the 46th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {191–200},
numpages = {10},
keywords = {accessibility, massive open online courses, community of practice, computing education, global computing education},
location = {Lisbon, Portugal},
series = {ICSE-SEET '24}
}

@inproceedings{10.1145/3568813.3600122,
author = {Shah, Anshul and Hogan, Emma and Agarwal, Vardhan and Driscoll, John and Porter, Leo and Griswold, William G. and Soosai Raj, Adalbert Gerald},
title = {An Empirical Evaluation of Live Coding in CS1},
year = {2023},
isbn = {9781450399760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568813.3600122},
doi = {10.1145/3568813.3600122},
abstract = {Background and Context. Live coding is a teaching method in which an instructor dynamically writes code in front of students in an effort to impart skills such as incremental development and debugging. By contrast, traditional, static-code examples typically involve an instructor annotating or explaining components of pre-written code. Despite recommendations to use live coding and a wealth of qualitative analyses that identify perceived learning benefits of it, there are a lack of empirical evaluations to confirm those learning benefits, especially with respect to students’ programming processes. Objectives. Our work aims to provide a holistic, empirical comparison of a live-coding pedagogy with a static-code one. We evaluated the impact of a live-coding pedagogy on three main areas: 1) students’ adherence to effective programming processes, 2) their performance on exams and assignments, and 3) their lecture experiences, such as engagement during lecture and perceptions of code examples. Method. In our treatment-control quasi-experimental setup, one lecture group saw live-coding examples while the other saw only static-code ones. Both lecture groups were taught by the same instructor, were taught the exact same content, and completed the same assignments and exams. We collected compilation-level programming process data, student performance on exam and homework questions, and feedback via a survey and course evaluations. Findings. Our findings showed no statistically significant differences between the live-coding and static-code groups on programming process metrics related to incremental development, debugging, and productivity. Similarly, there was no difference between the groups on course performance on assignments and exams. Finally, student feedback suggests that more students in the live-coding group reported that lectures were too fast and failed to facilitate note-taking, potentially mitigating the perceived benefits of live coding. Implications. Live coding alone may not lead to many of the perceived and intended benefits that prior work identifies, but future work may investigate how to realize these benefits while minimizing the drawbacks we identified.},
booktitle = {Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 1},
pages = {476–494},
numpages = {19},
keywords = {Cognitive Apprenticeship, course performance, live coding, pedagogical techniques, programming processes, student perceptions},
location = {Chicago, IL, USA},
series = {ICER '23}
}

@inproceedings{10.1145/3626252.3630753,
author = {Liut, Michael and Ly, Anna and Xu, Jessica Jia-Ni and Banson, Justice and Vrbik, Paul and Hardin, Caroline D.},
title = {"I Didn't Know": Examining Student Understanding of Academic Dishonesty in Computer Science},
year = {2024},
isbn = {9798400704239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626252.3630753},
doi = {10.1145/3626252.3630753},
abstract = {In contrast with studies that have identified why students commit academic offences, many educators are familiar with the excuse that an accused student did not know the behavior counted as dishonest. Given the variations in policy and the ways collaboration and code sharing occur in professional and hobbyist spaces, this might be plausible. Mismatches between students' conceptions of academic honesty and course policy can have major consequences, from being kicked out of programs to being too nervous to study with peers. In this work, we investigate what students understand about academic integrity in computer science courses and if there are differences based on university, country, demographic, or online versus in-person courses. We present a study that surveys undergraduate computer science students (N = 1,011) at three universities (Australia, Canada, and the United States of America). The results show that all three institutions take academic integrity seriously, and their students are aware of its importance, but confusion on what is covered under the policies is common. Interestingly, the results also show that course instructors play a huge role as to what students perceive to be a violation of the academic integrity policy at their institution. By understanding student's perspectives on academic integrity, educators can better develop policies and practices that reduce inadvertent and mistaken violations of academic integrity policies.},
booktitle = {Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1},
pages = {757–763},
numpages = {7},
keywords = {academic dishonesty, academic integrity, assessment, cheating, computer science students, computing students, education},
location = {Portland, OR, USA},
series = {SIGCSE 2024}
}

@inproceedings{10.1145/3555858.3555905,
author = {Healy, John and Cullen, Charlie},
title = {Navigating Complexity: Investigating How Students Move Through the Game Design Process},
year = {2022},
isbn = {9781450397957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555858.3555905},
doi = {10.1145/3555858.3555905},
abstract = {Game design has become a common feature at many higher education institutions since the first programmes emerged in the early 90s and a growing body of research around games education has emerged in recent years. This paper is part of an ongoing research project investigating how game design takes place and how designers navigate the complexities of the discipline. Student development logs (DevLogs) were produced throughout a six-week game design project and were analysed using a grounded theory approach. The study took a naturalistic approach and attempted to capture the process by which students moved from project brief to final deliverable. Through this analysis, four specific design approaches applied by students were identified: Open-Ended Design, Player Experience Design, World Design and Technical Design. In addition to this, we observed an overarching process of design moving from initiation to outcome and specifically we discuss the complexity of design approaches and their interrelated nature. Finally, we ask game design educators to consider if this complexity is necessary and if so how can we design learning to better support their academic development.},
booktitle = {Proceedings of the 17th International Conference on the Foundations of Digital Games},
articleno = {61},
numpages = {4},
location = {Athens, Greece},
series = {FDG '22}
}

@inproceedings{10.1145/3610969.3611121,
author = {Bowers, David S. and Hayes, Alan and Prickett, Tom and Crick, Tom and Streater, Kevin and Sharp, Chris},
title = {The Institute of Coding Accreditation Standard: Exploring the Use of a Professional Skills Framework to Address the UK Skills Gap},
year = {2023},
isbn = {9798400708763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610969.3611121},
doi = {10.1145/3610969.3611121},
abstract = {Computing comprises a broad spectrum of subjects and specialisms, with a rich variety of undergraduate courses (including Computer Engineering, Computer Science, Cybersecurity, Information Systems, Information Technology and Software Engineering) offered by universities worldwide. This breadth presents challenges for employers considering employing computing graduates and hence desiring to know both the topics studied and the skills/competencies accumulated by graduates to be able to make appropriate job offers. Small to medium enterprises (SMEs) may not have the resources to provide graduate training programmes, and therefore need ‘work-ready’ graduates. This paper explores and evaluates the feasibility of benchmarking students’ achievements against an industry-led skills framework, Skills for the Information Age (SFIA), to distinguish between what graduates know, have done or are competent in. The approach taken was evolutionary prototyping, informed by expert review. The work generated an accreditation standard that could be implemented or used as a model to enhance an existing accreditation standard. In contrast to academic approaches to competency-based education, or abstract notions of generic skills, this work focused on defining an output standard expressed in terms of employer needs and expectations captured in the SFIA skills framework. We show how a course meeting the proposed standard would satisfy the UK benchmarks for an undergraduate computing degree. By badging SFIA knowledge and competencies, such a course would enhance its learning outcomes, offering clarity for employers and career benefits to students.},
booktitle = {Proceedings of the 2023 Conference on United Kingdom &amp; Ireland Computing Education Research},
articleno = {10},
numpages = {7},
keywords = {IT competencies, SFIA, accreditation standard, skills frameworks},
location = {Swansea, Wales Uk},
series = {UKICER '23}
}

@proceedings{10.1145/3606094,
title = {ICDEL '23: Proceedings of the 2023 8th International Conference on Distance Education and Learning},
year = {2023},
isbn = {9798400700422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Beijing, China}
}

@inproceedings{10.1145/3608218.3608239,
author = {Hong, Wenqi and Dai, Yu},
title = {Analysis on the Function and Promotion Strategy of Metaverse Empowering Online-Merge-Offline Teaching},
year = {2023},
isbn = {9798400708220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3608218.3608239},
doi = {10.1145/3608218.3608239},
abstract = {As an online digital space parallel to the real world, the metaverse plays an important role in empowering online-merge-offline(OMO) teaching. Through analysis, it is found that the functions of metaverse in integrated teaching are mainly embodied in six aspects: aggregating online course resources, promoting online learning support services, reforming online teaching methods, expanding the learning field of online education, creating the interactive learning mode of virtual and real, and deepening the learning experience of online education. To better empower the metaverse to integrate education, online-merge-offline(OMO) teaching can be improved by expanding the organizational form, emphasizing the cognitive training and optimizing the teaching strategies of learning space.},
booktitle = {Proceedings of the 2023 6th International Conference on Big Data and Education},
pages = {6–12},
numpages = {7},
keywords = {metaverse, online education, online-merge-offline(OMO) teaching},
location = {Jinan, China},
series = {ICBDE '23}
}

@inproceedings{10.1145/3631802.3631825,
author = {W\"{o}rister, Florian and Knobelsdorf, Maria},
title = {A Block-Based Programming Environment for Teaching Low-Level Computing (Discussion Paper)},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631825},
doi = {10.1145/3631802.3631825},
abstract = {The block-based programming paradigm has gained popularity across various application areas, including programming education, physical computing, and creative arts and media. While initially targeting young learners, environments such as Scratch have demonstrated the versatility of this paradigm beyond its original audience. This paper explores the potential application of block-based code manipulation in the field of low-level programming, which is a fundamental component of many computer science curricula. However, existing visualization tools for teaching low-level computing do not leverage the benefits of block-based programming, such as eliminating the need to memorize syntax. This paper presents Blocksambler, a prototype of a block-based programming environment for teaching low-level computing. The purpose of this paper is two-fold, in the first theoretical part we explore the potential application of block-based code manipulation in the field of low-level programming, which is a fundamental component of many computer science curricula. In the second part, we introduce Blocksambler, a prototype of a block-based programming environment for teaching low-level computing. Blocksambler is built upon Blockly, a JavaScript library designed for creating customized block-based programming tools and for the purpose of teaching low-level computing. Reflecting the relevance, benefits, and limitations of Blocksambler for educational purposes, we conclude the discussion started in the first part of the paper and outline further steps.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {22},
numpages = {7},
keywords = {block-based programming, computer science education, low-level computing},
location = {Koli, Finland},
series = {Koli Calling '23}
}

@article{10.1145/3664808,
author = {Karas, Zachary and Bansal, Aakash and Zhang, Yifan and Li, Toby and McMillan, Collin and Huang, Yu},
title = {A Tale of Two Comprehensions? Analyzing Student Programmer Attention during Code Summarization},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {7},
issn = {1049-331X},
url = {https://doi.org/10.1145/3664808},
doi = {10.1145/3664808},
abstract = {Code summarization is the task of creating short, natural language descriptions of source code. It is an important part of code comprehension and a powerful method of documentation. Previous work has made progress in identifying where programmers focus in code as they write their own summaries (i.e., Writing). However, there is currently a gap in studying programmers’ attention as they read code with pre-written summaries (i.e., Reading). As a result, it is currently unknown how these two forms of code comprehension compare: Reading and Writing. Also, there is a limited understanding of programmer attention with respect to program semantics. We address these shortcomings with a human eye-tracking study (n = 27) comparing Reading and Writing. We examined programmers’ attention with respect to fine-grained program semantics, including their attention sequences (i.e., scan paths). We find distinctions in programmer attention across the comprehension tasks, similarities in reading patterns between them, and differences mediated by demographic factors. This can help guide code comprehension in both computer science education and automated code summarization. Furthermore, we mapped programmers’ gaze data onto the Abstract Syntax Tree to explore another representation of human attention. We find that visual behavior on this structure is not always consistent with that on source code.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
articleno = {193},
numpages = {37},
keywords = {Cognitive science, code summarization, eye-tracking, expertise, code comprehension}
}

@inproceedings{10.1145/3653666.3656093,
author = {Johnson, Michael J. and Hovey, Christopher Lynnly and Sanders, Sherri and Stallworth, Cedric and Mendes, Ryan and Parker, Andrea G. and Choi, Adrian and Francis, Sherilyn and Sackitey, Darley and Chernova, Sonia and Patel, Maithili and Tan, Xiang Zhi and Arriaga, Rosa I. and Johnson, Britney L. and DiSalvo, Betsy},
title = {Lessons Learned from Developing and Implementing a High School CS Bridge Program},
year = {2024},
isbn = {9798400706264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653666.3656093},
doi = {10.1145/3653666.3656093},
abstract = {This experience report describes the design and implementation of the BridgeUP STEM program, which connects women and gender non-binary individuals from historically underrepresented racial or ethnic groups to learning opportunities in computing subfields. Faculty, staff, and students from Georgia Tech collaborated with NCWIT to provide participants computer programming and research experience. We describe the program and present reflections from the facilitation team to capture knowledge and advice on successful and unsuccessful strategies for developing an effective bridge program. We provide a summary of lessons learned by those involved and conclude with guiding questions to help others in designing successful programs.},
booktitle = {Proceedings of the 2024 on RESPECT Annual Conference},
pages = {51–59},
numpages = {9},
keywords = {bridge programs, computer science, diversity, equity, and inclusion, student recruitment},
location = {Atlanta, GA, USA},
series = {RESPECT 2024}
}

@article{10.1145/3691354,
author = {Rajapakse, Chathura and Ariyarathna, Wathsala and Selvakan, Shanmugalingam},
title = {A Self-Efficacy Theory-Based Study on the Teachers’ Readiness to Teach Artificial Intelligence in Public Schools in Sri Lanka},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
url = {https://doi.org/10.1145/3691354},
doi = {10.1145/3691354},
abstract = {Objectives. This article explores teacher readiness for introducing artificial intelligence (AI) into Sri Lankan schools, drawing on self-efficacy theory. Similar to some other countries, Sri Lanka plans to integrate AI into the school curriculum soon. However, a key question remains: Are teachers prepared to teach this advanced technical subject to schoolchildren? Assessing teacher readiness is crucial, as it is intricately linked to the overall success of this initiative and will inform the development of appropriate policies.Participants. This study surveyed over 1,300 Sri Lankan public school teachers who teach Information and Communication Technology (ICT) using the snowball sampling approach. The respondents represent approximately 20% of the total ICT teacher population in Sri Lanka. Their readiness to teach AI was assessed using a general teacher self-efficacy scale specifically developed based on the well-established Self-Efficacy Theory. While key demographic factors like gender, education level and educational background were also collected, their impact analysis is not included in this article.Study Method. The study was conducted based on the premise that teachers’ readiness to teach AI hinges on their self-efficacy towards teaching AI in the classroom. This premise was substantiated through a review of existing research, and a conceptual model of teachers’ self-efficacy for AI instruction was developed. To assess this model, a nationwide survey targeting school ICT teachers was conducted. The questionnaire used in the survey was based on existing research on evaluating teacher self-efficacy. Data analysis, focusing on testing and validating the conceptual model, primarily employed the PLS-SEM approach.Findings. This study identified several key findings: (1) Teachers generally reported low self-efficacy regarding their ability to teach AI; (2) Teachers’ self-efficacy was most influenced by their emotional and physiological states, as well as their imaginary experiences related to teaching AI; (3) Surprisingly, mastery experiences had a lesser impact on their self-efficacy for teaching AI; and (4) Neither vicarious experiences (observing others teach AI) nor verbal persuasion had a significant impact on teachers’ self-efficacy. Additionally, the study revealed that the teachers’ own level of expertise in AI, along with their social capital, is insufficient to deliver a standard AI curriculum.Conclusions. The analysis of the results found that Sri Lankan teachers currently lack the readiness to teach AI in schools effectively. Potential lapses in certain sources of self-efficacy were also identified. It further revealed the need for a more systemic approach to teacher professional development. Therefore, the study recommends further research exploring the potential of incorporating a socio-technical systems perspective into the government’s teacher training initiatives.},
journal = {ACM Trans. Comput. Educ.},
month = nov,
articleno = {47},
numpages = {25},
keywords = {Teaching Artificial Intelligence, K-12 Education, Readiness, Self-Efficacy Theory, Teacher Professional Development, AI4K12}
}

@article{10.1145/3490685,
author = {Aho, Alfred and Ullman, Jeffrey},
title = {Abstractions, their algorithms, and their compilers},
year = {2022},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/3490685},
doi = {10.1145/3490685},
abstract = {Jeffrey D. Ullman and Alfred V. Aho are recipients of the 2020 ACM A.M. Turing award. They were recognized for creating fundamental algorithms and theory underlying programming language implementation and for synthesizing these results and those of others in their highly influential books, which educated generations of computer scientists.},
journal = {Commun. ACM},
month = jan,
pages = {76–91},
numpages = {16}
}

@inproceedings{10.1145/3545945.3569842,
author = {Garcia, Dan and Fries, Mary and Ball, Michael and Fox, Pamela and Gelosi, Deanna and Mock, Lauren and Dastur, Della and Briccetti, Dave and Kahn, Bob},
title = {BJC Sparks: A New Functional-First Middle School CS Curriculum},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569842},
doi = {10.1145/3545945.3569842},
abstract = {We present our experience with the design and implementation of BJC Sparks, a new introductory computer science curriculum for middle school piloted by ten teachers in the 2021-2022 academic year. Our curriculum is functional-first, using abstraction, functional decomposition, immutable data, and powerful higher-order functions such as map, keep, and combine. Based on a decade of experience teaching thousands of first-time programmers, we believe starting with functions teaches students the habit of solving problems functionally when possible, helping to avoid bugs in later programming experiences.BJC Sparks uses the Snap! blocks-based programming environment, which brings the expressive power of a general-purpose programming language without requiring mastery of the syntax of a text-based language. We take advantage of new Snap! features to develop microworlds: customized projects that simplify the blocks palette to further lower cognitive load. We designed projects that use Snap!'s multimedia features to explore data science, encryption, and computational media. We end with physical computing using the BBC micro:bit that students program using MicroBlocks. In terms of findings, we confirmed that it indeed was possible to capture the interest of middle school students with functional programming activities, especially through the use of microworlds. We were surprised at the strong student and teacher interest in our micro:bit lessons, which many wanted as a stand-alone unit. Finally, it became quite clear that multimedia and data science projects offer promising avenues for teaching functional programming through fun, engaging, and personally relevant activities.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {451–457},
numpages = {7},
keywords = {blocks-based programming, data science, functional programming, media computation, middle school, snap!},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3706598.3713443,
author = {Jia, Kaiyue and Yu, Junnan},
title = {Technologies for Children's AI Learning: Design Features and Future Opportunities},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713443},
doi = {10.1145/3706598.3713443},
abstract = {With the growing integration of AI into daily life, various technologies have been developed to teach children about AI. However, differences in their designs highlight the need for a thorough understanding of these tools to make the most of current technological resources and guide the effective development of future learning tools. Through a systematic search, we identified 64 different AI learning tools for children and analyzed their design features, including both static design features (i.e., presentation formats and learning content) and interactive design features (i.e., learning activity types and design features that potentially enhance the effectiveness of the activities). Our findings reveal the current trends and gaps in the design of children’s AI learning technologies. Based on these insights, we reflect on future design opportunities and provide recommendations for creating new, effective learning technologies to advance AI education for the next generations.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1196},
numpages = {22},
keywords = {AI literacy, AI learning tool, Learning technology, Design},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3706598.3713131,
author = {Devasia, Nisha and Zhao, Runhua and Lee, Jin Ha},
title = {Does the Story Matter? Applying Narrative Theory to an Educational Misinformation Escape Room Game},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713131},
doi = {10.1145/3706598.3713131},
abstract = {Rapid spread of harmful misinformation has led to a dire need for effective media literacy interventions, to which educational games have been suggested as a possible solution. Researchers and educators have created several games that increase media literacy and resilience to misinformation. However, the existing body of misinformation education games rarely focus upon the socio-emotional influences that factor into misinformation belief. Misinformation correction and serious games have both explored narrative as a method to engage with people on an emotional basis. To this end, we investigated how 123 young adults (mean age = 22.98) experienced narrative transportation and identification in two narrative-centered misinformation escape room games developed for library settings. We found that propensity for certain misinformation contexts, such as engagement with fan culture and likelihood to share on social media platforms, significantly affected how participants experienced specific measures of narrative immersion within the games. We discuss design implications for tailoring educational interventions to specific misinformation contexts.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {963},
numpages = {15},
keywords = {Misinformation, Education, Escape room game, Narrative theory},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3545945.3569780,
author = {Lachney, Michael and Yadav, Aman and Drazin, Matt and Green, Briana},
title = {Community Embedded Computing Education: Shaping Young People's Perceptions of Self-confidence and Personal Expression with Computer Science in a Youth Boxing Gym},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569780},
doi = {10.1145/3545945.3569780},
abstract = {Efforts in the United States to broaden the participation of racial and ethnic minorities in K12 computer science (CS) education often focus on the need for children to change themselves to meet the standards and norms of traditional computing education. Less attention has been paid to how CS education itself might change and adapt to cultural contexts and locations that children already participate in and find of value. In this paper, we describe efforts to change the shape and culture of CS education by designing and implementing it in and for a youth boxing gym in an African American community. We report findings from a boxing inspired CS curriculum that was designed in collaboration with boxing coaches, mentors, and academic staff members at the gym. It was piloted over the course of two six-hour workshops with 15 middle and high school age children. We focused on two issues that are relevant to broadening participation and reshaping the culture of CS education: (1) how the curriculum changed children's perceptions of CS and (2) the creative adaptations that children made to the curricular content and materials. An analysis of pre- and post-surveys using a nonparametric test showed significant positive changes in children's responses to the constructs of self-confidence and expression with CS. Vignettes of children appropriating and personalizing technologies from the workshops are used to unpack these findings. We end with a discussion about the implications of these findings for culture-based CS education in both school and community contexts.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {1103–1109},
numpages = {7},
keywords = {broadening participation, community expertise, computer science attitudes, computer science education, culturally responsive computing, sports},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3545945.3569783,
author = {Lee, Janice and Fancsali, Cheri and Clough, Symantha},
title = {Reaching for "All": Understanding the Challenges and Needs of Schools Lagging in CS for All Efforts},
year = {2023},
isbn = {9781450394314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545945.3569783},
doi = {10.1145/3545945.3569783},
abstract = {Over the last decade, substantial strides have been made in the CS for All movement, with the widespread the enactment of policies that promote the implementation of CS education in K-12 schools. Despite this progress, at the current rate of growth, it is estimated that it will take four decades to actually reach CS for all students. This sobering finding highlights the urgent need to understand why so many schools are lagging in implementation, and to identify solutions that could address this gap.Using school-level survey data and administrative school records, we investigate the barriers to scaling up CS education in New York City. Common to many school reform initiatives, some schools were early adopters and eagerly embraced the call to provide CS to all students. Others, despite years of effort and support, have yet to offer CS or serve a small percentage of their enrollment. Our findings suggest that while "normative'' perceptions of CS (e.g., beliefs about it's value) are similar among lower- and higher-implementing schools, some "technical'' challenges-such as lacking an implementation plan and shared school-wide vision for CS-and "political'' challenges-such as the lack of support from administrators, are greater for schools struggling to offer CS. Though these findings focus on one district, they are relevant to the many others engaged in CS for all efforts. This study builds on previous research by shedding light on the distinct challenges and needs of "lagging'' schools, and provides insight into effective strategies for bringing CS education to all.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1},
pages = {80–86},
numpages = {7},
keywords = {broadening participation, cs for all, k-12 education, scaling up},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@inproceedings{10.1145/3544549.3573842,
author = {Long, Duri},
title = {Conducting Remote Design Research on Embodied, Collaborative Museum Exhibits},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3573842},
doi = {10.1145/3544549.3573842},
abstract = {Research activities in interaction design and HCI were widely altered by the COVID-19 pandemic, with many studies shifting online as health concerns inhibited in-person research. Tangible and collaborative activities are often used in informal learning spaces and child-computer interaction, but they are neither designed for nor easily adapted to online formats. In this case study, I present findings and reflections on my experience adapting an in-situ study of embodied, collaborative museum exhibits to a remote user study during COVID-19. I identify several considerations and notes of inspiration for researchers working on similar projects, which I hope can aid in furthering iterative design research on embodied and/or collaborative activities both during the ongoing pandemic and in other current and future contexts that require remote research or interactions. The reflections I present in this case study additionally play a role in documenting the ongoing history of interaction design as researchers adapt to the rapidly changing global circumstances caused by COVID-19.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {379},
numpages = {8},
keywords = {Additional Keywords and Phrases AI literacy, COVID-19, at-home learning, collaborative, design research, embodied, informal learning, methods, museum exhibits, pandemic, remote user study, tangible},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3498765.3498821,
author = {Liu, Xuedan},
title = {The Development and Practice of Children's Spelling Ability for a Foreign Language},
year = {2022},
isbn = {9781450385114},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498765.3498821},
doi = {10.1145/3498765.3498821},
abstract = {In China there is a trend in English language teaching for children which is to focus on listening and speaking instead of grammar and spelling. This research seeks to develop children's spelling ability in foreign language learning as English for instance to prove it is actually beneficial for children's language ability to acquire spelling skills in a foreign language in their early age. The research provides experiments and data in children's reading, speaking, writing and listening in both mother language and the foreign language after helping the children acquire the foreign language spelling ability.},
booktitle = {Proceedings of the 13th International Conference on Education Technology and Computers},
pages = {359–363},
numpages = {5},
keywords = {Children, Data, Experiment, Foreign language learning, Spelling},
location = {Wuhan, China},
series = {ICETC '21}
}

@inproceedings{10.1145/3689187.3709606,
author = {Hamouda, Sally and Marshall, Linda and Sanders, Kate and Tshukudu, Ethel and Adelakun-Adeyemo, Oluwatoyin and Becker, Brett A. and Dodoo, Emma R. and Korsah, G. Ayorkor and Luvhengo, Sandani and Ola, Oluwakemi and Parkinson, Jack and Sanusi, Ismaila Temitayo},
title = {Computing Education in African Countries: A Literature Review and Contextualised Learning Materials},
year = {2025},
isbn = {9798400712081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689187.3709606},
doi = {10.1145/3689187.3709606},
abstract = {This report begins with a literature review of computing education in Africa. We found a substantial body of work, scattered over more than 80 venues, which we have brought together here for the first time. Several important themes emerge in this dataset, including the need to contextualise computing education.In the second part of this report we investigate contextualisation further. We present a pilot study, grounded in the literature review, of the development of course materials, sample code, and programming assignments for introductory programming, contextualised for six African countries: Botswana, Egypt, Ghana, Nigeria, South Africa, and Zambia. We include the materials, report on a preliminary evaluation of the materials by fellow educators in African countries, and suggest a process by which other educators could develop materials for their local contexts.},
booktitle = {2024 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {1–33},
numpages = {33},
keywords = {Africa, African, Algeria, Angola, Botswana, Burundi, CS1, CS1 materials, Cameroon, Egypt, Ethiopia, Ghana, Kenya, Lesotho, Liberia, Libya, Mauritius, Morocco, Mozambique, Namibia, Nigeria, Rwanda, Senegal, South Africa, Sudan, Tanzania, Uganda, Zambia, computer science education, computing education, contextualisation, introductory programming, literature review},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3528231.3528357,
author = {Tisha, Sirazum Munira and Oregon, Rufino A. and Baumgartner, Gerald and Alegre, Fernando and Moreno, Juana},
title = {An automatic grading system for a high school-level computational thinking course},
year = {2023},
isbn = {9781450393362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3528231.3528357},
doi = {10.1145/3528231.3528357},
abstract = {Automatic grading systems help lessen the load of manual grading. Most existent autograders are based on unit testing, which focuses on the correctness of the code, but has limited scope for judging code quality. Moreover, it is cumbersome to implement unit testing for evaluating graphical output code. We propose an autograder that can effectively judge the code quality of the visual output codes created by students enrolled in a high school-level computational thinking course. We aim to provide suggestions to teachers on an essential aspect of their grading, namely the level of student competency in using abstraction within their codes. A dataset from five different assignments, including open-ended problems, is used to evaluate the effectiveness of our autograder. Our initial experiments show that our method can classify the students' submissions even for open-ended problems, where existing autograders fail to do so. Additionally, survey responses from course teachers support the importance of our work.},
booktitle = {Proceedings of the 4th International Workshop on Software Engineering Education for the Next Generation},
pages = {20–27},
numpages = {8},
keywords = {code quality, lexical analysis, machine learning, open-ended problems},
location = {Pittsburgh, Pennsylvania},
series = {SEENG '22}
}

@inproceedings{10.1145/3568739.3568785,
author = {Li, Haiyang},
title = {Problems and Doctrine: Learning Situation Analysis and Application Research of Virtual Teaching from the Perspective of Artificial},
year = {2023},
isbn = {9781450398091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568739.3568785},
doi = {10.1145/3568739.3568785},
abstract = {Virtual teaching is an inevitable trend of informatization and intelligence of school education in the era of artificial intelligence. In response to the current high-incidence stage of the epidemic, the Ministry of Education has put forward the requirement of "suspending classes without stopping school, and without extension of teaching", which highlights the advantages of virtual teaching. Through in-depth practice and research, it is found that there is a "distortion" phenomenon in the teaching process and assessment results. Through the practice, combing and research of virtual teaching under the epidemic situation in recent years, comprehensively consider the interaction data between teachers and students, and students and students in the teaching process before, during and after class, and expound the remaining problems of virtual teaching. The analysis of learning situation before, during and after class, put forward effective teaching design to deal with "distorted" virtual teaching, and construct a new teaching process structure design idea, so as to construct the structure of educators, teaching resources, teaching environment, teaching data, and students. An efficient virtual teaching dynamic generation and development system composed of educators and other elements.},
booktitle = {Proceedings of the 6th International Conference on Digital Technology in Education},
pages = {269–276},
numpages = {8},
keywords = {Artificial intelligence, Epidemic, Improvement strategies, Virtual teaching, learning situation analysis},
location = {Hangzhou, China},
series = {ICDTE '22}
}

@article{10.1145/3631715,
author = {Williams, Krystal L. and Dillon, Edward and Carter, Shanice and Jones, Janelle and Melchior, Shelly},
title = {CS=Me: Exploring Factors that Shape Black Women's CS Identity at the Intersections of Race and Gender},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
url = {https://doi.org/10.1145/3631715},
doi = {10.1145/3631715},
abstract = {Improving equity and inclusion for underrepresented groups in the field of Computer Science (CS) has garnered much attention. In particular, there is a long-standing need for diversity efforts that center on the experiences of Black women, and specific actions to increase their representation—especially given the biases that they often encounter in the field. There is limited research concerning Black women in CS, specifically their conceptions of the field and their overarching CS identity development. More research in this area is especially important given the marginalization that Black women often experience at the intersections of their race and gender. Guided by a combination of critical theoretical lenses, this qualitative study examines Black women's conceptions of what it means to be a computer scientist and the degree to which those conceptions map onto how they see themselves in the field. Moreover, we explore experiences that help to bolster Black women's CS identity. The findings highlight key aspects of what it means to be a computer scientist for the Black women in this study—notably the ability to use computing to make societal contributions. Also, the results accentuate key nuances in the participants’ personal CS identification, particularly as it relates to the resilience required to overcome unique barriers that many Black women encounter when engaging within the field. Moreover, the findings highlight the importance of social support systems to facilitate Black women's CS identity development. Implications for policy and practice within education and industry are discussed.},
journal = {ACM Trans. Comput. Educ.},
month = apr,
articleno = {21},
numpages = {20},
keywords = {Black women, computer science identity, role strain and adaptation, intersectionality}
}

@inproceedings{10.1145/3613904.3642607,
author = {Kaspersen, Magnus H\o{}holt and Musaeus, Line Have and Bilstrup, Karl-Emil Kj\ae{}r and Petersen, Marianne Graves and Iversen, Ole Sejer and Dindler, Christian and Dalsgaard, Peter},
title = {From Primary Education to Premium Workforce: Drawing on K-12 Approaches for Developing AI Literacy},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642607},
doi = {10.1145/3613904.3642607},
abstract = {Advances in artificial intelligence present a need for fostering AI literacy in workplaces. While there is a lack of research on how this can be achieved, there are documented successful approaches in child-computer interaction (CCI), albeit aimed at K-12 education. We present an in-vivo explorative case study of how CCI approaches can be adopted for adult professionals via a full-day workshop developed in collaboration with a trade union to upskill workers. Analyzing data from pre- and post-surveys, a follow-up survey, and materials produced by participants (n=53), we demonstrate how this increased participants’ knowledge of AI while their self-efficacy and empowerment did not improve. This is similar to findings from K-12 education, pointing to self-efficacy and empowerment as major challenges for AI literacy across sectors. We discuss the role of ambassadorships and professional organizations in addressing these issues, and indicate research directions for the CHI community.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {268},
numpages = {16},
keywords = {AI Literacy, K-12, Machine Learning, education, trade unions},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3613904.3641910,
author = {Wu, Zihan and Ericson, Barbara J.},
title = {SQL Puzzles: Evaluating Micro Parsons Problems With Different Feedbacks as Practice for Novices},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3641910},
doi = {10.1145/3613904.3641910},
abstract = {This paper investigates using micro Parsons problems as a novel practice approach for learning Structured Query Language (SQL). In micro Parsons problems learners arrange predefined code fragments to form a SQL statement instead of typing the code. SQL is a standard language for working with relational databases. Targeting beginner-level SQL statements, we evaluated the efficacy of micro Parsons problems with block-based feedback and execution-based feedback compared to traditional text-entry problems. To delve into learners’ experiences and preferences for the three problem types, we conducted a within-subjects think-aloud study with 12 participants. We found that learners reported very different preferences. Factors they considered included perceived learning, task authenticity, and prior knowledge. Next, we conducted two between-subjects classroom studies to evaluate the effectiveness of micro Parsons problems with different feedback types versus text-entry problems for SQL practice. We found that learners who practiced by solving Parsons problems with block-based feedback had a significantly higher learning gain than those who practiced with traditional text-entry problems.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {930},
numpages = {15},
keywords = {SQL education, empirical study, learning, programming puzzle},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3706598.3713125,
author = {Thompson, Gabriella and Dombrowski, Lynn and Smith, Angela D. R.},
title = {Embracing Social Justice within a Computing Curriculum to Foster Social Change},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713125},
doi = {10.1145/3706598.3713125},
abstract = {To ensure that technology serves as a tool for empowerment rather than oppression, Human-Computer Interaction (HCI) scholars have examined the ethical considerations of HCI research to explore pathways that inspire social change. In this work, we consider post-secondary education as one such pathway to social change. We engaged in a qualitative content analysis of the course, Introduction to Social Justice Informatics, with 47 students to understand how students developed knowledge of social justice and what sociotechnical tools facilitated their learning. We found that course materials coupled with peer discussion and reflective practice contributed to their development of critical consciousness. We discuss the significance of critical consciousness as a grounding theoretical approach within a social justice computing curriculum and the role of hope within social justice efforts and the workplace. We conclude by providing collectivist design strategies to nurture hope in the workplace.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {586},
numpages = {17},
keywords = {social justice, computing education, social change, critical consciousness, hope},
location = {
},
series = {CHI '25}
}

@article{10.1145/3487051,
author = {Kao, Yvonne and Matlen, Bryan and Weintrop, David},
title = {From One Language to the Next: Applications of Analogical Transfer for Programming Education},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
url = {https://doi.org/10.1145/3487051},
doi = {10.1145/3487051},
abstract = {The 1980s and 1990s saw a robust connection between computer science education and cognitive psychology as researchers worked to understand how students learn to program. More recently, academic disciplines such as science and engineering have begun drawing on cognitive psychology research and theories of learning to create instructional materials and teacher professional development materials based on theories of learning, to some success. In this paper, we follow a similar approach by highlighting common areas of interest between computer science education and cognitive psychology–specifically theories of analogical transfer–and discuss how cross-pollination of theoretical constructs between disciplines can support research on the teaching and learning of multiple programming languages. We will also discuss areas where computing education research can adapt the existing theories from cognitive psychology to develop domain-specific theories of knowledge transfer in computing and feed back into cognitive psychology research to inform larger debates about the nature of cognition and learning.},
journal = {ACM Trans. Comput. Educ.},
month = nov,
articleno = {42},
numpages = {21},
keywords = {Cognitive psychology, transfer, computer science education, programming}
}

@inproceedings{10.1145/3544548.3581378,
author = {Solyst, Jaemarie and Xie, Shixian and Yang, Ellia and Stewart, Angela E.B. and Eslami, Motahhare and Hammer, Jessica and Ogan, Amy},
title = {“I Would Like to Design”: Black Girls Analyzing and Ideating Fair and Accountable AI},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581378},
doi = {10.1145/3544548.3581378},
abstract = {Artificial intelligence (AI) literacy is especially important for those who may not be well-represented in technology design. We worked with ten Black girls in fifth and sixth grade from a predominantly Black school to understand their perceptions around fair and accountable AI and how they can have an empowered role in the creation of AI. Thematic analysis of discussions and activity artifacts from a summer camp and after-school session revealed a number of findings around how Black girls: perceive AI, primarily consider fairness as niceness and equality (but may need support considering other notions, such as equity), consider accountability, and envision a just future. We also discuss how the learners can be positioned as decision-making designers in creating AI technology, as well as how AI literacy learning experiences can be empowering.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {452},
numpages = {14},
keywords = {AI ethics, AI literacy, Black girls, artificial intelligence, design},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1145/3608113,
author = {Wong-Villacres, Marisol and Kutay, Cat and Lazem, Shaimaa and Ahmed, Nova and Abad, Cristina and Collazos, Cesar and Elbassuoni, Shady and Islam, Farzana and Singh, Deepa and Mayeesha, Tasmiah Tahsin and Ujakpa, Martin Mabeifam and Zaman, Tariq and Bidwell, Nicola J.},
title = {Making Ethics at Home in Global CS Education: Provoking Stories from the Souths},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3608113},
doi = {10.1145/3608113},
abstract = {Despite the increase in university courses and curricula on the ethics of computing there are few studies about how computer science (CS) programs should account for the diverse ways ethical dilemmas and approaches to ethics are situated in cultural, philosophical, and governance systems, religions, and languages. We draw on the experiences and insights of 46 university educators and practitioners in Latin America, South-Asia, Africa, the Middle East, and Australian First Nations who participated in surveys and interviews. Our modest study seeks to prompt conversation about ethics and computing in the Global Souths and inform revisions to the Association of Computer Machinery's curricular guidelines for the Society, Ethics and Professionalism knowledge area in undergraduate CS programs. Participants describe frictions between static and anticipatory approaches to ethics in globalised regulations and formal codes of ethics and professional conduct and local practices, values, and impacts of technologies in the Global Souths. Codes and regulations are instruments for international control and their gap with local realities can cause harm, despite local efforts to compensate. However, our insights also illustrate opportunities for university teaching to link more closely to priorities, actions, and experiences in the Global Souths and enrich students’ education in the Global North.},
journal = {ACM J. Comput. Sustain. Soc.},
month = jan,
articleno = {1},
numpages = {26},
keywords = {Ethics of technology, Code of Ethics and Professional Conduct, CS curricula, Global South}
}

@article{10.1145/3702242,
author = {Sanusi, Ismaila Temitayo and Martin, Fred and Ma, Ruizhe and Gonzales, Joseph E. and Mahipal, Vaishali and Oyelere, Solomon Sunday and Suhonen, Jarkko and Tukiainen, Markku},
title = {AI MyData: Fostering Middle School Students’ Engagement with Machine Learning through an Ethics-Infused AI Curriculum},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {4},
url = {https://doi.org/10.1145/3702242},
doi = {10.1145/3702242},
abstract = {As initiatives on AI education in K-12 learning contexts continues to evolve, researchers have developed curricula among other resources to promote AI across grade levels. Yet, there is a need for more effort regarding curriculum, tools, and pedagogy, as well as assessment techniques to popularize AI at the middle school level. Drawing on prior work, we created original curriculum activities with innovative use of existing technology, a new computational teaching tool, and a series of approaches and assessments to evaluate students’ engagement with the learning resources. Our curriculum called AI MyData comprises elements of ML and data science infused with ethical orientation. In this article, we describe the novel AI curriculum and further discuss how we engaged students in learning and critiquing AI ethical dilemmas. We gathered data from two pilot studies conducted in the Northeast United States, one Artificial Intelligence Afterschool (AIA) program, and one virtual AI summer camp. The AIA program was carried out in a local public school with four middle school students aged 12 to 13; the program consisted of eleven 2-hour sessions. The summer camp consisted of 2-hour sessions over 4 consecutive days, with 18 students aged 12 to 15. We facilitated both pilot programs with hands-on plugged and unplugged activities. The method of capturing data included artifact collection, structured interviews, written assessments, and a pre- to post-questionnaire tapping participants’ dispositions about AI and its societal implication. Participant artifacts, written assessments, survey, observation, and analysis of tasks completed revealed that the children improved in their knowledge of AI. In addition, the AI curriculum units and accompanying approaches developed for this study successfully engaged the participants, even without prior knowledge of related concepts. We also found an indication that introducing ethics of AI to adolescents will help their development as ethically responsive citizens. Our study results also indicate that lessons establishing links with students’ personal lives (e.g., letting students choose personally meaningful datasets) and societal implications using unplugged activities and interactive tools were particularly valuable for promoting AI and the integration of AI in middle school education across the subject domains and settings. Based on these results, we discuss our findings, identify their limitations, and propose future work.},
journal = {ACM Trans. Comput. Educ.},
month = dec,
articleno = {55},
numpages = {37},
keywords = {data science, machine learning, plugged activities, unplugged activities, datasets, middle school}
}

@article{10.1145/3737884,
author = {Albluwi, Ibrahim and Hriez, Raghda and Lister, Raymond},
title = {Varying Program Input to Assess Code Reading Skills},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3737884},
doi = {10.1145/3737884},
abstract = {Explain-in-Plain-English (EiPE) questions are used by some researchers and educators to assess code reading skills. EiPE questions require students to briefly explain (in plain English) the purpose of a given piece of code, without restating what the code does line-by-line. The premise is that novices who can explain the purpose of a piece of code have higher code-reading skills than those who can trace the code but cannot see its high-level purpose. However, using natural language in EiPE questions poses challenges. Students (especially those whose first language is not English) may struggle to convey their understanding of the code unambiguously. Also, grading responses written in natural language is time-consuming, requires the design of a rubric, and is difficult to automate. We propose a new code reading question type that addresses these issues with EiPE questions. Given a piece of code involving repetition (in the form of iteration or recursion), the student is asked to provide the output for a set of inputs, where the output for some of these inputs cannot be determined using code tracing alone and requires higher-level code comprehension. In empirical evaluations, using CS1 exams, think-aloud interviews with students, and interviews with instructors, we found that assessments of code reading skills using the new question type are highly consistent with the assessments using EiPE questions, yet are more reliable. These results put forward the proposed question type as another way to assess high-level code-reading skills without the issues associated with expressing in natural language or grading responses expressed in natural language.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Educ.},
month = jun,
keywords = {Introductory Programming, CS1, Code Reading, Code Tracing, Explain-in-Plain-English}
}

@article{10.1145/3484495,
author = {Lee, Clifford H. and Gobir, Nimah and Gurn, Alex and Soep, Elisabeth},
title = {In the Black Mirror: Youth Investigations into Artificial Intelligence},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
url = {https://doi.org/10.1145/3484495},
doi = {10.1145/3484495},
abstract = {Over the past two decades, innovations powered by artificial intelligence (AI) have extended into nearly all facets of human experience. Our ethnographic research suggests that while young people sense they can't “trust” AI, many are not sure how it works or how much control they have over its growing role in their lives. In this study, we attempt to answer the following questions: (1) What can we learn about young people's understanding of AI when they produce media with and about it? and (2) What are the design features of an ethics-centered pedagogy that promotes STEM engagement via AI? To answer these questions, we co-developed and documented three projects at YR Media, a national network of youth journalists and artists who create multimedia for public distribution. Participants are predominantly youth of color and those contending with economic and other barriers to full participation in STEM fields. Findings showed that by creating a learning ecology that centered the cultures and experiences of its learners while leveraging familiar tools for critical analysis, youth deepened their understanding of AI. Our study also showed that providing opportunities for youth to produce ethics-centered interactive stories interrogating invisibilized AI functionalities, and to release those stories to the public, empowered them to creatively express their understandings and apprehensions about AI.},
journal = {ACM Trans. Comput. Educ.},
month = oct,
articleno = {25},
numpages = {25},
keywords = {Critical pedagogy, artificial intelligence, ethics-centered, engagement, agency, computational thinking, machine learning, media}
}

@article{10.1145/3618115,
author = {Killen, Heather and Coenraad, Merijke and Byrne, Virginia and Cabrera, Lautaro and Mills, Kelly and Ketelhut, Diane Jass and Plane, Jandelyn D.},
title = {Teacher Education to Integrate Computational Thinking into Elementary Science: A Design-Based Research Study},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {4},
url = {https://doi.org/10.1145/3618115},
doi = {10.1145/3618115},
abstract = {Computational thinking (CT) is playing an increasingly relevant role within disciplinary teaching in elementary school, particularly in science. However, many teachers are unfamiliar with CT, either because their education occurred before the popularization of CT or because CT instruction was not included in their pre-service coursework. For these teachers, CT professional development (PD) becomes a primary mechanism to close their CT knowledge gap. While CT PD has demonstrated success at increasing teacher's CT understanding, researchers have reported varied outcomes in supporting teachers to write CT-integrated lesson plans. To explore how we might support teachers to integrate CT into elementary science, we employed design-based research (DBR) in a dual-track design of in-class CT instruction for pre-service undergraduates within an elementary science methods class paired with a collaborative, multi-month PD opportunity for pre- and in-service teachers. In this article, we reflect on our 5-year period of DBR and present our design insights and implications for CT instruction and curriculum design from each iteration. Our findings on best practices will inform both teacher educators and PD providers within CT education. Our work will also be of interest to researchers considering DBR for technology-based educational projects.},
journal = {ACM Trans. Comput. Educ.},
month = nov,
articleno = {41},
numpages = {36},
keywords = {Teacher education, professional development, computational thinking, culturally responsive teaching, design-based research}
}

@article{10.1145/3569897,
author = {Van Mechelen, Maarten and Smith, Rachel Charlotte and Schaper, Marie-Monique and Tamashiro, Mariana and Bilstrup, Karl-Emil and Lunding, Mille and Graves Petersen, Marianne and Sejer Iversen, Ole},
title = {Emerging Technologies in K–12 Education: A Future HCI Research Agenda},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1073-0516},
url = {https://doi.org/10.1145/3569897},
doi = {10.1145/3569897},
abstract = {This systematic mapping review sheds light on how emerging technologies have been introduced and taught in various K–12 learning settings, particularly with regard to artificial intelligence (AI), machine learning (ML), the internet of things (IoT), augmented reality (AR), and virtual reality (VR). These technologies are rapidly being integrated into children's everyday lives, but their functions and implications are rarely understood due to their complex and distributed nature. The review provides a rigorous overview of the state of the art based on 107 records published across the fields of human-computer interaction, learning sciences, computing education, and child–computer interaction between 2010 and 2020.&nbsp;The findings show the urgent need on a global scale for inter- and transdisciplinary research that can integrate these dispersed contributions into a more coherent field of research and practice. The article presents nine discussion points for developing a shared agenda to mature the field. Based on the HCI community's expertise in human-centred approaches to technology and aspects of learning, we argue that the community is ideally positioned to take a leading role in the realisation of this future research agenda.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = jun,
articleno = {47},
numpages = {40},
keywords = {K–12 education, emerging technologies, computing education, computational literacy}
}

@inproceedings{10.1145/3491102.3501930,
author = {Sabie, Samar and Jackson, Steven J. and Ju, Wendy and Parikh, Tapan},
title = {Unmaking as Agonism: Using Participatory Design with Youth to Surface Difference in an Intergenerational Urban Context},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501930},
doi = {10.1145/3491102.3501930},
abstract = {Design has been used to contest existing socio-technical arrangements, provoke conversations around matters of concern, and operationalize radical theories such as agonism, which embraces difference and contention. However, the focus is usually on creating something new: a product, interface or artifact. In this paper, we investigate what happens when critical unmaking is deployed as a deliberate design strategy in an intergenerational, agonistic urban context. Specifically, we report on how youth in a six-week design internship used unmaking as a design move to subvert conventional narratives about their surrounding urban context. We analyze how this led to conflictual encounters at the local senior center, and compare it to the other, making-centric proposals which received favorable feedback but failed to raise the same important discussions. Through this ethnographic account, we argue that critical unmaking is important yet overlooked, and should be in the repertoire of design moves available for agonism and provocation.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {324},
numpages = {16},
keywords = {agonism, civic engagement, critical unmaking, making, older adults, participatory design, unmaking, virtual reality, youth},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@proceedings{10.1145/3592979,
title = {PASC '23: Proceedings of the Platform for Advanced Scientific Computing Conference},
year = {2023},
isbn = {9798400701900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The PASC Conference series is an international platform for the exchange of competence in scientific computing and computational science, with a strong focus on methods, tools, algorithms, application challenges, and novel techniques and usage of high-performance computing.},
location = {Davos, Switzerland}
}

@proceedings{10.1145/3528231,
title = {SEENG '22: Proceedings of the 4th International Workshop on Software Engineering Education for the Next Generation},
year = {2022},
isbn = {9781450393362},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This workshop, the fourth in the series since ICSE 2017, brings together scholars, educators, and other stakeholders to discuss the unique needs and challenges of software engineering education for the next generation. Building on its predecessors, the workshop employs a highly interactive format, structured around short presentations to generate discussion topics, an activity to select the most interesting topics, and structured breakout sessions to allow participants to address those topics.},
location = {Pittsburgh, Pennsylvania}
}

@proceedings{10.1145/3568739,
title = {ICDTE '22: Proceedings of the 6th International Conference on Digital Technology in Education},
year = {2022},
isbn = {9781450398091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Hangzhou, China}
}

@proceedings{10.1145/3588243,
title = {IC4E '23: Proceedings of the 2023 14th International Conference on E-Education, E-Business, E-Management and E-Learning},
year = {2023},
isbn = {9798400700651},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Shenzhen, China}
}

